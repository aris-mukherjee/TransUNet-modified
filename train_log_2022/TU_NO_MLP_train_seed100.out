2022-01-06 14:04:59,675 Logging directory: /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/TransUNet/log_dir/trRUNMC_cv1_r3/i2i2l/
2022-01-06 14:04:59,675 Tensorboard directory: /scratch_net/biwidl217_second/arismu/Tensorboard/trRUNMC_cv1_r3/i2i2l/
2022-01-06 14:04:59,675 ============================================================
2022-01-06 14:04:59,675 EXPERIMENT NAME: trRUNMC_cv1_r3/i2i2l/
2022-01-06 14:04:59,676 ============================================================
2022-01-06 14:04:59,676 Loading data...
2022-01-06 14:04:59,676 Reading NCI - RUNMC images...
2022-01-06 14:04:59,676 Data root directory: /itet-stor/arismu/bmicdatasets-originals/Originals/Challenge_Datasets/NCI_Prostate/
2022-01-06 14:04:59,678 Already preprocessed this configuration. Loading now!
2022-01-06 14:04:59,700 Training Images: (256, 256, 286)
2022-01-06 14:04:59,700 Training Labels: (256, 256, 286)
2022-01-06 14:04:59,700 Validation Images: (256, 256, 98)
2022-01-06 14:04:59,700 Validation Labels: (256, 256, 98)
2022-01-06 14:04:59,700 ============================================================
2022-01-06 14:04:59,747 17 iterations per epoch. 6800 max iterations 
  0%|                                         | 0/400 [00:00<?, ?it/s]2022-01-06 14:05:02,418 iteration 1 : loss : 0.766655, loss_ce: 0.861426
2022-01-06 14:05:03,571 iteration 2 : loss : 0.726669, loss_ce: 0.777779
2022-01-06 14:05:04,695 iteration 3 : loss : 0.696652, loss_ce: 0.709853
2022-01-06 14:05:05,881 iteration 4 : loss : 0.634722, loss_ce: 0.643802
2022-01-06 14:05:06,973 iteration 5 : loss : 0.607599, loss_ce: 0.572747
2022-01-06 14:05:08,210 iteration 6 : loss : 0.565452, loss_ce: 0.516028
2022-01-06 14:05:09,411 iteration 7 : loss : 0.547100, loss_ce: 0.470936
2022-01-06 14:05:10,608 iteration 8 : loss : 0.512519, loss_ce: 0.422897
2022-01-06 14:05:11,661 iteration 9 : loss : 0.474733, loss_ce: 0.401541
2022-01-06 14:05:12,868 iteration 10 : loss : 0.457569, loss_ce: 0.362026
2022-01-06 14:05:14,084 iteration 11 : loss : 0.434578, loss_ce: 0.326878
2022-01-06 14:05:15,346 iteration 12 : loss : 0.415935, loss_ce: 0.307087
2022-01-06 14:05:16,483 iteration 13 : loss : 0.396747, loss_ce: 0.270000
2022-01-06 14:05:17,735 iteration 14 : loss : 0.399011, loss_ce: 0.263005
2022-01-06 14:05:18,911 iteration 15 : loss : 0.368470, loss_ce: 0.228325
2022-01-06 14:05:20,055 iteration 16 : loss : 0.372896, loss_ce: 0.222018
2022-01-06 14:05:21,161 iteration 17 : loss : 0.388934, loss_ce: 0.197002
  0%|                               | 1/400 [00:21<2:23:02, 21.51s/it]2022-01-06 14:05:22,458 iteration 18 : loss : 0.349911, loss_ce: 0.189882
2022-01-06 14:05:23,659 iteration 19 : loss : 0.356597, loss_ce: 0.164363
2022-01-06 14:05:24,980 iteration 20 : loss : 0.335026, loss_ce: 0.176949
2022-01-06 14:05:26,122 iteration 21 : loss : 0.368033, loss_ce: 0.179988
2022-01-06 14:05:27,202 iteration 22 : loss : 0.332999, loss_ce: 0.161002
2022-01-06 14:05:28,434 iteration 23 : loss : 0.332688, loss_ce: 0.162677
2022-01-06 14:05:29,669 iteration 24 : loss : 0.314200, loss_ce: 0.163673
2022-01-06 14:05:30,933 iteration 25 : loss : 0.357893, loss_ce: 0.209386
2022-01-06 14:05:32,095 iteration 26 : loss : 0.307852, loss_ce: 0.161614
2022-01-06 14:05:33,199 iteration 27 : loss : 0.321513, loss_ce: 0.153095
2022-01-06 14:05:34,280 iteration 28 : loss : 0.284519, loss_ce: 0.129794
2022-01-06 14:05:35,482 iteration 29 : loss : 0.317159, loss_ce: 0.160918
2022-01-06 14:05:36,602 iteration 30 : loss : 0.276630, loss_ce: 0.128168
2022-01-06 14:05:37,697 iteration 31 : loss : 0.281454, loss_ce: 0.108978
2022-01-06 14:05:38,840 iteration 32 : loss : 0.305256, loss_ce: 0.154663
2022-01-06 14:05:40,056 iteration 33 : loss : 0.305229, loss_ce: 0.136076
2022-01-06 14:05:41,346 iteration 34 : loss : 0.258674, loss_ce: 0.108391
  0%|▏                              | 2/400 [00:41<2:17:24, 20.71s/it]2022-01-06 14:05:42,615 iteration 35 : loss : 0.274457, loss_ce: 0.129793
2022-01-06 14:05:43,805 iteration 36 : loss : 0.312866, loss_ce: 0.114403
2022-01-06 14:05:45,023 iteration 37 : loss : 0.261506, loss_ce: 0.102628
2022-01-06 14:05:46,205 iteration 38 : loss : 0.282950, loss_ce: 0.109571
2022-01-06 14:05:47,264 iteration 39 : loss : 0.307715, loss_ce: 0.127574
2022-01-06 14:05:48,398 iteration 40 : loss : 0.317801, loss_ce: 0.143480
2022-01-06 14:05:49,434 iteration 41 : loss : 0.263353, loss_ce: 0.108454
2022-01-06 14:05:50,633 iteration 42 : loss : 0.280652, loss_ce: 0.123604
2022-01-06 14:05:51,806 iteration 43 : loss : 0.246669, loss_ce: 0.104043
2022-01-06 14:05:52,884 iteration 44 : loss : 0.275660, loss_ce: 0.139515
2022-01-06 14:05:54,151 iteration 45 : loss : 0.294497, loss_ce: 0.118503
2022-01-06 14:05:55,346 iteration 46 : loss : 0.270551, loss_ce: 0.118324
2022-01-06 14:05:56,545 iteration 47 : loss : 0.267893, loss_ce: 0.098726
2022-01-06 14:05:57,742 iteration 48 : loss : 0.241486, loss_ce: 0.103277
2022-01-06 14:05:59,004 iteration 49 : loss : 0.307980, loss_ce: 0.126256
2022-01-06 14:06:00,260 iteration 50 : loss : 0.257466, loss_ce: 0.101309
2022-01-06 14:06:01,526 iteration 51 : loss : 0.225059, loss_ce: 0.093356
  1%|▏                              | 3/400 [01:01<2:15:26, 20.47s/it]2022-01-06 14:06:02,702 iteration 52 : loss : 0.248314, loss_ce: 0.098407
2022-01-06 14:06:03,789 iteration 53 : loss : 0.228921, loss_ce: 0.105636
2022-01-06 14:06:05,051 iteration 54 : loss : 0.261763, loss_ce: 0.109215
2022-01-06 14:06:06,264 iteration 55 : loss : 0.299690, loss_ce: 0.116156
2022-01-06 14:06:07,494 iteration 56 : loss : 0.257451, loss_ce: 0.125029
2022-01-06 14:06:08,644 iteration 57 : loss : 0.259597, loss_ce: 0.117882
2022-01-06 14:06:09,766 iteration 58 : loss : 0.259010, loss_ce: 0.127602
2022-01-06 14:06:10,897 iteration 59 : loss : 0.237010, loss_ce: 0.100980
2022-01-06 14:06:12,062 iteration 60 : loss : 0.266788, loss_ce: 0.121657
2022-01-06 14:06:13,246 iteration 61 : loss : 0.251347, loss_ce: 0.110427
2022-01-06 14:06:14,522 iteration 62 : loss : 0.276881, loss_ce: 0.103960
2022-01-06 14:06:15,665 iteration 63 : loss : 0.264853, loss_ce: 0.123235
2022-01-06 14:06:16,876 iteration 64 : loss : 0.242650, loss_ce: 0.101653
2022-01-06 14:06:18,074 iteration 65 : loss : 0.269981, loss_ce: 0.106118
2022-01-06 14:06:19,321 iteration 66 : loss : 0.265421, loss_ce: 0.105981
2022-01-06 14:06:20,478 iteration 67 : loss : 0.259536, loss_ce: 0.111781
2022-01-06 14:06:21,586 iteration 68 : loss : 0.245777, loss_ce: 0.111203
  1%|▎                              | 4/400 [01:21<2:14:01, 20.31s/it]2022-01-06 14:06:22,887 iteration 69 : loss : 0.291897, loss_ce: 0.132860
2022-01-06 14:06:24,047 iteration 70 : loss : 0.266832, loss_ce: 0.105010
2022-01-06 14:06:25,231 iteration 71 : loss : 0.286677, loss_ce: 0.153746
2022-01-06 14:06:26,445 iteration 72 : loss : 0.245495, loss_ce: 0.110226
2022-01-06 14:06:27,681 iteration 73 : loss : 0.228099, loss_ce: 0.086877
2022-01-06 14:06:28,901 iteration 74 : loss : 0.219118, loss_ce: 0.085719
2022-01-06 14:06:30,075 iteration 75 : loss : 0.217930, loss_ce: 0.105252
2022-01-06 14:06:31,189 iteration 76 : loss : 0.251699, loss_ce: 0.119652
2022-01-06 14:06:32,415 iteration 77 : loss : 0.288128, loss_ce: 0.134441
2022-01-06 14:06:33,620 iteration 78 : loss : 0.216670, loss_ce: 0.083745
2022-01-06 14:06:34,847 iteration 79 : loss : 0.281289, loss_ce: 0.125703
2022-01-06 14:06:35,931 iteration 80 : loss : 0.226611, loss_ce: 0.084659
2022-01-06 14:06:37,214 iteration 81 : loss : 0.310570, loss_ce: 0.149859
2022-01-06 14:06:38,378 iteration 82 : loss : 0.285156, loss_ce: 0.100491
2022-01-06 14:06:39,500 iteration 83 : loss : 0.306799, loss_ce: 0.102659
2022-01-06 14:06:40,805 iteration 84 : loss : 0.294813, loss_ce: 0.137341
2022-01-06 14:06:40,805 Training Data Eval:
2022-01-06 14:06:46,678   Average segmentation loss on training set: 0.2963
2022-01-06 14:06:46,678 Validation Data Eval:
2022-01-06 14:06:48,833   Average segmentation loss on validation set: 0.3233
2022-01-06 14:06:54,679 Found new lowest validation loss at iteration 84! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_MLP_best_val_loss_seed100.pth
2022-01-06 14:06:55,782 iteration 85 : loss : 0.273173, loss_ce: 0.132836
  1%|▍                              | 5/400 [01:56<2:46:41, 25.32s/it]2022-01-06 14:06:57,057 iteration 86 : loss : 0.237844, loss_ce: 0.113907
2022-01-06 14:06:58,188 iteration 87 : loss : 0.268980, loss_ce: 0.114616
2022-01-06 14:06:59,278 iteration 88 : loss : 0.234426, loss_ce: 0.118899
2022-01-06 14:07:00,449 iteration 89 : loss : 0.302183, loss_ce: 0.128621
2022-01-06 14:07:01,597 iteration 90 : loss : 0.218478, loss_ce: 0.099355
2022-01-06 14:07:02,819 iteration 91 : loss : 0.233772, loss_ce: 0.109253
2022-01-06 14:07:04,061 iteration 92 : loss : 0.219521, loss_ce: 0.082604
2022-01-06 14:07:05,146 iteration 93 : loss : 0.200422, loss_ce: 0.066398
2022-01-06 14:07:06,360 iteration 94 : loss : 0.224016, loss_ce: 0.099200
2022-01-06 14:07:07,529 iteration 95 : loss : 0.304890, loss_ce: 0.151458
2022-01-06 14:07:08,819 iteration 96 : loss : 0.267267, loss_ce: 0.108513
2022-01-06 14:07:10,048 iteration 97 : loss : 0.273910, loss_ce: 0.108944
2022-01-06 14:07:11,317 iteration 98 : loss : 0.280943, loss_ce: 0.114228
2022-01-06 14:07:12,516 iteration 99 : loss : 0.273217, loss_ce: 0.108359
2022-01-06 14:07:13,770 iteration 100 : loss : 0.217582, loss_ce: 0.095058
2022-01-06 14:07:14,957 iteration 101 : loss : 0.240010, loss_ce: 0.099549
2022-01-06 14:07:16,136 iteration 102 : loss : 0.217553, loss_ce: 0.094748
  2%|▍                              | 6/400 [02:16<2:35:09, 23.63s/it]2022-01-06 14:07:17,433 iteration 103 : loss : 0.249127, loss_ce: 0.090139
2022-01-06 14:07:18,705 iteration 104 : loss : 0.231694, loss_ce: 0.092824
2022-01-06 14:07:19,953 iteration 105 : loss : 0.205200, loss_ce: 0.083635
2022-01-06 14:07:21,294 iteration 106 : loss : 0.232440, loss_ce: 0.093761
2022-01-06 14:07:22,542 iteration 107 : loss : 0.276872, loss_ce: 0.098781
2022-01-06 14:07:23,731 iteration 108 : loss : 0.236674, loss_ce: 0.081955
2022-01-06 14:07:24,856 iteration 109 : loss : 0.281587, loss_ce: 0.122089
2022-01-06 14:07:26,038 iteration 110 : loss : 0.232069, loss_ce: 0.097815
2022-01-06 14:07:27,341 iteration 111 : loss : 0.217666, loss_ce: 0.096197
2022-01-06 14:07:28,556 iteration 112 : loss : 0.228311, loss_ce: 0.087984
2022-01-06 14:07:29,700 iteration 113 : loss : 0.191999, loss_ce: 0.070768
2022-01-06 14:07:30,970 iteration 114 : loss : 0.214536, loss_ce: 0.075806
2022-01-06 14:07:32,167 iteration 115 : loss : 0.190484, loss_ce: 0.088107
2022-01-06 14:07:33,428 iteration 116 : loss : 0.237598, loss_ce: 0.113452
2022-01-06 14:07:34,608 iteration 117 : loss : 0.224434, loss_ce: 0.094084
2022-01-06 14:07:35,816 iteration 118 : loss : 0.279109, loss_ce: 0.148064
2022-01-06 14:07:37,038 iteration 119 : loss : 0.200175, loss_ce: 0.077028
  2%|▌                              | 7/400 [02:37<2:28:56, 22.74s/it]2022-01-06 14:07:38,388 iteration 120 : loss : 0.218827, loss_ce: 0.093902
2022-01-06 14:07:39,625 iteration 121 : loss : 0.278296, loss_ce: 0.115275
2022-01-06 14:07:40,730 iteration 122 : loss : 0.240011, loss_ce: 0.105869
2022-01-06 14:07:41,995 iteration 123 : loss : 0.215624, loss_ce: 0.095576
2022-01-06 14:07:43,269 iteration 124 : loss : 0.268945, loss_ce: 0.103575
2022-01-06 14:07:44,466 iteration 125 : loss : 0.197482, loss_ce: 0.095009
2022-01-06 14:07:45,722 iteration 126 : loss : 0.261393, loss_ce: 0.096758
2022-01-06 14:07:46,987 iteration 127 : loss : 0.179990, loss_ce: 0.070300
2022-01-06 14:07:48,256 iteration 128 : loss : 0.178614, loss_ce: 0.080892
2022-01-06 14:07:49,421 iteration 129 : loss : 0.222478, loss_ce: 0.076629
2022-01-06 14:07:50,706 iteration 130 : loss : 0.231099, loss_ce: 0.105060
2022-01-06 14:07:51,909 iteration 131 : loss : 0.229659, loss_ce: 0.109964
2022-01-06 14:07:53,051 iteration 132 : loss : 0.220455, loss_ce: 0.082702
2022-01-06 14:07:54,325 iteration 133 : loss : 0.231014, loss_ce: 0.080704
2022-01-06 14:07:55,545 iteration 134 : loss : 0.263300, loss_ce: 0.106329
2022-01-06 14:07:56,776 iteration 135 : loss : 0.208861, loss_ce: 0.091744
2022-01-06 14:07:57,967 iteration 136 : loss : 0.250045, loss_ce: 0.109827
  2%|▌                              | 8/400 [02:58<2:24:46, 22.16s/it]2022-01-06 14:07:59,157 iteration 137 : loss : 0.164583, loss_ce: 0.054485
2022-01-06 14:08:00,321 iteration 138 : loss : 0.254507, loss_ce: 0.133384
2022-01-06 14:08:01,611 iteration 139 : loss : 0.206916, loss_ce: 0.078402
2022-01-06 14:08:02,948 iteration 140 : loss : 0.240556, loss_ce: 0.092202
2022-01-06 14:08:04,169 iteration 141 : loss : 0.234852, loss_ce: 0.091964
2022-01-06 14:08:05,310 iteration 142 : loss : 0.189171, loss_ce: 0.075886
2022-01-06 14:08:06,463 iteration 143 : loss : 0.215372, loss_ce: 0.083988
2022-01-06 14:08:07,736 iteration 144 : loss : 0.276317, loss_ce: 0.094754
2022-01-06 14:08:09,166 iteration 145 : loss : 0.220772, loss_ce: 0.095662
2022-01-06 14:08:10,341 iteration 146 : loss : 0.231361, loss_ce: 0.078666
2022-01-06 14:08:11,532 iteration 147 : loss : 0.229828, loss_ce: 0.091420
2022-01-06 14:08:12,746 iteration 148 : loss : 0.202367, loss_ce: 0.086835
2022-01-06 14:08:13,976 iteration 149 : loss : 0.213040, loss_ce: 0.082699
2022-01-06 14:08:15,182 iteration 150 : loss : 0.266277, loss_ce: 0.139611
2022-01-06 14:08:16,403 iteration 151 : loss : 0.226803, loss_ce: 0.100072
2022-01-06 14:08:17,703 iteration 152 : loss : 0.221605, loss_ce: 0.092088
2022-01-06 14:08:18,985 iteration 153 : loss : 0.176675, loss_ce: 0.074659
  2%|▋                              | 9/400 [03:19<2:22:04, 21.80s/it]2022-01-06 14:08:20,241 iteration 154 : loss : 0.165883, loss_ce: 0.066880
2022-01-06 14:08:21,479 iteration 155 : loss : 0.220902, loss_ce: 0.089903
2022-01-06 14:08:22,710 iteration 156 : loss : 0.179606, loss_ce: 0.074181
2022-01-06 14:08:23,973 iteration 157 : loss : 0.252882, loss_ce: 0.096581
2022-01-06 14:08:25,211 iteration 158 : loss : 0.212634, loss_ce: 0.099549
2022-01-06 14:08:26,439 iteration 159 : loss : 0.216865, loss_ce: 0.087327
2022-01-06 14:08:27,649 iteration 160 : loss : 0.281502, loss_ce: 0.098299
2022-01-06 14:08:28,902 iteration 161 : loss : 0.225219, loss_ce: 0.104681
2022-01-06 14:08:30,228 iteration 162 : loss : 0.201214, loss_ce: 0.087009
2022-01-06 14:08:31,324 iteration 163 : loss : 0.161494, loss_ce: 0.064172
2022-01-06 14:08:32,607 iteration 164 : loss : 0.186265, loss_ce: 0.071778
2022-01-06 14:08:33,807 iteration 165 : loss : 0.204939, loss_ce: 0.068714
2022-01-06 14:08:35,108 iteration 166 : loss : 0.218270, loss_ce: 0.088524
2022-01-06 14:08:36,386 iteration 167 : loss : 0.211255, loss_ce: 0.098521
2022-01-06 14:08:37,634 iteration 168 : loss : 0.232295, loss_ce: 0.098176
2022-01-06 14:08:38,832 iteration 169 : loss : 0.212063, loss_ce: 0.088999
2022-01-06 14:08:38,832 Training Data Eval:
2022-01-06 14:08:44,803   Average segmentation loss on training set: 0.2177
2022-01-06 14:08:44,804 Validation Data Eval:
2022-01-06 14:08:46,857   Average segmentation loss on validation set: 0.2338
2022-01-06 14:08:52,898 Found new lowest validation loss at iteration 169! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_MLP_best_val_loss_seed100.pth
2022-01-06 14:08:54,050 iteration 170 : loss : 0.213962, loss_ce: 0.096902
  2%|▊                             | 10/400 [03:54<2:48:19, 25.90s/it]2022-01-06 14:08:55,304 iteration 171 : loss : 0.215799, loss_ce: 0.103070
2022-01-06 14:08:56,328 iteration 172 : loss : 0.252192, loss_ce: 0.088465
2022-01-06 14:08:57,409 iteration 173 : loss : 0.202213, loss_ce: 0.085932
2022-01-06 14:08:58,664 iteration 174 : loss : 0.208686, loss_ce: 0.076626
2022-01-06 14:08:59,802 iteration 175 : loss : 0.233068, loss_ce: 0.098108
2022-01-06 14:09:01,086 iteration 176 : loss : 0.224346, loss_ce: 0.077475
2022-01-06 14:09:02,248 iteration 177 : loss : 0.274103, loss_ce: 0.103059
2022-01-06 14:09:03,487 iteration 178 : loss : 0.214600, loss_ce: 0.068816
2022-01-06 14:09:04,717 iteration 179 : loss : 0.210591, loss_ce: 0.087396
2022-01-06 14:09:05,973 iteration 180 : loss : 0.212777, loss_ce: 0.074117
2022-01-06 14:09:07,249 iteration 181 : loss : 0.162250, loss_ce: 0.059472
2022-01-06 14:09:08,504 iteration 182 : loss : 0.179430, loss_ce: 0.070050
2022-01-06 14:09:09,779 iteration 183 : loss : 0.187136, loss_ce: 0.074079
2022-01-06 14:09:11,003 iteration 184 : loss : 0.185278, loss_ce: 0.077675
2022-01-06 14:09:12,176 iteration 185 : loss : 0.218090, loss_ce: 0.104850
2022-01-06 14:09:13,344 iteration 186 : loss : 0.194416, loss_ce: 0.084810
2022-01-06 14:09:14,500 iteration 187 : loss : 0.242337, loss_ce: 0.112849
  3%|▊                             | 11/400 [04:14<2:37:05, 24.23s/it]2022-01-06 14:09:15,816 iteration 188 : loss : 0.259532, loss_ce: 0.104371
2022-01-06 14:09:17,078 iteration 189 : loss : 0.219725, loss_ce: 0.086521
2022-01-06 14:09:18,363 iteration 190 : loss : 0.225783, loss_ce: 0.073326
2022-01-06 14:09:19,583 iteration 191 : loss : 0.239483, loss_ce: 0.098126
2022-01-06 14:09:20,682 iteration 192 : loss : 0.159511, loss_ce: 0.067540
2022-01-06 14:09:21,853 iteration 193 : loss : 0.305279, loss_ce: 0.125664
2022-01-06 14:09:23,006 iteration 194 : loss : 0.284849, loss_ce: 0.123113
2022-01-06 14:09:24,331 iteration 195 : loss : 0.152667, loss_ce: 0.062528
2022-01-06 14:09:25,574 iteration 196 : loss : 0.191437, loss_ce: 0.069553
2022-01-06 14:09:26,791 iteration 197 : loss : 0.185187, loss_ce: 0.072380
2022-01-06 14:09:28,044 iteration 198 : loss : 0.255672, loss_ce: 0.113973
2022-01-06 14:09:29,304 iteration 199 : loss : 0.200568, loss_ce: 0.093870
2022-01-06 14:09:30,499 iteration 200 : loss : 0.258752, loss_ce: 0.094504
2022-01-06 14:09:31,714 iteration 201 : loss : 0.223274, loss_ce: 0.090054
2022-01-06 14:09:32,803 iteration 202 : loss : 0.209277, loss_ce: 0.065732
2022-01-06 14:09:34,103 iteration 203 : loss : 0.170168, loss_ce: 0.066331
2022-01-06 14:09:35,299 iteration 204 : loss : 0.207313, loss_ce: 0.096974
  3%|▉                             | 12/400 [04:35<2:29:58, 23.19s/it]2022-01-06 14:09:36,677 iteration 205 : loss : 0.215911, loss_ce: 0.080607
2022-01-06 14:09:37,903 iteration 206 : loss : 0.306543, loss_ce: 0.125390
2022-01-06 14:09:39,213 iteration 207 : loss : 0.261099, loss_ce: 0.107155
2022-01-06 14:09:40,480 iteration 208 : loss : 0.226558, loss_ce: 0.106309
2022-01-06 14:09:41,720 iteration 209 : loss : 0.182927, loss_ce: 0.069595
2022-01-06 14:09:42,928 iteration 210 : loss : 0.189800, loss_ce: 0.086011
2022-01-06 14:09:44,156 iteration 211 : loss : 0.171721, loss_ce: 0.077485
2022-01-06 14:09:45,447 iteration 212 : loss : 0.180417, loss_ce: 0.080058
2022-01-06 14:09:46,738 iteration 213 : loss : 0.240484, loss_ce: 0.106790
2022-01-06 14:09:47,950 iteration 214 : loss : 0.240301, loss_ce: 0.094558
2022-01-06 14:09:49,126 iteration 215 : loss : 0.231392, loss_ce: 0.087735
2022-01-06 14:09:50,421 iteration 216 : loss : 0.207321, loss_ce: 0.083016
2022-01-06 14:09:51,723 iteration 217 : loss : 0.227895, loss_ce: 0.097493
2022-01-06 14:09:52,916 iteration 218 : loss : 0.221752, loss_ce: 0.096605
2022-01-06 14:09:54,101 iteration 219 : loss : 0.220112, loss_ce: 0.101863
2022-01-06 14:09:55,340 iteration 220 : loss : 0.188720, loss_ce: 0.073858
2022-01-06 14:09:56,603 iteration 221 : loss : 0.218974, loss_ce: 0.096931
  3%|▉                             | 13/400 [04:56<2:25:51, 22.61s/it]2022-01-06 14:09:57,894 iteration 222 : loss : 0.189416, loss_ce: 0.073829
2022-01-06 14:09:59,092 iteration 223 : loss : 0.301034, loss_ce: 0.134404
2022-01-06 14:10:00,292 iteration 224 : loss : 0.208929, loss_ce: 0.069476
2022-01-06 14:10:01,658 iteration 225 : loss : 0.189204, loss_ce: 0.086764
2022-01-06 14:10:02,915 iteration 226 : loss : 0.221752, loss_ce: 0.090800
2022-01-06 14:10:04,120 iteration 227 : loss : 0.199147, loss_ce: 0.079128
2022-01-06 14:10:05,332 iteration 228 : loss : 0.204440, loss_ce: 0.079090
2022-01-06 14:10:06,591 iteration 229 : loss : 0.252262, loss_ce: 0.103688
2022-01-06 14:10:07,751 iteration 230 : loss : 0.164521, loss_ce: 0.073394
2022-01-06 14:10:08,972 iteration 231 : loss : 0.220204, loss_ce: 0.085561
2022-01-06 14:10:10,175 iteration 232 : loss : 0.226457, loss_ce: 0.090337
2022-01-06 14:10:11,307 iteration 233 : loss : 0.227267, loss_ce: 0.109214
2022-01-06 14:10:12,524 iteration 234 : loss : 0.292369, loss_ce: 0.123387
2022-01-06 14:10:13,706 iteration 235 : loss : 0.198239, loss_ce: 0.081880
2022-01-06 14:10:14,924 iteration 236 : loss : 0.319269, loss_ce: 0.136341
2022-01-06 14:10:16,082 iteration 237 : loss : 0.269773, loss_ce: 0.128180
2022-01-06 14:10:17,355 iteration 238 : loss : 0.222655, loss_ce: 0.078031
  4%|█                             | 14/400 [05:17<2:21:53, 22.06s/it]2022-01-06 14:10:18,632 iteration 239 : loss : 0.241941, loss_ce: 0.092557
2022-01-06 14:10:19,963 iteration 240 : loss : 0.214387, loss_ce: 0.075221
2022-01-06 14:10:21,229 iteration 241 : loss : 0.194452, loss_ce: 0.080767
2022-01-06 14:10:22,421 iteration 242 : loss : 0.183346, loss_ce: 0.068245
2022-01-06 14:10:23,613 iteration 243 : loss : 0.192787, loss_ce: 0.078197
2022-01-06 14:10:24,826 iteration 244 : loss : 0.209084, loss_ce: 0.083555
2022-01-06 14:10:25,994 iteration 245 : loss : 0.204137, loss_ce: 0.072995
2022-01-06 14:10:27,197 iteration 246 : loss : 0.237255, loss_ce: 0.089917
2022-01-06 14:10:28,449 iteration 247 : loss : 0.231422, loss_ce: 0.104575
2022-01-06 14:10:29,726 iteration 248 : loss : 0.174760, loss_ce: 0.077522
2022-01-06 14:10:30,949 iteration 249 : loss : 0.175339, loss_ce: 0.070233
2022-01-06 14:10:32,181 iteration 250 : loss : 0.222652, loss_ce: 0.081065
2022-01-06 14:10:33,345 iteration 251 : loss : 0.156287, loss_ce: 0.068406
2022-01-06 14:10:34,663 iteration 252 : loss : 0.203080, loss_ce: 0.100423
2022-01-06 14:10:35,904 iteration 253 : loss : 0.147676, loss_ce: 0.051354
2022-01-06 14:10:37,116 iteration 254 : loss : 0.181999, loss_ce: 0.083814
2022-01-06 14:10:37,257 Training Data Eval:
2022-01-06 14:10:43,316   Average segmentation loss on training set: 0.4333
2022-01-06 14:10:43,317 Validation Data Eval:
2022-01-06 14:10:45,405   Average segmentation loss on validation set: 0.5134
2022-01-06 14:10:46,571 iteration 255 : loss : 0.242435, loss_ce: 0.118292
  4%|█▏                            | 15/400 [05:46<2:35:21, 24.21s/it]2022-01-06 14:10:47,805 iteration 256 : loss : 0.165918, loss_ce: 0.062039
2022-01-06 14:10:48,990 iteration 257 : loss : 0.210037, loss_ce: 0.088050
2022-01-06 14:10:50,281 iteration 258 : loss : 0.207242, loss_ce: 0.091645
2022-01-06 14:10:51,511 iteration 259 : loss : 0.216791, loss_ce: 0.091643
2022-01-06 14:10:52,670 iteration 260 : loss : 0.176433, loss_ce: 0.094048
2022-01-06 14:10:53,938 iteration 261 : loss : 0.179300, loss_ce: 0.076118
2022-01-06 14:10:55,228 iteration 262 : loss : 0.331369, loss_ce: 0.157264
2022-01-06 14:10:56,389 iteration 263 : loss : 0.208218, loss_ce: 0.094324
2022-01-06 14:10:57,687 iteration 264 : loss : 0.234169, loss_ce: 0.080288
2022-01-06 14:10:59,042 iteration 265 : loss : 0.266649, loss_ce: 0.149309
2022-01-06 14:11:00,301 iteration 266 : loss : 0.206994, loss_ce: 0.088686
2022-01-06 14:11:01,650 iteration 267 : loss : 0.151552, loss_ce: 0.053650
2022-01-06 14:11:02,917 iteration 268 : loss : 0.229515, loss_ce: 0.101704
2022-01-06 14:11:04,164 iteration 269 : loss : 0.282267, loss_ce: 0.130301
2022-01-06 14:11:05,372 iteration 270 : loss : 0.245546, loss_ce: 0.083943
2022-01-06 14:11:06,576 iteration 271 : loss : 0.225133, loss_ce: 0.093395
2022-01-06 14:11:07,753 iteration 272 : loss : 0.182599, loss_ce: 0.069292
  4%|█▏                            | 16/400 [06:08<2:29:08, 23.30s/it]2022-01-06 14:11:09,120 iteration 273 : loss : 0.214917, loss_ce: 0.082199
2022-01-06 14:11:10,345 iteration 274 : loss : 0.266815, loss_ce: 0.120748
2022-01-06 14:11:11,545 iteration 275 : loss : 0.270907, loss_ce: 0.080673
2022-01-06 14:11:12,747 iteration 276 : loss : 0.175192, loss_ce: 0.072273
2022-01-06 14:11:13,938 iteration 277 : loss : 0.158153, loss_ce: 0.065657
2022-01-06 14:11:15,185 iteration 278 : loss : 0.229102, loss_ce: 0.104087
2022-01-06 14:11:16,413 iteration 279 : loss : 0.200192, loss_ce: 0.080701
2022-01-06 14:11:17,641 iteration 280 : loss : 0.206429, loss_ce: 0.095651
2022-01-06 14:11:18,928 iteration 281 : loss : 0.174070, loss_ce: 0.077318
2022-01-06 14:11:20,168 iteration 282 : loss : 0.167175, loss_ce: 0.062681
2022-01-06 14:11:21,354 iteration 283 : loss : 0.186922, loss_ce: 0.067593
2022-01-06 14:11:22,598 iteration 284 : loss : 0.173603, loss_ce: 0.086510
2022-01-06 14:11:23,864 iteration 285 : loss : 0.210844, loss_ce: 0.070667
2022-01-06 14:11:24,963 iteration 286 : loss : 0.177060, loss_ce: 0.072292
2022-01-06 14:11:26,185 iteration 287 : loss : 0.179800, loss_ce: 0.080245
2022-01-06 14:11:27,435 iteration 288 : loss : 0.170729, loss_ce: 0.066928
2022-01-06 14:11:28,645 iteration 289 : loss : 0.239809, loss_ce: 0.094781
  4%|█▎                            | 17/400 [06:28<2:24:06, 22.58s/it]2022-01-06 14:11:29,982 iteration 290 : loss : 0.202625, loss_ce: 0.089972
2022-01-06 14:11:31,161 iteration 291 : loss : 0.241745, loss_ce: 0.089070
2022-01-06 14:11:32,337 iteration 292 : loss : 0.223053, loss_ce: 0.103941
2022-01-06 14:11:33,633 iteration 293 : loss : 0.199758, loss_ce: 0.072809
2022-01-06 14:11:34,842 iteration 294 : loss : 0.180789, loss_ce: 0.080850
2022-01-06 14:11:36,139 iteration 295 : loss : 0.204042, loss_ce: 0.095317
2022-01-06 14:11:37,441 iteration 296 : loss : 0.250808, loss_ce: 0.104170
2022-01-06 14:11:38,695 iteration 297 : loss : 0.209824, loss_ce: 0.100788
2022-01-06 14:11:39,905 iteration 298 : loss : 0.205397, loss_ce: 0.092113
2022-01-06 14:11:41,054 iteration 299 : loss : 0.219299, loss_ce: 0.085586
2022-01-06 14:11:42,246 iteration 300 : loss : 0.206443, loss_ce: 0.088531
2022-01-06 14:11:43,376 iteration 301 : loss : 0.235969, loss_ce: 0.081072
2022-01-06 14:11:44,692 iteration 302 : loss : 0.184574, loss_ce: 0.064057
2022-01-06 14:11:45,975 iteration 303 : loss : 0.159879, loss_ce: 0.054925
2022-01-06 14:11:47,253 iteration 304 : loss : 0.224582, loss_ce: 0.086596
2022-01-06 14:11:48,440 iteration 305 : loss : 0.178160, loss_ce: 0.070554
2022-01-06 14:11:49,652 iteration 306 : loss : 0.188360, loss_ce: 0.070779
  4%|█▎                            | 18/400 [06:49<2:20:44, 22.11s/it]2022-01-06 14:11:50,866 iteration 307 : loss : 0.287301, loss_ce: 0.114688
2022-01-06 14:11:51,995 iteration 308 : loss : 0.194528, loss_ce: 0.083073
2022-01-06 14:11:53,149 iteration 309 : loss : 0.183281, loss_ce: 0.085796
2022-01-06 14:11:54,384 iteration 310 : loss : 0.220576, loss_ce: 0.105621
2022-01-06 14:11:55,652 iteration 311 : loss : 0.225691, loss_ce: 0.099030
2022-01-06 14:11:56,929 iteration 312 : loss : 0.156157, loss_ce: 0.066321
2022-01-06 14:11:58,106 iteration 313 : loss : 0.191448, loss_ce: 0.085390
2022-01-06 14:11:59,333 iteration 314 : loss : 0.160261, loss_ce: 0.074343
2022-01-06 14:12:00,612 iteration 315 : loss : 0.242916, loss_ce: 0.110811
2022-01-06 14:12:01,832 iteration 316 : loss : 0.217933, loss_ce: 0.083819
2022-01-06 14:12:02,980 iteration 317 : loss : 0.212235, loss_ce: 0.073770
2022-01-06 14:12:04,165 iteration 318 : loss : 0.191065, loss_ce: 0.085752
2022-01-06 14:12:05,461 iteration 319 : loss : 0.149027, loss_ce: 0.062723
2022-01-06 14:12:06,635 iteration 320 : loss : 0.220616, loss_ce: 0.086483
2022-01-06 14:12:07,781 iteration 321 : loss : 0.182362, loss_ce: 0.089757
2022-01-06 14:12:09,013 iteration 322 : loss : 0.194475, loss_ce: 0.082517
2022-01-06 14:12:10,303 iteration 323 : loss : 0.167930, loss_ce: 0.075171
  5%|█▍                            | 19/400 [07:10<2:17:36, 21.67s/it]2022-01-06 14:12:11,502 iteration 324 : loss : 0.206804, loss_ce: 0.083203
2022-01-06 14:12:12,712 iteration 325 : loss : 0.215515, loss_ce: 0.078507
2022-01-06 14:12:13,928 iteration 326 : loss : 0.185264, loss_ce: 0.063176
2022-01-06 14:12:15,256 iteration 327 : loss : 0.228133, loss_ce: 0.083162
2022-01-06 14:12:16,526 iteration 328 : loss : 0.204371, loss_ce: 0.083049
2022-01-06 14:12:17,745 iteration 329 : loss : 0.177695, loss_ce: 0.065730
2022-01-06 14:12:18,845 iteration 330 : loss : 0.186462, loss_ce: 0.082610
2022-01-06 14:12:20,135 iteration 331 : loss : 0.155193, loss_ce: 0.057343
2022-01-06 14:12:21,417 iteration 332 : loss : 0.152909, loss_ce: 0.065580
2022-01-06 14:12:22,566 iteration 333 : loss : 0.224571, loss_ce: 0.094169
2022-01-06 14:12:23,842 iteration 334 : loss : 0.164176, loss_ce: 0.079381
2022-01-06 14:12:25,036 iteration 335 : loss : 0.242588, loss_ce: 0.094700
2022-01-06 14:12:26,319 iteration 336 : loss : 0.173764, loss_ce: 0.076047
2022-01-06 14:12:27,501 iteration 337 : loss : 0.194110, loss_ce: 0.077359
2022-01-06 14:12:28,639 iteration 338 : loss : 0.188376, loss_ce: 0.088766
2022-01-06 14:12:29,866 iteration 339 : loss : 0.177849, loss_ce: 0.064556
2022-01-06 14:12:29,866 Training Data Eval:
2022-01-06 14:12:35,917   Average segmentation loss on training set: 0.4104
2022-01-06 14:12:35,917 Validation Data Eval:
2022-01-06 14:12:38,023   Average segmentation loss on validation set: 0.4754
2022-01-06 14:12:39,243 iteration 340 : loss : 0.211840, loss_ce: 0.076838
  5%|█▌                            | 20/400 [07:39<2:31:02, 23.85s/it]2022-01-06 14:12:40,609 iteration 341 : loss : 0.277716, loss_ce: 0.144031
2022-01-06 14:12:41,917 iteration 342 : loss : 0.185282, loss_ce: 0.077161
2022-01-06 14:12:43,292 iteration 343 : loss : 0.241513, loss_ce: 0.124338
2022-01-06 14:12:44,506 iteration 344 : loss : 0.195212, loss_ce: 0.077925
2022-01-06 14:12:45,766 iteration 345 : loss : 0.225591, loss_ce: 0.089537
2022-01-06 14:12:47,098 iteration 346 : loss : 0.189293, loss_ce: 0.068612
2022-01-06 14:12:48,290 iteration 347 : loss : 0.180788, loss_ce: 0.074399
2022-01-06 14:12:49,640 iteration 348 : loss : 0.207823, loss_ce: 0.082565
2022-01-06 14:12:50,849 iteration 349 : loss : 0.176448, loss_ce: 0.078916
2022-01-06 14:12:51,971 iteration 350 : loss : 0.142457, loss_ce: 0.049751
2022-01-06 14:12:53,289 iteration 351 : loss : 0.160542, loss_ce: 0.056406
2022-01-06 14:12:54,526 iteration 352 : loss : 0.189973, loss_ce: 0.073380
2022-01-06 14:12:55,879 iteration 353 : loss : 0.161408, loss_ce: 0.057458
2022-01-06 14:12:57,130 iteration 354 : loss : 0.194132, loss_ce: 0.089258
2022-01-06 14:12:58,434 iteration 355 : loss : 0.215032, loss_ce: 0.087212
2022-01-06 14:12:59,601 iteration 356 : loss : 0.191858, loss_ce: 0.081927
2022-01-06 14:13:00,805 iteration 357 : loss : 0.218978, loss_ce: 0.089161
  5%|█▌                            | 21/400 [08:01<2:26:20, 23.17s/it]2022-01-06 14:13:02,185 iteration 358 : loss : 0.201723, loss_ce: 0.075686
2022-01-06 14:13:03,470 iteration 359 : loss : 0.126243, loss_ce: 0.062484
2022-01-06 14:13:04,692 iteration 360 : loss : 0.219953, loss_ce: 0.062731
2022-01-06 14:13:05,952 iteration 361 : loss : 0.213346, loss_ce: 0.080409
2022-01-06 14:13:07,080 iteration 362 : loss : 0.160074, loss_ce: 0.063639
2022-01-06 14:13:08,411 iteration 363 : loss : 0.172223, loss_ce: 0.085309
2022-01-06 14:13:09,657 iteration 364 : loss : 0.257273, loss_ce: 0.133096
2022-01-06 14:13:10,874 iteration 365 : loss : 0.151394, loss_ce: 0.057016
2022-01-06 14:13:12,097 iteration 366 : loss : 0.163803, loss_ce: 0.063450
2022-01-06 14:13:13,378 iteration 367 : loss : 0.167558, loss_ce: 0.077290
2022-01-06 14:13:14,537 iteration 368 : loss : 0.170112, loss_ce: 0.053023
2022-01-06 14:13:15,805 iteration 369 : loss : 0.151420, loss_ce: 0.072265
2022-01-06 14:13:16,942 iteration 370 : loss : 0.191921, loss_ce: 0.062862
2022-01-06 14:13:18,248 iteration 371 : loss : 0.164685, loss_ce: 0.073340
2022-01-06 14:13:19,553 iteration 372 : loss : 0.197242, loss_ce: 0.074034
2022-01-06 14:13:20,799 iteration 373 : loss : 0.164545, loss_ce: 0.060176
2022-01-06 14:13:21,988 iteration 374 : loss : 0.136174, loss_ce: 0.061239
  6%|█▋                            | 22/400 [08:22<2:22:10, 22.57s/it]2022-01-06 14:13:23,323 iteration 375 : loss : 0.154303, loss_ce: 0.063464
2022-01-06 14:13:24,581 iteration 376 : loss : 0.165385, loss_ce: 0.064600
2022-01-06 14:13:25,852 iteration 377 : loss : 0.207876, loss_ce: 0.070834
2022-01-06 14:13:27,115 iteration 378 : loss : 0.208550, loss_ce: 0.096413
2022-01-06 14:13:28,277 iteration 379 : loss : 0.225969, loss_ce: 0.089078
2022-01-06 14:13:29,557 iteration 380 : loss : 0.175687, loss_ce: 0.083594
2022-01-06 14:13:30,674 iteration 381 : loss : 0.138304, loss_ce: 0.049228
2022-01-06 14:13:32,038 iteration 382 : loss : 0.165539, loss_ce: 0.078268
2022-01-06 14:13:33,322 iteration 383 : loss : 0.190402, loss_ce: 0.077286
2022-01-06 14:13:34,539 iteration 384 : loss : 0.153538, loss_ce: 0.063073
2022-01-06 14:13:35,719 iteration 385 : loss : 0.141556, loss_ce: 0.058720
2022-01-06 14:13:36,997 iteration 386 : loss : 0.185383, loss_ce: 0.070471
2022-01-06 14:13:38,246 iteration 387 : loss : 0.204301, loss_ce: 0.085687
2022-01-06 14:13:39,539 iteration 388 : loss : 0.170289, loss_ce: 0.070263
2022-01-06 14:13:40,735 iteration 389 : loss : 0.181532, loss_ce: 0.062478
2022-01-06 14:13:42,004 iteration 390 : loss : 0.168145, loss_ce: 0.085909
2022-01-06 14:13:43,203 iteration 391 : loss : 0.172358, loss_ce: 0.063329
  6%|█▋                            | 23/400 [08:43<2:19:14, 22.16s/it]2022-01-06 14:13:44,649 iteration 392 : loss : 0.145302, loss_ce: 0.065878
2022-01-06 14:13:45,834 iteration 393 : loss : 0.196999, loss_ce: 0.097804
2022-01-06 14:13:47,091 iteration 394 : loss : 0.229942, loss_ce: 0.091959
2022-01-06 14:13:48,300 iteration 395 : loss : 0.192958, loss_ce: 0.079365
2022-01-06 14:13:49,597 iteration 396 : loss : 0.147641, loss_ce: 0.061429
2022-01-06 14:13:50,886 iteration 397 : loss : 0.148626, loss_ce: 0.060230
2022-01-06 14:13:52,146 iteration 398 : loss : 0.168747, loss_ce: 0.080581
2022-01-06 14:13:53,360 iteration 399 : loss : 0.119523, loss_ce: 0.045720
2022-01-06 14:13:54,645 iteration 400 : loss : 0.184814, loss_ce: 0.080986
2022-01-06 14:13:55,777 iteration 401 : loss : 0.197928, loss_ce: 0.073929
2022-01-06 14:13:57,079 iteration 402 : loss : 0.221605, loss_ce: 0.096720
2022-01-06 14:13:58,418 iteration 403 : loss : 0.206420, loss_ce: 0.080740
2022-01-06 14:13:59,656 iteration 404 : loss : 0.187323, loss_ce: 0.056916
2022-01-06 14:14:00,940 iteration 405 : loss : 0.223085, loss_ce: 0.080367
2022-01-06 14:14:02,124 iteration 406 : loss : 0.251823, loss_ce: 0.113223
2022-01-06 14:14:03,304 iteration 407 : loss : 0.250028, loss_ce: 0.094320
2022-01-06 14:14:04,496 iteration 408 : loss : 0.187157, loss_ce: 0.064174
  6%|█▊                            | 24/400 [09:04<2:17:14, 21.90s/it]2022-01-06 14:14:05,951 iteration 409 : loss : 0.170465, loss_ce: 0.068353
2022-01-06 14:14:07,164 iteration 410 : loss : 0.212521, loss_ce: 0.091933
2022-01-06 14:14:08,431 iteration 411 : loss : 0.182713, loss_ce: 0.074642
2022-01-06 14:14:09,656 iteration 412 : loss : 0.243119, loss_ce: 0.098230
2022-01-06 14:14:10,869 iteration 413 : loss : 0.164054, loss_ce: 0.067593
2022-01-06 14:14:12,125 iteration 414 : loss : 0.202662, loss_ce: 0.087691
2022-01-06 14:14:13,423 iteration 415 : loss : 0.185567, loss_ce: 0.092037
2022-01-06 14:14:14,503 iteration 416 : loss : 0.195060, loss_ce: 0.071555
2022-01-06 14:14:15,726 iteration 417 : loss : 0.254368, loss_ce: 0.105064
2022-01-06 14:14:16,971 iteration 418 : loss : 0.199712, loss_ce: 0.081957
2022-01-06 14:14:18,168 iteration 419 : loss : 0.225080, loss_ce: 0.097792
2022-01-06 14:14:19,399 iteration 420 : loss : 0.195967, loss_ce: 0.072427
2022-01-06 14:14:20,522 iteration 421 : loss : 0.226616, loss_ce: 0.065322
2022-01-06 14:14:21,811 iteration 422 : loss : 0.201122, loss_ce: 0.067610
2022-01-06 14:14:23,022 iteration 423 : loss : 0.184802, loss_ce: 0.068214
2022-01-06 14:14:24,229 iteration 424 : loss : 0.236568, loss_ce: 0.098000
2022-01-06 14:14:24,229 Training Data Eval:
2022-01-06 14:14:30,252   Average segmentation loss on training set: 0.4378
2022-01-06 14:14:30,253 Validation Data Eval:
2022-01-06 14:14:32,324   Average segmentation loss on validation set: 0.5096
2022-01-06 14:14:33,631 iteration 425 : loss : 0.242104, loss_ce: 0.105151
  6%|█▉                            | 25/400 [09:33<2:30:26, 24.07s/it]2022-01-06 14:14:34,984 iteration 426 : loss : 0.192481, loss_ce: 0.074290
2022-01-06 14:14:36,159 iteration 427 : loss : 0.208536, loss_ce: 0.086037
2022-01-06 14:14:37,508 iteration 428 : loss : 0.205293, loss_ce: 0.093744
2022-01-06 14:14:38,703 iteration 429 : loss : 0.175289, loss_ce: 0.067553
2022-01-06 14:14:39,838 iteration 430 : loss : 0.199838, loss_ce: 0.091837
2022-01-06 14:14:41,115 iteration 431 : loss : 0.155009, loss_ce: 0.073024
2022-01-06 14:14:42,365 iteration 432 : loss : 0.145084, loss_ce: 0.050358
2022-01-06 14:14:43,499 iteration 433 : loss : 0.179568, loss_ce: 0.068660
2022-01-06 14:14:44,661 iteration 434 : loss : 0.183558, loss_ce: 0.060907
2022-01-06 14:14:45,914 iteration 435 : loss : 0.151256, loss_ce: 0.061161
2022-01-06 14:14:47,192 iteration 436 : loss : 0.199296, loss_ce: 0.057469
2022-01-06 14:14:48,380 iteration 437 : loss : 0.171910, loss_ce: 0.069237
2022-01-06 14:14:49,560 iteration 438 : loss : 0.154105, loss_ce: 0.059363
2022-01-06 14:14:50,747 iteration 439 : loss : 0.185196, loss_ce: 0.077912
2022-01-06 14:14:52,119 iteration 440 : loss : 0.191685, loss_ce: 0.083914
2022-01-06 14:14:53,314 iteration 441 : loss : 0.161909, loss_ce: 0.073651
2022-01-06 14:14:54,497 iteration 442 : loss : 0.173886, loss_ce: 0.064964
  6%|█▉                            | 26/400 [09:54<2:24:03, 23.11s/it]2022-01-06 14:14:55,835 iteration 443 : loss : 0.204645, loss_ce: 0.081812
2022-01-06 14:14:57,009 iteration 444 : loss : 0.169593, loss_ce: 0.066279
2022-01-06 14:14:58,173 iteration 445 : loss : 0.205887, loss_ce: 0.080403
2022-01-06 14:14:59,384 iteration 446 : loss : 0.171652, loss_ce: 0.058400
2022-01-06 14:15:00,674 iteration 447 : loss : 0.129855, loss_ce: 0.047869
2022-01-06 14:15:01,805 iteration 448 : loss : 0.283878, loss_ce: 0.106418
2022-01-06 14:15:02,990 iteration 449 : loss : 0.206543, loss_ce: 0.052032
2022-01-06 14:15:04,126 iteration 450 : loss : 0.201221, loss_ce: 0.077845
2022-01-06 14:15:05,351 iteration 451 : loss : 0.227567, loss_ce: 0.105146
2022-01-06 14:15:06,594 iteration 452 : loss : 0.176314, loss_ce: 0.059989
2022-01-06 14:15:07,787 iteration 453 : loss : 0.149544, loss_ce: 0.055480
2022-01-06 14:15:09,119 iteration 454 : loss : 0.191302, loss_ce: 0.079789
2022-01-06 14:15:10,384 iteration 455 : loss : 0.186847, loss_ce: 0.086133
2022-01-06 14:15:11,578 iteration 456 : loss : 0.159609, loss_ce: 0.057936
2022-01-06 14:15:12,765 iteration 457 : loss : 0.177057, loss_ce: 0.088751
2022-01-06 14:15:13,952 iteration 458 : loss : 0.188659, loss_ce: 0.093517
2022-01-06 14:15:15,192 iteration 459 : loss : 0.180656, loss_ce: 0.078836
  7%|██                            | 27/400 [10:15<2:19:08, 22.38s/it]2022-01-06 14:15:16,408 iteration 460 : loss : 0.203890, loss_ce: 0.098246
2022-01-06 14:15:17,597 iteration 461 : loss : 0.151671, loss_ce: 0.064245
2022-01-06 14:15:18,903 iteration 462 : loss : 0.187362, loss_ce: 0.066282
2022-01-06 14:15:20,136 iteration 463 : loss : 0.212816, loss_ce: 0.077223
2022-01-06 14:15:21,525 iteration 464 : loss : 0.240183, loss_ce: 0.087748
2022-01-06 14:15:22,801 iteration 465 : loss : 0.211620, loss_ce: 0.078287
2022-01-06 14:15:23,988 iteration 466 : loss : 0.179714, loss_ce: 0.072918
2022-01-06 14:15:25,206 iteration 467 : loss : 0.261845, loss_ce: 0.118937
2022-01-06 14:15:26,398 iteration 468 : loss : 0.172687, loss_ce: 0.071522
2022-01-06 14:15:27,671 iteration 469 : loss : 0.177244, loss_ce: 0.079128
2022-01-06 14:15:28,892 iteration 470 : loss : 0.193915, loss_ce: 0.088406
2022-01-06 14:15:30,115 iteration 471 : loss : 0.288149, loss_ce: 0.145740
2022-01-06 14:15:31,378 iteration 472 : loss : 0.137795, loss_ce: 0.062598
2022-01-06 14:15:32,614 iteration 473 : loss : 0.181986, loss_ce: 0.079710
2022-01-06 14:15:33,797 iteration 474 : loss : 0.261779, loss_ce: 0.101515
2022-01-06 14:15:35,008 iteration 475 : loss : 0.232697, loss_ce: 0.088274
2022-01-06 14:15:36,315 iteration 476 : loss : 0.198629, loss_ce: 0.074092
  7%|██                            | 28/400 [10:36<2:16:26, 22.01s/it]2022-01-06 14:15:37,591 iteration 477 : loss : 0.144549, loss_ce: 0.060233
2022-01-06 14:15:38,718 iteration 478 : loss : 0.156999, loss_ce: 0.067271
2022-01-06 14:15:39,974 iteration 479 : loss : 0.162990, loss_ce: 0.055386
2022-01-06 14:15:41,192 iteration 480 : loss : 0.212283, loss_ce: 0.064550
2022-01-06 14:15:42,370 iteration 481 : loss : 0.130939, loss_ce: 0.036477
2022-01-06 14:15:43,696 iteration 482 : loss : 0.216420, loss_ce: 0.065501
2022-01-06 14:15:44,860 iteration 483 : loss : 0.242772, loss_ce: 0.108003
2022-01-06 14:15:46,115 iteration 484 : loss : 0.218988, loss_ce: 0.072673
2022-01-06 14:15:47,366 iteration 485 : loss : 0.195421, loss_ce: 0.075153
2022-01-06 14:15:48,727 iteration 486 : loss : 0.148833, loss_ce: 0.057420
2022-01-06 14:15:49,944 iteration 487 : loss : 0.188180, loss_ce: 0.060095
2022-01-06 14:15:51,066 iteration 488 : loss : 0.141246, loss_ce: 0.064392
2022-01-06 14:15:52,337 iteration 489 : loss : 0.191036, loss_ce: 0.088848
2022-01-06 14:15:53,561 iteration 490 : loss : 0.202045, loss_ce: 0.089052
2022-01-06 14:15:54,869 iteration 491 : loss : 0.194408, loss_ce: 0.068702
2022-01-06 14:15:56,053 iteration 492 : loss : 0.182874, loss_ce: 0.095750
2022-01-06 14:15:57,225 iteration 493 : loss : 0.192097, loss_ce: 0.090747
  7%|██▏                           | 29/400 [10:57<2:14:03, 21.68s/it]2022-01-06 14:15:58,509 iteration 494 : loss : 0.218617, loss_ce: 0.097264
2022-01-06 14:15:59,658 iteration 495 : loss : 0.186951, loss_ce: 0.062569
2022-01-06 14:16:00,898 iteration 496 : loss : 0.196149, loss_ce: 0.080278
2022-01-06 14:16:02,217 iteration 497 : loss : 0.221701, loss_ce: 0.086197
2022-01-06 14:16:03,562 iteration 498 : loss : 0.189438, loss_ce: 0.081016
2022-01-06 14:16:04,818 iteration 499 : loss : 0.188679, loss_ce: 0.065116
2022-01-06 14:16:06,198 iteration 500 : loss : 0.218168, loss_ce: 0.075389
2022-01-06 14:16:07,426 iteration 501 : loss : 0.202133, loss_ce: 0.071414
2022-01-06 14:16:08,587 iteration 502 : loss : 0.171113, loss_ce: 0.071776
2022-01-06 14:16:09,932 iteration 503 : loss : 0.165986, loss_ce: 0.050011
2022-01-06 14:16:11,283 iteration 504 : loss : 0.174020, loss_ce: 0.063199
2022-01-06 14:16:12,464 iteration 505 : loss : 0.183137, loss_ce: 0.074351
2022-01-06 14:16:13,705 iteration 506 : loss : 0.201546, loss_ce: 0.079286
2022-01-06 14:16:14,984 iteration 507 : loss : 0.163736, loss_ce: 0.060601
2022-01-06 14:16:16,196 iteration 508 : loss : 0.163065, loss_ce: 0.065817
2022-01-06 14:16:17,457 iteration 509 : loss : 0.160963, loss_ce: 0.069442
2022-01-06 14:16:17,457 Training Data Eval:
2022-01-06 14:16:23,440   Average segmentation loss on training set: 0.4646
2022-01-06 14:16:23,440 Validation Data Eval:
2022-01-06 14:16:25,485   Average segmentation loss on validation set: 0.5605
2022-01-06 14:16:26,599 iteration 510 : loss : 0.170723, loss_ce: 0.065272
  8%|██▎                           | 30/400 [11:26<2:27:53, 23.98s/it]2022-01-06 14:16:27,718 iteration 511 : loss : 0.134131, loss_ce: 0.056276
2022-01-06 14:16:29,132 iteration 512 : loss : 0.162279, loss_ce: 0.063625
2022-01-06 14:16:30,289 iteration 513 : loss : 0.216149, loss_ce: 0.100759
2022-01-06 14:16:31,550 iteration 514 : loss : 0.175500, loss_ce: 0.064660
2022-01-06 14:16:32,734 iteration 515 : loss : 0.132054, loss_ce: 0.049786
2022-01-06 14:16:33,852 iteration 516 : loss : 0.159633, loss_ce: 0.069597
2022-01-06 14:16:35,262 iteration 517 : loss : 0.175881, loss_ce: 0.061375
2022-01-06 14:16:36,515 iteration 518 : loss : 0.213751, loss_ce: 0.102634
2022-01-06 14:16:37,688 iteration 519 : loss : 0.180819, loss_ce: 0.070617
2022-01-06 14:16:38,858 iteration 520 : loss : 0.180257, loss_ce: 0.080391
2022-01-06 14:16:40,192 iteration 521 : loss : 0.225947, loss_ce: 0.060341
2022-01-06 14:16:41,363 iteration 522 : loss : 0.168112, loss_ce: 0.063043
2022-01-06 14:16:42,623 iteration 523 : loss : 0.167672, loss_ce: 0.060021
2022-01-06 14:16:43,706 iteration 524 : loss : 0.156831, loss_ce: 0.066822
2022-01-06 14:16:44,927 iteration 525 : loss : 0.176217, loss_ce: 0.072593
2022-01-06 14:16:46,185 iteration 526 : loss : 0.181485, loss_ce: 0.068697
2022-01-06 14:16:47,397 iteration 527 : loss : 0.190364, loss_ce: 0.079915
  8%|██▎                           | 31/400 [11:47<2:21:39, 23.03s/it]2022-01-06 14:16:48,738 iteration 528 : loss : 0.114955, loss_ce: 0.045730
2022-01-06 14:16:49,940 iteration 529 : loss : 0.153332, loss_ce: 0.072045
2022-01-06 14:16:51,104 iteration 530 : loss : 0.141408, loss_ce: 0.053869
2022-01-06 14:16:52,372 iteration 531 : loss : 0.209400, loss_ce: 0.072922
2022-01-06 14:16:53,487 iteration 532 : loss : 0.204830, loss_ce: 0.052882
2022-01-06 14:16:54,735 iteration 533 : loss : 0.160761, loss_ce: 0.062589
2022-01-06 14:16:55,977 iteration 534 : loss : 0.187050, loss_ce: 0.087036
2022-01-06 14:16:57,224 iteration 535 : loss : 0.164039, loss_ce: 0.059049
2022-01-06 14:16:58,371 iteration 536 : loss : 0.159272, loss_ce: 0.073427
2022-01-06 14:16:59,591 iteration 537 : loss : 0.181992, loss_ce: 0.068308
2022-01-06 14:17:00,790 iteration 538 : loss : 0.130494, loss_ce: 0.046123
2022-01-06 14:17:01,963 iteration 539 : loss : 0.153256, loss_ce: 0.056580
2022-01-06 14:17:03,119 iteration 540 : loss : 0.210349, loss_ce: 0.088806
2022-01-06 14:17:04,336 iteration 541 : loss : 0.170592, loss_ce: 0.081179
2022-01-06 14:17:05,480 iteration 542 : loss : 0.154250, loss_ce: 0.054260
2022-01-06 14:17:06,650 iteration 543 : loss : 0.187740, loss_ce: 0.064688
2022-01-06 14:17:07,858 iteration 544 : loss : 0.184944, loss_ce: 0.072907
  8%|██▍                           | 32/400 [12:08<2:16:32, 22.26s/it]2022-01-06 14:17:09,122 iteration 545 : loss : 0.214283, loss_ce: 0.095117
2022-01-06 14:17:10,339 iteration 546 : loss : 0.178579, loss_ce: 0.066329
2022-01-06 14:17:11,549 iteration 547 : loss : 0.246744, loss_ce: 0.119071
2022-01-06 14:17:12,685 iteration 548 : loss : 0.195665, loss_ce: 0.075996
2022-01-06 14:17:13,917 iteration 549 : loss : 0.178918, loss_ce: 0.072650
2022-01-06 14:17:15,256 iteration 550 : loss : 0.210457, loss_ce: 0.087812
2022-01-06 14:17:16,508 iteration 551 : loss : 0.198029, loss_ce: 0.063600
2022-01-06 14:17:17,785 iteration 552 : loss : 0.192470, loss_ce: 0.083107
2022-01-06 14:17:19,066 iteration 553 : loss : 0.177523, loss_ce: 0.083010
2022-01-06 14:17:20,207 iteration 554 : loss : 0.180424, loss_ce: 0.070343
2022-01-06 14:17:21,464 iteration 555 : loss : 0.127515, loss_ce: 0.059235
2022-01-06 14:17:22,795 iteration 556 : loss : 0.165635, loss_ce: 0.065676
2022-01-06 14:17:24,002 iteration 557 : loss : 0.186152, loss_ce: 0.081255
2022-01-06 14:17:25,182 iteration 558 : loss : 0.163788, loss_ce: 0.056070
2022-01-06 14:17:26,435 iteration 559 : loss : 0.182106, loss_ce: 0.075125
2022-01-06 14:17:27,697 iteration 560 : loss : 0.214609, loss_ce: 0.083369
2022-01-06 14:17:28,861 iteration 561 : loss : 0.175728, loss_ce: 0.056226
  8%|██▍                           | 33/400 [12:29<2:13:51, 21.88s/it]2022-01-06 14:17:30,117 iteration 562 : loss : 0.203019, loss_ce: 0.085294
2022-01-06 14:17:31,358 iteration 563 : loss : 0.181900, loss_ce: 0.082325
2022-01-06 14:17:32,640 iteration 564 : loss : 0.128450, loss_ce: 0.053417
2022-01-06 14:17:33,891 iteration 565 : loss : 0.241713, loss_ce: 0.131190
2022-01-06 14:17:35,022 iteration 566 : loss : 0.130277, loss_ce: 0.050511
2022-01-06 14:17:36,311 iteration 567 : loss : 0.199540, loss_ce: 0.063828
2022-01-06 14:17:37,502 iteration 568 : loss : 0.168578, loss_ce: 0.061276
2022-01-06 14:17:38,721 iteration 569 : loss : 0.147855, loss_ce: 0.045281
2022-01-06 14:17:40,034 iteration 570 : loss : 0.205931, loss_ce: 0.080436
2022-01-06 14:17:41,318 iteration 571 : loss : 0.152543, loss_ce: 0.052683
2022-01-06 14:17:42,581 iteration 572 : loss : 0.170626, loss_ce: 0.062770
2022-01-06 14:17:43,771 iteration 573 : loss : 0.162231, loss_ce: 0.056851
2022-01-06 14:17:44,979 iteration 574 : loss : 0.150729, loss_ce: 0.068014
2022-01-06 14:17:46,265 iteration 575 : loss : 0.149462, loss_ce: 0.057716
2022-01-06 14:17:47,487 iteration 576 : loss : 0.156118, loss_ce: 0.070253
2022-01-06 14:17:48,728 iteration 577 : loss : 0.221338, loss_ce: 0.072788
2022-01-06 14:17:49,962 iteration 578 : loss : 0.145835, loss_ce: 0.048331
  8%|██▌                           | 34/400 [12:50<2:12:02, 21.65s/it]2022-01-06 14:17:51,225 iteration 579 : loss : 0.138264, loss_ce: 0.040751
2022-01-06 14:17:52,430 iteration 580 : loss : 0.159826, loss_ce: 0.067089
2022-01-06 14:17:53,664 iteration 581 : loss : 0.188537, loss_ce: 0.074609
2022-01-06 14:17:54,914 iteration 582 : loss : 0.143943, loss_ce: 0.052673
2022-01-06 14:17:56,118 iteration 583 : loss : 0.255011, loss_ce: 0.130022
2022-01-06 14:17:57,228 iteration 584 : loss : 0.184679, loss_ce: 0.074019
2022-01-06 14:17:58,416 iteration 585 : loss : 0.222633, loss_ce: 0.090708
2022-01-06 14:17:59,701 iteration 586 : loss : 0.162223, loss_ce: 0.077350
2022-01-06 14:18:00,908 iteration 587 : loss : 0.189910, loss_ce: 0.084706
2022-01-06 14:18:02,180 iteration 588 : loss : 0.153774, loss_ce: 0.058909
2022-01-06 14:18:03,401 iteration 589 : loss : 0.150349, loss_ce: 0.061264
2022-01-06 14:18:04,577 iteration 590 : loss : 0.193152, loss_ce: 0.092913
2022-01-06 14:18:05,794 iteration 591 : loss : 0.145251, loss_ce: 0.054578
2022-01-06 14:18:07,003 iteration 592 : loss : 0.203010, loss_ce: 0.079310
2022-01-06 14:18:08,299 iteration 593 : loss : 0.140585, loss_ce: 0.050459
2022-01-06 14:18:09,607 iteration 594 : loss : 0.180105, loss_ce: 0.077225
2022-01-06 14:18:09,607 Training Data Eval:
2022-01-06 14:18:15,680   Average segmentation loss on training set: 0.6647
2022-01-06 14:18:15,680 Validation Data Eval:
2022-01-06 14:18:17,738   Average segmentation loss on validation set: 0.6232
2022-01-06 14:18:19,017 iteration 595 : loss : 0.162454, loss_ce: 0.051798
  9%|██▋                           | 35/400 [13:19<2:25:13, 23.87s/it]2022-01-06 14:18:20,255 iteration 596 : loss : 0.131136, loss_ce: 0.054088
2022-01-06 14:18:21,475 iteration 597 : loss : 0.183528, loss_ce: 0.094710
2022-01-06 14:18:22,714 iteration 598 : loss : 0.163551, loss_ce: 0.058831
2022-01-06 14:18:23,978 iteration 599 : loss : 0.132913, loss_ce: 0.051607
2022-01-06 14:18:25,090 iteration 600 : loss : 0.157368, loss_ce: 0.068638
2022-01-06 14:18:26,316 iteration 601 : loss : 0.160593, loss_ce: 0.058703
2022-01-06 14:18:27,602 iteration 602 : loss : 0.135341, loss_ce: 0.059808
2022-01-06 14:18:28,715 iteration 603 : loss : 0.174935, loss_ce: 0.072348
2022-01-06 14:18:29,935 iteration 604 : loss : 0.222927, loss_ce: 0.088727
2022-01-06 14:18:31,170 iteration 605 : loss : 0.135743, loss_ce: 0.050035
2022-01-06 14:18:32,360 iteration 606 : loss : 0.196328, loss_ce: 0.089449
2022-01-06 14:18:33,580 iteration 607 : loss : 0.175909, loss_ce: 0.065824
2022-01-06 14:18:34,936 iteration 608 : loss : 0.138199, loss_ce: 0.045560
2022-01-06 14:18:36,123 iteration 609 : loss : 0.119905, loss_ce: 0.044767
2022-01-06 14:18:37,413 iteration 610 : loss : 0.130929, loss_ce: 0.051111
2022-01-06 14:18:38,635 iteration 611 : loss : 0.194819, loss_ce: 0.065855
2022-01-06 14:18:39,858 iteration 612 : loss : 0.131769, loss_ce: 0.051078
  9%|██▋                           | 36/400 [13:40<2:19:16, 22.96s/it]2022-01-06 14:18:41,157 iteration 613 : loss : 0.156692, loss_ce: 0.064456
2022-01-06 14:18:42,500 iteration 614 : loss : 0.125189, loss_ce: 0.049353
2022-01-06 14:18:43,705 iteration 615 : loss : 0.142394, loss_ce: 0.055091
2022-01-06 14:18:44,856 iteration 616 : loss : 0.189428, loss_ce: 0.099876
2022-01-06 14:18:46,081 iteration 617 : loss : 0.145752, loss_ce: 0.060808
2022-01-06 14:18:47,237 iteration 618 : loss : 0.234739, loss_ce: 0.092759
2022-01-06 14:18:48,443 iteration 619 : loss : 0.192885, loss_ce: 0.067027
2022-01-06 14:18:49,774 iteration 620 : loss : 0.191014, loss_ce: 0.082710
2022-01-06 14:18:50,935 iteration 621 : loss : 0.182109, loss_ce: 0.066606
2022-01-06 14:18:52,167 iteration 622 : loss : 0.149079, loss_ce: 0.056989
2022-01-06 14:18:53,457 iteration 623 : loss : 0.151136, loss_ce: 0.044401
2022-01-06 14:18:54,646 iteration 624 : loss : 0.164968, loss_ce: 0.047285
2022-01-06 14:18:55,793 iteration 625 : loss : 0.236289, loss_ce: 0.088556
2022-01-06 14:18:56,951 iteration 626 : loss : 0.168158, loss_ce: 0.050894
2022-01-06 14:18:58,150 iteration 627 : loss : 0.130169, loss_ce: 0.038090
2022-01-06 14:18:59,345 iteration 628 : loss : 0.172063, loss_ce: 0.086011
2022-01-06 14:19:00,599 iteration 629 : loss : 0.125054, loss_ce: 0.044253
  9%|██▊                           | 37/400 [14:00<2:14:53, 22.30s/it]2022-01-06 14:19:01,851 iteration 630 : loss : 0.129903, loss_ce: 0.051827
2022-01-06 14:19:03,099 iteration 631 : loss : 0.182670, loss_ce: 0.081027
2022-01-06 14:19:04,310 iteration 632 : loss : 0.177779, loss_ce: 0.076363
2022-01-06 14:19:05,554 iteration 633 : loss : 0.136050, loss_ce: 0.049143
2022-01-06 14:19:06,822 iteration 634 : loss : 0.164035, loss_ce: 0.079018
2022-01-06 14:19:08,042 iteration 635 : loss : 0.222890, loss_ce: 0.093801
2022-01-06 14:19:09,353 iteration 636 : loss : 0.189754, loss_ce: 0.089562
2022-01-06 14:19:10,518 iteration 637 : loss : 0.214151, loss_ce: 0.084074
2022-01-06 14:19:11,707 iteration 638 : loss : 0.208757, loss_ce: 0.104663
2022-01-06 14:19:12,991 iteration 639 : loss : 0.166189, loss_ce: 0.067547
2022-01-06 14:19:14,185 iteration 640 : loss : 0.139514, loss_ce: 0.056228
2022-01-06 14:19:15,434 iteration 641 : loss : 0.165271, loss_ce: 0.060408
2022-01-06 14:19:16,683 iteration 642 : loss : 0.128936, loss_ce: 0.045092
2022-01-06 14:19:17,905 iteration 643 : loss : 0.170139, loss_ce: 0.064322
2022-01-06 14:19:19,123 iteration 644 : loss : 0.142887, loss_ce: 0.058378
2022-01-06 14:19:20,362 iteration 645 : loss : 0.201163, loss_ce: 0.092289
2022-01-06 14:19:21,567 iteration 646 : loss : 0.130585, loss_ce: 0.055412
 10%|██▊                           | 38/400 [14:21<2:12:07, 21.90s/it]2022-01-06 14:19:22,837 iteration 647 : loss : 0.203877, loss_ce: 0.064217
2022-01-06 14:19:24,001 iteration 648 : loss : 0.176630, loss_ce: 0.079731
2022-01-06 14:19:25,171 iteration 649 : loss : 0.201672, loss_ce: 0.094166
2022-01-06 14:19:26,442 iteration 650 : loss : 0.175122, loss_ce: 0.048176
2022-01-06 14:19:27,694 iteration 651 : loss : 0.169443, loss_ce: 0.082978
2022-01-06 14:19:28,854 iteration 652 : loss : 0.141106, loss_ce: 0.059341
2022-01-06 14:19:30,038 iteration 653 : loss : 0.157363, loss_ce: 0.072024
2022-01-06 14:19:31,265 iteration 654 : loss : 0.146896, loss_ce: 0.053898
2022-01-06 14:19:32,406 iteration 655 : loss : 0.177982, loss_ce: 0.069070
2022-01-06 14:19:33,683 iteration 656 : loss : 0.137897, loss_ce: 0.054454
2022-01-06 14:19:34,902 iteration 657 : loss : 0.168291, loss_ce: 0.065341
2022-01-06 14:19:36,131 iteration 658 : loss : 0.137398, loss_ce: 0.050854
2022-01-06 14:19:37,413 iteration 659 : loss : 0.198171, loss_ce: 0.066411
2022-01-06 14:19:38,648 iteration 660 : loss : 0.132860, loss_ce: 0.049033
2022-01-06 14:19:39,872 iteration 661 : loss : 0.134479, loss_ce: 0.056678
2022-01-06 14:19:41,101 iteration 662 : loss : 0.154752, loss_ce: 0.058391
2022-01-06 14:19:42,310 iteration 663 : loss : 0.197479, loss_ce: 0.098259
 10%|██▉                           | 39/400 [14:42<2:09:39, 21.55s/it]2022-01-06 14:19:43,546 iteration 664 : loss : 0.211628, loss_ce: 0.100242
2022-01-06 14:19:44,815 iteration 665 : loss : 0.167917, loss_ce: 0.058938
2022-01-06 14:19:46,043 iteration 666 : loss : 0.187587, loss_ce: 0.064518
2022-01-06 14:19:47,321 iteration 667 : loss : 0.177249, loss_ce: 0.069547
2022-01-06 14:19:48,546 iteration 668 : loss : 0.111195, loss_ce: 0.042037
2022-01-06 14:19:49,837 iteration 669 : loss : 0.152550, loss_ce: 0.062535
2022-01-06 14:19:51,117 iteration 670 : loss : 0.177054, loss_ce: 0.068094
2022-01-06 14:19:52,367 iteration 671 : loss : 0.120671, loss_ce: 0.047575
2022-01-06 14:19:53,500 iteration 672 : loss : 0.204306, loss_ce: 0.098009
2022-01-06 14:19:54,710 iteration 673 : loss : 0.169857, loss_ce: 0.058678
2022-01-06 14:19:56,106 iteration 674 : loss : 0.151570, loss_ce: 0.068900
2022-01-06 14:19:57,322 iteration 675 : loss : 0.113794, loss_ce: 0.038918
2022-01-06 14:19:58,513 iteration 676 : loss : 0.157095, loss_ce: 0.057639
2022-01-06 14:19:59,872 iteration 677 : loss : 0.116450, loss_ce: 0.044228
2022-01-06 14:20:01,133 iteration 678 : loss : 0.168358, loss_ce: 0.048989
2022-01-06 14:20:02,300 iteration 679 : loss : 0.156497, loss_ce: 0.065023
2022-01-06 14:20:02,300 Training Data Eval:
2022-01-06 14:20:08,280   Average segmentation loss on training set: 0.2899
2022-01-06 14:20:08,281 Validation Data Eval:
2022-01-06 14:20:10,340   Average segmentation loss on validation set: 0.3782
2022-01-06 14:20:11,512 iteration 680 : loss : 0.172845, loss_ce: 0.067154
 10%|███                           | 40/400 [15:11<2:23:05, 23.85s/it]2022-01-06 14:20:12,957 iteration 681 : loss : 0.133143, loss_ce: 0.056735
2022-01-06 14:20:14,170 iteration 682 : loss : 0.138912, loss_ce: 0.044607
2022-01-06 14:20:15,338 iteration 683 : loss : 0.138012, loss_ce: 0.037538
2022-01-06 14:20:16,635 iteration 684 : loss : 0.146248, loss_ce: 0.055501
2022-01-06 14:20:17,948 iteration 685 : loss : 0.140654, loss_ce: 0.043628
2022-01-06 14:20:19,219 iteration 686 : loss : 0.150799, loss_ce: 0.063322
2022-01-06 14:20:20,479 iteration 687 : loss : 0.120671, loss_ce: 0.048187
2022-01-06 14:20:21,776 iteration 688 : loss : 0.148614, loss_ce: 0.053174
2022-01-06 14:20:23,035 iteration 689 : loss : 0.139642, loss_ce: 0.061879
2022-01-06 14:20:24,249 iteration 690 : loss : 0.158175, loss_ce: 0.072089
2022-01-06 14:20:25,534 iteration 691 : loss : 0.163942, loss_ce: 0.067203
2022-01-06 14:20:26,680 iteration 692 : loss : 0.138505, loss_ce: 0.057420
2022-01-06 14:20:27,885 iteration 693 : loss : 0.189153, loss_ce: 0.079363
2022-01-06 14:20:29,116 iteration 694 : loss : 0.247913, loss_ce: 0.081143
2022-01-06 14:20:30,335 iteration 695 : loss : 0.132136, loss_ce: 0.044273
2022-01-06 14:20:31,507 iteration 696 : loss : 0.194885, loss_ce: 0.053863
2022-01-06 14:20:32,617 iteration 697 : loss : 0.126008, loss_ce: 0.048210
 10%|███                           | 41/400 [15:32<2:17:46, 23.03s/it]2022-01-06 14:20:33,925 iteration 698 : loss : 0.124287, loss_ce: 0.052730
2022-01-06 14:20:35,174 iteration 699 : loss : 0.186683, loss_ce: 0.077831
2022-01-06 14:20:36,344 iteration 700 : loss : 0.206484, loss_ce: 0.080347
2022-01-06 14:20:37,528 iteration 701 : loss : 0.209305, loss_ce: 0.080945
2022-01-06 14:20:38,800 iteration 702 : loss : 0.126069, loss_ce: 0.051220
2022-01-06 14:20:40,038 iteration 703 : loss : 0.155440, loss_ce: 0.049894
2022-01-06 14:20:41,361 iteration 704 : loss : 0.175486, loss_ce: 0.056085
2022-01-06 14:20:42,628 iteration 705 : loss : 0.174496, loss_ce: 0.075288
2022-01-06 14:20:43,898 iteration 706 : loss : 0.173100, loss_ce: 0.050326
2022-01-06 14:20:45,085 iteration 707 : loss : 0.170139, loss_ce: 0.066928
2022-01-06 14:20:46,322 iteration 708 : loss : 0.170352, loss_ce: 0.057295
2022-01-06 14:20:47,571 iteration 709 : loss : 0.127719, loss_ce: 0.046249
2022-01-06 14:20:48,730 iteration 710 : loss : 0.157670, loss_ce: 0.075614
2022-01-06 14:20:50,014 iteration 711 : loss : 0.134007, loss_ce: 0.055057
2022-01-06 14:20:51,223 iteration 712 : loss : 0.204580, loss_ce: 0.086714
2022-01-06 14:20:52,369 iteration 713 : loss : 0.136298, loss_ce: 0.055515
2022-01-06 14:20:53,553 iteration 714 : loss : 0.139514, loss_ce: 0.047628
 10%|███▏                          | 42/400 [15:53<2:13:37, 22.40s/it]2022-01-06 14:20:54,944 iteration 715 : loss : 0.132289, loss_ce: 0.054235
2022-01-06 14:20:56,154 iteration 716 : loss : 0.146662, loss_ce: 0.056639
2022-01-06 14:20:57,364 iteration 717 : loss : 0.181398, loss_ce: 0.061432
2022-01-06 14:20:58,587 iteration 718 : loss : 0.170916, loss_ce: 0.070155
2022-01-06 14:20:59,914 iteration 719 : loss : 0.121438, loss_ce: 0.049800
2022-01-06 14:21:01,100 iteration 720 : loss : 0.171007, loss_ce: 0.069263
2022-01-06 14:21:02,384 iteration 721 : loss : 0.145736, loss_ce: 0.057103
2022-01-06 14:21:03,634 iteration 722 : loss : 0.163407, loss_ce: 0.060986
2022-01-06 14:21:04,823 iteration 723 : loss : 0.133805, loss_ce: 0.042571
2022-01-06 14:21:06,187 iteration 724 : loss : 0.156837, loss_ce: 0.066156
2022-01-06 14:21:07,337 iteration 725 : loss : 0.127321, loss_ce: 0.045203
2022-01-06 14:21:08,575 iteration 726 : loss : 0.260662, loss_ce: 0.079688
2022-01-06 14:21:09,844 iteration 727 : loss : 0.176690, loss_ce: 0.056666
2022-01-06 14:21:11,260 iteration 728 : loss : 0.156689, loss_ce: 0.071725
2022-01-06 14:21:12,358 iteration 729 : loss : 0.120075, loss_ce: 0.050116
2022-01-06 14:21:13,619 iteration 730 : loss : 0.140792, loss_ce: 0.061850
2022-01-06 14:21:14,887 iteration 731 : loss : 0.127503, loss_ce: 0.048120
 11%|███▏                          | 43/400 [16:15<2:11:20, 22.07s/it]2022-01-06 14:21:16,320 iteration 732 : loss : 0.122664, loss_ce: 0.050172
2022-01-06 14:21:17,584 iteration 733 : loss : 0.233364, loss_ce: 0.112599
2022-01-06 14:21:18,752 iteration 734 : loss : 0.190093, loss_ce: 0.073464
2022-01-06 14:21:19,985 iteration 735 : loss : 0.140455, loss_ce: 0.057799
2022-01-06 14:21:21,243 iteration 736 : loss : 0.146931, loss_ce: 0.053131
2022-01-06 14:21:22,544 iteration 737 : loss : 0.125182, loss_ce: 0.043739
2022-01-06 14:21:23,776 iteration 738 : loss : 0.145405, loss_ce: 0.060062
2022-01-06 14:21:24,977 iteration 739 : loss : 0.163595, loss_ce: 0.071270
2022-01-06 14:21:26,184 iteration 740 : loss : 0.200081, loss_ce: 0.097943
2022-01-06 14:21:27,424 iteration 741 : loss : 0.149019, loss_ce: 0.058588
2022-01-06 14:21:28,740 iteration 742 : loss : 0.175158, loss_ce: 0.062412
2022-01-06 14:21:29,938 iteration 743 : loss : 0.168821, loss_ce: 0.061275
2022-01-06 14:21:31,158 iteration 744 : loss : 0.190164, loss_ce: 0.067932
2022-01-06 14:21:32,358 iteration 745 : loss : 0.154929, loss_ce: 0.064058
2022-01-06 14:21:33,605 iteration 746 : loss : 0.227946, loss_ce: 0.110705
2022-01-06 14:21:34,886 iteration 747 : loss : 0.174771, loss_ce: 0.056461
2022-01-06 14:21:36,029 iteration 748 : loss : 0.150613, loss_ce: 0.060036
 11%|███▎                          | 44/400 [16:36<2:09:20, 21.80s/it]2022-01-06 14:21:37,344 iteration 749 : loss : 0.173712, loss_ce: 0.070096
2022-01-06 14:21:38,635 iteration 750 : loss : 0.143173, loss_ce: 0.060476
2022-01-06 14:21:39,787 iteration 751 : loss : 0.137614, loss_ce: 0.056583
2022-01-06 14:21:41,024 iteration 752 : loss : 0.173560, loss_ce: 0.058148
2022-01-06 14:21:42,354 iteration 753 : loss : 0.129443, loss_ce: 0.052486
2022-01-06 14:21:43,545 iteration 754 : loss : 0.188022, loss_ce: 0.073598
2022-01-06 14:21:44,781 iteration 755 : loss : 0.114219, loss_ce: 0.043700
2022-01-06 14:21:45,955 iteration 756 : loss : 0.125665, loss_ce: 0.048915
2022-01-06 14:21:47,154 iteration 757 : loss : 0.153670, loss_ce: 0.062620
2022-01-06 14:21:48,397 iteration 758 : loss : 0.130594, loss_ce: 0.053853
2022-01-06 14:21:49,531 iteration 759 : loss : 0.107568, loss_ce: 0.038218
2022-01-06 14:21:50,797 iteration 760 : loss : 0.184139, loss_ce: 0.058363
2022-01-06 14:21:52,079 iteration 761 : loss : 0.153439, loss_ce: 0.042941
2022-01-06 14:21:53,332 iteration 762 : loss : 0.156220, loss_ce: 0.077262
2022-01-06 14:21:54,541 iteration 763 : loss : 0.155848, loss_ce: 0.059100
2022-01-06 14:21:55,709 iteration 764 : loss : 0.133106, loss_ce: 0.049083
2022-01-06 14:21:55,709 Training Data Eval:
2022-01-06 14:22:01,719   Average segmentation loss on training set: 0.4600
2022-01-06 14:22:01,720 Validation Data Eval:
2022-01-06 14:22:03,780   Average segmentation loss on validation set: 0.5470
2022-01-06 14:22:05,046 iteration 765 : loss : 0.133332, loss_ce: 0.053706
 11%|███▍                          | 45/400 [17:05<2:21:46, 23.96s/it]2022-01-06 14:22:06,321 iteration 766 : loss : 0.188375, loss_ce: 0.070618
2022-01-06 14:22:07,609 iteration 767 : loss : 0.168205, loss_ce: 0.070900
2022-01-06 14:22:08,732 iteration 768 : loss : 0.163731, loss_ce: 0.055054
2022-01-06 14:22:09,970 iteration 769 : loss : 0.231936, loss_ce: 0.069242
2022-01-06 14:22:11,143 iteration 770 : loss : 0.109871, loss_ce: 0.042030
2022-01-06 14:22:12,454 iteration 771 : loss : 0.147205, loss_ce: 0.055746
2022-01-06 14:22:13,744 iteration 772 : loss : 0.091559, loss_ce: 0.040924
2022-01-06 14:22:14,965 iteration 773 : loss : 0.158363, loss_ce: 0.058346
2022-01-06 14:22:16,170 iteration 774 : loss : 0.148606, loss_ce: 0.070374
2022-01-06 14:22:17,363 iteration 775 : loss : 0.158224, loss_ce: 0.062418
2022-01-06 14:22:18,475 iteration 776 : loss : 0.158926, loss_ce: 0.059538
2022-01-06 14:22:19,772 iteration 777 : loss : 0.148740, loss_ce: 0.065678
2022-01-06 14:22:21,021 iteration 778 : loss : 0.099018, loss_ce: 0.039906
2022-01-06 14:22:22,253 iteration 779 : loss : 0.106116, loss_ce: 0.047843
2022-01-06 14:22:23,410 iteration 780 : loss : 0.165246, loss_ce: 0.061011
2022-01-06 14:22:24,697 iteration 781 : loss : 0.132447, loss_ce: 0.055234
2022-01-06 14:22:25,972 iteration 782 : loss : 0.191517, loss_ce: 0.069626
 12%|███▍                          | 46/400 [17:26<2:15:59, 23.05s/it]2022-01-06 14:22:27,311 iteration 783 : loss : 0.113102, loss_ce: 0.047243
2022-01-06 14:22:28,527 iteration 784 : loss : 0.187008, loss_ce: 0.071184
2022-01-06 14:22:29,790 iteration 785 : loss : 0.146848, loss_ce: 0.064653
2022-01-06 14:22:30,994 iteration 786 : loss : 0.119980, loss_ce: 0.039989
2022-01-06 14:22:32,217 iteration 787 : loss : 0.145461, loss_ce: 0.062480
2022-01-06 14:22:33,467 iteration 788 : loss : 0.106856, loss_ce: 0.037813
2022-01-06 14:22:34,849 iteration 789 : loss : 0.184545, loss_ce: 0.094119
2022-01-06 14:22:36,136 iteration 790 : loss : 0.102604, loss_ce: 0.034113
2022-01-06 14:22:37,329 iteration 791 : loss : 0.162182, loss_ce: 0.057354
2022-01-06 14:22:38,586 iteration 792 : loss : 0.123036, loss_ce: 0.046161
2022-01-06 14:22:39,704 iteration 793 : loss : 0.156440, loss_ce: 0.055489
2022-01-06 14:22:40,912 iteration 794 : loss : 0.150853, loss_ce: 0.055872
2022-01-06 14:22:42,145 iteration 795 : loss : 0.125456, loss_ce: 0.057239
2022-01-06 14:22:43,372 iteration 796 : loss : 0.137421, loss_ce: 0.056914
2022-01-06 14:22:44,542 iteration 797 : loss : 0.158335, loss_ce: 0.063229
2022-01-06 14:22:45,723 iteration 798 : loss : 0.183723, loss_ce: 0.062082
2022-01-06 14:22:46,992 iteration 799 : loss : 0.106427, loss_ce: 0.036632
 12%|███▌                          | 47/400 [17:47<2:12:02, 22.44s/it]2022-01-06 14:22:48,338 iteration 800 : loss : 0.118234, loss_ce: 0.036404
2022-01-06 14:22:49,511 iteration 801 : loss : 0.174754, loss_ce: 0.061917
2022-01-06 14:22:50,721 iteration 802 : loss : 0.193780, loss_ce: 0.072368
2022-01-06 14:22:52,035 iteration 803 : loss : 0.219867, loss_ce: 0.114391
2022-01-06 14:22:53,178 iteration 804 : loss : 0.173010, loss_ce: 0.076117
2022-01-06 14:22:54,325 iteration 805 : loss : 0.231293, loss_ce: 0.066250
2022-01-06 14:22:55,560 iteration 806 : loss : 0.170207, loss_ce: 0.056566
2022-01-06 14:22:56,888 iteration 807 : loss : 0.191986, loss_ce: 0.071206
2022-01-06 14:22:58,155 iteration 808 : loss : 0.166290, loss_ce: 0.063766
2022-01-06 14:22:59,354 iteration 809 : loss : 0.201313, loss_ce: 0.070315
2022-01-06 14:23:00,633 iteration 810 : loss : 0.196789, loss_ce: 0.067405
2022-01-06 14:23:01,800 iteration 811 : loss : 0.100127, loss_ce: 0.042787
2022-01-06 14:23:03,097 iteration 812 : loss : 0.163452, loss_ce: 0.064678
2022-01-06 14:23:04,297 iteration 813 : loss : 0.117074, loss_ce: 0.047459
2022-01-06 14:23:05,531 iteration 814 : loss : 0.142045, loss_ce: 0.062816
2022-01-06 14:23:06,804 iteration 815 : loss : 0.136819, loss_ce: 0.054740
2022-01-06 14:23:07,875 iteration 816 : loss : 0.111379, loss_ce: 0.039157
 12%|███▌                          | 48/400 [18:08<2:08:55, 21.98s/it]2022-01-06 14:23:09,131 iteration 817 : loss : 0.125800, loss_ce: 0.043557
2022-01-06 14:23:10,391 iteration 818 : loss : 0.144794, loss_ce: 0.060750
2022-01-06 14:23:11,599 iteration 819 : loss : 0.134620, loss_ce: 0.052758
2022-01-06 14:23:12,933 iteration 820 : loss : 0.139535, loss_ce: 0.055581
2022-01-06 14:23:14,040 iteration 821 : loss : 0.163307, loss_ce: 0.051138
2022-01-06 14:23:15,306 iteration 822 : loss : 0.109145, loss_ce: 0.042882
2022-01-06 14:23:16,529 iteration 823 : loss : 0.153003, loss_ce: 0.055773
2022-01-06 14:23:17,728 iteration 824 : loss : 0.125177, loss_ce: 0.045401
2022-01-06 14:23:18,951 iteration 825 : loss : 0.120009, loss_ce: 0.034893
2022-01-06 14:23:20,217 iteration 826 : loss : 0.139526, loss_ce: 0.057298
2022-01-06 14:23:21,411 iteration 827 : loss : 0.192728, loss_ce: 0.053807
2022-01-06 14:23:22,676 iteration 828 : loss : 0.134174, loss_ce: 0.054331
2022-01-06 14:23:23,877 iteration 829 : loss : 0.116126, loss_ce: 0.041626
2022-01-06 14:23:25,213 iteration 830 : loss : 0.135854, loss_ce: 0.050513
2022-01-06 14:23:26,439 iteration 831 : loss : 0.144841, loss_ce: 0.060576
2022-01-06 14:23:27,626 iteration 832 : loss : 0.126209, loss_ce: 0.049553
2022-01-06 14:23:28,786 iteration 833 : loss : 0.098020, loss_ce: 0.048132
 12%|███▋                          | 49/400 [18:29<2:06:40, 21.65s/it]2022-01-06 14:23:30,021 iteration 834 : loss : 0.190735, loss_ce: 0.072026
2022-01-06 14:23:31,303 iteration 835 : loss : 0.178277, loss_ce: 0.085395
2022-01-06 14:23:32,544 iteration 836 : loss : 0.129202, loss_ce: 0.051967
2022-01-06 14:23:33,735 iteration 837 : loss : 0.146081, loss_ce: 0.065106
2022-01-06 14:23:34,920 iteration 838 : loss : 0.181796, loss_ce: 0.088270
2022-01-06 14:23:36,229 iteration 839 : loss : 0.140358, loss_ce: 0.047010
2022-01-06 14:23:37,472 iteration 840 : loss : 0.227092, loss_ce: 0.110861
2022-01-06 14:23:38,693 iteration 841 : loss : 0.162590, loss_ce: 0.075692
2022-01-06 14:23:40,056 iteration 842 : loss : 0.147151, loss_ce: 0.045418
2022-01-06 14:23:41,244 iteration 843 : loss : 0.164703, loss_ce: 0.058632
2022-01-06 14:23:42,519 iteration 844 : loss : 0.150213, loss_ce: 0.062524
2022-01-06 14:23:43,703 iteration 845 : loss : 0.133356, loss_ce: 0.044719
2022-01-06 14:23:44,939 iteration 846 : loss : 0.190301, loss_ce: 0.064075
2022-01-06 14:23:46,233 iteration 847 : loss : 0.148734, loss_ce: 0.072733
2022-01-06 14:23:47,371 iteration 848 : loss : 0.149286, loss_ce: 0.058271
2022-01-06 14:23:48,642 iteration 849 : loss : 0.131662, loss_ce: 0.053612
2022-01-06 14:23:48,642 Training Data Eval:
2022-01-06 14:23:54,661   Average segmentation loss on training set: 0.2511
2022-01-06 14:23:54,661 Validation Data Eval:
2022-01-06 14:23:56,727   Average segmentation loss on validation set: 0.2669
2022-01-06 14:23:58,054 iteration 850 : loss : 0.188121, loss_ce: 0.074553
 12%|███▊                          | 50/400 [18:58<2:19:38, 23.94s/it]2022-01-06 14:23:59,390 iteration 851 : loss : 0.164076, loss_ce: 0.080155
2022-01-06 14:24:00,655 iteration 852 : loss : 0.135562, loss_ce: 0.050418
2022-01-06 14:24:01,916 iteration 853 : loss : 0.116670, loss_ce: 0.045030
2022-01-06 14:24:03,079 iteration 854 : loss : 0.139773, loss_ce: 0.052328
2022-01-06 14:24:04,202 iteration 855 : loss : 0.125398, loss_ce: 0.047045
2022-01-06 14:24:05,403 iteration 856 : loss : 0.175115, loss_ce: 0.066299
2022-01-06 14:24:06,755 iteration 857 : loss : 0.130395, loss_ce: 0.048657
2022-01-06 14:24:07,981 iteration 858 : loss : 0.160243, loss_ce: 0.054299
2022-01-06 14:24:09,309 iteration 859 : loss : 0.141979, loss_ce: 0.052706
2022-01-06 14:24:10,586 iteration 860 : loss : 0.161223, loss_ce: 0.061101
2022-01-06 14:24:11,808 iteration 861 : loss : 0.161439, loss_ce: 0.054737
2022-01-06 14:24:13,029 iteration 862 : loss : 0.152625, loss_ce: 0.038374
2022-01-06 14:24:14,183 iteration 863 : loss : 0.202345, loss_ce: 0.083561
2022-01-06 14:24:15,395 iteration 864 : loss : 0.132797, loss_ce: 0.054405
2022-01-06 14:24:16,538 iteration 865 : loss : 0.104621, loss_ce: 0.043804
2022-01-06 14:24:17,706 iteration 866 : loss : 0.120530, loss_ce: 0.043482
2022-01-06 14:24:18,894 iteration 867 : loss : 0.166553, loss_ce: 0.069810
 13%|███▊                          | 51/400 [19:19<2:13:50, 23.01s/it]2022-01-06 14:24:20,216 iteration 868 : loss : 0.130595, loss_ce: 0.050814
2022-01-06 14:24:21,396 iteration 869 : loss : 0.111760, loss_ce: 0.044684
2022-01-06 14:24:22,648 iteration 870 : loss : 0.160744, loss_ce: 0.042783
2022-01-06 14:24:23,902 iteration 871 : loss : 0.138250, loss_ce: 0.041509
2022-01-06 14:24:25,053 iteration 872 : loss : 0.124846, loss_ce: 0.040757
2022-01-06 14:24:26,221 iteration 873 : loss : 0.097160, loss_ce: 0.034982
2022-01-06 14:24:27,477 iteration 874 : loss : 0.143815, loss_ce: 0.056598
2022-01-06 14:24:28,710 iteration 875 : loss : 0.161692, loss_ce: 0.083168
2022-01-06 14:24:29,907 iteration 876 : loss : 0.129109, loss_ce: 0.054627
2022-01-06 14:24:31,052 iteration 877 : loss : 0.117655, loss_ce: 0.044840
2022-01-06 14:24:32,222 iteration 878 : loss : 0.115701, loss_ce: 0.037478
2022-01-06 14:24:33,457 iteration 879 : loss : 0.123755, loss_ce: 0.046313
2022-01-06 14:24:34,627 iteration 880 : loss : 0.130306, loss_ce: 0.049895
2022-01-06 14:24:35,897 iteration 881 : loss : 0.147725, loss_ce: 0.058641
2022-01-06 14:24:37,074 iteration 882 : loss : 0.114026, loss_ce: 0.053550
2022-01-06 14:24:38,256 iteration 883 : loss : 0.165569, loss_ce: 0.058722
2022-01-06 14:24:39,612 iteration 884 : loss : 0.128652, loss_ce: 0.045672
 13%|███▉                          | 52/400 [19:39<2:09:27, 22.32s/it]2022-01-06 14:24:40,965 iteration 885 : loss : 0.140874, loss_ce: 0.060776
2022-01-06 14:24:42,156 iteration 886 : loss : 0.165756, loss_ce: 0.057804
2022-01-06 14:24:43,353 iteration 887 : loss : 0.157805, loss_ce: 0.067601
2022-01-06 14:24:44,586 iteration 888 : loss : 0.141216, loss_ce: 0.058820
2022-01-06 14:24:45,778 iteration 889 : loss : 0.138422, loss_ce: 0.045374
2022-01-06 14:24:47,072 iteration 890 : loss : 0.109496, loss_ce: 0.043326
2022-01-06 14:24:48,318 iteration 891 : loss : 0.140307, loss_ce: 0.065658
2022-01-06 14:24:49,535 iteration 892 : loss : 0.166737, loss_ce: 0.055145
2022-01-06 14:24:50,842 iteration 893 : loss : 0.174254, loss_ce: 0.077326
2022-01-06 14:24:52,065 iteration 894 : loss : 0.120301, loss_ce: 0.048654
2022-01-06 14:24:53,300 iteration 895 : loss : 0.125356, loss_ce: 0.041089
2022-01-06 14:24:54,591 iteration 896 : loss : 0.177923, loss_ce: 0.054637
2022-01-06 14:24:55,747 iteration 897 : loss : 0.106291, loss_ce: 0.038928
2022-01-06 14:24:57,116 iteration 898 : loss : 0.135776, loss_ce: 0.051117
2022-01-06 14:24:58,347 iteration 899 : loss : 0.154493, loss_ce: 0.053449
2022-01-06 14:24:59,617 iteration 900 : loss : 0.136160, loss_ce: 0.053492
2022-01-06 14:25:00,864 iteration 901 : loss : 0.162307, loss_ce: 0.089343
 13%|███▉                          | 53/400 [20:01<2:07:15, 22.00s/it]2022-01-06 14:25:02,186 iteration 902 : loss : 0.158411, loss_ce: 0.064698
2022-01-06 14:25:03,477 iteration 903 : loss : 0.128670, loss_ce: 0.044743
2022-01-06 14:25:04,698 iteration 904 : loss : 0.107029, loss_ce: 0.047293
2022-01-06 14:25:05,887 iteration 905 : loss : 0.175967, loss_ce: 0.052051
2022-01-06 14:25:07,075 iteration 906 : loss : 0.117573, loss_ce: 0.046661
2022-01-06 14:25:08,382 iteration 907 : loss : 0.121943, loss_ce: 0.053121
2022-01-06 14:25:09,554 iteration 908 : loss : 0.138348, loss_ce: 0.065886
2022-01-06 14:25:10,830 iteration 909 : loss : 0.157789, loss_ce: 0.067847
2022-01-06 14:25:12,078 iteration 910 : loss : 0.169865, loss_ce: 0.071821
2022-01-06 14:25:13,340 iteration 911 : loss : 0.155963, loss_ce: 0.055250
2022-01-06 14:25:14,475 iteration 912 : loss : 0.159851, loss_ce: 0.057952
2022-01-06 14:25:15,767 iteration 913 : loss : 0.122505, loss_ce: 0.048382
2022-01-06 14:25:16,975 iteration 914 : loss : 0.129029, loss_ce: 0.059006
2022-01-06 14:25:18,262 iteration 915 : loss : 0.126560, loss_ce: 0.053018
2022-01-06 14:25:19,479 iteration 916 : loss : 0.156849, loss_ce: 0.067343
2022-01-06 14:25:20,723 iteration 917 : loss : 0.127609, loss_ce: 0.048327
2022-01-06 14:25:21,988 iteration 918 : loss : 0.139064, loss_ce: 0.050240
 14%|████                          | 54/400 [20:22<2:05:20, 21.74s/it]2022-01-06 14:25:23,374 iteration 919 : loss : 0.126307, loss_ce: 0.045038
2022-01-06 14:25:24,538 iteration 920 : loss : 0.191513, loss_ce: 0.113977
2022-01-06 14:25:25,875 iteration 921 : loss : 0.119452, loss_ce: 0.044807
2022-01-06 14:25:26,994 iteration 922 : loss : 0.127912, loss_ce: 0.052700
2022-01-06 14:25:28,176 iteration 923 : loss : 0.147808, loss_ce: 0.059021
2022-01-06 14:25:29,403 iteration 924 : loss : 0.094769, loss_ce: 0.029017
2022-01-06 14:25:30,575 iteration 925 : loss : 0.131969, loss_ce: 0.040765
2022-01-06 14:25:31,740 iteration 926 : loss : 0.129201, loss_ce: 0.051378
2022-01-06 14:25:32,982 iteration 927 : loss : 0.145930, loss_ce: 0.060235
2022-01-06 14:25:34,144 iteration 928 : loss : 0.128315, loss_ce: 0.046703
2022-01-06 14:25:35,544 iteration 929 : loss : 0.132334, loss_ce: 0.058978
2022-01-06 14:25:36,788 iteration 930 : loss : 0.140065, loss_ce: 0.071891
2022-01-06 14:25:38,038 iteration 931 : loss : 0.142421, loss_ce: 0.051135
2022-01-06 14:25:39,286 iteration 932 : loss : 0.185082, loss_ce: 0.061603
2022-01-06 14:25:40,682 iteration 933 : loss : 0.169280, loss_ce: 0.058989
2022-01-06 14:25:41,886 iteration 934 : loss : 0.131387, loss_ce: 0.052613
2022-01-06 14:25:41,886 Training Data Eval:
2022-01-06 14:25:47,886   Average segmentation loss on training set: 0.5211
2022-01-06 14:25:47,887 Validation Data Eval:
2022-01-06 14:25:49,934   Average segmentation loss on validation set: 0.4580
2022-01-06 14:25:51,129 iteration 935 : loss : 0.116615, loss_ce: 0.043890
 14%|████▏                         | 55/400 [20:51<2:17:45, 23.96s/it]2022-01-06 14:25:52,406 iteration 936 : loss : 0.124361, loss_ce: 0.042707
2022-01-06 14:25:53,608 iteration 937 : loss : 0.128674, loss_ce: 0.051024
2022-01-06 14:25:54,873 iteration 938 : loss : 0.130618, loss_ce: 0.057742
2022-01-06 14:25:56,102 iteration 939 : loss : 0.133550, loss_ce: 0.052390
2022-01-06 14:25:57,313 iteration 940 : loss : 0.129945, loss_ce: 0.046158
2022-01-06 14:25:58,583 iteration 941 : loss : 0.196135, loss_ce: 0.068638
2022-01-06 14:25:59,802 iteration 942 : loss : 0.135634, loss_ce: 0.041121
2022-01-06 14:26:00,944 iteration 943 : loss : 0.151171, loss_ce: 0.054052
2022-01-06 14:26:02,147 iteration 944 : loss : 0.122314, loss_ce: 0.047204
2022-01-06 14:26:03,368 iteration 945 : loss : 0.154275, loss_ce: 0.085584
2022-01-06 14:26:04,607 iteration 946 : loss : 0.088740, loss_ce: 0.033355
2022-01-06 14:26:05,965 iteration 947 : loss : 0.163285, loss_ce: 0.071137
2022-01-06 14:26:07,155 iteration 948 : loss : 0.120238, loss_ce: 0.046547
2022-01-06 14:26:08,416 iteration 949 : loss : 0.109242, loss_ce: 0.040353
2022-01-06 14:26:09,673 iteration 950 : loss : 0.179024, loss_ce: 0.048263
2022-01-06 14:26:10,991 iteration 951 : loss : 0.125823, loss_ce: 0.051227
2022-01-06 14:26:12,153 iteration 952 : loss : 0.122951, loss_ce: 0.039646
 14%|████▏                         | 56/400 [21:12<2:12:18, 23.08s/it]2022-01-06 14:26:13,456 iteration 953 : loss : 0.114150, loss_ce: 0.036057
2022-01-06 14:26:14,817 iteration 954 : loss : 0.092654, loss_ce: 0.032309
2022-01-06 14:26:16,027 iteration 955 : loss : 0.118519, loss_ce: 0.050036
2022-01-06 14:26:17,242 iteration 956 : loss : 0.111832, loss_ce: 0.042939
2022-01-06 14:26:18,330 iteration 957 : loss : 0.173402, loss_ce: 0.058766
2022-01-06 14:26:19,549 iteration 958 : loss : 0.134964, loss_ce: 0.039597
2022-01-06 14:26:20,732 iteration 959 : loss : 0.111893, loss_ce: 0.046102
2022-01-06 14:26:21,899 iteration 960 : loss : 0.148250, loss_ce: 0.057591
2022-01-06 14:26:23,143 iteration 961 : loss : 0.152484, loss_ce: 0.053884
2022-01-06 14:26:24,341 iteration 962 : loss : 0.106316, loss_ce: 0.034269
2022-01-06 14:26:25,619 iteration 963 : loss : 0.161309, loss_ce: 0.056572
2022-01-06 14:26:26,871 iteration 964 : loss : 0.095264, loss_ce: 0.038463
2022-01-06 14:26:28,070 iteration 965 : loss : 0.095013, loss_ce: 0.036696
2022-01-06 14:26:29,398 iteration 966 : loss : 0.159153, loss_ce: 0.074659
2022-01-06 14:26:30,626 iteration 967 : loss : 0.215599, loss_ce: 0.091722
2022-01-06 14:26:31,881 iteration 968 : loss : 0.131732, loss_ce: 0.051162
2022-01-06 14:26:33,172 iteration 969 : loss : 0.086452, loss_ce: 0.032455
 14%|████▎                         | 57/400 [21:33<2:08:23, 22.46s/it]2022-01-06 14:26:34,443 iteration 970 : loss : 0.152221, loss_ce: 0.068381
2022-01-06 14:26:35,767 iteration 971 : loss : 0.134134, loss_ce: 0.045495
2022-01-06 14:26:36,930 iteration 972 : loss : 0.092356, loss_ce: 0.032413
2022-01-06 14:26:38,097 iteration 973 : loss : 0.105853, loss_ce: 0.041195
2022-01-06 14:26:39,321 iteration 974 : loss : 0.132240, loss_ce: 0.047800
2022-01-06 14:26:40,498 iteration 975 : loss : 0.172654, loss_ce: 0.059808
2022-01-06 14:26:41,696 iteration 976 : loss : 0.153189, loss_ce: 0.047644
2022-01-06 14:26:43,007 iteration 977 : loss : 0.127238, loss_ce: 0.047429
2022-01-06 14:26:44,286 iteration 978 : loss : 0.121158, loss_ce: 0.054964
2022-01-06 14:26:45,547 iteration 979 : loss : 0.110727, loss_ce: 0.046355
2022-01-06 14:26:46,687 iteration 980 : loss : 0.116919, loss_ce: 0.040329
2022-01-06 14:26:47,980 iteration 981 : loss : 0.091857, loss_ce: 0.036337
2022-01-06 14:26:49,123 iteration 982 : loss : 0.138497, loss_ce: 0.061403
2022-01-06 14:26:50,380 iteration 983 : loss : 0.147584, loss_ce: 0.061138
2022-01-06 14:26:51,608 iteration 984 : loss : 0.134553, loss_ce: 0.061083
2022-01-06 14:26:52,820 iteration 985 : loss : 0.124003, loss_ce: 0.042090
2022-01-06 14:26:54,024 iteration 986 : loss : 0.136022, loss_ce: 0.044771
 14%|████▎                         | 58/400 [21:54<2:05:15, 21.98s/it]2022-01-06 14:26:55,311 iteration 987 : loss : 0.113302, loss_ce: 0.044492
2022-01-06 14:26:56,617 iteration 988 : loss : 0.127181, loss_ce: 0.050761
2022-01-06 14:26:57,784 iteration 989 : loss : 0.154912, loss_ce: 0.051726
2022-01-06 14:26:59,072 iteration 990 : loss : 0.097018, loss_ce: 0.044842
2022-01-06 14:27:00,347 iteration 991 : loss : 0.119240, loss_ce: 0.058089
2022-01-06 14:27:01,509 iteration 992 : loss : 0.130645, loss_ce: 0.055786
2022-01-06 14:27:02,875 iteration 993 : loss : 0.135696, loss_ce: 0.048947
2022-01-06 14:27:04,150 iteration 994 : loss : 0.117648, loss_ce: 0.039870
2022-01-06 14:27:05,377 iteration 995 : loss : 0.130188, loss_ce: 0.044352
2022-01-06 14:27:06,553 iteration 996 : loss : 0.088736, loss_ce: 0.031670
2022-01-06 14:27:07,807 iteration 997 : loss : 0.138759, loss_ce: 0.060149
2022-01-06 14:27:09,032 iteration 998 : loss : 0.131412, loss_ce: 0.054294
2022-01-06 14:27:10,354 iteration 999 : loss : 0.085489, loss_ce: 0.035275
2022-01-06 14:27:11,606 iteration 1000 : loss : 0.139748, loss_ce: 0.060201
2022-01-06 14:27:12,866 iteration 1001 : loss : 0.143335, loss_ce: 0.058903
2022-01-06 14:27:14,022 iteration 1002 : loss : 0.136864, loss_ce: 0.045425
2022-01-06 14:27:15,210 iteration 1003 : loss : 0.117877, loss_ce: 0.047894
 15%|████▍                         | 59/400 [22:15<2:03:33, 21.74s/it]2022-01-06 14:27:16,477 iteration 1004 : loss : 0.112075, loss_ce: 0.045814
2022-01-06 14:27:17,801 iteration 1005 : loss : 0.179076, loss_ce: 0.062812
2022-01-06 14:27:19,029 iteration 1006 : loss : 0.139741, loss_ce: 0.059666
2022-01-06 14:27:20,279 iteration 1007 : loss : 0.118211, loss_ce: 0.041227
2022-01-06 14:27:21,517 iteration 1008 : loss : 0.198969, loss_ce: 0.065260
2022-01-06 14:27:22,760 iteration 1009 : loss : 0.122470, loss_ce: 0.044427
2022-01-06 14:27:23,940 iteration 1010 : loss : 0.131377, loss_ce: 0.046197
2022-01-06 14:27:25,149 iteration 1011 : loss : 0.124906, loss_ce: 0.055329
2022-01-06 14:27:26,463 iteration 1012 : loss : 0.194980, loss_ce: 0.062333
2022-01-06 14:27:27,642 iteration 1013 : loss : 0.146091, loss_ce: 0.051167
2022-01-06 14:27:28,905 iteration 1014 : loss : 0.203318, loss_ce: 0.082065
2022-01-06 14:27:30,101 iteration 1015 : loss : 0.105341, loss_ce: 0.048858
2022-01-06 14:27:31,221 iteration 1016 : loss : 0.098252, loss_ce: 0.048625
2022-01-06 14:27:32,566 iteration 1017 : loss : 0.113200, loss_ce: 0.039806
2022-01-06 14:27:33,851 iteration 1018 : loss : 0.111701, loss_ce: 0.049940
2022-01-06 14:27:35,140 iteration 1019 : loss : 0.144992, loss_ce: 0.057863
2022-01-06 14:27:35,141 Training Data Eval:
2022-01-06 14:27:41,103   Average segmentation loss on training set: 0.2676
2022-01-06 14:27:41,104 Validation Data Eval:
2022-01-06 14:27:43,151   Average segmentation loss on validation set: 0.2444
2022-01-06 14:27:44,461 iteration 1020 : loss : 0.116385, loss_ce: 0.040264
 15%|████▌                         | 60/400 [22:44<2:15:59, 24.00s/it]2022-01-06 14:27:45,868 iteration 1021 : loss : 0.123523, loss_ce: 0.045203
2022-01-06 14:27:47,104 iteration 1022 : loss : 0.095463, loss_ce: 0.034924
2022-01-06 14:27:48,282 iteration 1023 : loss : 0.151539, loss_ce: 0.053446
2022-01-06 14:27:49,553 iteration 1024 : loss : 0.090846, loss_ce: 0.041623
2022-01-06 14:27:50,749 iteration 1025 : loss : 0.135339, loss_ce: 0.064350
2022-01-06 14:27:52,007 iteration 1026 : loss : 0.144049, loss_ce: 0.047918
2022-01-06 14:27:53,216 iteration 1027 : loss : 0.144751, loss_ce: 0.066505
2022-01-06 14:27:54,385 iteration 1028 : loss : 0.106399, loss_ce: 0.038429
2022-01-06 14:27:55,636 iteration 1029 : loss : 0.176476, loss_ce: 0.050521
2022-01-06 14:27:56,888 iteration 1030 : loss : 0.100912, loss_ce: 0.035787
2022-01-06 14:27:58,201 iteration 1031 : loss : 0.136067, loss_ce: 0.059733
2022-01-06 14:27:59,359 iteration 1032 : loss : 0.111253, loss_ce: 0.040665
2022-01-06 14:28:00,655 iteration 1033 : loss : 0.099347, loss_ce: 0.038152
2022-01-06 14:28:01,952 iteration 1034 : loss : 0.093622, loss_ce: 0.040933
2022-01-06 14:28:03,211 iteration 1035 : loss : 0.189776, loss_ce: 0.080932
2022-01-06 14:28:04,544 iteration 1036 : loss : 0.144757, loss_ce: 0.051790
2022-01-06 14:28:05,623 iteration 1037 : loss : 0.108379, loss_ce: 0.045753
 15%|████▌                         | 61/400 [23:05<2:10:46, 23.15s/it]2022-01-06 14:28:06,834 iteration 1038 : loss : 0.099723, loss_ce: 0.044663
2022-01-06 14:28:08,055 iteration 1039 : loss : 0.174252, loss_ce: 0.064037
2022-01-06 14:28:09,191 iteration 1040 : loss : 0.138496, loss_ce: 0.049589
2022-01-06 14:28:10,508 iteration 1041 : loss : 0.114632, loss_ce: 0.041165
2022-01-06 14:28:11,816 iteration 1042 : loss : 0.150395, loss_ce: 0.051290
2022-01-06 14:28:12,994 iteration 1043 : loss : 0.094319, loss_ce: 0.034846
2022-01-06 14:28:14,214 iteration 1044 : loss : 0.095578, loss_ce: 0.035061
2022-01-06 14:28:15,415 iteration 1045 : loss : 0.130286, loss_ce: 0.068105
2022-01-06 14:28:16,655 iteration 1046 : loss : 0.141704, loss_ce: 0.045406
2022-01-06 14:28:17,988 iteration 1047 : loss : 0.125141, loss_ce: 0.050345
2022-01-06 14:28:19,229 iteration 1048 : loss : 0.155825, loss_ce: 0.063216
2022-01-06 14:28:20,391 iteration 1049 : loss : 0.145297, loss_ce: 0.060076
2022-01-06 14:28:21,589 iteration 1050 : loss : 0.109270, loss_ce: 0.042244
2022-01-06 14:28:22,839 iteration 1051 : loss : 0.116271, loss_ce: 0.047238
2022-01-06 14:28:24,071 iteration 1052 : loss : 0.126258, loss_ce: 0.054938
2022-01-06 14:28:25,436 iteration 1053 : loss : 0.109703, loss_ce: 0.044436
2022-01-06 14:28:26,625 iteration 1054 : loss : 0.089637, loss_ce: 0.035050
 16%|████▋                         | 62/400 [23:26<2:06:44, 22.50s/it]2022-01-06 14:28:27,887 iteration 1055 : loss : 0.098981, loss_ce: 0.034652
2022-01-06 14:28:29,102 iteration 1056 : loss : 0.187558, loss_ce: 0.084373
2022-01-06 14:28:30,363 iteration 1057 : loss : 0.127623, loss_ce: 0.037786
2022-01-06 14:28:31,513 iteration 1058 : loss : 0.107413, loss_ce: 0.042502
2022-01-06 14:28:32,843 iteration 1059 : loss : 0.111291, loss_ce: 0.044289
2022-01-06 14:28:34,046 iteration 1060 : loss : 0.106282, loss_ce: 0.041545
2022-01-06 14:28:35,322 iteration 1061 : loss : 0.072837, loss_ce: 0.030326
2022-01-06 14:28:36,546 iteration 1062 : loss : 0.127502, loss_ce: 0.052209
2022-01-06 14:28:37,866 iteration 1063 : loss : 0.127832, loss_ce: 0.045218
2022-01-06 14:28:39,062 iteration 1064 : loss : 0.127207, loss_ce: 0.052578
2022-01-06 14:28:40,209 iteration 1065 : loss : 0.143076, loss_ce: 0.052777
2022-01-06 14:28:41,475 iteration 1066 : loss : 0.108041, loss_ce: 0.047783
2022-01-06 14:28:42,884 iteration 1067 : loss : 0.149316, loss_ce: 0.058629
2022-01-06 14:28:44,175 iteration 1068 : loss : 0.160931, loss_ce: 0.062747
2022-01-06 14:28:45,334 iteration 1069 : loss : 0.153582, loss_ce: 0.063082
2022-01-06 14:28:46,689 iteration 1070 : loss : 0.118080, loss_ce: 0.036741
2022-01-06 14:28:47,919 iteration 1071 : loss : 0.109854, loss_ce: 0.040022
 16%|████▋                         | 63/400 [23:48<2:04:20, 22.14s/it]2022-01-06 14:28:49,187 iteration 1072 : loss : 0.100828, loss_ce: 0.041957
2022-01-06 14:28:50,536 iteration 1073 : loss : 0.116462, loss_ce: 0.040520
2022-01-06 14:28:51,746 iteration 1074 : loss : 0.090474, loss_ce: 0.037747
2022-01-06 14:28:52,999 iteration 1075 : loss : 0.100054, loss_ce: 0.034947
2022-01-06 14:28:54,195 iteration 1076 : loss : 0.123098, loss_ce: 0.043777
2022-01-06 14:28:55,498 iteration 1077 : loss : 0.259337, loss_ce: 0.088308
2022-01-06 14:28:56,667 iteration 1078 : loss : 0.109767, loss_ce: 0.041357
2022-01-06 14:28:57,912 iteration 1079 : loss : 0.092810, loss_ce: 0.040494
2022-01-06 14:28:59,182 iteration 1080 : loss : 0.123695, loss_ce: 0.051945
2022-01-06 14:29:00,451 iteration 1081 : loss : 0.099823, loss_ce: 0.040386
2022-01-06 14:29:01,689 iteration 1082 : loss : 0.114297, loss_ce: 0.052929
2022-01-06 14:29:02,919 iteration 1083 : loss : 0.155334, loss_ce: 0.054482
2022-01-06 14:29:04,208 iteration 1084 : loss : 0.161628, loss_ce: 0.044672
2022-01-06 14:29:05,413 iteration 1085 : loss : 0.134587, loss_ce: 0.054357
2022-01-06 14:29:06,676 iteration 1086 : loss : 0.142815, loss_ce: 0.046670
2022-01-06 14:29:07,837 iteration 1087 : loss : 0.094374, loss_ce: 0.037888
2022-01-06 14:29:09,048 iteration 1088 : loss : 0.123061, loss_ce: 0.064800
 16%|████▊                         | 64/400 [24:09<2:02:17, 21.84s/it]2022-01-06 14:29:10,218 iteration 1089 : loss : 0.113978, loss_ce: 0.049826
2022-01-06 14:29:11,444 iteration 1090 : loss : 0.096825, loss_ce: 0.034772
2022-01-06 14:29:12,728 iteration 1091 : loss : 0.129910, loss_ce: 0.063520
2022-01-06 14:29:13,961 iteration 1092 : loss : 0.133172, loss_ce: 0.044724
2022-01-06 14:29:15,200 iteration 1093 : loss : 0.157227, loss_ce: 0.049510
2022-01-06 14:29:16,558 iteration 1094 : loss : 0.177787, loss_ce: 0.052273
2022-01-06 14:29:17,696 iteration 1095 : loss : 0.132659, loss_ce: 0.041502
2022-01-06 14:29:18,866 iteration 1096 : loss : 0.086661, loss_ce: 0.036484
2022-01-06 14:29:20,013 iteration 1097 : loss : 0.108186, loss_ce: 0.039671
2022-01-06 14:29:21,319 iteration 1098 : loss : 0.131013, loss_ce: 0.052905
2022-01-06 14:29:22,529 iteration 1099 : loss : 0.104087, loss_ce: 0.035610
2022-01-06 14:29:23,808 iteration 1100 : loss : 0.137770, loss_ce: 0.057071
2022-01-06 14:29:25,084 iteration 1101 : loss : 0.115290, loss_ce: 0.045681
2022-01-06 14:29:26,291 iteration 1102 : loss : 0.143984, loss_ce: 0.049555
2022-01-06 14:29:27,530 iteration 1103 : loss : 0.081825, loss_ce: 0.034513
2022-01-06 14:29:28,732 iteration 1104 : loss : 0.140210, loss_ce: 0.054495
2022-01-06 14:29:28,732 Training Data Eval:
2022-01-06 14:29:34,776   Average segmentation loss on training set: 0.3007
2022-01-06 14:29:34,777 Validation Data Eval:
2022-01-06 14:29:36,838   Average segmentation loss on validation set: 0.3863
2022-01-06 14:29:38,076 iteration 1105 : loss : 0.107309, loss_ce: 0.045466
 16%|████▉                         | 65/400 [24:38<2:13:58, 24.00s/it]2022-01-06 14:29:39,424 iteration 1106 : loss : 0.118902, loss_ce: 0.048997
2022-01-06 14:29:40,649 iteration 1107 : loss : 0.108919, loss_ce: 0.046396
2022-01-06 14:29:41,841 iteration 1108 : loss : 0.130392, loss_ce: 0.041005
2022-01-06 14:29:43,155 iteration 1109 : loss : 0.106169, loss_ce: 0.041994
2022-01-06 14:29:44,370 iteration 1110 : loss : 0.093130, loss_ce: 0.034043
2022-01-06 14:29:45,710 iteration 1111 : loss : 0.177641, loss_ce: 0.061450
2022-01-06 14:29:46,870 iteration 1112 : loss : 0.135213, loss_ce: 0.057822
2022-01-06 14:29:48,106 iteration 1113 : loss : 0.127394, loss_ce: 0.040842
2022-01-06 14:29:49,349 iteration 1114 : loss : 0.110728, loss_ce: 0.038073
2022-01-06 14:29:50,589 iteration 1115 : loss : 0.092860, loss_ce: 0.033882
2022-01-06 14:29:51,714 iteration 1116 : loss : 0.108746, loss_ce: 0.046102
2022-01-06 14:29:52,882 iteration 1117 : loss : 0.120316, loss_ce: 0.045420
2022-01-06 14:29:54,141 iteration 1118 : loss : 0.100009, loss_ce: 0.040541
2022-01-06 14:29:55,400 iteration 1119 : loss : 0.120575, loss_ce: 0.051044
2022-01-06 14:29:56,622 iteration 1120 : loss : 0.116553, loss_ce: 0.033534
2022-01-06 14:29:57,807 iteration 1121 : loss : 0.119581, loss_ce: 0.047136
2022-01-06 14:29:58,944 iteration 1122 : loss : 0.109412, loss_ce: 0.035411
 16%|████▉                         | 66/400 [24:59<2:08:20, 23.06s/it]2022-01-06 14:30:00,353 iteration 1123 : loss : 0.068536, loss_ce: 0.024872
2022-01-06 14:30:01,577 iteration 1124 : loss : 0.066838, loss_ce: 0.032459
2022-01-06 14:30:02,765 iteration 1125 : loss : 0.138522, loss_ce: 0.038337
2022-01-06 14:30:03,961 iteration 1126 : loss : 0.100141, loss_ce: 0.035526
2022-01-06 14:30:05,155 iteration 1127 : loss : 0.094066, loss_ce: 0.034639
2022-01-06 14:30:06,476 iteration 1128 : loss : 0.114391, loss_ce: 0.044705
2022-01-06 14:30:07,834 iteration 1129 : loss : 0.124793, loss_ce: 0.051317
2022-01-06 14:30:09,001 iteration 1130 : loss : 0.109550, loss_ce: 0.047844
2022-01-06 14:30:10,305 iteration 1131 : loss : 0.134428, loss_ce: 0.057072
2022-01-06 14:30:11,658 iteration 1132 : loss : 0.104579, loss_ce: 0.042047
2022-01-06 14:30:12,982 iteration 1133 : loss : 0.130968, loss_ce: 0.051896
2022-01-06 14:30:14,308 iteration 1134 : loss : 0.138822, loss_ce: 0.041173
2022-01-06 14:30:15,502 iteration 1135 : loss : 0.155913, loss_ce: 0.057948
2022-01-06 14:30:16,721 iteration 1136 : loss : 0.083706, loss_ce: 0.035854
2022-01-06 14:30:17,940 iteration 1137 : loss : 0.112919, loss_ce: 0.048280
2022-01-06 14:30:19,217 iteration 1138 : loss : 0.146686, loss_ce: 0.060544
2022-01-06 14:30:20,397 iteration 1139 : loss : 0.142852, loss_ce: 0.055563
 17%|█████                         | 67/400 [25:20<2:05:18, 22.58s/it]2022-01-06 14:30:21,689 iteration 1140 : loss : 0.065987, loss_ce: 0.027104
2022-01-06 14:30:22,947 iteration 1141 : loss : 0.133130, loss_ce: 0.063531
2022-01-06 14:30:24,325 iteration 1142 : loss : 0.112731, loss_ce: 0.046993
2022-01-06 14:30:25,588 iteration 1143 : loss : 0.108259, loss_ce: 0.045322
2022-01-06 14:30:26,865 iteration 1144 : loss : 0.127698, loss_ce: 0.046028
2022-01-06 14:30:28,109 iteration 1145 : loss : 0.104910, loss_ce: 0.037542
2022-01-06 14:30:29,322 iteration 1146 : loss : 0.106291, loss_ce: 0.038944
2022-01-06 14:30:30,544 iteration 1147 : loss : 0.091737, loss_ce: 0.033889
2022-01-06 14:30:31,755 iteration 1148 : loss : 0.104433, loss_ce: 0.040888
2022-01-06 14:30:33,004 iteration 1149 : loss : 0.078593, loss_ce: 0.036335
2022-01-06 14:30:34,221 iteration 1150 : loss : 0.114106, loss_ce: 0.042023
2022-01-06 14:30:35,453 iteration 1151 : loss : 0.111858, loss_ce: 0.041478
2022-01-06 14:30:36,660 iteration 1152 : loss : 0.120843, loss_ce: 0.045112
2022-01-06 14:30:38,032 iteration 1153 : loss : 0.128476, loss_ce: 0.037079
2022-01-06 14:30:39,337 iteration 1154 : loss : 0.106906, loss_ce: 0.030857
2022-01-06 14:30:40,666 iteration 1155 : loss : 0.093283, loss_ce: 0.030845
2022-01-06 14:30:42,056 iteration 1156 : loss : 0.143983, loss_ce: 0.055792
 17%|█████                         | 68/400 [25:42<2:03:22, 22.30s/it]2022-01-06 14:30:43,393 iteration 1157 : loss : 0.111434, loss_ce: 0.040106
2022-01-06 14:30:44,626 iteration 1158 : loss : 0.087403, loss_ce: 0.029632
2022-01-06 14:30:45,764 iteration 1159 : loss : 0.106541, loss_ce: 0.036462
2022-01-06 14:30:46,976 iteration 1160 : loss : 0.101003, loss_ce: 0.048508
2022-01-06 14:30:48,250 iteration 1161 : loss : 0.115585, loss_ce: 0.045515
2022-01-06 14:30:49,509 iteration 1162 : loss : 0.108653, loss_ce: 0.048465
2022-01-06 14:30:50,750 iteration 1163 : loss : 0.079683, loss_ce: 0.039773
2022-01-06 14:30:52,068 iteration 1164 : loss : 0.133810, loss_ce: 0.059592
2022-01-06 14:30:53,216 iteration 1165 : loss : 0.116693, loss_ce: 0.052672
2022-01-06 14:30:54,406 iteration 1166 : loss : 0.144778, loss_ce: 0.049056
2022-01-06 14:30:55,664 iteration 1167 : loss : 0.121006, loss_ce: 0.053171
2022-01-06 14:30:56,982 iteration 1168 : loss : 0.168401, loss_ce: 0.064519
2022-01-06 14:30:58,138 iteration 1169 : loss : 0.127947, loss_ce: 0.045206
2022-01-06 14:30:59,398 iteration 1170 : loss : 0.130503, loss_ce: 0.042409
2022-01-06 14:31:00,773 iteration 1171 : loss : 0.183704, loss_ce: 0.052769
2022-01-06 14:31:01,918 iteration 1172 : loss : 0.098453, loss_ce: 0.036482
2022-01-06 14:31:03,061 iteration 1173 : loss : 0.111906, loss_ce: 0.037622
 17%|█████▏                        | 69/400 [26:03<2:00:53, 21.91s/it]2022-01-06 14:31:04,395 iteration 1174 : loss : 0.104347, loss_ce: 0.035304
2022-01-06 14:31:05,717 iteration 1175 : loss : 0.131996, loss_ce: 0.048190
2022-01-06 14:31:06,906 iteration 1176 : loss : 0.124659, loss_ce: 0.040173
2022-01-06 14:31:08,107 iteration 1177 : loss : 0.125122, loss_ce: 0.060080
2022-01-06 14:31:09,268 iteration 1178 : loss : 0.117808, loss_ce: 0.042029
2022-01-06 14:31:10,559 iteration 1179 : loss : 0.092983, loss_ce: 0.038804
2022-01-06 14:31:11,825 iteration 1180 : loss : 0.109988, loss_ce: 0.042864
2022-01-06 14:31:12,994 iteration 1181 : loss : 0.147440, loss_ce: 0.048120
2022-01-06 14:31:14,204 iteration 1182 : loss : 0.110527, loss_ce: 0.047300
2022-01-06 14:31:15,426 iteration 1183 : loss : 0.093992, loss_ce: 0.037494
2022-01-06 14:31:16,664 iteration 1184 : loss : 0.125528, loss_ce: 0.061019
2022-01-06 14:31:17,872 iteration 1185 : loss : 0.110983, loss_ce: 0.059303
2022-01-06 14:31:19,120 iteration 1186 : loss : 0.080567, loss_ce: 0.025580
2022-01-06 14:31:20,276 iteration 1187 : loss : 0.084007, loss_ce: 0.034797
2022-01-06 14:31:21,559 iteration 1188 : loss : 0.100814, loss_ce: 0.038181
2022-01-06 14:31:22,845 iteration 1189 : loss : 0.150656, loss_ce: 0.039042
2022-01-06 14:31:22,846 Training Data Eval:
2022-01-06 14:31:28,831   Average segmentation loss on training set: 5.2089
2022-01-06 14:31:28,832 Validation Data Eval:
2022-01-06 14:31:30,883   Average segmentation loss on validation set: 5.0901
2022-01-06 14:31:32,076 iteration 1190 : loss : 0.113394, loss_ce: 0.039225
 18%|█████▎                        | 70/400 [26:32<2:12:12, 24.04s/it]2022-01-06 14:31:33,383 iteration 1191 : loss : 0.081451, loss_ce: 0.031934
2022-01-06 14:31:34,597 iteration 1192 : loss : 0.141920, loss_ce: 0.074248
2022-01-06 14:31:35,742 iteration 1193 : loss : 0.104434, loss_ce: 0.041297
2022-01-06 14:31:36,985 iteration 1194 : loss : 0.167935, loss_ce: 0.051897
2022-01-06 14:31:38,206 iteration 1195 : loss : 0.130690, loss_ce: 0.051364
2022-01-06 14:31:39,448 iteration 1196 : loss : 0.102667, loss_ce: 0.040255
2022-01-06 14:31:40,729 iteration 1197 : loss : 0.163340, loss_ce: 0.052235
2022-01-06 14:31:41,885 iteration 1198 : loss : 0.097310, loss_ce: 0.035105
2022-01-06 14:31:43,109 iteration 1199 : loss : 0.115082, loss_ce: 0.056672
2022-01-06 14:31:44,254 iteration 1200 : loss : 0.107521, loss_ce: 0.041514
2022-01-06 14:31:45,453 iteration 1201 : loss : 0.149173, loss_ce: 0.040273
2022-01-06 14:31:46,762 iteration 1202 : loss : 0.080622, loss_ce: 0.031187
2022-01-06 14:31:48,053 iteration 1203 : loss : 0.117409, loss_ce: 0.049302
2022-01-06 14:31:49,190 iteration 1204 : loss : 0.129657, loss_ce: 0.035881
2022-01-06 14:31:50,458 iteration 1205 : loss : 0.115292, loss_ce: 0.048010
2022-01-06 14:31:51,634 iteration 1206 : loss : 0.100531, loss_ce: 0.033007
2022-01-06 14:31:52,930 iteration 1207 : loss : 0.086693, loss_ce: 0.035619
 18%|█████▎                        | 71/400 [26:53<2:06:35, 23.09s/it]2022-01-06 14:31:54,179 iteration 1208 : loss : 0.104144, loss_ce: 0.039559
2022-01-06 14:31:55,408 iteration 1209 : loss : 0.091522, loss_ce: 0.029632
2022-01-06 14:31:56,542 iteration 1210 : loss : 0.104894, loss_ce: 0.037772
2022-01-06 14:31:57,838 iteration 1211 : loss : 0.102164, loss_ce: 0.042989
2022-01-06 14:31:59,042 iteration 1212 : loss : 0.084688, loss_ce: 0.032237
2022-01-06 14:32:00,141 iteration 1213 : loss : 0.077145, loss_ce: 0.027592
2022-01-06 14:32:01,464 iteration 1214 : loss : 0.107655, loss_ce: 0.043220
2022-01-06 14:32:02,669 iteration 1215 : loss : 0.077858, loss_ce: 0.030729
2022-01-06 14:32:03,875 iteration 1216 : loss : 0.105599, loss_ce: 0.036033
2022-01-06 14:32:05,236 iteration 1217 : loss : 0.131552, loss_ce: 0.047686
2022-01-06 14:32:06,367 iteration 1218 : loss : 0.113181, loss_ce: 0.072570
2022-01-06 14:32:07,752 iteration 1219 : loss : 0.094026, loss_ce: 0.043727
2022-01-06 14:32:08,953 iteration 1220 : loss : 0.125512, loss_ce: 0.056423
2022-01-06 14:32:10,252 iteration 1221 : loss : 0.117053, loss_ce: 0.040667
2022-01-06 14:32:11,437 iteration 1222 : loss : 0.132542, loss_ce: 0.044247
2022-01-06 14:32:12,631 iteration 1223 : loss : 0.091983, loss_ce: 0.034128
2022-01-06 14:32:13,834 iteration 1224 : loss : 0.179470, loss_ce: 0.077100
 18%|█████▍                        | 72/400 [27:14<2:02:37, 22.43s/it]2022-01-06 14:32:15,110 iteration 1225 : loss : 0.114011, loss_ce: 0.033428
2022-01-06 14:32:16,362 iteration 1226 : loss : 0.063303, loss_ce: 0.021815
2022-01-06 14:32:17,534 iteration 1227 : loss : 0.085272, loss_ce: 0.031004
2022-01-06 14:32:18,710 iteration 1228 : loss : 0.103507, loss_ce: 0.034037
2022-01-06 14:32:20,064 iteration 1229 : loss : 0.114155, loss_ce: 0.052756
2022-01-06 14:32:21,325 iteration 1230 : loss : 0.070560, loss_ce: 0.028964
2022-01-06 14:32:22,556 iteration 1231 : loss : 0.079569, loss_ce: 0.027827
2022-01-06 14:32:23,718 iteration 1232 : loss : 0.106872, loss_ce: 0.042458
2022-01-06 14:32:25,018 iteration 1233 : loss : 0.079159, loss_ce: 0.024896
2022-01-06 14:32:26,366 iteration 1234 : loss : 0.097996, loss_ce: 0.033398
2022-01-06 14:32:27,489 iteration 1235 : loss : 0.121640, loss_ce: 0.040662
2022-01-06 14:32:28,761 iteration 1236 : loss : 0.102424, loss_ce: 0.048254
2022-01-06 14:32:29,969 iteration 1237 : loss : 0.137648, loss_ce: 0.045215
2022-01-06 14:32:31,113 iteration 1238 : loss : 0.100672, loss_ce: 0.035598
2022-01-06 14:32:32,316 iteration 1239 : loss : 0.108779, loss_ce: 0.048557
2022-01-06 14:32:33,641 iteration 1240 : loss : 0.118922, loss_ce: 0.047019
2022-01-06 14:32:34,861 iteration 1241 : loss : 0.082748, loss_ce: 0.032302
 18%|█████▍                        | 73/400 [27:35<1:59:57, 22.01s/it]2022-01-06 14:32:36,192 iteration 1242 : loss : 0.102669, loss_ce: 0.050412
2022-01-06 14:32:37,415 iteration 1243 : loss : 0.108425, loss_ce: 0.041859
2022-01-06 14:32:38,754 iteration 1244 : loss : 0.097011, loss_ce: 0.041958
2022-01-06 14:32:40,050 iteration 1245 : loss : 0.098573, loss_ce: 0.038397
2022-01-06 14:32:41,246 iteration 1246 : loss : 0.105986, loss_ce: 0.050260
2022-01-06 14:32:42,666 iteration 1247 : loss : 0.140401, loss_ce: 0.054884
2022-01-06 14:32:43,826 iteration 1248 : loss : 0.103057, loss_ce: 0.037659
2022-01-06 14:32:45,044 iteration 1249 : loss : 0.103893, loss_ce: 0.050010
2022-01-06 14:32:46,290 iteration 1250 : loss : 0.187777, loss_ce: 0.044922
2022-01-06 14:32:47,615 iteration 1251 : loss : 0.080394, loss_ce: 0.031218
2022-01-06 14:32:48,776 iteration 1252 : loss : 0.144461, loss_ce: 0.048573
2022-01-06 14:32:49,993 iteration 1253 : loss : 0.099265, loss_ce: 0.037198
2022-01-06 14:32:51,222 iteration 1254 : loss : 0.086745, loss_ce: 0.031318
2022-01-06 14:32:52,590 iteration 1255 : loss : 0.106651, loss_ce: 0.038043
2022-01-06 14:32:53,807 iteration 1256 : loss : 0.088871, loss_ce: 0.030154
2022-01-06 14:32:54,931 iteration 1257 : loss : 0.074322, loss_ce: 0.025235
2022-01-06 14:32:56,076 iteration 1258 : loss : 0.091890, loss_ce: 0.043918
 18%|█████▌                        | 74/400 [27:56<1:58:17, 21.77s/it]2022-01-06 14:32:57,341 iteration 1259 : loss : 0.091883, loss_ce: 0.031966
2022-01-06 14:32:58,701 iteration 1260 : loss : 0.064192, loss_ce: 0.023183
2022-01-06 14:32:59,901 iteration 1261 : loss : 0.090337, loss_ce: 0.024982
2022-01-06 14:33:01,050 iteration 1262 : loss : 0.078452, loss_ce: 0.025941
2022-01-06 14:33:02,268 iteration 1263 : loss : 0.090281, loss_ce: 0.033173
2022-01-06 14:33:03,513 iteration 1264 : loss : 0.103389, loss_ce: 0.047866
2022-01-06 14:33:04,742 iteration 1265 : loss : 0.079909, loss_ce: 0.026984
2022-01-06 14:33:05,924 iteration 1266 : loss : 0.087354, loss_ce: 0.034524
2022-01-06 14:33:07,099 iteration 1267 : loss : 0.123421, loss_ce: 0.051469
2022-01-06 14:33:08,270 iteration 1268 : loss : 0.083056, loss_ce: 0.029683
2022-01-06 14:33:09,459 iteration 1269 : loss : 0.110970, loss_ce: 0.055203
2022-01-06 14:33:10,677 iteration 1270 : loss : 0.077767, loss_ce: 0.032196
2022-01-06 14:33:11,972 iteration 1271 : loss : 0.103833, loss_ce: 0.045398
2022-01-06 14:33:13,057 iteration 1272 : loss : 0.073497, loss_ce: 0.027270
2022-01-06 14:33:14,217 iteration 1273 : loss : 0.110780, loss_ce: 0.039026
2022-01-06 14:33:15,461 iteration 1274 : loss : 0.102828, loss_ce: 0.043320
2022-01-06 14:33:15,462 Training Data Eval:
2022-01-06 14:33:21,538   Average segmentation loss on training set: 0.1870
2022-01-06 14:33:21,538 Validation Data Eval:
2022-01-06 14:33:23,615   Average segmentation loss on validation set: 0.1817
2022-01-06 14:33:29,881 Found new lowest validation loss at iteration 1274! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_MLP_best_val_loss_seed100.pth
2022-01-06 14:33:31,026 iteration 1275 : loss : 0.090852, loss_ce: 0.034957
 19%|█████▋                        | 75/400 [28:31<2:19:21, 25.73s/it]2022-01-06 14:33:32,224 iteration 1276 : loss : 0.083241, loss_ce: 0.034672
2022-01-06 14:33:33,534 iteration 1277 : loss : 0.122565, loss_ce: 0.046688
2022-01-06 14:33:34,706 iteration 1278 : loss : 0.125863, loss_ce: 0.051731
2022-01-06 14:33:35,858 iteration 1279 : loss : 0.096643, loss_ce: 0.040578
2022-01-06 14:33:37,057 iteration 1280 : loss : 0.089882, loss_ce: 0.041967
2022-01-06 14:33:38,170 iteration 1281 : loss : 0.082796, loss_ce: 0.030489
2022-01-06 14:33:39,321 iteration 1282 : loss : 0.090303, loss_ce: 0.035301
2022-01-06 14:33:40,384 iteration 1283 : loss : 0.175293, loss_ce: 0.055491
2022-01-06 14:33:41,564 iteration 1284 : loss : 0.110309, loss_ce: 0.040072
2022-01-06 14:33:42,722 iteration 1285 : loss : 0.083671, loss_ce: 0.027159
2022-01-06 14:33:44,017 iteration 1286 : loss : 0.120368, loss_ce: 0.040473
2022-01-06 14:33:45,304 iteration 1287 : loss : 0.060390, loss_ce: 0.023639
2022-01-06 14:33:46,480 iteration 1288 : loss : 0.101281, loss_ce: 0.037853
2022-01-06 14:33:47,706 iteration 1289 : loss : 0.143827, loss_ce: 0.051035
2022-01-06 14:33:48,886 iteration 1290 : loss : 0.142496, loss_ce: 0.050824
2022-01-06 14:33:50,060 iteration 1291 : loss : 0.104121, loss_ce: 0.035897
2022-01-06 14:33:51,223 iteration 1292 : loss : 0.134885, loss_ce: 0.076331
 19%|█████▋                        | 76/400 [28:51<2:09:57, 24.07s/it]2022-01-06 14:33:52,618 iteration 1293 : loss : 0.122407, loss_ce: 0.058991
2022-01-06 14:33:53,750 iteration 1294 : loss : 0.111177, loss_ce: 0.039302
2022-01-06 14:33:55,031 iteration 1295 : loss : 0.139199, loss_ce: 0.049089
2022-01-06 14:33:56,279 iteration 1296 : loss : 0.115267, loss_ce: 0.040304
2022-01-06 14:33:57,374 iteration 1297 : loss : 0.068189, loss_ce: 0.027672
2022-01-06 14:33:58,688 iteration 1298 : loss : 0.096286, loss_ce: 0.038315
2022-01-06 14:33:59,873 iteration 1299 : loss : 0.109995, loss_ce: 0.048627
2022-01-06 14:34:01,184 iteration 1300 : loss : 0.151731, loss_ce: 0.062404
2022-01-06 14:34:02,492 iteration 1301 : loss : 0.153333, loss_ce: 0.045499
2022-01-06 14:34:03,632 iteration 1302 : loss : 0.121763, loss_ce: 0.039016
2022-01-06 14:34:04,910 iteration 1303 : loss : 0.093594, loss_ce: 0.029662
2022-01-06 14:34:06,151 iteration 1304 : loss : 0.100643, loss_ce: 0.043701
2022-01-06 14:34:07,484 iteration 1305 : loss : 0.084707, loss_ce: 0.030475
2022-01-06 14:34:08,708 iteration 1306 : loss : 0.089660, loss_ce: 0.036537
2022-01-06 14:34:09,868 iteration 1307 : loss : 0.114333, loss_ce: 0.046341
2022-01-06 14:34:11,084 iteration 1308 : loss : 0.118672, loss_ce: 0.047185
2022-01-06 14:34:12,264 iteration 1309 : loss : 0.080703, loss_ce: 0.032975
 19%|█████▊                        | 77/400 [29:12<2:04:40, 23.16s/it]2022-01-06 14:34:13,529 iteration 1310 : loss : 0.096445, loss_ce: 0.037093
2022-01-06 14:34:14,889 iteration 1311 : loss : 0.118872, loss_ce: 0.044799
2022-01-06 14:34:16,118 iteration 1312 : loss : 0.094745, loss_ce: 0.033026
2022-01-06 14:34:17,339 iteration 1313 : loss : 0.100393, loss_ce: 0.040135
2022-01-06 14:34:18,643 iteration 1314 : loss : 0.135755, loss_ce: 0.062151
2022-01-06 14:34:19,963 iteration 1315 : loss : 0.118228, loss_ce: 0.056264
2022-01-06 14:34:21,146 iteration 1316 : loss : 0.085601, loss_ce: 0.034980
2022-01-06 14:34:22,302 iteration 1317 : loss : 0.122822, loss_ce: 0.037970
2022-01-06 14:34:23,477 iteration 1318 : loss : 0.096630, loss_ce: 0.034378
2022-01-06 14:34:24,750 iteration 1319 : loss : 0.114799, loss_ce: 0.042964
2022-01-06 14:34:25,914 iteration 1320 : loss : 0.088703, loss_ce: 0.030890
2022-01-06 14:34:27,146 iteration 1321 : loss : 0.126534, loss_ce: 0.057268
2022-01-06 14:34:28,344 iteration 1322 : loss : 0.081093, loss_ce: 0.029466
2022-01-06 14:34:29,657 iteration 1323 : loss : 0.097552, loss_ce: 0.030854
2022-01-06 14:34:30,988 iteration 1324 : loss : 0.065507, loss_ce: 0.025607
2022-01-06 14:34:32,193 iteration 1325 : loss : 0.086301, loss_ce: 0.032789
2022-01-06 14:34:33,424 iteration 1326 : loss : 0.112443, loss_ce: 0.042661
 20%|█████▊                        | 78/400 [29:33<2:01:03, 22.56s/it]2022-01-06 14:34:34,682 iteration 1327 : loss : 0.079763, loss_ce: 0.029825
2022-01-06 14:34:35,910 iteration 1328 : loss : 0.066373, loss_ce: 0.027776
2022-01-06 14:34:37,126 iteration 1329 : loss : 0.093680, loss_ce: 0.037168
2022-01-06 14:34:38,444 iteration 1330 : loss : 0.115533, loss_ce: 0.038034
2022-01-06 14:34:39,668 iteration 1331 : loss : 0.098428, loss_ce: 0.039378
2022-01-06 14:34:40,862 iteration 1332 : loss : 0.104084, loss_ce: 0.037672
2022-01-06 14:34:42,058 iteration 1333 : loss : 0.093577, loss_ce: 0.035898
2022-01-06 14:34:43,346 iteration 1334 : loss : 0.086198, loss_ce: 0.035611
2022-01-06 14:34:44,617 iteration 1335 : loss : 0.071606, loss_ce: 0.026038
2022-01-06 14:34:45,802 iteration 1336 : loss : 0.075005, loss_ce: 0.035195
2022-01-06 14:34:47,108 iteration 1337 : loss : 0.112198, loss_ce: 0.044333
2022-01-06 14:34:48,315 iteration 1338 : loss : 0.076059, loss_ce: 0.030028
2022-01-06 14:34:49,513 iteration 1339 : loss : 0.119210, loss_ce: 0.044535
2022-01-06 14:34:50,702 iteration 1340 : loss : 0.081736, loss_ce: 0.032759
2022-01-06 14:34:51,971 iteration 1341 : loss : 0.136134, loss_ce: 0.046265
2022-01-06 14:34:53,286 iteration 1342 : loss : 0.100086, loss_ce: 0.038003
2022-01-06 14:34:54,400 iteration 1343 : loss : 0.099938, loss_ce: 0.035099
 20%|█████▉                        | 79/400 [29:54<1:58:08, 22.08s/it]2022-01-06 14:34:55,731 iteration 1344 : loss : 0.076363, loss_ce: 0.028390
2022-01-06 14:34:56,903 iteration 1345 : loss : 0.094241, loss_ce: 0.021970
2022-01-06 14:34:58,184 iteration 1346 : loss : 0.096956, loss_ce: 0.029884
2022-01-06 14:34:59,416 iteration 1347 : loss : 0.105505, loss_ce: 0.046886
2022-01-06 14:35:00,588 iteration 1348 : loss : 0.114134, loss_ce: 0.050738
2022-01-06 14:35:01,811 iteration 1349 : loss : 0.164971, loss_ce: 0.065639
2022-01-06 14:35:03,009 iteration 1350 : loss : 0.127777, loss_ce: 0.039225
2022-01-06 14:35:04,331 iteration 1351 : loss : 0.173947, loss_ce: 0.073222
2022-01-06 14:35:05,480 iteration 1352 : loss : 0.124457, loss_ce: 0.050441
2022-01-06 14:35:06,698 iteration 1353 : loss : 0.118602, loss_ce: 0.055074
2022-01-06 14:35:07,919 iteration 1354 : loss : 0.123541, loss_ce: 0.047292
2022-01-06 14:35:09,040 iteration 1355 : loss : 0.092479, loss_ce: 0.031528
2022-01-06 14:35:10,349 iteration 1356 : loss : 0.106669, loss_ce: 0.037535
2022-01-06 14:35:11,490 iteration 1357 : loss : 0.088832, loss_ce: 0.038340
2022-01-06 14:35:12,680 iteration 1358 : loss : 0.114594, loss_ce: 0.049844
2022-01-06 14:35:14,010 iteration 1359 : loss : 0.092169, loss_ce: 0.048316
2022-01-06 14:35:14,010 Training Data Eval:
2022-01-06 14:35:20,000   Average segmentation loss on training set: 0.3775
2022-01-06 14:35:20,001 Validation Data Eval:
2022-01-06 14:35:22,042   Average segmentation loss on validation set: 0.3291
2022-01-06 14:35:23,278 iteration 1360 : loss : 0.100999, loss_ce: 0.039802
 20%|██████                        | 80/400 [30:23<2:08:38, 24.12s/it]2022-01-06 14:35:24,627 iteration 1361 : loss : 0.122118, loss_ce: 0.037595
2022-01-06 14:35:25,816 iteration 1362 : loss : 0.146090, loss_ce: 0.048812
2022-01-06 14:35:27,051 iteration 1363 : loss : 0.095002, loss_ce: 0.034600
2022-01-06 14:35:28,185 iteration 1364 : loss : 0.121330, loss_ce: 0.058612
2022-01-06 14:35:29,361 iteration 1365 : loss : 0.150188, loss_ce: 0.045259
2022-01-06 14:35:30,679 iteration 1366 : loss : 0.116310, loss_ce: 0.043315
2022-01-06 14:35:31,913 iteration 1367 : loss : 0.118970, loss_ce: 0.066522
2022-01-06 14:35:33,348 iteration 1368 : loss : 0.075630, loss_ce: 0.029683
2022-01-06 14:35:34,545 iteration 1369 : loss : 0.107870, loss_ce: 0.044238
2022-01-06 14:35:35,702 iteration 1370 : loss : 0.103472, loss_ce: 0.042240
2022-01-06 14:35:36,903 iteration 1371 : loss : 0.080817, loss_ce: 0.031398
2022-01-06 14:35:38,303 iteration 1372 : loss : 0.104981, loss_ce: 0.038926
2022-01-06 14:35:39,501 iteration 1373 : loss : 0.092363, loss_ce: 0.038870
2022-01-06 14:35:40,672 iteration 1374 : loss : 0.090983, loss_ce: 0.034844
2022-01-06 14:35:41,861 iteration 1375 : loss : 0.089208, loss_ce: 0.039401
2022-01-06 14:35:43,075 iteration 1376 : loss : 0.081003, loss_ce: 0.028650
2022-01-06 14:35:44,373 iteration 1377 : loss : 0.089754, loss_ce: 0.046263
 20%|██████                        | 81/400 [30:44<2:03:25, 23.22s/it]2022-01-06 14:35:45,778 iteration 1378 : loss : 0.110411, loss_ce: 0.056391
2022-01-06 14:35:47,044 iteration 1379 : loss : 0.113917, loss_ce: 0.051008
2022-01-06 14:35:48,260 iteration 1380 : loss : 0.128099, loss_ce: 0.045933
2022-01-06 14:35:49,540 iteration 1381 : loss : 0.113487, loss_ce: 0.051454
2022-01-06 14:35:50,794 iteration 1382 : loss : 0.117567, loss_ce: 0.046286
2022-01-06 14:35:51,957 iteration 1383 : loss : 0.115192, loss_ce: 0.034262
2022-01-06 14:35:53,230 iteration 1384 : loss : 0.084428, loss_ce: 0.035762
2022-01-06 14:35:54,491 iteration 1385 : loss : 0.073216, loss_ce: 0.028093
2022-01-06 14:35:55,704 iteration 1386 : loss : 0.097637, loss_ce: 0.042292
2022-01-06 14:35:57,062 iteration 1387 : loss : 0.105295, loss_ce: 0.039182
2022-01-06 14:35:58,257 iteration 1388 : loss : 0.110860, loss_ce: 0.042211
2022-01-06 14:35:59,458 iteration 1389 : loss : 0.086879, loss_ce: 0.033538
2022-01-06 14:36:00,710 iteration 1390 : loss : 0.084045, loss_ce: 0.030106
2022-01-06 14:36:01,943 iteration 1391 : loss : 0.114901, loss_ce: 0.032919
2022-01-06 14:36:03,181 iteration 1392 : loss : 0.087187, loss_ce: 0.028114
2022-01-06 14:36:04,373 iteration 1393 : loss : 0.099923, loss_ce: 0.036000
2022-01-06 14:36:05,574 iteration 1394 : loss : 0.071656, loss_ce: 0.030093
 20%|██████▏                       | 82/400 [31:05<1:59:50, 22.61s/it]2022-01-06 14:36:06,861 iteration 1395 : loss : 0.084840, loss_ce: 0.034308
2022-01-06 14:36:08,042 iteration 1396 : loss : 0.080350, loss_ce: 0.032720
2022-01-06 14:36:09,294 iteration 1397 : loss : 0.118450, loss_ce: 0.039577
2022-01-06 14:36:10,635 iteration 1398 : loss : 0.067175, loss_ce: 0.026424
2022-01-06 14:36:11,904 iteration 1399 : loss : 0.129460, loss_ce: 0.057406
2022-01-06 14:36:13,107 iteration 1400 : loss : 0.114858, loss_ce: 0.058858
2022-01-06 14:36:14,310 iteration 1401 : loss : 0.102117, loss_ce: 0.030623
2022-01-06 14:36:15,461 iteration 1402 : loss : 0.086381, loss_ce: 0.036432
2022-01-06 14:36:16,820 iteration 1403 : loss : 0.115922, loss_ce: 0.044935
2022-01-06 14:36:18,099 iteration 1404 : loss : 0.086590, loss_ce: 0.029339
2022-01-06 14:36:19,352 iteration 1405 : loss : 0.089296, loss_ce: 0.040408
2022-01-06 14:36:20,582 iteration 1406 : loss : 0.082679, loss_ce: 0.030740
2022-01-06 14:36:21,733 iteration 1407 : loss : 0.099688, loss_ce: 0.046451
2022-01-06 14:36:22,919 iteration 1408 : loss : 0.118542, loss_ce: 0.049119
2022-01-06 14:36:24,244 iteration 1409 : loss : 0.147546, loss_ce: 0.046110
2022-01-06 14:36:25,495 iteration 1410 : loss : 0.105486, loss_ce: 0.036758
2022-01-06 14:36:26,686 iteration 1411 : loss : 0.083997, loss_ce: 0.034553
 21%|██████▏                       | 83/400 [31:26<1:57:03, 22.16s/it]2022-01-06 14:36:27,856 iteration 1412 : loss : 0.101193, loss_ce: 0.035963
2022-01-06 14:36:29,144 iteration 1413 : loss : 0.060845, loss_ce: 0.019977
2022-01-06 14:36:30,370 iteration 1414 : loss : 0.086350, loss_ce: 0.036108
2022-01-06 14:36:31,519 iteration 1415 : loss : 0.077617, loss_ce: 0.027261
2022-01-06 14:36:32,762 iteration 1416 : loss : 0.075862, loss_ce: 0.024082
2022-01-06 14:36:34,042 iteration 1417 : loss : 0.079550, loss_ce: 0.026882
2022-01-06 14:36:35,170 iteration 1418 : loss : 0.095607, loss_ce: 0.030259
2022-01-06 14:36:36,526 iteration 1419 : loss : 0.084903, loss_ce: 0.026997
2022-01-06 14:36:37,793 iteration 1420 : loss : 0.102902, loss_ce: 0.026944
2022-01-06 14:36:38,947 iteration 1421 : loss : 0.098554, loss_ce: 0.043626
2022-01-06 14:36:40,148 iteration 1422 : loss : 0.085301, loss_ce: 0.028220
2022-01-06 14:36:41,325 iteration 1423 : loss : 0.110792, loss_ce: 0.045924
2022-01-06 14:36:42,609 iteration 1424 : loss : 0.103113, loss_ce: 0.033758
2022-01-06 14:36:43,934 iteration 1425 : loss : 0.117809, loss_ce: 0.048362
2022-01-06 14:36:44,996 iteration 1426 : loss : 0.076139, loss_ce: 0.030250
2022-01-06 14:36:46,188 iteration 1427 : loss : 0.075393, loss_ce: 0.032943
2022-01-06 14:36:47,417 iteration 1428 : loss : 0.102765, loss_ce: 0.036537
 21%|██████▎                       | 84/400 [31:47<1:54:27, 21.73s/it]2022-01-06 14:36:48,827 iteration 1429 : loss : 0.112314, loss_ce: 0.045061
2022-01-06 14:36:49,981 iteration 1430 : loss : 0.080551, loss_ce: 0.027577
2022-01-06 14:36:51,190 iteration 1431 : loss : 0.093482, loss_ce: 0.041140
2022-01-06 14:36:52,438 iteration 1432 : loss : 0.092544, loss_ce: 0.036804
2022-01-06 14:36:53,783 iteration 1433 : loss : 0.109109, loss_ce: 0.051210
2022-01-06 14:36:55,065 iteration 1434 : loss : 0.074635, loss_ce: 0.028333
2022-01-06 14:36:56,159 iteration 1435 : loss : 0.086809, loss_ce: 0.029023
2022-01-06 14:36:57,467 iteration 1436 : loss : 0.084722, loss_ce: 0.035340
2022-01-06 14:36:58,597 iteration 1437 : loss : 0.140631, loss_ce: 0.034242
2022-01-06 14:36:59,715 iteration 1438 : loss : 0.077143, loss_ce: 0.038132
2022-01-06 14:37:00,951 iteration 1439 : loss : 0.070113, loss_ce: 0.022391
2022-01-06 14:37:02,271 iteration 1440 : loss : 0.089951, loss_ce: 0.038179
2022-01-06 14:37:03,460 iteration 1441 : loss : 0.071582, loss_ce: 0.029177
2022-01-06 14:37:04,622 iteration 1442 : loss : 0.084226, loss_ce: 0.032331
2022-01-06 14:37:05,880 iteration 1443 : loss : 0.087029, loss_ce: 0.031998
2022-01-06 14:37:07,161 iteration 1444 : loss : 0.073199, loss_ce: 0.031438
2022-01-06 14:37:07,161 Training Data Eval:
2022-01-06 14:37:13,170   Average segmentation loss on training set: 0.4359
2022-01-06 14:37:13,171 Validation Data Eval:
2022-01-06 14:37:15,222   Average segmentation loss on validation set: 0.5461
2022-01-06 14:37:16,493 iteration 1445 : loss : 0.082108, loss_ce: 0.025745
 21%|██████▍                       | 85/400 [32:16<2:05:39, 23.94s/it]2022-01-06 14:37:17,847 iteration 1446 : loss : 0.082521, loss_ce: 0.029645
2022-01-06 14:37:19,115 iteration 1447 : loss : 0.114940, loss_ce: 0.043285
2022-01-06 14:37:20,306 iteration 1448 : loss : 0.096135, loss_ce: 0.046388
2022-01-06 14:37:21,539 iteration 1449 : loss : 0.099878, loss_ce: 0.038491
2022-01-06 14:37:22,652 iteration 1450 : loss : 0.073497, loss_ce: 0.030065
2022-01-06 14:37:23,813 iteration 1451 : loss : 0.078953, loss_ce: 0.026225
2022-01-06 14:37:25,057 iteration 1452 : loss : 0.145845, loss_ce: 0.033537
2022-01-06 14:37:26,363 iteration 1453 : loss : 0.091875, loss_ce: 0.030781
2022-01-06 14:37:27,525 iteration 1454 : loss : 0.079771, loss_ce: 0.034868
2022-01-06 14:37:28,702 iteration 1455 : loss : 0.074563, loss_ce: 0.031020
2022-01-06 14:37:29,934 iteration 1456 : loss : 0.070126, loss_ce: 0.025035
2022-01-06 14:37:31,177 iteration 1457 : loss : 0.067119, loss_ce: 0.028533
2022-01-06 14:37:32,400 iteration 1458 : loss : 0.068915, loss_ce: 0.023308
2022-01-06 14:37:33,516 iteration 1459 : loss : 0.068392, loss_ce: 0.027939
2022-01-06 14:37:34,807 iteration 1460 : loss : 0.097675, loss_ce: 0.046924
2022-01-06 14:37:36,029 iteration 1461 : loss : 0.107628, loss_ce: 0.032302
2022-01-06 14:37:37,264 iteration 1462 : loss : 0.086920, loss_ce: 0.033251
 22%|██████▍                       | 86/400 [32:37<2:00:17, 22.99s/it]2022-01-06 14:37:38,556 iteration 1463 : loss : 0.077099, loss_ce: 0.034840
2022-01-06 14:37:39,837 iteration 1464 : loss : 0.097588, loss_ce: 0.031520
2022-01-06 14:37:41,144 iteration 1465 : loss : 0.109949, loss_ce: 0.050043
2022-01-06 14:37:42,365 iteration 1466 : loss : 0.098895, loss_ce: 0.030394
2022-01-06 14:37:43,572 iteration 1467 : loss : 0.083746, loss_ce: 0.030124
2022-01-06 14:37:44,827 iteration 1468 : loss : 0.083174, loss_ce: 0.028509
2022-01-06 14:37:46,091 iteration 1469 : loss : 0.077846, loss_ce: 0.032726
2022-01-06 14:37:47,332 iteration 1470 : loss : 0.060775, loss_ce: 0.021881
2022-01-06 14:37:48,485 iteration 1471 : loss : 0.093513, loss_ce: 0.026345
2022-01-06 14:37:49,700 iteration 1472 : loss : 0.073132, loss_ce: 0.026065
2022-01-06 14:37:50,995 iteration 1473 : loss : 0.078408, loss_ce: 0.038771
2022-01-06 14:37:52,156 iteration 1474 : loss : 0.113174, loss_ce: 0.044972
2022-01-06 14:37:53,373 iteration 1475 : loss : 0.078949, loss_ce: 0.025865
2022-01-06 14:37:54,657 iteration 1476 : loss : 0.122813, loss_ce: 0.066377
2022-01-06 14:37:55,865 iteration 1477 : loss : 0.079514, loss_ce: 0.028749
2022-01-06 14:37:57,152 iteration 1478 : loss : 0.079810, loss_ce: 0.028318
2022-01-06 14:37:58,405 iteration 1479 : loss : 0.101820, loss_ce: 0.039200
 22%|██████▌                       | 87/400 [32:58<1:57:01, 22.43s/it]2022-01-06 14:37:59,695 iteration 1480 : loss : 0.089986, loss_ce: 0.046061
2022-01-06 14:38:01,002 iteration 1481 : loss : 0.079879, loss_ce: 0.032183
2022-01-06 14:38:02,196 iteration 1482 : loss : 0.062045, loss_ce: 0.027603
2022-01-06 14:38:03,327 iteration 1483 : loss : 0.070885, loss_ce: 0.030243
2022-01-06 14:38:04,482 iteration 1484 : loss : 0.094561, loss_ce: 0.039615
2022-01-06 14:38:05,666 iteration 1485 : loss : 0.083476, loss_ce: 0.032092
2022-01-06 14:38:06,872 iteration 1486 : loss : 0.112052, loss_ce: 0.038903
2022-01-06 14:38:08,179 iteration 1487 : loss : 0.117752, loss_ce: 0.051564
2022-01-06 14:38:09,367 iteration 1488 : loss : 0.086497, loss_ce: 0.033047
2022-01-06 14:38:10,611 iteration 1489 : loss : 0.089084, loss_ce: 0.034553
2022-01-06 14:38:11,823 iteration 1490 : loss : 0.075812, loss_ce: 0.029904
2022-01-06 14:38:12,974 iteration 1491 : loss : 0.110464, loss_ce: 0.049250
2022-01-06 14:38:14,240 iteration 1492 : loss : 0.081579, loss_ce: 0.035486
2022-01-06 14:38:15,602 iteration 1493 : loss : 0.102920, loss_ce: 0.036067
2022-01-06 14:38:16,863 iteration 1494 : loss : 0.107013, loss_ce: 0.033856
2022-01-06 14:38:18,091 iteration 1495 : loss : 0.125639, loss_ce: 0.042645
2022-01-06 14:38:19,324 iteration 1496 : loss : 0.084968, loss_ce: 0.035889
 22%|██████▌                       | 88/400 [33:19<1:54:17, 21.98s/it]2022-01-06 14:38:20,581 iteration 1497 : loss : 0.082461, loss_ce: 0.030768
2022-01-06 14:38:21,692 iteration 1498 : loss : 0.062231, loss_ce: 0.023911
2022-01-06 14:38:22,875 iteration 1499 : loss : 0.099038, loss_ce: 0.042937
2022-01-06 14:38:24,295 iteration 1500 : loss : 0.122154, loss_ce: 0.039708
2022-01-06 14:38:25,454 iteration 1501 : loss : 0.085410, loss_ce: 0.029491
2022-01-06 14:38:26,722 iteration 1502 : loss : 0.110739, loss_ce: 0.039412
2022-01-06 14:38:28,067 iteration 1503 : loss : 0.086204, loss_ce: 0.028169
2022-01-06 14:38:29,284 iteration 1504 : loss : 0.091248, loss_ce: 0.035947
2022-01-06 14:38:30,490 iteration 1505 : loss : 0.106721, loss_ce: 0.043861
2022-01-06 14:38:31,802 iteration 1506 : loss : 0.080485, loss_ce: 0.029525
2022-01-06 14:38:33,011 iteration 1507 : loss : 0.076527, loss_ce: 0.033103
2022-01-06 14:38:34,268 iteration 1508 : loss : 0.168378, loss_ce: 0.046211
2022-01-06 14:38:35,495 iteration 1509 : loss : 0.106716, loss_ce: 0.040535
2022-01-06 14:38:36,709 iteration 1510 : loss : 0.103305, loss_ce: 0.033476
2022-01-06 14:38:38,069 iteration 1511 : loss : 0.092953, loss_ce: 0.049147
2022-01-06 14:38:39,300 iteration 1512 : loss : 0.096137, loss_ce: 0.040800
2022-01-06 14:38:40,539 iteration 1513 : loss : 0.077599, loss_ce: 0.030268
 22%|██████▋                       | 89/400 [33:40<1:52:43, 21.75s/it]2022-01-06 14:38:41,869 iteration 1514 : loss : 0.099292, loss_ce: 0.043688
2022-01-06 14:38:43,111 iteration 1515 : loss : 0.073288, loss_ce: 0.030593
2022-01-06 14:38:44,383 iteration 1516 : loss : 0.097200, loss_ce: 0.044010
2022-01-06 14:38:45,568 iteration 1517 : loss : 0.085816, loss_ce: 0.040476
2022-01-06 14:38:46,715 iteration 1518 : loss : 0.076088, loss_ce: 0.031724
2022-01-06 14:38:47,936 iteration 1519 : loss : 0.092393, loss_ce: 0.038199
2022-01-06 14:38:49,352 iteration 1520 : loss : 0.093646, loss_ce: 0.036564
2022-01-06 14:38:50,527 iteration 1521 : loss : 0.081346, loss_ce: 0.028134
2022-01-06 14:38:51,815 iteration 1522 : loss : 0.070651, loss_ce: 0.022627
2022-01-06 14:38:53,039 iteration 1523 : loss : 0.096529, loss_ce: 0.037470
2022-01-06 14:38:54,197 iteration 1524 : loss : 0.078746, loss_ce: 0.024875
2022-01-06 14:38:55,393 iteration 1525 : loss : 0.069365, loss_ce: 0.029912
2022-01-06 14:38:56,590 iteration 1526 : loss : 0.136679, loss_ce: 0.052467
2022-01-06 14:38:57,903 iteration 1527 : loss : 0.071559, loss_ce: 0.026051
2022-01-06 14:38:59,155 iteration 1528 : loss : 0.088834, loss_ce: 0.031564
2022-01-06 14:39:00,354 iteration 1529 : loss : 0.120184, loss_ce: 0.034739
2022-01-06 14:39:00,355 Training Data Eval:
2022-01-06 14:39:06,374   Average segmentation loss on training set: 0.1417
2022-01-06 14:39:06,375 Validation Data Eval:
2022-01-06 14:39:08,443   Average segmentation loss on validation set: 0.1358
2022-01-06 14:39:14,522 Found new lowest validation loss at iteration 1529! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_MLP_best_val_loss_seed100.pth
2022-01-06 14:39:15,654 iteration 1530 : loss : 0.081759, loss_ce: 0.038101
 22%|██████▊                       | 90/400 [34:15<2:13:06, 25.76s/it]2022-01-06 14:39:16,919 iteration 1531 : loss : 0.090324, loss_ce: 0.038928
2022-01-06 14:39:18,130 iteration 1532 : loss : 0.119327, loss_ce: 0.044425
2022-01-06 14:39:19,237 iteration 1533 : loss : 0.095678, loss_ce: 0.035541
2022-01-06 14:39:20,378 iteration 1534 : loss : 0.062682, loss_ce: 0.033659
2022-01-06 14:39:21,482 iteration 1535 : loss : 0.099893, loss_ce: 0.044482
2022-01-06 14:39:22,663 iteration 1536 : loss : 0.091801, loss_ce: 0.029509
2022-01-06 14:39:23,793 iteration 1537 : loss : 0.088648, loss_ce: 0.044932
2022-01-06 14:39:24,878 iteration 1538 : loss : 0.157672, loss_ce: 0.047247
2022-01-06 14:39:26,048 iteration 1539 : loss : 0.108462, loss_ce: 0.045276
2022-01-06 14:39:27,303 iteration 1540 : loss : 0.075347, loss_ce: 0.035232
2022-01-06 14:39:28,588 iteration 1541 : loss : 0.073116, loss_ce: 0.029226
2022-01-06 14:39:29,919 iteration 1542 : loss : 0.093915, loss_ce: 0.037871
2022-01-06 14:39:31,139 iteration 1543 : loss : 0.100814, loss_ce: 0.028958
2022-01-06 14:39:32,371 iteration 1544 : loss : 0.064377, loss_ce: 0.026622
2022-01-06 14:39:33,521 iteration 1545 : loss : 0.100356, loss_ce: 0.038343
2022-01-06 14:39:34,726 iteration 1546 : loss : 0.091818, loss_ce: 0.031909
2022-01-06 14:39:35,987 iteration 1547 : loss : 0.063326, loss_ce: 0.027317
 23%|██████▊                       | 91/400 [34:36<2:04:17, 24.13s/it]2022-01-06 14:39:37,248 iteration 1548 : loss : 0.111976, loss_ce: 0.034850
2022-01-06 14:39:38,383 iteration 1549 : loss : 0.108721, loss_ce: 0.031309
2022-01-06 14:39:39,649 iteration 1550 : loss : 0.080607, loss_ce: 0.025502
2022-01-06 14:39:40,882 iteration 1551 : loss : 0.085899, loss_ce: 0.031584
2022-01-06 14:39:42,067 iteration 1552 : loss : 0.081096, loss_ce: 0.021789
2022-01-06 14:39:43,330 iteration 1553 : loss : 0.081650, loss_ce: 0.036864
2022-01-06 14:39:44,433 iteration 1554 : loss : 0.122410, loss_ce: 0.066590
2022-01-06 14:39:45,614 iteration 1555 : loss : 0.066340, loss_ce: 0.023531
2022-01-06 14:39:46,956 iteration 1556 : loss : 0.091304, loss_ce: 0.035040
2022-01-06 14:39:48,159 iteration 1557 : loss : 0.065364, loss_ce: 0.022581
2022-01-06 14:39:49,301 iteration 1558 : loss : 0.086746, loss_ce: 0.029111
2022-01-06 14:39:50,609 iteration 1559 : loss : 0.099319, loss_ce: 0.038190
2022-01-06 14:39:51,780 iteration 1560 : loss : 0.081538, loss_ce: 0.040960
2022-01-06 14:39:52,989 iteration 1561 : loss : 0.066234, loss_ce: 0.028902
2022-01-06 14:39:54,190 iteration 1562 : loss : 0.075254, loss_ce: 0.035879
2022-01-06 14:39:55,424 iteration 1563 : loss : 0.108186, loss_ce: 0.043291
2022-01-06 14:39:56,605 iteration 1564 : loss : 0.084401, loss_ce: 0.030032
 23%|██████▉                       | 92/400 [34:56<1:58:28, 23.08s/it]2022-01-06 14:39:57,838 iteration 1565 : loss : 0.064594, loss_ce: 0.034115
2022-01-06 14:39:59,094 iteration 1566 : loss : 0.082741, loss_ce: 0.030375
2022-01-06 14:40:00,309 iteration 1567 : loss : 0.088059, loss_ce: 0.038512
2022-01-06 14:40:01,639 iteration 1568 : loss : 0.069390, loss_ce: 0.026628
2022-01-06 14:40:02,832 iteration 1569 : loss : 0.090703, loss_ce: 0.041432
2022-01-06 14:40:04,054 iteration 1570 : loss : 0.079220, loss_ce: 0.029321
2022-01-06 14:40:05,263 iteration 1571 : loss : 0.105290, loss_ce: 0.037213
2022-01-06 14:40:06,580 iteration 1572 : loss : 0.109723, loss_ce: 0.038283
2022-01-06 14:40:07,692 iteration 1573 : loss : 0.051562, loss_ce: 0.022141
2022-01-06 14:40:08,955 iteration 1574 : loss : 0.083473, loss_ce: 0.032361
2022-01-06 14:40:10,262 iteration 1575 : loss : 0.067526, loss_ce: 0.028029
2022-01-06 14:40:11,558 iteration 1576 : loss : 0.072943, loss_ce: 0.025046
2022-01-06 14:40:12,683 iteration 1577 : loss : 0.080080, loss_ce: 0.032524
2022-01-06 14:40:13,883 iteration 1578 : loss : 0.063433, loss_ce: 0.021180
2022-01-06 14:40:15,180 iteration 1579 : loss : 0.095244, loss_ce: 0.030330
2022-01-06 14:40:16,431 iteration 1580 : loss : 0.124788, loss_ce: 0.042435
2022-01-06 14:40:17,805 iteration 1581 : loss : 0.090605, loss_ce: 0.037066
 23%|██████▉                       | 93/400 [35:18<1:55:12, 22.52s/it]2022-01-06 14:40:19,115 iteration 1582 : loss : 0.054164, loss_ce: 0.019859
2022-01-06 14:40:20,368 iteration 1583 : loss : 0.085439, loss_ce: 0.028274
2022-01-06 14:40:21,502 iteration 1584 : loss : 0.078057, loss_ce: 0.026366
2022-01-06 14:40:22,755 iteration 1585 : loss : 0.185571, loss_ce: 0.058786
2022-01-06 14:40:24,081 iteration 1586 : loss : 0.088125, loss_ce: 0.024209
2022-01-06 14:40:25,322 iteration 1587 : loss : 0.101509, loss_ce: 0.031319
2022-01-06 14:40:26,628 iteration 1588 : loss : 0.071446, loss_ce: 0.029604
2022-01-06 14:40:27,862 iteration 1589 : loss : 0.076625, loss_ce: 0.024694
2022-01-06 14:40:29,140 iteration 1590 : loss : 0.090203, loss_ce: 0.038963
2022-01-06 14:40:30,337 iteration 1591 : loss : 0.124590, loss_ce: 0.065969
2022-01-06 14:40:31,545 iteration 1592 : loss : 0.098158, loss_ce: 0.042923
2022-01-06 14:40:32,869 iteration 1593 : loss : 0.075725, loss_ce: 0.023377
2022-01-06 14:40:34,121 iteration 1594 : loss : 0.069191, loss_ce: 0.035715
2022-01-06 14:40:35,372 iteration 1595 : loss : 0.093819, loss_ce: 0.035299
2022-01-06 14:40:36,607 iteration 1596 : loss : 0.069433, loss_ce: 0.027181
2022-01-06 14:40:37,791 iteration 1597 : loss : 0.073771, loss_ce: 0.025731
2022-01-06 14:40:39,066 iteration 1598 : loss : 0.069580, loss_ce: 0.031507
 24%|███████                       | 94/400 [35:39<1:52:54, 22.14s/it]2022-01-06 14:40:40,393 iteration 1599 : loss : 0.071649, loss_ce: 0.035425
2022-01-06 14:40:41,581 iteration 1600 : loss : 0.098412, loss_ce: 0.032563
2022-01-06 14:40:42,817 iteration 1601 : loss : 0.089859, loss_ce: 0.032764
2022-01-06 14:40:44,015 iteration 1602 : loss : 0.089206, loss_ce: 0.039276
2022-01-06 14:40:45,335 iteration 1603 : loss : 0.076161, loss_ce: 0.030506
2022-01-06 14:40:46,536 iteration 1604 : loss : 0.078571, loss_ce: 0.030786
2022-01-06 14:40:47,858 iteration 1605 : loss : 0.082745, loss_ce: 0.029280
2022-01-06 14:40:49,135 iteration 1606 : loss : 0.082359, loss_ce: 0.023473
2022-01-06 14:40:50,345 iteration 1607 : loss : 0.073798, loss_ce: 0.031168
2022-01-06 14:40:51,584 iteration 1608 : loss : 0.057757, loss_ce: 0.022081
2022-01-06 14:40:52,831 iteration 1609 : loss : 0.065181, loss_ce: 0.022827
2022-01-06 14:40:54,036 iteration 1610 : loss : 0.130090, loss_ce: 0.055665
2022-01-06 14:40:55,281 iteration 1611 : loss : 0.085176, loss_ce: 0.031509
2022-01-06 14:40:56,503 iteration 1612 : loss : 0.083316, loss_ce: 0.038296
2022-01-06 14:40:57,745 iteration 1613 : loss : 0.089939, loss_ce: 0.040934
2022-01-06 14:40:58,992 iteration 1614 : loss : 0.070560, loss_ce: 0.020927
2022-01-06 14:40:58,992 Training Data Eval:
2022-01-06 14:41:04,991   Average segmentation loss on training set: 0.1284
2022-01-06 14:41:04,991 Validation Data Eval:
2022-01-06 14:41:07,032   Average segmentation loss on validation set: 0.1339
2022-01-06 14:41:12,862 Found new lowest validation loss at iteration 1614! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_MLP_best_val_loss_seed100.pth
2022-01-06 14:41:14,137 iteration 1615 : loss : 0.091905, loss_ce: 0.046341
 24%|███████▏                      | 95/400 [36:14<2:12:15, 26.02s/it]2022-01-06 14:41:15,335 iteration 1616 : loss : 0.078689, loss_ce: 0.023457
2022-01-06 14:41:16,427 iteration 1617 : loss : 0.057965, loss_ce: 0.025885
2022-01-06 14:41:17,590 iteration 1618 : loss : 0.045595, loss_ce: 0.017346
2022-01-06 14:41:18,823 iteration 1619 : loss : 0.063483, loss_ce: 0.028854
2022-01-06 14:41:19,913 iteration 1620 : loss : 0.077070, loss_ce: 0.030158
2022-01-06 14:41:21,128 iteration 1621 : loss : 0.080904, loss_ce: 0.031450
2022-01-06 14:41:22,272 iteration 1622 : loss : 0.102333, loss_ce: 0.035563
2022-01-06 14:41:23,377 iteration 1623 : loss : 0.054196, loss_ce: 0.023926
2022-01-06 14:41:24,615 iteration 1624 : loss : 0.083733, loss_ce: 0.032129
2022-01-06 14:41:25,757 iteration 1625 : loss : 0.058940, loss_ce: 0.029579
2022-01-06 14:41:26,967 iteration 1626 : loss : 0.057416, loss_ce: 0.021389
2022-01-06 14:41:28,142 iteration 1627 : loss : 0.109277, loss_ce: 0.026703
2022-01-06 14:41:29,398 iteration 1628 : loss : 0.085674, loss_ce: 0.024897
2022-01-06 14:41:30,642 iteration 1629 : loss : 0.095898, loss_ce: 0.041697
2022-01-06 14:41:31,785 iteration 1630 : loss : 0.092777, loss_ce: 0.038348
2022-01-06 14:41:33,016 iteration 1631 : loss : 0.075873, loss_ce: 0.034290
2022-01-06 14:41:34,128 iteration 1632 : loss : 0.052515, loss_ce: 0.023319
 24%|███████▏                      | 96/400 [36:34<2:02:39, 24.21s/it]2022-01-06 14:41:35,498 iteration 1633 : loss : 0.057183, loss_ce: 0.019195
2022-01-06 14:41:36,763 iteration 1634 : loss : 0.071114, loss_ce: 0.034892
2022-01-06 14:41:37,968 iteration 1635 : loss : 0.090900, loss_ce: 0.041341
2022-01-06 14:41:39,073 iteration 1636 : loss : 0.068508, loss_ce: 0.026377
2022-01-06 14:41:40,366 iteration 1637 : loss : 0.073546, loss_ce: 0.024367
2022-01-06 14:41:41,616 iteration 1638 : loss : 0.056751, loss_ce: 0.021400
2022-01-06 14:41:42,925 iteration 1639 : loss : 0.067080, loss_ce: 0.027081
2022-01-06 14:41:44,146 iteration 1640 : loss : 0.079522, loss_ce: 0.033608
2022-01-06 14:41:45,398 iteration 1641 : loss : 0.069088, loss_ce: 0.026142
2022-01-06 14:41:46,751 iteration 1642 : loss : 0.090403, loss_ce: 0.033237
2022-01-06 14:41:48,010 iteration 1643 : loss : 0.047294, loss_ce: 0.019311
2022-01-06 14:41:49,256 iteration 1644 : loss : 0.087674, loss_ce: 0.037039
2022-01-06 14:41:50,391 iteration 1645 : loss : 0.066316, loss_ce: 0.022783
2022-01-06 14:41:51,603 iteration 1646 : loss : 0.064503, loss_ce: 0.025227
2022-01-06 14:41:52,926 iteration 1647 : loss : 0.072030, loss_ce: 0.037086
2022-01-06 14:41:54,140 iteration 1648 : loss : 0.072122, loss_ce: 0.030482
2022-01-06 14:41:55,481 iteration 1649 : loss : 0.096634, loss_ce: 0.029869
 24%|███████▎                      | 97/400 [36:55<1:57:55, 23.35s/it]2022-01-06 14:41:56,775 iteration 1650 : loss : 0.085495, loss_ce: 0.027517
2022-01-06 14:41:58,089 iteration 1651 : loss : 0.113831, loss_ce: 0.050836
2022-01-06 14:41:59,383 iteration 1652 : loss : 0.118727, loss_ce: 0.038772
2022-01-06 14:42:00,660 iteration 1653 : loss : 0.090542, loss_ce: 0.042905
2022-01-06 14:42:01,874 iteration 1654 : loss : 0.069937, loss_ce: 0.020844
2022-01-06 14:42:03,066 iteration 1655 : loss : 0.076027, loss_ce: 0.024532
2022-01-06 14:42:04,261 iteration 1656 : loss : 0.051341, loss_ce: 0.022070
2022-01-06 14:42:05,569 iteration 1657 : loss : 0.072246, loss_ce: 0.032653
2022-01-06 14:42:06,737 iteration 1658 : loss : 0.064103, loss_ce: 0.026298
2022-01-06 14:42:07,966 iteration 1659 : loss : 0.080860, loss_ce: 0.033954
2022-01-06 14:42:09,339 iteration 1660 : loss : 0.110082, loss_ce: 0.050252
2022-01-06 14:42:10,554 iteration 1661 : loss : 0.067071, loss_ce: 0.027245
2022-01-06 14:42:11,744 iteration 1662 : loss : 0.078539, loss_ce: 0.028498
2022-01-06 14:42:13,032 iteration 1663 : loss : 0.090662, loss_ce: 0.037077
2022-01-06 14:42:14,319 iteration 1664 : loss : 0.075901, loss_ce: 0.026030
2022-01-06 14:42:15,487 iteration 1665 : loss : 0.043806, loss_ce: 0.019617
2022-01-06 14:42:16,669 iteration 1666 : loss : 0.065669, loss_ce: 0.025907
 24%|███████▎                      | 98/400 [37:16<1:54:16, 22.70s/it]2022-01-06 14:42:17,953 iteration 1667 : loss : 0.068270, loss_ce: 0.029335
2022-01-06 14:42:19,237 iteration 1668 : loss : 0.084614, loss_ce: 0.034935
2022-01-06 14:42:20,429 iteration 1669 : loss : 0.103900, loss_ce: 0.048764
2022-01-06 14:42:21,626 iteration 1670 : loss : 0.087029, loss_ce: 0.028533
2022-01-06 14:42:22,869 iteration 1671 : loss : 0.077842, loss_ce: 0.030978
2022-01-06 14:42:24,047 iteration 1672 : loss : 0.060090, loss_ce: 0.018083
2022-01-06 14:42:25,219 iteration 1673 : loss : 0.048968, loss_ce: 0.019270
2022-01-06 14:42:26,523 iteration 1674 : loss : 0.084721, loss_ce: 0.028897
2022-01-06 14:42:27,715 iteration 1675 : loss : 0.050773, loss_ce: 0.020191
2022-01-06 14:42:28,991 iteration 1676 : loss : 0.068565, loss_ce: 0.031196
2022-01-06 14:42:30,277 iteration 1677 : loss : 0.074268, loss_ce: 0.026260
2022-01-06 14:42:31,520 iteration 1678 : loss : 0.134007, loss_ce: 0.049498
2022-01-06 14:42:32,772 iteration 1679 : loss : 0.074752, loss_ce: 0.021366
2022-01-06 14:42:34,002 iteration 1680 : loss : 0.063351, loss_ce: 0.028805
2022-01-06 14:42:35,201 iteration 1681 : loss : 0.099215, loss_ce: 0.024901
2022-01-06 14:42:36,378 iteration 1682 : loss : 0.069444, loss_ce: 0.031363
2022-01-06 14:42:37,640 iteration 1683 : loss : 0.085077, loss_ce: 0.039926
 25%|███████▍                      | 99/400 [37:37<1:51:17, 22.18s/it]2022-01-06 14:42:38,927 iteration 1684 : loss : 0.066841, loss_ce: 0.023392
2022-01-06 14:42:40,098 iteration 1685 : loss : 0.064079, loss_ce: 0.023960
2022-01-06 14:42:41,381 iteration 1686 : loss : 0.070836, loss_ce: 0.027673
2022-01-06 14:42:42,537 iteration 1687 : loss : 0.061358, loss_ce: 0.021039
2022-01-06 14:42:43,776 iteration 1688 : loss : 0.069472, loss_ce: 0.024535
2022-01-06 14:42:45,019 iteration 1689 : loss : 0.093001, loss_ce: 0.040580
2022-01-06 14:42:46,156 iteration 1690 : loss : 0.073040, loss_ce: 0.022665
2022-01-06 14:42:47,312 iteration 1691 : loss : 0.092276, loss_ce: 0.031735
2022-01-06 14:42:48,561 iteration 1692 : loss : 0.064373, loss_ce: 0.025916
2022-01-06 14:42:49,870 iteration 1693 : loss : 0.064986, loss_ce: 0.023561
2022-01-06 14:42:51,139 iteration 1694 : loss : 0.076201, loss_ce: 0.019091
2022-01-06 14:42:52,327 iteration 1695 : loss : 0.065087, loss_ce: 0.018697
2022-01-06 14:42:53,551 iteration 1696 : loss : 0.103593, loss_ce: 0.047536
2022-01-06 14:42:54,794 iteration 1697 : loss : 0.084653, loss_ce: 0.036837
2022-01-06 14:42:56,034 iteration 1698 : loss : 0.060179, loss_ce: 0.021972
2022-01-06 14:42:57,226 iteration 1699 : loss : 0.066185, loss_ce: 0.028904
2022-01-06 14:42:57,226 Training Data Eval:
2022-01-06 14:43:03,215   Average segmentation loss on training set: 0.1964
2022-01-06 14:43:03,215 Validation Data Eval:
2022-01-06 14:43:05,277   Average segmentation loss on validation set: 0.3506
2022-01-06 14:43:06,528 iteration 1700 : loss : 0.072794, loss_ce: 0.029730
 25%|███████▎                     | 100/400 [38:06<2:00:58, 24.20s/it]2022-01-06 14:43:07,800 iteration 1701 : loss : 0.053783, loss_ce: 0.022074
2022-01-06 14:43:09,004 iteration 1702 : loss : 0.059913, loss_ce: 0.022584
2022-01-06 14:43:10,288 iteration 1703 : loss : 0.105755, loss_ce: 0.046267
2022-01-06 14:43:11,496 iteration 1704 : loss : 0.058829, loss_ce: 0.026957
2022-01-06 14:43:12,752 iteration 1705 : loss : 0.082071, loss_ce: 0.028868
2022-01-06 14:43:14,046 iteration 1706 : loss : 0.091389, loss_ce: 0.034808
2022-01-06 14:43:15,228 iteration 1707 : loss : 0.077236, loss_ce: 0.036033
2022-01-06 14:43:16,444 iteration 1708 : loss : 0.136558, loss_ce: 0.040721
2022-01-06 14:43:17,650 iteration 1709 : loss : 0.079458, loss_ce: 0.031994
2022-01-06 14:43:18,914 iteration 1710 : loss : 0.080603, loss_ce: 0.026790
2022-01-06 14:43:20,125 iteration 1711 : loss : 0.100816, loss_ce: 0.048579
2022-01-06 14:43:21,270 iteration 1712 : loss : 0.060956, loss_ce: 0.024775
2022-01-06 14:43:22,506 iteration 1713 : loss : 0.084396, loss_ce: 0.025265
2022-01-06 14:43:23,795 iteration 1714 : loss : 0.074008, loss_ce: 0.023145
2022-01-06 14:43:25,066 iteration 1715 : loss : 0.082458, loss_ce: 0.038592
2022-01-06 14:43:26,194 iteration 1716 : loss : 0.070638, loss_ce: 0.022192
2022-01-06 14:43:27,456 iteration 1717 : loss : 0.100359, loss_ce: 0.031195
 25%|███████▎                     | 101/400 [38:27<1:55:40, 23.21s/it]2022-01-06 14:43:28,735 iteration 1718 : loss : 0.068953, loss_ce: 0.030329
2022-01-06 14:43:29,925 iteration 1719 : loss : 0.088927, loss_ce: 0.019968
2022-01-06 14:43:31,182 iteration 1720 : loss : 0.084140, loss_ce: 0.024461
2022-01-06 14:43:32,361 iteration 1721 : loss : 0.072277, loss_ce: 0.022782
2022-01-06 14:43:33,605 iteration 1722 : loss : 0.084424, loss_ce: 0.024994
2022-01-06 14:43:34,765 iteration 1723 : loss : 0.105468, loss_ce: 0.034679
2022-01-06 14:43:36,010 iteration 1724 : loss : 0.049386, loss_ce: 0.017134
2022-01-06 14:43:37,234 iteration 1725 : loss : 0.101835, loss_ce: 0.035544
2022-01-06 14:43:38,462 iteration 1726 : loss : 0.080002, loss_ce: 0.028329
2022-01-06 14:43:39,763 iteration 1727 : loss : 0.084856, loss_ce: 0.038859
2022-01-06 14:43:40,902 iteration 1728 : loss : 0.079354, loss_ce: 0.029612
2022-01-06 14:43:42,167 iteration 1729 : loss : 0.090761, loss_ce: 0.036538
2022-01-06 14:43:43,465 iteration 1730 : loss : 0.056508, loss_ce: 0.023507
2022-01-06 14:43:44,688 iteration 1731 : loss : 0.105754, loss_ce: 0.051863
2022-01-06 14:43:45,904 iteration 1732 : loss : 0.055468, loss_ce: 0.023058
2022-01-06 14:43:47,103 iteration 1733 : loss : 0.061854, loss_ce: 0.025736
2022-01-06 14:43:48,319 iteration 1734 : loss : 0.077069, loss_ce: 0.030463
 26%|███████▍                     | 102/400 [38:48<1:51:47, 22.51s/it]2022-01-06 14:43:49,743 iteration 1735 : loss : 0.064817, loss_ce: 0.026832
2022-01-06 14:43:50,916 iteration 1736 : loss : 0.089731, loss_ce: 0.032213
2022-01-06 14:43:52,269 iteration 1737 : loss : 0.075715, loss_ce: 0.032515
2022-01-06 14:43:53,479 iteration 1738 : loss : 0.063421, loss_ce: 0.030098
2022-01-06 14:43:54,629 iteration 1739 : loss : 0.066117, loss_ce: 0.023708
2022-01-06 14:43:55,902 iteration 1740 : loss : 0.051410, loss_ce: 0.020515
2022-01-06 14:43:57,150 iteration 1741 : loss : 0.068781, loss_ce: 0.023624
2022-01-06 14:43:58,407 iteration 1742 : loss : 0.065149, loss_ce: 0.022267
2022-01-06 14:43:59,539 iteration 1743 : loss : 0.075888, loss_ce: 0.027751
2022-01-06 14:44:00,840 iteration 1744 : loss : 0.099992, loss_ce: 0.038662
2022-01-06 14:44:02,122 iteration 1745 : loss : 0.071298, loss_ce: 0.027242
2022-01-06 14:44:03,350 iteration 1746 : loss : 0.087674, loss_ce: 0.042915
2022-01-06 14:44:04,527 iteration 1747 : loss : 0.089351, loss_ce: 0.037728
2022-01-06 14:44:05,765 iteration 1748 : loss : 0.080779, loss_ce: 0.026738
2022-01-06 14:44:06,868 iteration 1749 : loss : 0.060125, loss_ce: 0.019275
2022-01-06 14:44:08,111 iteration 1750 : loss : 0.145893, loss_ce: 0.035137
2022-01-06 14:44:09,436 iteration 1751 : loss : 0.090250, loss_ce: 0.031215
 26%|███████▍                     | 103/400 [39:09<1:49:21, 22.09s/it]2022-01-06 14:44:10,791 iteration 1752 : loss : 0.162391, loss_ce: 0.043935
2022-01-06 14:44:11,885 iteration 1753 : loss : 0.081205, loss_ce: 0.027265
2022-01-06 14:44:13,164 iteration 1754 : loss : 0.075801, loss_ce: 0.026989
2022-01-06 14:44:14,337 iteration 1755 : loss : 0.060794, loss_ce: 0.025140
2022-01-06 14:44:15,571 iteration 1756 : loss : 0.126550, loss_ce: 0.050369
2022-01-06 14:44:16,733 iteration 1757 : loss : 0.051448, loss_ce: 0.021452
2022-01-06 14:44:17,933 iteration 1758 : loss : 0.065198, loss_ce: 0.029185
2022-01-06 14:44:19,109 iteration 1759 : loss : 0.088523, loss_ce: 0.022706
2022-01-06 14:44:20,286 iteration 1760 : loss : 0.077542, loss_ce: 0.028902
2022-01-06 14:44:21,578 iteration 1761 : loss : 0.075738, loss_ce: 0.024905
2022-01-06 14:44:22,688 iteration 1762 : loss : 0.054923, loss_ce: 0.021703
2022-01-06 14:44:23,834 iteration 1763 : loss : 0.070022, loss_ce: 0.026970
2022-01-06 14:44:24,938 iteration 1764 : loss : 0.051746, loss_ce: 0.016340
2022-01-06 14:44:26,171 iteration 1765 : loss : 0.062857, loss_ce: 0.026976
2022-01-06 14:44:27,444 iteration 1766 : loss : 0.077485, loss_ce: 0.034294
2022-01-06 14:44:28,613 iteration 1767 : loss : 0.062660, loss_ce: 0.026937
2022-01-06 14:44:29,825 iteration 1768 : loss : 0.085875, loss_ce: 0.031318
 26%|███████▌                     | 104/400 [39:30<1:46:28, 21.58s/it]2022-01-06 14:44:31,091 iteration 1769 : loss : 0.087351, loss_ce: 0.033606
2022-01-06 14:44:32,240 iteration 1770 : loss : 0.070772, loss_ce: 0.027052
2022-01-06 14:44:33,347 iteration 1771 : loss : 0.075553, loss_ce: 0.029659
2022-01-06 14:44:34,495 iteration 1772 : loss : 0.056037, loss_ce: 0.026596
2022-01-06 14:44:35,764 iteration 1773 : loss : 0.102734, loss_ce: 0.035007
2022-01-06 14:44:36,989 iteration 1774 : loss : 0.045963, loss_ce: 0.015574
2022-01-06 14:44:38,235 iteration 1775 : loss : 0.069189, loss_ce: 0.026980
2022-01-06 14:44:39,390 iteration 1776 : loss : 0.073100, loss_ce: 0.033573
2022-01-06 14:44:40,543 iteration 1777 : loss : 0.063158, loss_ce: 0.022253
2022-01-06 14:44:41,676 iteration 1778 : loss : 0.084124, loss_ce: 0.029581
2022-01-06 14:44:42,939 iteration 1779 : loss : 0.079462, loss_ce: 0.024200
2022-01-06 14:44:44,141 iteration 1780 : loss : 0.092323, loss_ce: 0.044038
2022-01-06 14:44:45,272 iteration 1781 : loss : 0.067482, loss_ce: 0.025425
2022-01-06 14:44:46,503 iteration 1782 : loss : 0.069032, loss_ce: 0.030981
2022-01-06 14:44:47,721 iteration 1783 : loss : 0.063886, loss_ce: 0.021236
2022-01-06 14:44:48,875 iteration 1784 : loss : 0.056196, loss_ce: 0.024167
2022-01-06 14:44:48,875 Training Data Eval:
2022-01-06 14:44:54,784   Average segmentation loss on training set: 0.5435
2022-01-06 14:44:54,784 Validation Data Eval:
2022-01-06 14:44:56,818   Average segmentation loss on validation set: 0.5033
2022-01-06 14:44:58,076 iteration 1785 : loss : 0.067450, loss_ce: 0.027256
 26%|███████▌                     | 105/400 [39:58<1:55:55, 23.58s/it]2022-01-06 14:44:59,343 iteration 1786 : loss : 0.065484, loss_ce: 0.030666
2022-01-06 14:45:00,444 iteration 1787 : loss : 0.071732, loss_ce: 0.026525
2022-01-06 14:45:01,700 iteration 1788 : loss : 0.067519, loss_ce: 0.032726
2022-01-06 14:45:02,938 iteration 1789 : loss : 0.066055, loss_ce: 0.027552
2022-01-06 14:45:04,154 iteration 1790 : loss : 0.057366, loss_ce: 0.026762
2022-01-06 14:45:05,423 iteration 1791 : loss : 0.090929, loss_ce: 0.037396
2022-01-06 14:45:06,563 iteration 1792 : loss : 0.077643, loss_ce: 0.029008
2022-01-06 14:45:07,719 iteration 1793 : loss : 0.123382, loss_ce: 0.045327
2022-01-06 14:45:08,902 iteration 1794 : loss : 0.057172, loss_ce: 0.021229
2022-01-06 14:45:10,153 iteration 1795 : loss : 0.061214, loss_ce: 0.023003
2022-01-06 14:45:11,351 iteration 1796 : loss : 0.083637, loss_ce: 0.033380
2022-01-06 14:45:12,647 iteration 1797 : loss : 0.086356, loss_ce: 0.032747
2022-01-06 14:45:13,918 iteration 1798 : loss : 0.068000, loss_ce: 0.021958
2022-01-06 14:45:15,067 iteration 1799 : loss : 0.065681, loss_ce: 0.026605
2022-01-06 14:45:16,276 iteration 1800 : loss : 0.072472, loss_ce: 0.029325
2022-01-06 14:45:17,415 iteration 1801 : loss : 0.083429, loss_ce: 0.027197
2022-01-06 14:45:18,595 iteration 1802 : loss : 0.092841, loss_ce: 0.030135
 26%|███████▋                     | 106/400 [40:18<1:51:02, 22.66s/it]2022-01-06 14:45:19,950 iteration 1803 : loss : 0.092408, loss_ce: 0.034075
2022-01-06 14:45:21,218 iteration 1804 : loss : 0.073796, loss_ce: 0.023601
2022-01-06 14:45:22,374 iteration 1805 : loss : 0.066982, loss_ce: 0.016026
2022-01-06 14:45:23,527 iteration 1806 : loss : 0.074215, loss_ce: 0.033699
2022-01-06 14:45:24,685 iteration 1807 : loss : 0.094459, loss_ce: 0.033373
2022-01-06 14:45:25,874 iteration 1808 : loss : 0.053763, loss_ce: 0.022913
2022-01-06 14:45:27,042 iteration 1809 : loss : 0.056324, loss_ce: 0.023507
2022-01-06 14:45:28,138 iteration 1810 : loss : 0.056536, loss_ce: 0.022399
2022-01-06 14:45:29,398 iteration 1811 : loss : 0.086911, loss_ce: 0.039147
2022-01-06 14:45:30,669 iteration 1812 : loss : 0.066598, loss_ce: 0.023472
2022-01-06 14:45:31,994 iteration 1813 : loss : 0.061833, loss_ce: 0.025911
2022-01-06 14:45:33,210 iteration 1814 : loss : 0.056664, loss_ce: 0.025467
2022-01-06 14:45:34,332 iteration 1815 : loss : 0.066867, loss_ce: 0.023797
2022-01-06 14:45:35,427 iteration 1816 : loss : 0.052060, loss_ce: 0.022257
2022-01-06 14:45:36,699 iteration 1817 : loss : 0.069209, loss_ce: 0.026723
2022-01-06 14:45:37,901 iteration 1818 : loss : 0.086067, loss_ce: 0.035608
2022-01-06 14:45:39,155 iteration 1819 : loss : 0.068313, loss_ce: 0.028491
 27%|███████▊                     | 107/400 [40:39<1:47:35, 22.03s/it]2022-01-06 14:45:40,504 iteration 1820 : loss : 0.055117, loss_ce: 0.021676
2022-01-06 14:45:41,660 iteration 1821 : loss : 0.052039, loss_ce: 0.021665
2022-01-06 14:45:42,864 iteration 1822 : loss : 0.057091, loss_ce: 0.022032
2022-01-06 14:45:43,972 iteration 1823 : loss : 0.046425, loss_ce: 0.015220
2022-01-06 14:45:45,048 iteration 1824 : loss : 0.051265, loss_ce: 0.020009
2022-01-06 14:45:46,290 iteration 1825 : loss : 0.075268, loss_ce: 0.028558
2022-01-06 14:45:47,373 iteration 1826 : loss : 0.073613, loss_ce: 0.037202
2022-01-06 14:45:48,681 iteration 1827 : loss : 0.179531, loss_ce: 0.053476
2022-01-06 14:45:49,857 iteration 1828 : loss : 0.054469, loss_ce: 0.020462
2022-01-06 14:45:51,026 iteration 1829 : loss : 0.106325, loss_ce: 0.037089
2022-01-06 14:45:52,201 iteration 1830 : loss : 0.074326, loss_ce: 0.028854
2022-01-06 14:45:53,479 iteration 1831 : loss : 0.093502, loss_ce: 0.044129
2022-01-06 14:45:54,656 iteration 1832 : loss : 0.061238, loss_ce: 0.019987
2022-01-06 14:45:55,846 iteration 1833 : loss : 0.078467, loss_ce: 0.036240
2022-01-06 14:45:57,144 iteration 1834 : loss : 0.077233, loss_ce: 0.031289
2022-01-06 14:45:58,306 iteration 1835 : loss : 0.074048, loss_ce: 0.039824
2022-01-06 14:45:59,498 iteration 1836 : loss : 0.082548, loss_ce: 0.030201
 27%|███████▊                     | 108/400 [40:59<1:44:45, 21.53s/it]2022-01-06 14:46:00,820 iteration 1837 : loss : 0.061556, loss_ce: 0.025504
2022-01-06 14:46:02,077 iteration 1838 : loss : 0.070323, loss_ce: 0.032054
2022-01-06 14:46:03,246 iteration 1839 : loss : 0.078328, loss_ce: 0.031962
2022-01-06 14:46:04,354 iteration 1840 : loss : 0.067945, loss_ce: 0.031328
2022-01-06 14:46:05,593 iteration 1841 : loss : 0.097015, loss_ce: 0.042885
2022-01-06 14:46:06,858 iteration 1842 : loss : 0.074521, loss_ce: 0.027205
2022-01-06 14:46:07,982 iteration 1843 : loss : 0.056250, loss_ce: 0.025482
2022-01-06 14:46:09,181 iteration 1844 : loss : 0.074131, loss_ce: 0.027640
2022-01-06 14:46:10,310 iteration 1845 : loss : 0.062192, loss_ce: 0.023498
2022-01-06 14:46:11,631 iteration 1846 : loss : 0.060237, loss_ce: 0.026282
2022-01-06 14:46:12,884 iteration 1847 : loss : 0.074995, loss_ce: 0.027662
2022-01-06 14:46:14,083 iteration 1848 : loss : 0.050373, loss_ce: 0.021518
2022-01-06 14:46:15,180 iteration 1849 : loss : 0.061092, loss_ce: 0.025786
2022-01-06 14:46:16,434 iteration 1850 : loss : 0.093978, loss_ce: 0.023586
2022-01-06 14:46:17,671 iteration 1851 : loss : 0.079684, loss_ce: 0.030301
2022-01-06 14:46:18,878 iteration 1852 : loss : 0.055663, loss_ce: 0.024186
2022-01-06 14:46:20,087 iteration 1853 : loss : 0.050462, loss_ce: 0.011994
 27%|███████▉                     | 109/400 [41:20<1:43:02, 21.25s/it]2022-01-06 14:46:21,322 iteration 1854 : loss : 0.076494, loss_ce: 0.028196
2022-01-06 14:46:22,497 iteration 1855 : loss : 0.061066, loss_ce: 0.020609
2022-01-06 14:46:23,691 iteration 1856 : loss : 0.064105, loss_ce: 0.016435
2022-01-06 14:46:24,889 iteration 1857 : loss : 0.098779, loss_ce: 0.037912
2022-01-06 14:46:26,075 iteration 1858 : loss : 0.069018, loss_ce: 0.030015
2022-01-06 14:46:27,223 iteration 1859 : loss : 0.054073, loss_ce: 0.027273
2022-01-06 14:46:28,556 iteration 1860 : loss : 0.089320, loss_ce: 0.041819
2022-01-06 14:46:29,780 iteration 1861 : loss : 0.078389, loss_ce: 0.026531
2022-01-06 14:46:30,964 iteration 1862 : loss : 0.066413, loss_ce: 0.025880
2022-01-06 14:46:32,210 iteration 1863 : loss : 0.071113, loss_ce: 0.026637
2022-01-06 14:46:33,422 iteration 1864 : loss : 0.087036, loss_ce: 0.035225
2022-01-06 14:46:34,718 iteration 1865 : loss : 0.065691, loss_ce: 0.029387
2022-01-06 14:46:35,980 iteration 1866 : loss : 0.058738, loss_ce: 0.025836
2022-01-06 14:46:37,205 iteration 1867 : loss : 0.126116, loss_ce: 0.047079
2022-01-06 14:46:38,376 iteration 1868 : loss : 0.057610, loss_ce: 0.028312
2022-01-06 14:46:39,582 iteration 1869 : loss : 0.079720, loss_ce: 0.031323
2022-01-06 14:46:39,583 Training Data Eval:
2022-01-06 14:46:45,518   Average segmentation loss on training set: 0.4247
2022-01-06 14:46:45,519 Validation Data Eval:
2022-01-06 14:46:47,532   Average segmentation loss on validation set: 0.5383
2022-01-06 14:46:48,668 iteration 1870 : loss : 0.074491, loss_ce: 0.026515
 28%|███████▉                     | 110/400 [41:48<1:53:18, 23.44s/it]2022-01-06 14:46:49,911 iteration 1871 : loss : 0.058503, loss_ce: 0.026113
2022-01-06 14:46:51,054 iteration 1872 : loss : 0.062538, loss_ce: 0.021770
2022-01-06 14:46:52,255 iteration 1873 : loss : 0.078038, loss_ce: 0.034931
2022-01-06 14:46:53,498 iteration 1874 : loss : 0.072690, loss_ce: 0.033188
2022-01-06 14:46:54,784 iteration 1875 : loss : 0.052203, loss_ce: 0.024887
2022-01-06 14:46:56,063 iteration 1876 : loss : 0.070917, loss_ce: 0.021141
2022-01-06 14:46:57,311 iteration 1877 : loss : 0.052494, loss_ce: 0.017577
2022-01-06 14:46:58,441 iteration 1878 : loss : 0.059414, loss_ce: 0.021643
2022-01-06 14:46:59,598 iteration 1879 : loss : 0.063352, loss_ce: 0.028828
2022-01-06 14:47:00,747 iteration 1880 : loss : 0.054016, loss_ce: 0.018132
2022-01-06 14:47:01,901 iteration 1881 : loss : 0.047626, loss_ce: 0.015729
2022-01-06 14:47:03,112 iteration 1882 : loss : 0.085977, loss_ce: 0.021076
2022-01-06 14:47:04,231 iteration 1883 : loss : 0.053111, loss_ce: 0.021614
2022-01-06 14:47:05,504 iteration 1884 : loss : 0.064104, loss_ce: 0.023985
2022-01-06 14:47:06,639 iteration 1885 : loss : 0.049296, loss_ce: 0.017625
2022-01-06 14:47:07,860 iteration 1886 : loss : 0.074712, loss_ce: 0.029941
2022-01-06 14:47:09,013 iteration 1887 : loss : 0.087807, loss_ce: 0.039917
 28%|████████                     | 111/400 [42:09<1:48:27, 22.52s/it]2022-01-06 14:47:10,305 iteration 1888 : loss : 0.055406, loss_ce: 0.016638
2022-01-06 14:47:11,582 iteration 1889 : loss : 0.080173, loss_ce: 0.033842
2022-01-06 14:47:12,687 iteration 1890 : loss : 0.054182, loss_ce: 0.019480
2022-01-06 14:47:13,880 iteration 1891 : loss : 0.055106, loss_ce: 0.028182
2022-01-06 14:47:15,059 iteration 1892 : loss : 0.060983, loss_ce: 0.020993
2022-01-06 14:47:16,298 iteration 1893 : loss : 0.089738, loss_ce: 0.034090
2022-01-06 14:47:17,484 iteration 1894 : loss : 0.077909, loss_ce: 0.031631
2022-01-06 14:47:18,714 iteration 1895 : loss : 0.054051, loss_ce: 0.017587
2022-01-06 14:47:19,935 iteration 1896 : loss : 0.071676, loss_ce: 0.032742
2022-01-06 14:47:21,150 iteration 1897 : loss : 0.078695, loss_ce: 0.027409
2022-01-06 14:47:22,460 iteration 1898 : loss : 0.065879, loss_ce: 0.027154
2022-01-06 14:47:23,587 iteration 1899 : loss : 0.051538, loss_ce: 0.019838
2022-01-06 14:47:24,887 iteration 1900 : loss : 0.067544, loss_ce: 0.028267
2022-01-06 14:47:26,215 iteration 1901 : loss : 0.057540, loss_ce: 0.020780
2022-01-06 14:47:27,410 iteration 1902 : loss : 0.071798, loss_ce: 0.025195
2022-01-06 14:47:28,664 iteration 1903 : loss : 0.070738, loss_ce: 0.028048
2022-01-06 14:47:29,955 iteration 1904 : loss : 0.056142, loss_ce: 0.019467
 28%|████████                     | 112/400 [42:30<1:45:48, 22.05s/it]2022-01-06 14:47:31,276 iteration 1905 : loss : 0.069811, loss_ce: 0.028710
2022-01-06 14:47:32,490 iteration 1906 : loss : 0.061602, loss_ce: 0.018514
2022-01-06 14:47:33,686 iteration 1907 : loss : 0.066343, loss_ce: 0.026400
2022-01-06 14:47:35,041 iteration 1908 : loss : 0.076674, loss_ce: 0.022633
2022-01-06 14:47:36,282 iteration 1909 : loss : 0.058628, loss_ce: 0.022218
2022-01-06 14:47:37,531 iteration 1910 : loss : 0.127806, loss_ce: 0.028713
2022-01-06 14:47:38,817 iteration 1911 : loss : 0.057802, loss_ce: 0.020150
2022-01-06 14:47:39,957 iteration 1912 : loss : 0.062859, loss_ce: 0.021793
2022-01-06 14:47:41,181 iteration 1913 : loss : 0.068716, loss_ce: 0.032053
2022-01-06 14:47:42,408 iteration 1914 : loss : 0.062519, loss_ce: 0.030052
2022-01-06 14:47:43,657 iteration 1915 : loss : 0.049593, loss_ce: 0.017929
2022-01-06 14:47:44,867 iteration 1916 : loss : 0.079155, loss_ce: 0.031730
2022-01-06 14:47:46,072 iteration 1917 : loss : 0.066206, loss_ce: 0.028733
2022-01-06 14:47:47,310 iteration 1918 : loss : 0.063393, loss_ce: 0.025054
2022-01-06 14:47:48,469 iteration 1919 : loss : 0.065282, loss_ce: 0.025424
2022-01-06 14:47:49,632 iteration 1920 : loss : 0.067450, loss_ce: 0.028081
2022-01-06 14:47:50,921 iteration 1921 : loss : 0.098017, loss_ce: 0.046419
 28%|████████▏                    | 113/400 [42:51<1:43:54, 21.72s/it]2022-01-06 14:47:52,147 iteration 1922 : loss : 0.069822, loss_ce: 0.031230
2022-01-06 14:47:53,337 iteration 1923 : loss : 0.047108, loss_ce: 0.021739
2022-01-06 14:47:54,600 iteration 1924 : loss : 0.044286, loss_ce: 0.021421
2022-01-06 14:47:55,824 iteration 1925 : loss : 0.076394, loss_ce: 0.032192
2022-01-06 14:47:57,005 iteration 1926 : loss : 0.047199, loss_ce: 0.020950
2022-01-06 14:47:58,176 iteration 1927 : loss : 0.058600, loss_ce: 0.024731
2022-01-06 14:47:59,472 iteration 1928 : loss : 0.087637, loss_ce: 0.038532
2022-01-06 14:48:00,617 iteration 1929 : loss : 0.060289, loss_ce: 0.021392
2022-01-06 14:48:01,762 iteration 1930 : loss : 0.064160, loss_ce: 0.028911
2022-01-06 14:48:02,991 iteration 1931 : loss : 0.109315, loss_ce: 0.030572
2022-01-06 14:48:04,258 iteration 1932 : loss : 0.075918, loss_ce: 0.024618
2022-01-06 14:48:05,519 iteration 1933 : loss : 0.074216, loss_ce: 0.034060
2022-01-06 14:48:06,602 iteration 1934 : loss : 0.042056, loss_ce: 0.015511
2022-01-06 14:48:07,817 iteration 1935 : loss : 0.075009, loss_ce: 0.026616
2022-01-06 14:48:09,107 iteration 1936 : loss : 0.058632, loss_ce: 0.024504
2022-01-06 14:48:10,247 iteration 1937 : loss : 0.065299, loss_ce: 0.016850
2022-01-06 14:48:11,372 iteration 1938 : loss : 0.077323, loss_ce: 0.026945
 28%|████████▎                    | 114/400 [43:11<1:41:42, 21.34s/it]2022-01-06 14:48:12,610 iteration 1939 : loss : 0.038094, loss_ce: 0.012079
2022-01-06 14:48:13,925 iteration 1940 : loss : 0.052967, loss_ce: 0.022867
2022-01-06 14:48:15,219 iteration 1941 : loss : 0.058462, loss_ce: 0.020440
2022-01-06 14:48:16,440 iteration 1942 : loss : 0.083373, loss_ce: 0.027166
2022-01-06 14:48:17,615 iteration 1943 : loss : 0.065485, loss_ce: 0.026366
2022-01-06 14:48:18,818 iteration 1944 : loss : 0.052651, loss_ce: 0.017361
2022-01-06 14:48:19,960 iteration 1945 : loss : 0.067817, loss_ce: 0.023647
2022-01-06 14:48:21,164 iteration 1946 : loss : 0.060212, loss_ce: 0.023401
2022-01-06 14:48:22,404 iteration 1947 : loss : 0.074647, loss_ce: 0.029963
2022-01-06 14:48:23,602 iteration 1948 : loss : 0.105679, loss_ce: 0.030490
2022-01-06 14:48:24,765 iteration 1949 : loss : 0.061907, loss_ce: 0.022815
2022-01-06 14:48:25,890 iteration 1950 : loss : 0.040127, loss_ce: 0.016661
2022-01-06 14:48:27,161 iteration 1951 : loss : 0.076712, loss_ce: 0.029373
2022-01-06 14:48:28,394 iteration 1952 : loss : 0.072379, loss_ce: 0.023987
2022-01-06 14:48:29,666 iteration 1953 : loss : 0.079286, loss_ce: 0.041377
2022-01-06 14:48:30,887 iteration 1954 : loss : 0.039279, loss_ce: 0.018610
2022-01-06 14:48:30,887 Training Data Eval:
2022-01-06 14:48:36,740   Average segmentation loss on training set: 0.0688
2022-01-06 14:48:36,741 Validation Data Eval:
2022-01-06 14:48:38,723   Average segmentation loss on validation set: 0.0995
2022-01-06 14:48:44,599 Found new lowest validation loss at iteration 1954! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_MLP_best_val_loss_seed100.pth
2022-01-06 14:48:45,815 iteration 1955 : loss : 0.072402, loss_ce: 0.021867
 29%|████████▎                    | 115/400 [43:46<2:00:02, 25.27s/it]2022-01-06 14:48:46,996 iteration 1956 : loss : 0.104262, loss_ce: 0.027404
2022-01-06 14:48:48,259 iteration 1957 : loss : 0.087822, loss_ce: 0.039460
2022-01-06 14:48:49,525 iteration 1958 : loss : 0.067207, loss_ce: 0.023320
2022-01-06 14:48:50,703 iteration 1959 : loss : 0.050928, loss_ce: 0.017965
2022-01-06 14:48:51,780 iteration 1960 : loss : 0.058061, loss_ce: 0.024095
2022-01-06 14:48:52,967 iteration 1961 : loss : 0.050705, loss_ce: 0.020156
2022-01-06 14:48:54,168 iteration 1962 : loss : 0.061296, loss_ce: 0.025959
2022-01-06 14:48:55,260 iteration 1963 : loss : 0.062355, loss_ce: 0.022356
2022-01-06 14:48:56,354 iteration 1964 : loss : 0.069449, loss_ce: 0.030037
2022-01-06 14:48:57,456 iteration 1965 : loss : 0.055606, loss_ce: 0.024003
2022-01-06 14:48:58,690 iteration 1966 : loss : 0.075267, loss_ce: 0.030576
2022-01-06 14:48:59,938 iteration 1967 : loss : 0.055452, loss_ce: 0.021335
2022-01-06 14:49:01,156 iteration 1968 : loss : 0.078082, loss_ce: 0.033352
2022-01-06 14:49:02,353 iteration 1969 : loss : 0.062389, loss_ce: 0.024784
2022-01-06 14:49:03,496 iteration 1970 : loss : 0.076127, loss_ce: 0.028174
2022-01-06 14:49:04,732 iteration 1971 : loss : 0.052863, loss_ce: 0.023152
2022-01-06 14:49:05,893 iteration 1972 : loss : 0.084570, loss_ce: 0.027202
 29%|████████▍                    | 116/400 [44:06<1:52:13, 23.71s/it]2022-01-06 14:49:07,008 iteration 1973 : loss : 0.050017, loss_ce: 0.019741
2022-01-06 14:49:08,232 iteration 1974 : loss : 0.074506, loss_ce: 0.027961
2022-01-06 14:49:09,412 iteration 1975 : loss : 0.060114, loss_ce: 0.022084
2022-01-06 14:49:10,678 iteration 1976 : loss : 0.059261, loss_ce: 0.025825
2022-01-06 14:49:11,865 iteration 1977 : loss : 0.066120, loss_ce: 0.018736
2022-01-06 14:49:12,962 iteration 1978 : loss : 0.038313, loss_ce: 0.016897
2022-01-06 14:49:14,062 iteration 1979 : loss : 0.071364, loss_ce: 0.026526
2022-01-06 14:49:15,278 iteration 1980 : loss : 0.076754, loss_ce: 0.036781
2022-01-06 14:49:16,424 iteration 1981 : loss : 0.072662, loss_ce: 0.024094
2022-01-06 14:49:17,569 iteration 1982 : loss : 0.068341, loss_ce: 0.026077
2022-01-06 14:49:18,832 iteration 1983 : loss : 0.061100, loss_ce: 0.028780
2022-01-06 14:49:19,944 iteration 1984 : loss : 0.069061, loss_ce: 0.019784
2022-01-06 14:49:21,153 iteration 1985 : loss : 0.045413, loss_ce: 0.018462
2022-01-06 14:49:22,367 iteration 1986 : loss : 0.073833, loss_ce: 0.034486
2022-01-06 14:49:23,612 iteration 1987 : loss : 0.043525, loss_ce: 0.017614
2022-01-06 14:49:24,734 iteration 1988 : loss : 0.084027, loss_ce: 0.030460
2022-01-06 14:49:25,918 iteration 1989 : loss : 0.055356, loss_ce: 0.025113
 29%|████████▍                    | 117/400 [44:26<1:46:37, 22.61s/it]2022-01-06 14:49:27,079 iteration 1990 : loss : 0.043829, loss_ce: 0.017926
2022-01-06 14:49:28,220 iteration 1991 : loss : 0.039513, loss_ce: 0.014798
2022-01-06 14:49:29,445 iteration 1992 : loss : 0.063665, loss_ce: 0.022433
2022-01-06 14:49:30,785 iteration 1993 : loss : 0.062781, loss_ce: 0.029932
2022-01-06 14:49:31,907 iteration 1994 : loss : 0.052271, loss_ce: 0.021343
2022-01-06 14:49:33,095 iteration 1995 : loss : 0.100001, loss_ce: 0.025939
2022-01-06 14:49:34,298 iteration 1996 : loss : 0.060893, loss_ce: 0.018516
2022-01-06 14:49:35,487 iteration 1997 : loss : 0.050132, loss_ce: 0.016048
2022-01-06 14:49:36,669 iteration 1998 : loss : 0.058016, loss_ce: 0.019684
2022-01-06 14:49:37,831 iteration 1999 : loss : 0.066285, loss_ce: 0.028830
2022-01-06 14:49:39,102 iteration 2000 : loss : 0.083600, loss_ce: 0.022578
2022-01-06 14:49:40,270 iteration 2001 : loss : 0.046307, loss_ce: 0.019239
2022-01-06 14:49:41,541 iteration 2002 : loss : 0.062143, loss_ce: 0.027361
2022-01-06 14:49:42,675 iteration 2003 : loss : 0.089434, loss_ce: 0.041233
2022-01-06 14:49:43,894 iteration 2004 : loss : 0.090513, loss_ce: 0.027885
2022-01-06 14:49:45,132 iteration 2005 : loss : 0.080772, loss_ce: 0.029748
2022-01-06 14:49:46,356 iteration 2006 : loss : 0.082690, loss_ce: 0.031231
 30%|████████▌                    | 118/400 [44:46<1:43:11, 21.96s/it]2022-01-06 14:49:47,638 iteration 2007 : loss : 0.055367, loss_ce: 0.023521
2022-01-06 14:49:48,770 iteration 2008 : loss : 0.052148, loss_ce: 0.020072
2022-01-06 14:49:49,929 iteration 2009 : loss : 0.090485, loss_ce: 0.030919
2022-01-06 14:49:51,049 iteration 2010 : loss : 0.074523, loss_ce: 0.027362
2022-01-06 14:49:52,225 iteration 2011 : loss : 0.071268, loss_ce: 0.029707
2022-01-06 14:49:53,435 iteration 2012 : loss : 0.106832, loss_ce: 0.032483
2022-01-06 14:49:54,682 iteration 2013 : loss : 0.060008, loss_ce: 0.022770
2022-01-06 14:49:55,896 iteration 2014 : loss : 0.042832, loss_ce: 0.015791
2022-01-06 14:49:56,989 iteration 2015 : loss : 0.060932, loss_ce: 0.022048
2022-01-06 14:49:58,162 iteration 2016 : loss : 0.062289, loss_ce: 0.020477
2022-01-06 14:49:59,238 iteration 2017 : loss : 0.042503, loss_ce: 0.016867
2022-01-06 14:50:00,433 iteration 2018 : loss : 0.076471, loss_ce: 0.029671
2022-01-06 14:50:01,559 iteration 2019 : loss : 0.043047, loss_ce: 0.018365
2022-01-06 14:50:02,763 iteration 2020 : loss : 0.066537, loss_ce: 0.026178
2022-01-06 14:50:03,997 iteration 2021 : loss : 0.035447, loss_ce: 0.014082
2022-01-06 14:50:05,258 iteration 2022 : loss : 0.060243, loss_ce: 0.030012
2022-01-06 14:50:06,437 iteration 2023 : loss : 0.076037, loss_ce: 0.021514
 30%|████████▋                    | 119/400 [45:06<1:40:10, 21.39s/it]2022-01-06 14:50:07,729 iteration 2024 : loss : 0.060227, loss_ce: 0.024096
2022-01-06 14:50:08,851 iteration 2025 : loss : 0.046278, loss_ce: 0.015368
2022-01-06 14:50:09,994 iteration 2026 : loss : 0.072738, loss_ce: 0.029351
2022-01-06 14:50:11,225 iteration 2027 : loss : 0.077352, loss_ce: 0.025795
2022-01-06 14:50:12,432 iteration 2028 : loss : 0.052606, loss_ce: 0.020777
2022-01-06 14:50:13,541 iteration 2029 : loss : 0.057728, loss_ce: 0.026826
2022-01-06 14:50:14,739 iteration 2030 : loss : 0.059355, loss_ce: 0.023329
2022-01-06 14:50:15,945 iteration 2031 : loss : 0.052909, loss_ce: 0.018694
2022-01-06 14:50:17,164 iteration 2032 : loss : 0.068680, loss_ce: 0.024091
2022-01-06 14:50:18,340 iteration 2033 : loss : 0.095988, loss_ce: 0.029320
2022-01-06 14:50:19,449 iteration 2034 : loss : 0.041032, loss_ce: 0.013271
2022-01-06 14:50:20,563 iteration 2035 : loss : 0.051786, loss_ce: 0.027501
2022-01-06 14:50:21,789 iteration 2036 : loss : 0.054100, loss_ce: 0.022193
2022-01-06 14:50:22,975 iteration 2037 : loss : 0.076030, loss_ce: 0.027739
2022-01-06 14:50:24,112 iteration 2038 : loss : 0.052445, loss_ce: 0.018842
2022-01-06 14:50:25,323 iteration 2039 : loss : 0.064812, loss_ce: 0.029734
2022-01-06 14:50:25,324 Training Data Eval:
2022-01-06 14:50:31,172   Average segmentation loss on training set: 0.0674
2022-01-06 14:50:31,173 Validation Data Eval:
2022-01-06 14:50:33,194   Average segmentation loss on validation set: 0.1224
2022-01-06 14:50:34,362 iteration 2040 : loss : 0.063506, loss_ce: 0.020404
 30%|████████▋                    | 120/400 [45:34<1:48:58, 23.35s/it]2022-01-06 14:50:35,607 iteration 2041 : loss : 0.058827, loss_ce: 0.024664
2022-01-06 14:50:36,768 iteration 2042 : loss : 0.036768, loss_ce: 0.016345
2022-01-06 14:50:37,955 iteration 2043 : loss : 0.045081, loss_ce: 0.020769
2022-01-06 14:50:39,204 iteration 2044 : loss : 0.049007, loss_ce: 0.020967
2022-01-06 14:50:40,478 iteration 2045 : loss : 0.068016, loss_ce: 0.027053
2022-01-06 14:50:41,698 iteration 2046 : loss : 0.076920, loss_ce: 0.029104
2022-01-06 14:50:42,907 iteration 2047 : loss : 0.068018, loss_ce: 0.025287
2022-01-06 14:50:44,022 iteration 2048 : loss : 0.058595, loss_ce: 0.018623
2022-01-06 14:50:45,192 iteration 2049 : loss : 0.042446, loss_ce: 0.015928
2022-01-06 14:50:46,423 iteration 2050 : loss : 0.068676, loss_ce: 0.025555
2022-01-06 14:50:47,656 iteration 2051 : loss : 0.072671, loss_ce: 0.028755
2022-01-06 14:50:48,807 iteration 2052 : loss : 0.053843, loss_ce: 0.024083
2022-01-06 14:50:49,951 iteration 2053 : loss : 0.053045, loss_ce: 0.019382
2022-01-06 14:50:51,156 iteration 2054 : loss : 0.080611, loss_ce: 0.023412
2022-01-06 14:50:52,367 iteration 2055 : loss : 0.072409, loss_ce: 0.032516
2022-01-06 14:50:53,570 iteration 2056 : loss : 0.057607, loss_ce: 0.023045
2022-01-06 14:50:54,805 iteration 2057 : loss : 0.043383, loss_ce: 0.018376
 30%|████████▊                    | 121/400 [45:55<1:44:32, 22.48s/it]2022-01-06 14:50:56,150 iteration 2058 : loss : 0.050033, loss_ce: 0.022369
2022-01-06 14:50:57,358 iteration 2059 : loss : 0.073097, loss_ce: 0.025818
2022-01-06 14:50:58,595 iteration 2060 : loss : 0.082336, loss_ce: 0.029954
2022-01-06 14:50:59,736 iteration 2061 : loss : 0.048609, loss_ce: 0.023237
2022-01-06 14:51:00,856 iteration 2062 : loss : 0.050725, loss_ce: 0.016389
2022-01-06 14:51:01,997 iteration 2063 : loss : 0.041842, loss_ce: 0.016575
2022-01-06 14:51:03,208 iteration 2064 : loss : 0.048569, loss_ce: 0.024067
2022-01-06 14:51:04,421 iteration 2065 : loss : 0.070999, loss_ce: 0.023810
2022-01-06 14:51:05,677 iteration 2066 : loss : 0.076140, loss_ce: 0.024730
2022-01-06 14:51:06,841 iteration 2067 : loss : 0.056255, loss_ce: 0.021302
2022-01-06 14:51:08,106 iteration 2068 : loss : 0.055281, loss_ce: 0.027060
2022-01-06 14:51:09,366 iteration 2069 : loss : 0.064836, loss_ce: 0.020858
2022-01-06 14:51:10,523 iteration 2070 : loss : 0.050898, loss_ce: 0.017010
2022-01-06 14:51:11,695 iteration 2071 : loss : 0.059538, loss_ce: 0.026189
2022-01-06 14:51:12,813 iteration 2072 : loss : 0.051511, loss_ce: 0.022522
2022-01-06 14:51:13,935 iteration 2073 : loss : 0.066208, loss_ce: 0.022359
2022-01-06 14:51:15,133 iteration 2074 : loss : 0.082761, loss_ce: 0.024889
 30%|████████▊                    | 122/400 [46:15<1:41:10, 21.84s/it]2022-01-06 14:51:16,400 iteration 2075 : loss : 0.058579, loss_ce: 0.022542
2022-01-06 14:51:17,649 iteration 2076 : loss : 0.085530, loss_ce: 0.026342
2022-01-06 14:51:18,843 iteration 2077 : loss : 0.056035, loss_ce: 0.018727
2022-01-06 14:51:19,967 iteration 2078 : loss : 0.051497, loss_ce: 0.020731
2022-01-06 14:51:21,141 iteration 2079 : loss : 0.058808, loss_ce: 0.021972
2022-01-06 14:51:22,406 iteration 2080 : loss : 0.061478, loss_ce: 0.022506
2022-01-06 14:51:23,630 iteration 2081 : loss : 0.061901, loss_ce: 0.025268
2022-01-06 14:51:24,835 iteration 2082 : loss : 0.072413, loss_ce: 0.030692
2022-01-06 14:51:26,017 iteration 2083 : loss : 0.053677, loss_ce: 0.018945
2022-01-06 14:51:27,208 iteration 2084 : loss : 0.063197, loss_ce: 0.023044
2022-01-06 14:51:28,370 iteration 2085 : loss : 0.065004, loss_ce: 0.026801
2022-01-06 14:51:29,591 iteration 2086 : loss : 0.046636, loss_ce: 0.020918
2022-01-06 14:51:30,734 iteration 2087 : loss : 0.069106, loss_ce: 0.021188
2022-01-06 14:51:32,066 iteration 2088 : loss : 0.072868, loss_ce: 0.031089
2022-01-06 14:51:33,240 iteration 2089 : loss : 0.042536, loss_ce: 0.015092
2022-01-06 14:51:34,486 iteration 2090 : loss : 0.066308, loss_ce: 0.021846
2022-01-06 14:51:35,679 iteration 2091 : loss : 0.045762, loss_ce: 0.017972
 31%|████████▉                    | 123/400 [46:35<1:39:00, 21.44s/it]2022-01-06 14:51:36,916 iteration 2092 : loss : 0.042008, loss_ce: 0.016886
2022-01-06 14:51:38,206 iteration 2093 : loss : 0.049662, loss_ce: 0.022577
2022-01-06 14:51:39,433 iteration 2094 : loss : 0.051051, loss_ce: 0.023311
2022-01-06 14:51:40,666 iteration 2095 : loss : 0.056953, loss_ce: 0.023092
2022-01-06 14:51:41,929 iteration 2096 : loss : 0.062543, loss_ce: 0.022833
2022-01-06 14:51:43,072 iteration 2097 : loss : 0.045504, loss_ce: 0.020009
2022-01-06 14:51:44,332 iteration 2098 : loss : 0.056876, loss_ce: 0.023510
2022-01-06 14:51:45,586 iteration 2099 : loss : 0.073843, loss_ce: 0.026979
2022-01-06 14:51:46,902 iteration 2100 : loss : 0.095709, loss_ce: 0.043996
2022-01-06 14:51:48,102 iteration 2101 : loss : 0.062228, loss_ce: 0.018541
2022-01-06 14:51:49,245 iteration 2102 : loss : 0.054025, loss_ce: 0.021779
2022-01-06 14:51:50,556 iteration 2103 : loss : 0.058472, loss_ce: 0.022348
2022-01-06 14:51:51,795 iteration 2104 : loss : 0.048715, loss_ce: 0.019769
2022-01-06 14:51:52,954 iteration 2105 : loss : 0.062046, loss_ce: 0.020290
2022-01-06 14:51:54,250 iteration 2106 : loss : 0.074867, loss_ce: 0.019556
2022-01-06 14:51:55,462 iteration 2107 : loss : 0.059791, loss_ce: 0.023341
2022-01-06 14:51:56,693 iteration 2108 : loss : 0.053070, loss_ce: 0.018366
 31%|████████▉                    | 124/400 [46:57<1:38:03, 21.32s/it]2022-01-06 14:51:57,962 iteration 2109 : loss : 0.076546, loss_ce: 0.031443
2022-01-06 14:51:59,144 iteration 2110 : loss : 0.063450, loss_ce: 0.018852
2022-01-06 14:52:00,496 iteration 2111 : loss : 0.064291, loss_ce: 0.020444
2022-01-06 14:52:01,850 iteration 2112 : loss : 0.046916, loss_ce: 0.019157
2022-01-06 14:52:03,015 iteration 2113 : loss : 0.056328, loss_ce: 0.021408
2022-01-06 14:52:04,197 iteration 2114 : loss : 0.058445, loss_ce: 0.021504
2022-01-06 14:52:05,277 iteration 2115 : loss : 0.078880, loss_ce: 0.023088
2022-01-06 14:52:06,522 iteration 2116 : loss : 0.043701, loss_ce: 0.016125
2022-01-06 14:52:07,810 iteration 2117 : loss : 0.059005, loss_ce: 0.019312
2022-01-06 14:52:08,999 iteration 2118 : loss : 0.073459, loss_ce: 0.033060
2022-01-06 14:52:10,227 iteration 2119 : loss : 0.075925, loss_ce: 0.020624
2022-01-06 14:52:11,429 iteration 2120 : loss : 0.068521, loss_ce: 0.024339
2022-01-06 14:52:12,740 iteration 2121 : loss : 0.067626, loss_ce: 0.026873
2022-01-06 14:52:13,874 iteration 2122 : loss : 0.055137, loss_ce: 0.024394
2022-01-06 14:52:15,040 iteration 2123 : loss : 0.064171, loss_ce: 0.038254
2022-01-06 14:52:16,282 iteration 2124 : loss : 0.070329, loss_ce: 0.025658
2022-01-06 14:52:16,282 Training Data Eval:
2022-01-06 14:52:22,101   Average segmentation loss on training set: 0.0896
2022-01-06 14:52:22,102 Validation Data Eval:
2022-01-06 14:52:24,117   Average segmentation loss on validation set: 0.1149
2022-01-06 14:52:25,324 iteration 2125 : loss : 0.035435, loss_ce: 0.011888
 31%|█████████                    | 125/400 [47:25<1:47:46, 23.51s/it]2022-01-06 14:52:26,575 iteration 2126 : loss : 0.040600, loss_ce: 0.014716
2022-01-06 14:52:27,843 iteration 2127 : loss : 0.046701, loss_ce: 0.019259
2022-01-06 14:52:28,964 iteration 2128 : loss : 0.047123, loss_ce: 0.023204
2022-01-06 14:52:30,168 iteration 2129 : loss : 0.069156, loss_ce: 0.027625
2022-01-06 14:52:31,301 iteration 2130 : loss : 0.082533, loss_ce: 0.026556
2022-01-06 14:52:32,591 iteration 2131 : loss : 0.093786, loss_ce: 0.039903
2022-01-06 14:52:33,861 iteration 2132 : loss : 0.050058, loss_ce: 0.020696
2022-01-06 14:52:35,090 iteration 2133 : loss : 0.070678, loss_ce: 0.027611
2022-01-06 14:52:36,322 iteration 2134 : loss : 0.097614, loss_ce: 0.032449
2022-01-06 14:52:37,529 iteration 2135 : loss : 0.061571, loss_ce: 0.022074
2022-01-06 14:52:38,732 iteration 2136 : loss : 0.060058, loss_ce: 0.023896
2022-01-06 14:52:39,928 iteration 2137 : loss : 0.087609, loss_ce: 0.025227
2022-01-06 14:52:41,080 iteration 2138 : loss : 0.100479, loss_ce: 0.028487
2022-01-06 14:52:42,192 iteration 2139 : loss : 0.057375, loss_ce: 0.028691
2022-01-06 14:52:43,357 iteration 2140 : loss : 0.058203, loss_ce: 0.018385
2022-01-06 14:52:44,561 iteration 2141 : loss : 0.066873, loss_ce: 0.026648
2022-01-06 14:52:45,762 iteration 2142 : loss : 0.094019, loss_ce: 0.051918
 32%|█████████▏                   | 126/400 [47:46<1:43:10, 22.59s/it]2022-01-06 14:52:47,052 iteration 2143 : loss : 0.051722, loss_ce: 0.014137
2022-01-06 14:52:48,225 iteration 2144 : loss : 0.090022, loss_ce: 0.024831
2022-01-06 14:52:49,322 iteration 2145 : loss : 0.040509, loss_ce: 0.018925
2022-01-06 14:52:50,695 iteration 2146 : loss : 0.060886, loss_ce: 0.022758
2022-01-06 14:52:51,911 iteration 2147 : loss : 0.047365, loss_ce: 0.017976
2022-01-06 14:52:53,106 iteration 2148 : loss : 0.059559, loss_ce: 0.023440
2022-01-06 14:52:54,316 iteration 2149 : loss : 0.091952, loss_ce: 0.030299
2022-01-06 14:52:55,564 iteration 2150 : loss : 0.072077, loss_ce: 0.026290
2022-01-06 14:52:56,815 iteration 2151 : loss : 0.050690, loss_ce: 0.023153
2022-01-06 14:52:57,943 iteration 2152 : loss : 0.041758, loss_ce: 0.017355
2022-01-06 14:52:59,110 iteration 2153 : loss : 0.063656, loss_ce: 0.020905
2022-01-06 14:53:00,233 iteration 2154 : loss : 0.052384, loss_ce: 0.022648
2022-01-06 14:53:01,384 iteration 2155 : loss : 0.048080, loss_ce: 0.019022
2022-01-06 14:53:02,621 iteration 2156 : loss : 0.070899, loss_ce: 0.025871
2022-01-06 14:53:03,836 iteration 2157 : loss : 0.049902, loss_ce: 0.021299
2022-01-06 14:53:05,003 iteration 2158 : loss : 0.062018, loss_ce: 0.023947
2022-01-06 14:53:06,250 iteration 2159 : loss : 0.089742, loss_ce: 0.028332
 32%|█████████▏                   | 127/400 [48:06<1:39:55, 21.96s/it]2022-01-06 14:53:07,418 iteration 2160 : loss : 0.044276, loss_ce: 0.014493
2022-01-06 14:53:08,622 iteration 2161 : loss : 0.047949, loss_ce: 0.018470
2022-01-06 14:53:09,810 iteration 2162 : loss : 0.053353, loss_ce: 0.021924
2022-01-06 14:53:10,889 iteration 2163 : loss : 0.077142, loss_ce: 0.025359
2022-01-06 14:53:12,097 iteration 2164 : loss : 0.047260, loss_ce: 0.019775
2022-01-06 14:53:13,290 iteration 2165 : loss : 0.038830, loss_ce: 0.012956
2022-01-06 14:53:14,553 iteration 2166 : loss : 0.080442, loss_ce: 0.036504
2022-01-06 14:53:15,748 iteration 2167 : loss : 0.067691, loss_ce: 0.033331
2022-01-06 14:53:16,936 iteration 2168 : loss : 0.050461, loss_ce: 0.018337
2022-01-06 14:53:18,154 iteration 2169 : loss : 0.045185, loss_ce: 0.017940
2022-01-06 14:53:19,318 iteration 2170 : loss : 0.047589, loss_ce: 0.015638
2022-01-06 14:53:20,493 iteration 2171 : loss : 0.033289, loss_ce: 0.010839
2022-01-06 14:53:21,567 iteration 2172 : loss : 0.053284, loss_ce: 0.026274
2022-01-06 14:53:22,755 iteration 2173 : loss : 0.057653, loss_ce: 0.018386
2022-01-06 14:53:23,975 iteration 2174 : loss : 0.060059, loss_ce: 0.025219
2022-01-06 14:53:25,095 iteration 2175 : loss : 0.060159, loss_ce: 0.019941
2022-01-06 14:53:26,254 iteration 2176 : loss : 0.053504, loss_ce: 0.024270
 32%|█████████▎                   | 128/400 [48:26<1:36:52, 21.37s/it]2022-01-06 14:53:27,573 iteration 2177 : loss : 0.096790, loss_ce: 0.029689
2022-01-06 14:53:28,698 iteration 2178 : loss : 0.043636, loss_ce: 0.019045
2022-01-06 14:53:29,872 iteration 2179 : loss : 0.051946, loss_ce: 0.021889
2022-01-06 14:53:31,110 iteration 2180 : loss : 0.041621, loss_ce: 0.013418
2022-01-06 14:53:32,287 iteration 2181 : loss : 0.044870, loss_ce: 0.014628
2022-01-06 14:53:33,465 iteration 2182 : loss : 0.051893, loss_ce: 0.017235
2022-01-06 14:53:34,624 iteration 2183 : loss : 0.058247, loss_ce: 0.018406
2022-01-06 14:53:35,756 iteration 2184 : loss : 0.051595, loss_ce: 0.023010
2022-01-06 14:53:36,883 iteration 2185 : loss : 0.058712, loss_ce: 0.023962
2022-01-06 14:53:38,213 iteration 2186 : loss : 0.098515, loss_ce: 0.040546
2022-01-06 14:53:39,333 iteration 2187 : loss : 0.054135, loss_ce: 0.029514
2022-01-06 14:53:40,602 iteration 2188 : loss : 0.051014, loss_ce: 0.020756
2022-01-06 14:53:41,730 iteration 2189 : loss : 0.070685, loss_ce: 0.023419
2022-01-06 14:53:42,979 iteration 2190 : loss : 0.058068, loss_ce: 0.023438
2022-01-06 14:53:44,081 iteration 2191 : loss : 0.044496, loss_ce: 0.019798
2022-01-06 14:53:45,262 iteration 2192 : loss : 0.071396, loss_ce: 0.029478
2022-01-06 14:53:46,533 iteration 2193 : loss : 0.055099, loss_ce: 0.022847
 32%|█████████▎                   | 129/400 [48:46<1:35:03, 21.05s/it]2022-01-06 14:53:47,737 iteration 2194 : loss : 0.068232, loss_ce: 0.031764
2022-01-06 14:53:48,877 iteration 2195 : loss : 0.045060, loss_ce: 0.017328
2022-01-06 14:53:50,080 iteration 2196 : loss : 0.054609, loss_ce: 0.022802
2022-01-06 14:53:51,320 iteration 2197 : loss : 0.091702, loss_ce: 0.029394
2022-01-06 14:53:52,480 iteration 2198 : loss : 0.050280, loss_ce: 0.022834
2022-01-06 14:53:53,842 iteration 2199 : loss : 0.063000, loss_ce: 0.027645
2022-01-06 14:53:55,008 iteration 2200 : loss : 0.049642, loss_ce: 0.020216
2022-01-06 14:53:56,192 iteration 2201 : loss : 0.076847, loss_ce: 0.026493
2022-01-06 14:53:57,365 iteration 2202 : loss : 0.121118, loss_ce: 0.039287
2022-01-06 14:53:58,591 iteration 2203 : loss : 0.039273, loss_ce: 0.012456
2022-01-06 14:53:59,825 iteration 2204 : loss : 0.052046, loss_ce: 0.020426
2022-01-06 14:54:00,887 iteration 2205 : loss : 0.049824, loss_ce: 0.020299
2022-01-06 14:54:01,994 iteration 2206 : loss : 0.039361, loss_ce: 0.017097
2022-01-06 14:54:03,187 iteration 2207 : loss : 0.062076, loss_ce: 0.027438
2022-01-06 14:54:04,352 iteration 2208 : loss : 0.057722, loss_ce: 0.025197
2022-01-06 14:54:05,604 iteration 2209 : loss : 0.052746, loss_ce: 0.019030
2022-01-06 14:54:05,604 Training Data Eval:
2022-01-06 14:54:11,477   Average segmentation loss on training set: 0.1235
2022-01-06 14:54:11,477 Validation Data Eval:
2022-01-06 14:54:13,486   Average segmentation loss on validation set: 0.2606
2022-01-06 14:54:14,708 iteration 2210 : loss : 0.059474, loss_ce: 0.025251
 32%|█████████▍                   | 130/400 [49:15<1:44:19, 23.18s/it]2022-01-06 14:54:15,964 iteration 2211 : loss : 0.046144, loss_ce: 0.022272
2022-01-06 14:54:17,057 iteration 2212 : loss : 0.037214, loss_ce: 0.012676
2022-01-06 14:54:18,180 iteration 2213 : loss : 0.044708, loss_ce: 0.016796
2022-01-06 14:54:19,392 iteration 2214 : loss : 0.044138, loss_ce: 0.019008
2022-01-06 14:54:20,569 iteration 2215 : loss : 0.036190, loss_ce: 0.014040
2022-01-06 14:54:21,714 iteration 2216 : loss : 0.052121, loss_ce: 0.024384
2022-01-06 14:54:22,817 iteration 2217 : loss : 0.054326, loss_ce: 0.022994
2022-01-06 14:54:24,023 iteration 2218 : loss : 0.056535, loss_ce: 0.023433
2022-01-06 14:54:25,189 iteration 2219 : loss : 0.046523, loss_ce: 0.019507
2022-01-06 14:54:26,387 iteration 2220 : loss : 0.048638, loss_ce: 0.022808
2022-01-06 14:54:27,652 iteration 2221 : loss : 0.058499, loss_ce: 0.024543
2022-01-06 14:54:28,790 iteration 2222 : loss : 0.069352, loss_ce: 0.021604
2022-01-06 14:54:30,118 iteration 2223 : loss : 0.074071, loss_ce: 0.025250
2022-01-06 14:54:31,269 iteration 2224 : loss : 0.111982, loss_ce: 0.025976
2022-01-06 14:54:32,444 iteration 2225 : loss : 0.061489, loss_ce: 0.024153
2022-01-06 14:54:33,785 iteration 2226 : loss : 0.084296, loss_ce: 0.029415
2022-01-06 14:54:35,020 iteration 2227 : loss : 0.072783, loss_ce: 0.024528
 33%|█████████▍                   | 131/400 [49:35<1:40:04, 22.32s/it]2022-01-06 14:54:36,305 iteration 2228 : loss : 0.087995, loss_ce: 0.032385
2022-01-06 14:54:37,500 iteration 2229 : loss : 0.062674, loss_ce: 0.027344
2022-01-06 14:54:38,639 iteration 2230 : loss : 0.066225, loss_ce: 0.024074
2022-01-06 14:54:39,766 iteration 2231 : loss : 0.040949, loss_ce: 0.018725
2022-01-06 14:54:40,899 iteration 2232 : loss : 0.041198, loss_ce: 0.016077
2022-01-06 14:54:42,047 iteration 2233 : loss : 0.049349, loss_ce: 0.015223
2022-01-06 14:54:43,270 iteration 2234 : loss : 0.091136, loss_ce: 0.030461
2022-01-06 14:54:44,376 iteration 2235 : loss : 0.064812, loss_ce: 0.030769
2022-01-06 14:54:45,520 iteration 2236 : loss : 0.063263, loss_ce: 0.023520
2022-01-06 14:54:46,703 iteration 2237 : loss : 0.056121, loss_ce: 0.023396
2022-01-06 14:54:47,854 iteration 2238 : loss : 0.089170, loss_ce: 0.028851
2022-01-06 14:54:49,069 iteration 2239 : loss : 0.066639, loss_ce: 0.026470
2022-01-06 14:54:50,176 iteration 2240 : loss : 0.049995, loss_ce: 0.019933
2022-01-06 14:54:51,401 iteration 2241 : loss : 0.060626, loss_ce: 0.022457
2022-01-06 14:54:52,512 iteration 2242 : loss : 0.057612, loss_ce: 0.023923
2022-01-06 14:54:53,632 iteration 2243 : loss : 0.077324, loss_ce: 0.021239
2022-01-06 14:54:54,793 iteration 2244 : loss : 0.043946, loss_ce: 0.016922
 33%|█████████▌                   | 132/400 [49:55<1:36:17, 21.56s/it]2022-01-06 14:54:55,990 iteration 2245 : loss : 0.045611, loss_ce: 0.018711
2022-01-06 14:54:57,180 iteration 2246 : loss : 0.093033, loss_ce: 0.031103
2022-01-06 14:54:58,346 iteration 2247 : loss : 0.078565, loss_ce: 0.030100
2022-01-06 14:54:59,405 iteration 2248 : loss : 0.037223, loss_ce: 0.016008
2022-01-06 14:55:00,650 iteration 2249 : loss : 0.054338, loss_ce: 0.021964
2022-01-06 14:55:01,964 iteration 2250 : loss : 0.051306, loss_ce: 0.022130
2022-01-06 14:55:03,060 iteration 2251 : loss : 0.057658, loss_ce: 0.024412
2022-01-06 14:55:04,274 iteration 2252 : loss : 0.047056, loss_ce: 0.016710
2022-01-06 14:55:05,393 iteration 2253 : loss : 0.067386, loss_ce: 0.022156
2022-01-06 14:55:06,631 iteration 2254 : loss : 0.065200, loss_ce: 0.026325
2022-01-06 14:55:07,856 iteration 2255 : loss : 0.057729, loss_ce: 0.026293
2022-01-06 14:55:09,125 iteration 2256 : loss : 0.065092, loss_ce: 0.026484
2022-01-06 14:55:10,277 iteration 2257 : loss : 0.070789, loss_ce: 0.022074
2022-01-06 14:55:11,450 iteration 2258 : loss : 0.059131, loss_ce: 0.022985
2022-01-06 14:55:12,683 iteration 2259 : loss : 0.054729, loss_ce: 0.020062
2022-01-06 14:55:13,884 iteration 2260 : loss : 0.073470, loss_ce: 0.027421
2022-01-06 14:55:15,024 iteration 2261 : loss : 0.078927, loss_ce: 0.021828
 33%|█████████▋                   | 133/400 [50:15<1:34:10, 21.16s/it]2022-01-06 14:55:16,432 iteration 2262 : loss : 0.054331, loss_ce: 0.022001
2022-01-06 14:55:17,623 iteration 2263 : loss : 0.047446, loss_ce: 0.018176
2022-01-06 14:55:18,835 iteration 2264 : loss : 0.071451, loss_ce: 0.023892
2022-01-06 14:55:20,051 iteration 2265 : loss : 0.049131, loss_ce: 0.018860
2022-01-06 14:55:21,227 iteration 2266 : loss : 0.053571, loss_ce: 0.018431
2022-01-06 14:55:22,453 iteration 2267 : loss : 0.073992, loss_ce: 0.023228
2022-01-06 14:55:23,562 iteration 2268 : loss : 0.071805, loss_ce: 0.028475
2022-01-06 14:55:24,775 iteration 2269 : loss : 0.067766, loss_ce: 0.028951
2022-01-06 14:55:25,938 iteration 2270 : loss : 0.064767, loss_ce: 0.022183
2022-01-06 14:55:27,089 iteration 2271 : loss : 0.083117, loss_ce: 0.046058
2022-01-06 14:55:28,211 iteration 2272 : loss : 0.073290, loss_ce: 0.018750
2022-01-06 14:55:29,442 iteration 2273 : loss : 0.075093, loss_ce: 0.022838
2022-01-06 14:55:30,553 iteration 2274 : loss : 0.040784, loss_ce: 0.011985
2022-01-06 14:55:31,717 iteration 2275 : loss : 0.051106, loss_ce: 0.022408
2022-01-06 14:55:32,913 iteration 2276 : loss : 0.073408, loss_ce: 0.025853
2022-01-06 14:55:34,072 iteration 2277 : loss : 0.047775, loss_ce: 0.024128
2022-01-06 14:55:35,248 iteration 2278 : loss : 0.051249, loss_ce: 0.024077
 34%|█████████▋                   | 134/400 [50:35<1:32:34, 20.88s/it]2022-01-06 14:55:36,627 iteration 2279 : loss : 0.077942, loss_ce: 0.036714
2022-01-06 14:55:37,836 iteration 2280 : loss : 0.059527, loss_ce: 0.030396
2022-01-06 14:55:38,933 iteration 2281 : loss : 0.064674, loss_ce: 0.026205
2022-01-06 14:55:40,240 iteration 2282 : loss : 0.051502, loss_ce: 0.019479
2022-01-06 14:55:41,359 iteration 2283 : loss : 0.055232, loss_ce: 0.020036
2022-01-06 14:55:42,642 iteration 2284 : loss : 0.100602, loss_ce: 0.043866
2022-01-06 14:55:43,765 iteration 2285 : loss : 0.051753, loss_ce: 0.020686
2022-01-06 14:55:44,921 iteration 2286 : loss : 0.038698, loss_ce: 0.016843
2022-01-06 14:55:46,143 iteration 2287 : loss : 0.066497, loss_ce: 0.020676
2022-01-06 14:55:47,230 iteration 2288 : loss : 0.054696, loss_ce: 0.020632
2022-01-06 14:55:48,440 iteration 2289 : loss : 0.055124, loss_ce: 0.018679
2022-01-06 14:55:49,635 iteration 2290 : loss : 0.045684, loss_ce: 0.018166
2022-01-06 14:55:50,794 iteration 2291 : loss : 0.057540, loss_ce: 0.017925
2022-01-06 14:55:52,040 iteration 2292 : loss : 0.075445, loss_ce: 0.021852
2022-01-06 14:55:53,136 iteration 2293 : loss : 0.098158, loss_ce: 0.057930
2022-01-06 14:55:54,297 iteration 2294 : loss : 0.057789, loss_ce: 0.024013
2022-01-06 14:55:54,297 Training Data Eval:
2022-01-06 14:56:00,127   Average segmentation loss on training set: 0.1194
2022-01-06 14:56:00,128 Validation Data Eval:
2022-01-06 14:56:02,125   Average segmentation loss on validation set: 0.2579
2022-01-06 14:56:03,312 iteration 2295 : loss : 0.054854, loss_ce: 0.021092
 34%|█████████▊                   | 135/400 [51:03<1:41:43, 23.03s/it]2022-01-06 14:56:04,554 iteration 2296 : loss : 0.066350, loss_ce: 0.026338
2022-01-06 14:56:05,734 iteration 2297 : loss : 0.050684, loss_ce: 0.013983
2022-01-06 14:56:07,046 iteration 2298 : loss : 0.053648, loss_ce: 0.020088
2022-01-06 14:56:08,216 iteration 2299 : loss : 0.079354, loss_ce: 0.019451
2022-01-06 14:56:09,499 iteration 2300 : loss : 0.055899, loss_ce: 0.017809
2022-01-06 14:56:10,663 iteration 2301 : loss : 0.056438, loss_ce: 0.023675
2022-01-06 14:56:11,823 iteration 2302 : loss : 0.081474, loss_ce: 0.036005
2022-01-06 14:56:12,967 iteration 2303 : loss : 0.061001, loss_ce: 0.018606
2022-01-06 14:56:14,067 iteration 2304 : loss : 0.087062, loss_ce: 0.030813
2022-01-06 14:56:15,310 iteration 2305 : loss : 0.064849, loss_ce: 0.022333
2022-01-06 14:56:16,454 iteration 2306 : loss : 0.064688, loss_ce: 0.022760
2022-01-06 14:56:17,577 iteration 2307 : loss : 0.047687, loss_ce: 0.016805
2022-01-06 14:56:18,769 iteration 2308 : loss : 0.053414, loss_ce: 0.027470
2022-01-06 14:56:19,986 iteration 2309 : loss : 0.048557, loss_ce: 0.020511
2022-01-06 14:56:21,247 iteration 2310 : loss : 0.051734, loss_ce: 0.023364
2022-01-06 14:56:22,539 iteration 2311 : loss : 0.065867, loss_ce: 0.024165
2022-01-06 14:56:23,735 iteration 2312 : loss : 0.056041, loss_ce: 0.025061
 34%|█████████▊                   | 136/400 [51:24<1:37:54, 22.25s/it]2022-01-06 14:56:24,972 iteration 2313 : loss : 0.048172, loss_ce: 0.022003
2022-01-06 14:56:26,103 iteration 2314 : loss : 0.058199, loss_ce: 0.024343
2022-01-06 14:56:27,294 iteration 2315 : loss : 0.047799, loss_ce: 0.018677
2022-01-06 14:56:28,514 iteration 2316 : loss : 0.053119, loss_ce: 0.027887
2022-01-06 14:56:29,608 iteration 2317 : loss : 0.060635, loss_ce: 0.022566
2022-01-06 14:56:30,882 iteration 2318 : loss : 0.038191, loss_ce: 0.017329
2022-01-06 14:56:32,188 iteration 2319 : loss : 0.076931, loss_ce: 0.027277
2022-01-06 14:56:33,330 iteration 2320 : loss : 0.048834, loss_ce: 0.020905
2022-01-06 14:56:34,527 iteration 2321 : loss : 0.053078, loss_ce: 0.015580
2022-01-06 14:56:35,733 iteration 2322 : loss : 0.054245, loss_ce: 0.021287
2022-01-06 14:56:37,006 iteration 2323 : loss : 0.055755, loss_ce: 0.017888
2022-01-06 14:56:38,224 iteration 2324 : loss : 0.050951, loss_ce: 0.014671
2022-01-06 14:56:39,414 iteration 2325 : loss : 0.061309, loss_ce: 0.031344
2022-01-06 14:56:40,635 iteration 2326 : loss : 0.060949, loss_ce: 0.034335
2022-01-06 14:56:41,791 iteration 2327 : loss : 0.045689, loss_ce: 0.016974
2022-01-06 14:56:43,075 iteration 2328 : loss : 0.059905, loss_ce: 0.020914
2022-01-06 14:56:44,240 iteration 2329 : loss : 0.069379, loss_ce: 0.025390
 34%|█████████▉                   | 137/400 [51:44<1:35:14, 21.73s/it]2022-01-06 14:56:45,466 iteration 2330 : loss : 0.038290, loss_ce: 0.015987
2022-01-06 14:56:46,584 iteration 2331 : loss : 0.062232, loss_ce: 0.019573
2022-01-06 14:56:47,857 iteration 2332 : loss : 0.048704, loss_ce: 0.021648
2022-01-06 14:56:48,970 iteration 2333 : loss : 0.055690, loss_ce: 0.029906
2022-01-06 14:56:50,146 iteration 2334 : loss : 0.061595, loss_ce: 0.014424
2022-01-06 14:56:51,465 iteration 2335 : loss : 0.070122, loss_ce: 0.033645
2022-01-06 14:56:52,598 iteration 2336 : loss : 0.050453, loss_ce: 0.024004
2022-01-06 14:56:53,777 iteration 2337 : loss : 0.051998, loss_ce: 0.019196
2022-01-06 14:56:54,994 iteration 2338 : loss : 0.049482, loss_ce: 0.017358
2022-01-06 14:56:56,168 iteration 2339 : loss : 0.081286, loss_ce: 0.031816
2022-01-06 14:56:57,382 iteration 2340 : loss : 0.063909, loss_ce: 0.027306
2022-01-06 14:56:58,555 iteration 2341 : loss : 0.045338, loss_ce: 0.016322
2022-01-06 14:56:59,779 iteration 2342 : loss : 0.031360, loss_ce: 0.010405
2022-01-06 14:57:00,948 iteration 2343 : loss : 0.039135, loss_ce: 0.014541
2022-01-06 14:57:02,025 iteration 2344 : loss : 0.052148, loss_ce: 0.020421
2022-01-06 14:57:03,153 iteration 2345 : loss : 0.045557, loss_ce: 0.017472
2022-01-06 14:57:04,256 iteration 2346 : loss : 0.056213, loss_ce: 0.019772
 34%|██████████                   | 138/400 [52:04<1:32:37, 21.21s/it]2022-01-06 14:57:05,559 iteration 2347 : loss : 0.058867, loss_ce: 0.022350
2022-01-06 14:57:06,702 iteration 2348 : loss : 0.058185, loss_ce: 0.018287
2022-01-06 14:57:07,903 iteration 2349 : loss : 0.081616, loss_ce: 0.043539
2022-01-06 14:57:09,004 iteration 2350 : loss : 0.052204, loss_ce: 0.022838
2022-01-06 14:57:10,229 iteration 2351 : loss : 0.072280, loss_ce: 0.020732
2022-01-06 14:57:11,555 iteration 2352 : loss : 0.053821, loss_ce: 0.024046
2022-01-06 14:57:12,653 iteration 2353 : loss : 0.047881, loss_ce: 0.020551
2022-01-06 14:57:13,866 iteration 2354 : loss : 0.067719, loss_ce: 0.018751
2022-01-06 14:57:15,009 iteration 2355 : loss : 0.055445, loss_ce: 0.025231
2022-01-06 14:57:16,325 iteration 2356 : loss : 0.070477, loss_ce: 0.024900
2022-01-06 14:57:17,601 iteration 2357 : loss : 0.075094, loss_ce: 0.031890
2022-01-06 14:57:18,901 iteration 2358 : loss : 0.064877, loss_ce: 0.024339
2022-01-06 14:57:20,147 iteration 2359 : loss : 0.091314, loss_ce: 0.021859
2022-01-06 14:57:21,216 iteration 2360 : loss : 0.132876, loss_ce: 0.030373
2022-01-06 14:57:22,333 iteration 2361 : loss : 0.040229, loss_ce: 0.014875
2022-01-06 14:57:23,534 iteration 2362 : loss : 0.058260, loss_ce: 0.024044
2022-01-06 14:57:24,722 iteration 2363 : loss : 0.049039, loss_ce: 0.020947
 35%|██████████                   | 139/400 [52:25<1:31:18, 20.99s/it]2022-01-06 14:57:25,978 iteration 2364 : loss : 0.085781, loss_ce: 0.042283
2022-01-06 14:57:27,096 iteration 2365 : loss : 0.048077, loss_ce: 0.016128
2022-01-06 14:57:28,305 iteration 2366 : loss : 0.051151, loss_ce: 0.021265
2022-01-06 14:57:29,455 iteration 2367 : loss : 0.077857, loss_ce: 0.023696
2022-01-06 14:57:30,748 iteration 2368 : loss : 0.062999, loss_ce: 0.018864
2022-01-06 14:57:31,914 iteration 2369 : loss : 0.089105, loss_ce: 0.025239
2022-01-06 14:57:33,057 iteration 2370 : loss : 0.045861, loss_ce: 0.014966
2022-01-06 14:57:34,293 iteration 2371 : loss : 0.045656, loss_ce: 0.019846
2022-01-06 14:57:35,388 iteration 2372 : loss : 0.042860, loss_ce: 0.020747
2022-01-06 14:57:36,580 iteration 2373 : loss : 0.037916, loss_ce: 0.015730
2022-01-06 14:57:37,827 iteration 2374 : loss : 0.056178, loss_ce: 0.023152
2022-01-06 14:57:38,959 iteration 2375 : loss : 0.051950, loss_ce: 0.017222
2022-01-06 14:57:40,200 iteration 2376 : loss : 0.056404, loss_ce: 0.024359
2022-01-06 14:57:41,362 iteration 2377 : loss : 0.052839, loss_ce: 0.012238
2022-01-06 14:57:42,656 iteration 2378 : loss : 0.077846, loss_ce: 0.043713
2022-01-06 14:57:43,864 iteration 2379 : loss : 0.064809, loss_ce: 0.020721
2022-01-06 14:57:43,864 Training Data Eval:
2022-01-06 14:57:49,678   Average segmentation loss on training set: 0.1919
2022-01-06 14:57:49,679 Validation Data Eval:
2022-01-06 14:57:51,667   Average segmentation loss on validation set: 0.1797
2022-01-06 14:57:52,996 iteration 2380 : loss : 0.042416, loss_ce: 0.017410
 35%|██████████▏                  | 140/400 [52:53<1:40:24, 23.17s/it]2022-01-06 14:57:54,182 iteration 2381 : loss : 0.065073, loss_ce: 0.024430
2022-01-06 14:57:55,420 iteration 2382 : loss : 0.067471, loss_ce: 0.030135
2022-01-06 14:57:56,666 iteration 2383 : loss : 0.060747, loss_ce: 0.023725
2022-01-06 14:57:57,854 iteration 2384 : loss : 0.062175, loss_ce: 0.018505
2022-01-06 14:57:59,050 iteration 2385 : loss : 0.054149, loss_ce: 0.025732
2022-01-06 14:58:00,322 iteration 2386 : loss : 0.066260, loss_ce: 0.021170
2022-01-06 14:58:01,446 iteration 2387 : loss : 0.054835, loss_ce: 0.018615
2022-01-06 14:58:02,642 iteration 2388 : loss : 0.037637, loss_ce: 0.013855
2022-01-06 14:58:03,768 iteration 2389 : loss : 0.057050, loss_ce: 0.021643
2022-01-06 14:58:04,950 iteration 2390 : loss : 0.037866, loss_ce: 0.016500
2022-01-06 14:58:06,078 iteration 2391 : loss : 0.055060, loss_ce: 0.019094
2022-01-06 14:58:07,197 iteration 2392 : loss : 0.051913, loss_ce: 0.022379
2022-01-06 14:58:08,323 iteration 2393 : loss : 0.049795, loss_ce: 0.024853
2022-01-06 14:58:09,521 iteration 2394 : loss : 0.097005, loss_ce: 0.024099
2022-01-06 14:58:10,859 iteration 2395 : loss : 0.064861, loss_ce: 0.022409
2022-01-06 14:58:12,125 iteration 2396 : loss : 0.034173, loss_ce: 0.013608
2022-01-06 14:58:13,307 iteration 2397 : loss : 0.054111, loss_ce: 0.017756
 35%|██████████▏                  | 141/400 [53:13<1:36:20, 22.32s/it]2022-01-06 14:58:14,677 iteration 2398 : loss : 0.052531, loss_ce: 0.027350
2022-01-06 14:58:15,779 iteration 2399 : loss : 0.046572, loss_ce: 0.016627
2022-01-06 14:58:16,945 iteration 2400 : loss : 0.038406, loss_ce: 0.011507
2022-01-06 14:58:18,141 iteration 2401 : loss : 0.085941, loss_ce: 0.035057
2022-01-06 14:58:19,295 iteration 2402 : loss : 0.048280, loss_ce: 0.018236
2022-01-06 14:58:20,478 iteration 2403 : loss : 0.046023, loss_ce: 0.017350
2022-01-06 14:58:21,625 iteration 2404 : loss : 0.062624, loss_ce: 0.028790
2022-01-06 14:58:22,789 iteration 2405 : loss : 0.046658, loss_ce: 0.017142
2022-01-06 14:58:24,030 iteration 2406 : loss : 0.059701, loss_ce: 0.024230
2022-01-06 14:58:25,144 iteration 2407 : loss : 0.036537, loss_ce: 0.012986
2022-01-06 14:58:26,327 iteration 2408 : loss : 0.054934, loss_ce: 0.022636
2022-01-06 14:58:27,517 iteration 2409 : loss : 0.055404, loss_ce: 0.024434
2022-01-06 14:58:28,736 iteration 2410 : loss : 0.071269, loss_ce: 0.019806
2022-01-06 14:58:29,840 iteration 2411 : loss : 0.047135, loss_ce: 0.015551
2022-01-06 14:58:31,091 iteration 2412 : loss : 0.088647, loss_ce: 0.031854
2022-01-06 14:58:32,390 iteration 2413 : loss : 0.060545, loss_ce: 0.030905
2022-01-06 14:58:33,631 iteration 2414 : loss : 0.049656, loss_ce: 0.020687
 36%|██████████▎                  | 142/400 [53:33<1:33:22, 21.72s/it]2022-01-06 14:58:34,888 iteration 2415 : loss : 0.037333, loss_ce: 0.012630
2022-01-06 14:58:36,055 iteration 2416 : loss : 0.053766, loss_ce: 0.027702
2022-01-06 14:58:37,257 iteration 2417 : loss : 0.054300, loss_ce: 0.017570
2022-01-06 14:58:38,481 iteration 2418 : loss : 0.066317, loss_ce: 0.020428
2022-01-06 14:58:39,676 iteration 2419 : loss : 0.057393, loss_ce: 0.030858
2022-01-06 14:58:40,822 iteration 2420 : loss : 0.041629, loss_ce: 0.015525
2022-01-06 14:58:42,140 iteration 2421 : loss : 0.051795, loss_ce: 0.015080
2022-01-06 14:58:43,372 iteration 2422 : loss : 0.050787, loss_ce: 0.016863
2022-01-06 14:58:44,577 iteration 2423 : loss : 0.056677, loss_ce: 0.025848
2022-01-06 14:58:45,692 iteration 2424 : loss : 0.054433, loss_ce: 0.018532
2022-01-06 14:58:46,962 iteration 2425 : loss : 0.045280, loss_ce: 0.021097
2022-01-06 14:58:48,138 iteration 2426 : loss : 0.036960, loss_ce: 0.013238
2022-01-06 14:58:49,446 iteration 2427 : loss : 0.081923, loss_ce: 0.020869
2022-01-06 14:58:50,716 iteration 2428 : loss : 0.064323, loss_ce: 0.029398
2022-01-06 14:58:52,003 iteration 2429 : loss : 0.059693, loss_ce: 0.027863
2022-01-06 14:58:53,208 iteration 2430 : loss : 0.049001, loss_ce: 0.019988
2022-01-06 14:58:54,349 iteration 2431 : loss : 0.035384, loss_ce: 0.014708
 36%|██████████▎                  | 143/400 [53:54<1:31:45, 21.42s/it]2022-01-06 14:58:55,645 iteration 2432 : loss : 0.054654, loss_ce: 0.021322
2022-01-06 14:58:56,846 iteration 2433 : loss : 0.067718, loss_ce: 0.025979
2022-01-06 14:58:58,026 iteration 2434 : loss : 0.036756, loss_ce: 0.011010
2022-01-06 14:58:59,346 iteration 2435 : loss : 0.062305, loss_ce: 0.029183
2022-01-06 14:59:00,423 iteration 2436 : loss : 0.049805, loss_ce: 0.020201
2022-01-06 14:59:01,536 iteration 2437 : loss : 0.035378, loss_ce: 0.014989
2022-01-06 14:59:02,875 iteration 2438 : loss : 0.048312, loss_ce: 0.020001
2022-01-06 14:59:04,092 iteration 2439 : loss : 0.059776, loss_ce: 0.025121
2022-01-06 14:59:05,261 iteration 2440 : loss : 0.039319, loss_ce: 0.013243
2022-01-06 14:59:06,419 iteration 2441 : loss : 0.060219, loss_ce: 0.017382
2022-01-06 14:59:07,554 iteration 2442 : loss : 0.055229, loss_ce: 0.019214
2022-01-06 14:59:08,670 iteration 2443 : loss : 0.042290, loss_ce: 0.018876
2022-01-06 14:59:09,900 iteration 2444 : loss : 0.041029, loss_ce: 0.014650
2022-01-06 14:59:10,978 iteration 2445 : loss : 0.080702, loss_ce: 0.018334
2022-01-06 14:59:12,280 iteration 2446 : loss : 0.043958, loss_ce: 0.014770
2022-01-06 14:59:13,549 iteration 2447 : loss : 0.053511, loss_ce: 0.023269
2022-01-06 14:59:14,740 iteration 2448 : loss : 0.050300, loss_ce: 0.016508
 36%|██████████▍                  | 144/400 [54:15<1:30:04, 21.11s/it]2022-01-06 14:59:15,964 iteration 2449 : loss : 0.054801, loss_ce: 0.020535
2022-01-06 14:59:17,226 iteration 2450 : loss : 0.041278, loss_ce: 0.015347
2022-01-06 14:59:18,478 iteration 2451 : loss : 0.070645, loss_ce: 0.028538
2022-01-06 14:59:19,569 iteration 2452 : loss : 0.037550, loss_ce: 0.015816
2022-01-06 14:59:20,731 iteration 2453 : loss : 0.070926, loss_ce: 0.020734
2022-01-06 14:59:21,954 iteration 2454 : loss : 0.048266, loss_ce: 0.023583
2022-01-06 14:59:23,169 iteration 2455 : loss : 0.050276, loss_ce: 0.025703
2022-01-06 14:59:24,511 iteration 2456 : loss : 0.067070, loss_ce: 0.025643
2022-01-06 14:59:25,691 iteration 2457 : loss : 0.052970, loss_ce: 0.020192
2022-01-06 14:59:26,957 iteration 2458 : loss : 0.040816, loss_ce: 0.013501
2022-01-06 14:59:28,080 iteration 2459 : loss : 0.068268, loss_ce: 0.019792
2022-01-06 14:59:29,252 iteration 2460 : loss : 0.046877, loss_ce: 0.014931
2022-01-06 14:59:30,463 iteration 2461 : loss : 0.056349, loss_ce: 0.019653
2022-01-06 14:59:31,635 iteration 2462 : loss : 0.041829, loss_ce: 0.018020
2022-01-06 14:59:32,854 iteration 2463 : loss : 0.047301, loss_ce: 0.017413
2022-01-06 14:59:34,131 iteration 2464 : loss : 0.066724, loss_ce: 0.026504
2022-01-06 14:59:34,132 Training Data Eval:
2022-01-06 14:59:39,989   Average segmentation loss on training set: 0.0721
2022-01-06 14:59:39,989 Validation Data Eval:
2022-01-06 14:59:42,006   Average segmentation loss on validation set: 0.2026
2022-01-06 14:59:43,171 iteration 2465 : loss : 0.044327, loss_ce: 0.015978
 36%|██████████▌                  | 145/400 [54:43<1:39:02, 23.30s/it]2022-01-06 14:59:44,400 iteration 2466 : loss : 0.046633, loss_ce: 0.017553
2022-01-06 14:59:45,645 iteration 2467 : loss : 0.052639, loss_ce: 0.031910
2022-01-06 14:59:46,790 iteration 2468 : loss : 0.032510, loss_ce: 0.013407
2022-01-06 14:59:47,897 iteration 2469 : loss : 0.027093, loss_ce: 0.013634
2022-01-06 14:59:49,142 iteration 2470 : loss : 0.064013, loss_ce: 0.026050
2022-01-06 14:59:50,391 iteration 2471 : loss : 0.056706, loss_ce: 0.024150
2022-01-06 14:59:51,687 iteration 2472 : loss : 0.048934, loss_ce: 0.019049
2022-01-06 14:59:52,773 iteration 2473 : loss : 0.049109, loss_ce: 0.018638
2022-01-06 14:59:53,985 iteration 2474 : loss : 0.055256, loss_ce: 0.017472
2022-01-06 14:59:55,094 iteration 2475 : loss : 0.034944, loss_ce: 0.014053
2022-01-06 14:59:56,376 iteration 2476 : loss : 0.089349, loss_ce: 0.025161
2022-01-06 14:59:57,556 iteration 2477 : loss : 0.047970, loss_ce: 0.017650
2022-01-06 14:59:58,737 iteration 2478 : loss : 0.052390, loss_ce: 0.017533
2022-01-06 15:00:00,076 iteration 2479 : loss : 0.063828, loss_ce: 0.020647
2022-01-06 15:00:01,348 iteration 2480 : loss : 0.063744, loss_ce: 0.026483
2022-01-06 15:00:02,502 iteration 2481 : loss : 0.039305, loss_ce: 0.014536
2022-01-06 15:00:03,700 iteration 2482 : loss : 0.051063, loss_ce: 0.019549
 36%|██████████▌                  | 146/400 [55:04<1:35:08, 22.48s/it]2022-01-06 15:00:04,995 iteration 2483 : loss : 0.051082, loss_ce: 0.018631
2022-01-06 15:00:06,264 iteration 2484 : loss : 0.057949, loss_ce: 0.018363
2022-01-06 15:00:07,562 iteration 2485 : loss : 0.050286, loss_ce: 0.018567
2022-01-06 15:00:08,706 iteration 2486 : loss : 0.052346, loss_ce: 0.023000
2022-01-06 15:00:09,874 iteration 2487 : loss : 0.061498, loss_ce: 0.024472
2022-01-06 15:00:11,132 iteration 2488 : loss : 0.056253, loss_ce: 0.019740
2022-01-06 15:00:12,390 iteration 2489 : loss : 0.042277, loss_ce: 0.017043
2022-01-06 15:00:13,545 iteration 2490 : loss : 0.045290, loss_ce: 0.018197
2022-01-06 15:00:14,754 iteration 2491 : loss : 0.042668, loss_ce: 0.017069
2022-01-06 15:00:15,905 iteration 2492 : loss : 0.053793, loss_ce: 0.018593
2022-01-06 15:00:17,066 iteration 2493 : loss : 0.046252, loss_ce: 0.014647
2022-01-06 15:00:18,191 iteration 2494 : loss : 0.040505, loss_ce: 0.016444
2022-01-06 15:00:19,367 iteration 2495 : loss : 0.078066, loss_ce: 0.023956
2022-01-06 15:00:20,606 iteration 2496 : loss : 0.057777, loss_ce: 0.018763
2022-01-06 15:00:21,778 iteration 2497 : loss : 0.057955, loss_ce: 0.023497
2022-01-06 15:00:22,873 iteration 2498 : loss : 0.053453, loss_ce: 0.025084
2022-01-06 15:00:24,142 iteration 2499 : loss : 0.049211, loss_ce: 0.024680
 37%|██████████▋                  | 147/400 [55:24<1:32:10, 21.86s/it]2022-01-06 15:00:25,430 iteration 2500 : loss : 0.057328, loss_ce: 0.019892
2022-01-06 15:00:26,550 iteration 2501 : loss : 0.044146, loss_ce: 0.014447
2022-01-06 15:00:27,704 iteration 2502 : loss : 0.058008, loss_ce: 0.018133
2022-01-06 15:00:28,897 iteration 2503 : loss : 0.071404, loss_ce: 0.032622
2022-01-06 15:00:30,120 iteration 2504 : loss : 0.068161, loss_ce: 0.021425
2022-01-06 15:00:31,236 iteration 2505 : loss : 0.048953, loss_ce: 0.019891
2022-01-06 15:00:32,375 iteration 2506 : loss : 0.086038, loss_ce: 0.022698
2022-01-06 15:00:33,660 iteration 2507 : loss : 0.043817, loss_ce: 0.017136
2022-01-06 15:00:34,813 iteration 2508 : loss : 0.048388, loss_ce: 0.022193
2022-01-06 15:00:36,066 iteration 2509 : loss : 0.062974, loss_ce: 0.027195
2022-01-06 15:00:37,229 iteration 2510 : loss : 0.054095, loss_ce: 0.019856
2022-01-06 15:00:38,356 iteration 2511 : loss : 0.037962, loss_ce: 0.018007
2022-01-06 15:00:39,609 iteration 2512 : loss : 0.053751, loss_ce: 0.019975
2022-01-06 15:00:40,750 iteration 2513 : loss : 0.039176, loss_ce: 0.015730
2022-01-06 15:00:41,984 iteration 2514 : loss : 0.062486, loss_ce: 0.020946
2022-01-06 15:00:43,139 iteration 2515 : loss : 0.058280, loss_ce: 0.030504
2022-01-06 15:00:44,516 iteration 2516 : loss : 0.042165, loss_ce: 0.016627
 37%|██████████▋                  | 148/400 [55:44<1:29:57, 21.42s/it]2022-01-06 15:00:45,785 iteration 2517 : loss : 0.057738, loss_ce: 0.021418
2022-01-06 15:00:47,000 iteration 2518 : loss : 0.061680, loss_ce: 0.019825
2022-01-06 15:00:48,098 iteration 2519 : loss : 0.033766, loss_ce: 0.012232
2022-01-06 15:00:49,334 iteration 2520 : loss : 0.064233, loss_ce: 0.031489
2022-01-06 15:00:50,537 iteration 2521 : loss : 0.052839, loss_ce: 0.022450
2022-01-06 15:00:51,747 iteration 2522 : loss : 0.049895, loss_ce: 0.019805
2022-01-06 15:00:52,934 iteration 2523 : loss : 0.055243, loss_ce: 0.017335
2022-01-06 15:00:54,166 iteration 2524 : loss : 0.041289, loss_ce: 0.018090
2022-01-06 15:00:55,307 iteration 2525 : loss : 0.056058, loss_ce: 0.018270
2022-01-06 15:00:56,427 iteration 2526 : loss : 0.052959, loss_ce: 0.019016
2022-01-06 15:00:57,649 iteration 2527 : loss : 0.050420, loss_ce: 0.022590
2022-01-06 15:00:58,831 iteration 2528 : loss : 0.053578, loss_ce: 0.025705
2022-01-06 15:01:00,058 iteration 2529 : loss : 0.063251, loss_ce: 0.026880
2022-01-06 15:01:01,169 iteration 2530 : loss : 0.045472, loss_ce: 0.016091
2022-01-06 15:01:02,393 iteration 2531 : loss : 0.074026, loss_ce: 0.027406
2022-01-06 15:01:03,686 iteration 2532 : loss : 0.051141, loss_ce: 0.023522
2022-01-06 15:01:04,812 iteration 2533 : loss : 0.065281, loss_ce: 0.019697
 37%|██████████▊                  | 149/400 [56:05<1:28:10, 21.08s/it]2022-01-06 15:01:06,081 iteration 2534 : loss : 0.038406, loss_ce: 0.015211
2022-01-06 15:01:07,161 iteration 2535 : loss : 0.049129, loss_ce: 0.020460
2022-01-06 15:01:08,316 iteration 2536 : loss : 0.029305, loss_ce: 0.012849
2022-01-06 15:01:09,623 iteration 2537 : loss : 0.062540, loss_ce: 0.031846
2022-01-06 15:01:10,679 iteration 2538 : loss : 0.047953, loss_ce: 0.015867
2022-01-06 15:01:11,792 iteration 2539 : loss : 0.038706, loss_ce: 0.016041
2022-01-06 15:01:12,972 iteration 2540 : loss : 0.048602, loss_ce: 0.022522
2022-01-06 15:01:14,242 iteration 2541 : loss : 0.059993, loss_ce: 0.024980
2022-01-06 15:01:15,415 iteration 2542 : loss : 0.048699, loss_ce: 0.017089
2022-01-06 15:01:16,692 iteration 2543 : loss : 0.055343, loss_ce: 0.016904
2022-01-06 15:01:17,959 iteration 2544 : loss : 0.070246, loss_ce: 0.025435
2022-01-06 15:01:19,076 iteration 2545 : loss : 0.037049, loss_ce: 0.016216
2022-01-06 15:01:20,477 iteration 2546 : loss : 0.062859, loss_ce: 0.022808
2022-01-06 15:01:21,733 iteration 2547 : loss : 0.102777, loss_ce: 0.039386
2022-01-06 15:01:23,018 iteration 2548 : loss : 0.056479, loss_ce: 0.028884
2022-01-06 15:01:24,194 iteration 2549 : loss : 0.047493, loss_ce: 0.016132
2022-01-06 15:01:24,195 Training Data Eval:
2022-01-06 15:01:30,116   Average segmentation loss on training set: 0.0938
2022-01-06 15:01:30,116 Validation Data Eval:
2022-01-06 15:01:32,139   Average segmentation loss on validation set: 0.0981
2022-01-06 15:01:37,995 Found new lowest validation loss at iteration 2549! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_MLP_best_val_loss_seed100.pth
2022-01-06 15:01:39,169 iteration 2550 : loss : 0.049333, loss_ce: 0.012474
 38%|██████████▉                  | 150/400 [56:39<1:44:25, 25.06s/it]2022-01-06 15:01:40,385 iteration 2551 : loss : 0.045265, loss_ce: 0.015374
2022-01-06 15:01:41,561 iteration 2552 : loss : 0.035319, loss_ce: 0.014250
2022-01-06 15:01:42,737 iteration 2553 : loss : 0.038650, loss_ce: 0.012880
2022-01-06 15:01:43,881 iteration 2554 : loss : 0.046951, loss_ce: 0.019770
2022-01-06 15:01:45,001 iteration 2555 : loss : 0.043336, loss_ce: 0.015726
2022-01-06 15:01:46,158 iteration 2556 : loss : 0.050517, loss_ce: 0.021213
2022-01-06 15:01:47,302 iteration 2557 : loss : 0.037559, loss_ce: 0.015689
2022-01-06 15:01:48,423 iteration 2558 : loss : 0.033680, loss_ce: 0.011977
2022-01-06 15:01:49,501 iteration 2559 : loss : 0.042823, loss_ce: 0.021203
2022-01-06 15:01:50,766 iteration 2560 : loss : 0.034460, loss_ce: 0.012489
2022-01-06 15:01:51,919 iteration 2561 : loss : 0.066558, loss_ce: 0.027582
2022-01-06 15:01:53,103 iteration 2562 : loss : 0.038326, loss_ce: 0.016296
2022-01-06 15:01:54,171 iteration 2563 : loss : 0.060743, loss_ce: 0.024848
2022-01-06 15:01:55,273 iteration 2564 : loss : 0.033787, loss_ce: 0.013722
2022-01-06 15:01:56,414 iteration 2565 : loss : 0.034041, loss_ce: 0.015671
2022-01-06 15:01:57,532 iteration 2566 : loss : 0.033762, loss_ce: 0.012544
2022-01-06 15:01:58,718 iteration 2567 : loss : 0.057417, loss_ce: 0.013759
 38%|██████████▉                  | 151/400 [56:59<1:37:09, 23.41s/it]2022-01-06 15:01:59,952 iteration 2568 : loss : 0.055091, loss_ce: 0.026993
2022-01-06 15:02:01,117 iteration 2569 : loss : 0.040738, loss_ce: 0.017556
2022-01-06 15:02:02,397 iteration 2570 : loss : 0.045472, loss_ce: 0.014441
2022-01-06 15:02:03,570 iteration 2571 : loss : 0.046839, loss_ce: 0.014599
2022-01-06 15:02:04,669 iteration 2572 : loss : 0.058206, loss_ce: 0.020064
2022-01-06 15:02:05,833 iteration 2573 : loss : 0.036297, loss_ce: 0.014937
2022-01-06 15:02:06,975 iteration 2574 : loss : 0.044514, loss_ce: 0.020345
2022-01-06 15:02:08,089 iteration 2575 : loss : 0.034453, loss_ce: 0.012243
2022-01-06 15:02:09,207 iteration 2576 : loss : 0.034235, loss_ce: 0.010145
2022-01-06 15:02:10,431 iteration 2577 : loss : 0.054616, loss_ce: 0.028951
2022-01-06 15:02:11,583 iteration 2578 : loss : 0.039451, loss_ce: 0.013831
2022-01-06 15:02:12,774 iteration 2579 : loss : 0.066131, loss_ce: 0.025074
2022-01-06 15:02:13,885 iteration 2580 : loss : 0.043148, loss_ce: 0.015284
2022-01-06 15:02:15,098 iteration 2581 : loss : 0.051426, loss_ce: 0.020697
2022-01-06 15:02:16,359 iteration 2582 : loss : 0.029888, loss_ce: 0.013747
2022-01-06 15:02:17,503 iteration 2583 : loss : 0.039788, loss_ce: 0.018449
2022-01-06 15:02:18,654 iteration 2584 : loss : 0.059635, loss_ce: 0.019135
 38%|███████████                  | 152/400 [57:18<1:32:27, 22.37s/it]2022-01-06 15:02:19,902 iteration 2585 : loss : 0.062778, loss_ce: 0.022516
2022-01-06 15:02:21,291 iteration 2586 : loss : 0.074526, loss_ce: 0.024385
2022-01-06 15:02:22,560 iteration 2587 : loss : 0.079668, loss_ce: 0.029930
2022-01-06 15:02:23,745 iteration 2588 : loss : 0.070443, loss_ce: 0.025657
2022-01-06 15:02:24,851 iteration 2589 : loss : 0.075725, loss_ce: 0.020748
2022-01-06 15:02:26,090 iteration 2590 : loss : 0.047266, loss_ce: 0.016726
2022-01-06 15:02:27,263 iteration 2591 : loss : 0.028161, loss_ce: 0.011721
2022-01-06 15:02:28,390 iteration 2592 : loss : 0.036066, loss_ce: 0.018126
2022-01-06 15:02:29,460 iteration 2593 : loss : 0.041673, loss_ce: 0.019719
2022-01-06 15:02:30,648 iteration 2594 : loss : 0.065603, loss_ce: 0.028089
2022-01-06 15:02:31,754 iteration 2595 : loss : 0.046106, loss_ce: 0.016195
2022-01-06 15:02:32,902 iteration 2596 : loss : 0.051727, loss_ce: 0.016911
2022-01-06 15:02:34,062 iteration 2597 : loss : 0.068850, loss_ce: 0.021829
2022-01-06 15:02:35,247 iteration 2598 : loss : 0.044999, loss_ce: 0.019824
2022-01-06 15:02:36,451 iteration 2599 : loss : 0.033265, loss_ce: 0.013449
2022-01-06 15:02:37,602 iteration 2600 : loss : 0.046381, loss_ce: 0.018483
2022-01-06 15:02:38,716 iteration 2601 : loss : 0.056193, loss_ce: 0.018440
 38%|███████████                  | 153/400 [57:39<1:29:14, 21.68s/it]2022-01-06 15:02:39,939 iteration 2602 : loss : 0.061164, loss_ce: 0.025554
2022-01-06 15:02:41,139 iteration 2603 : loss : 0.075138, loss_ce: 0.016170
2022-01-06 15:02:42,319 iteration 2604 : loss : 0.045674, loss_ce: 0.017648
2022-01-06 15:02:43,492 iteration 2605 : loss : 0.068052, loss_ce: 0.025931
2022-01-06 15:02:44,680 iteration 2606 : loss : 0.051448, loss_ce: 0.019752
2022-01-06 15:02:45,886 iteration 2607 : loss : 0.053995, loss_ce: 0.021903
2022-01-06 15:02:47,016 iteration 2608 : loss : 0.046419, loss_ce: 0.016700
2022-01-06 15:02:48,243 iteration 2609 : loss : 0.039304, loss_ce: 0.017407
2022-01-06 15:02:49,444 iteration 2610 : loss : 0.056511, loss_ce: 0.021100
2022-01-06 15:02:50,683 iteration 2611 : loss : 0.046206, loss_ce: 0.020186
2022-01-06 15:02:51,898 iteration 2612 : loss : 0.051788, loss_ce: 0.019754
2022-01-06 15:02:53,153 iteration 2613 : loss : 0.059603, loss_ce: 0.020272
2022-01-06 15:02:54,378 iteration 2614 : loss : 0.033094, loss_ce: 0.013569
2022-01-06 15:02:55,580 iteration 2615 : loss : 0.044048, loss_ce: 0.019648
2022-01-06 15:02:56,841 iteration 2616 : loss : 0.048138, loss_ce: 0.020308
2022-01-06 15:02:58,051 iteration 2617 : loss : 0.045125, loss_ce: 0.017404
2022-01-06 15:02:59,307 iteration 2618 : loss : 0.052986, loss_ce: 0.019548
 38%|███████████▏                 | 154/400 [57:59<1:27:32, 21.35s/it]2022-01-06 15:03:00,726 iteration 2619 : loss : 0.050861, loss_ce: 0.021262
2022-01-06 15:03:01,953 iteration 2620 : loss : 0.056475, loss_ce: 0.021251
2022-01-06 15:03:03,030 iteration 2621 : loss : 0.040380, loss_ce: 0.017225
2022-01-06 15:03:04,269 iteration 2622 : loss : 0.076880, loss_ce: 0.022986
2022-01-06 15:03:05,440 iteration 2623 : loss : 0.034772, loss_ce: 0.013457
2022-01-06 15:03:06,710 iteration 2624 : loss : 0.066059, loss_ce: 0.024052
2022-01-06 15:03:07,977 iteration 2625 : loss : 0.041612, loss_ce: 0.018428
2022-01-06 15:03:09,217 iteration 2626 : loss : 0.045929, loss_ce: 0.019712
2022-01-06 15:03:10,340 iteration 2627 : loss : 0.039750, loss_ce: 0.020616
2022-01-06 15:03:11,484 iteration 2628 : loss : 0.037002, loss_ce: 0.013606
2022-01-06 15:03:12,586 iteration 2629 : loss : 0.043676, loss_ce: 0.012494
2022-01-06 15:03:13,822 iteration 2630 : loss : 0.090972, loss_ce: 0.016224
2022-01-06 15:03:14,929 iteration 2631 : loss : 0.068636, loss_ce: 0.037217
2022-01-06 15:03:16,062 iteration 2632 : loss : 0.043660, loss_ce: 0.014610
2022-01-06 15:03:17,265 iteration 2633 : loss : 0.026826, loss_ce: 0.008947
2022-01-06 15:03:18,417 iteration 2634 : loss : 0.050521, loss_ce: 0.015781
2022-01-06 15:03:18,418 Training Data Eval:
2022-01-06 15:03:24,263   Average segmentation loss on training set: 0.0462
2022-01-06 15:03:24,263 Validation Data Eval:
2022-01-06 15:03:26,244   Average segmentation loss on validation set: 0.1234
2022-01-06 15:03:27,393 iteration 2635 : loss : 0.046925, loss_ce: 0.017946
 39%|███████████▏                 | 155/400 [58:27<1:35:25, 23.37s/it]2022-01-06 15:03:28,569 iteration 2636 : loss : 0.034098, loss_ce: 0.012295
2022-01-06 15:03:29,720 iteration 2637 : loss : 0.037424, loss_ce: 0.016022
2022-01-06 15:03:31,002 iteration 2638 : loss : 0.060765, loss_ce: 0.022993
2022-01-06 15:03:32,171 iteration 2639 : loss : 0.041548, loss_ce: 0.015146
2022-01-06 15:03:33,379 iteration 2640 : loss : 0.067119, loss_ce: 0.023938
2022-01-06 15:03:34,567 iteration 2641 : loss : 0.040687, loss_ce: 0.013964
2022-01-06 15:03:35,787 iteration 2642 : loss : 0.050064, loss_ce: 0.014361
2022-01-06 15:03:36,950 iteration 2643 : loss : 0.049639, loss_ce: 0.023202
2022-01-06 15:03:38,158 iteration 2644 : loss : 0.044894, loss_ce: 0.017873
2022-01-06 15:03:39,273 iteration 2645 : loss : 0.045205, loss_ce: 0.018739
2022-01-06 15:03:40,410 iteration 2646 : loss : 0.054064, loss_ce: 0.018586
2022-01-06 15:03:41,683 iteration 2647 : loss : 0.062219, loss_ce: 0.023983
2022-01-06 15:03:42,833 iteration 2648 : loss : 0.052726, loss_ce: 0.019722
2022-01-06 15:03:44,035 iteration 2649 : loss : 0.058310, loss_ce: 0.021737
2022-01-06 15:03:45,376 iteration 2650 : loss : 0.053141, loss_ce: 0.022280
2022-01-06 15:03:46,545 iteration 2651 : loss : 0.048630, loss_ce: 0.018181
2022-01-06 15:03:47,826 iteration 2652 : loss : 0.051796, loss_ce: 0.020917
 39%|███████████▎                 | 156/400 [58:48<1:31:27, 22.49s/it]2022-01-06 15:03:49,170 iteration 2653 : loss : 0.053024, loss_ce: 0.020630
2022-01-06 15:03:50,292 iteration 2654 : loss : 0.036815, loss_ce: 0.014969
2022-01-06 15:03:51,468 iteration 2655 : loss : 0.041379, loss_ce: 0.018213
2022-01-06 15:03:52,647 iteration 2656 : loss : 0.037694, loss_ce: 0.018651
2022-01-06 15:03:53,886 iteration 2657 : loss : 0.041859, loss_ce: 0.020098
2022-01-06 15:03:55,159 iteration 2658 : loss : 0.063852, loss_ce: 0.019832
2022-01-06 15:03:56,379 iteration 2659 : loss : 0.049879, loss_ce: 0.018623
2022-01-06 15:03:57,669 iteration 2660 : loss : 0.050536, loss_ce: 0.020461
2022-01-06 15:03:58,751 iteration 2661 : loss : 0.045810, loss_ce: 0.016354
2022-01-06 15:04:00,030 iteration 2662 : loss : 0.054385, loss_ce: 0.020835
2022-01-06 15:04:01,287 iteration 2663 : loss : 0.054765, loss_ce: 0.022079
2022-01-06 15:04:02,405 iteration 2664 : loss : 0.052416, loss_ce: 0.018881
2022-01-06 15:04:03,556 iteration 2665 : loss : 0.042015, loss_ce: 0.019220
2022-01-06 15:04:04,832 iteration 2666 : loss : 0.048302, loss_ce: 0.017119
2022-01-06 15:04:05,961 iteration 2667 : loss : 0.039684, loss_ce: 0.013324
2022-01-06 15:04:07,232 iteration 2668 : loss : 0.060579, loss_ce: 0.024143
2022-01-06 15:04:08,394 iteration 2669 : loss : 0.053361, loss_ce: 0.017201
 39%|███████████▍                 | 157/400 [59:08<1:28:45, 21.91s/it]2022-01-06 15:04:09,560 iteration 2670 : loss : 0.034294, loss_ce: 0.013785
2022-01-06 15:04:10,662 iteration 2671 : loss : 0.065637, loss_ce: 0.031326
2022-01-06 15:04:11,820 iteration 2672 : loss : 0.074207, loss_ce: 0.033570
2022-01-06 15:04:12,963 iteration 2673 : loss : 0.042739, loss_ce: 0.013302
2022-01-06 15:04:14,173 iteration 2674 : loss : 0.045840, loss_ce: 0.015053
2022-01-06 15:04:15,414 iteration 2675 : loss : 0.036282, loss_ce: 0.012873
2022-01-06 15:04:16,571 iteration 2676 : loss : 0.031615, loss_ce: 0.011111
2022-01-06 15:04:17,745 iteration 2677 : loss : 0.040288, loss_ce: 0.015234
2022-01-06 15:04:18,844 iteration 2678 : loss : 0.045661, loss_ce: 0.020808
2022-01-06 15:04:20,034 iteration 2679 : loss : 0.042197, loss_ce: 0.016776
2022-01-06 15:04:21,293 iteration 2680 : loss : 0.048266, loss_ce: 0.018860
2022-01-06 15:04:22,480 iteration 2681 : loss : 0.041369, loss_ce: 0.014468
2022-01-06 15:04:23,696 iteration 2682 : loss : 0.046839, loss_ce: 0.017526
2022-01-06 15:04:24,829 iteration 2683 : loss : 0.038651, loss_ce: 0.015266
2022-01-06 15:04:26,027 iteration 2684 : loss : 0.038703, loss_ce: 0.017763
2022-01-06 15:04:27,261 iteration 2685 : loss : 0.038421, loss_ce: 0.017331
2022-01-06 15:04:28,487 iteration 2686 : loss : 0.092246, loss_ce: 0.022285
 40%|███████████▍                 | 158/400 [59:28<1:26:11, 21.37s/it]2022-01-06 15:04:29,695 iteration 2687 : loss : 0.046310, loss_ce: 0.020581
2022-01-06 15:04:30,858 iteration 2688 : loss : 0.038368, loss_ce: 0.014534
2022-01-06 15:04:31,983 iteration 2689 : loss : 0.046286, loss_ce: 0.015703
2022-01-06 15:04:33,215 iteration 2690 : loss : 0.085290, loss_ce: 0.020713
2022-01-06 15:04:34,337 iteration 2691 : loss : 0.034882, loss_ce: 0.014894
2022-01-06 15:04:35,463 iteration 2692 : loss : 0.039132, loss_ce: 0.013305
2022-01-06 15:04:36,587 iteration 2693 : loss : 0.037004, loss_ce: 0.013746
2022-01-06 15:04:37,789 iteration 2694 : loss : 0.053100, loss_ce: 0.021284
2022-01-06 15:04:38,959 iteration 2695 : loss : 0.048309, loss_ce: 0.023389
2022-01-06 15:04:40,138 iteration 2696 : loss : 0.039311, loss_ce: 0.016444
2022-01-06 15:04:41,278 iteration 2697 : loss : 0.063116, loss_ce: 0.028468
2022-01-06 15:04:42,414 iteration 2698 : loss : 0.038565, loss_ce: 0.011373
2022-01-06 15:04:43,580 iteration 2699 : loss : 0.043396, loss_ce: 0.018945
2022-01-06 15:04:44,710 iteration 2700 : loss : 0.043779, loss_ce: 0.014690
2022-01-06 15:04:46,023 iteration 2701 : loss : 0.058339, loss_ce: 0.021601
2022-01-06 15:04:47,233 iteration 2702 : loss : 0.054448, loss_ce: 0.026561
2022-01-06 15:04:48,474 iteration 2703 : loss : 0.057389, loss_ce: 0.021239
 40%|███████████▌                 | 159/400 [59:48<1:24:09, 20.95s/it]2022-01-06 15:04:49,794 iteration 2704 : loss : 0.073221, loss_ce: 0.025775
2022-01-06 15:04:50,920 iteration 2705 : loss : 0.036791, loss_ce: 0.016574
2022-01-06 15:04:52,147 iteration 2706 : loss : 0.069254, loss_ce: 0.027414
2022-01-06 15:04:53,297 iteration 2707 : loss : 0.059658, loss_ce: 0.030036
2022-01-06 15:04:54,520 iteration 2708 : loss : 0.057706, loss_ce: 0.026796
2022-01-06 15:04:55,700 iteration 2709 : loss : 0.035953, loss_ce: 0.014848
2022-01-06 15:04:56,813 iteration 2710 : loss : 0.030523, loss_ce: 0.013154
2022-01-06 15:04:58,026 iteration 2711 : loss : 0.047511, loss_ce: 0.020483
2022-01-06 15:04:59,245 iteration 2712 : loss : 0.050964, loss_ce: 0.015422
2022-01-06 15:05:00,372 iteration 2713 : loss : 0.032900, loss_ce: 0.016462
2022-01-06 15:05:01,627 iteration 2714 : loss : 0.043202, loss_ce: 0.016647
2022-01-06 15:05:02,856 iteration 2715 : loss : 0.082822, loss_ce: 0.032074
2022-01-06 15:05:04,089 iteration 2716 : loss : 0.037389, loss_ce: 0.013870
2022-01-06 15:05:05,252 iteration 2717 : loss : 0.054861, loss_ce: 0.021717
2022-01-06 15:05:06,475 iteration 2718 : loss : 0.088753, loss_ce: 0.026068
2022-01-06 15:05:07,618 iteration 2719 : loss : 0.039706, loss_ce: 0.014489
2022-01-06 15:05:07,618 Training Data Eval:
2022-01-06 15:05:13,505   Average segmentation loss on training set: 0.0459
2022-01-06 15:05:13,506 Validation Data Eval:
2022-01-06 15:05:15,510   Average segmentation loss on validation set: 0.0905
2022-01-06 15:05:22,075 Found new lowest validation loss at iteration 2719! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_MLP_best_val_loss_seed100.pth
2022-01-06 15:05:23,197 iteration 2720 : loss : 0.039085, loss_ce: 0.013212
 40%|██████████▊                | 160/400 [1:00:23<1:40:19, 25.08s/it]2022-01-06 15:05:24,432 iteration 2721 : loss : 0.048380, loss_ce: 0.022138
2022-01-06 15:05:25,590 iteration 2722 : loss : 0.054693, loss_ce: 0.019060
2022-01-06 15:05:26,779 iteration 2723 : loss : 0.045963, loss_ce: 0.016031
2022-01-06 15:05:27,939 iteration 2724 : loss : 0.049295, loss_ce: 0.017637
2022-01-06 15:05:29,063 iteration 2725 : loss : 0.053090, loss_ce: 0.017721
2022-01-06 15:05:30,385 iteration 2726 : loss : 0.102790, loss_ce: 0.037993
2022-01-06 15:05:31,611 iteration 2727 : loss : 0.047738, loss_ce: 0.019067
2022-01-06 15:05:32,770 iteration 2728 : loss : 0.032249, loss_ce: 0.013497
2022-01-06 15:05:33,913 iteration 2729 : loss : 0.060156, loss_ce: 0.025888
2022-01-06 15:05:34,981 iteration 2730 : loss : 0.032781, loss_ce: 0.012434
2022-01-06 15:05:36,142 iteration 2731 : loss : 0.040724, loss_ce: 0.016840
2022-01-06 15:05:37,378 iteration 2732 : loss : 0.047884, loss_ce: 0.021593
2022-01-06 15:05:38,565 iteration 2733 : loss : 0.044785, loss_ce: 0.016147
2022-01-06 15:05:39,725 iteration 2734 : loss : 0.057177, loss_ce: 0.019787
2022-01-06 15:05:40,996 iteration 2735 : loss : 0.044690, loss_ce: 0.014701
2022-01-06 15:05:42,145 iteration 2736 : loss : 0.033057, loss_ce: 0.013885
2022-01-06 15:05:43,371 iteration 2737 : loss : 0.052736, loss_ce: 0.019579
 40%|██████████▊                | 161/400 [1:00:43<1:34:02, 23.61s/it]2022-01-06 15:05:44,565 iteration 2738 : loss : 0.030472, loss_ce: 0.013395
2022-01-06 15:05:45,845 iteration 2739 : loss : 0.039278, loss_ce: 0.016138
2022-01-06 15:05:47,143 iteration 2740 : loss : 0.047729, loss_ce: 0.020074
2022-01-06 15:05:48,315 iteration 2741 : loss : 0.037342, loss_ce: 0.013683
2022-01-06 15:05:49,436 iteration 2742 : loss : 0.055409, loss_ce: 0.018667
2022-01-06 15:05:50,662 iteration 2743 : loss : 0.058434, loss_ce: 0.018289
2022-01-06 15:05:52,025 iteration 2744 : loss : 0.042679, loss_ce: 0.016400
2022-01-06 15:05:53,219 iteration 2745 : loss : 0.039009, loss_ce: 0.011719
2022-01-06 15:05:54,394 iteration 2746 : loss : 0.033738, loss_ce: 0.012542
2022-01-06 15:05:55,722 iteration 2747 : loss : 0.086100, loss_ce: 0.034493
2022-01-06 15:05:56,893 iteration 2748 : loss : 0.040095, loss_ce: 0.015255
2022-01-06 15:05:58,103 iteration 2749 : loss : 0.043442, loss_ce: 0.021304
2022-01-06 15:05:59,282 iteration 2750 : loss : 0.047821, loss_ce: 0.015580
2022-01-06 15:06:00,463 iteration 2751 : loss : 0.050545, loss_ce: 0.015635
2022-01-06 15:06:01,634 iteration 2752 : loss : 0.039739, loss_ce: 0.016364
2022-01-06 15:06:02,834 iteration 2753 : loss : 0.039273, loss_ce: 0.017479
2022-01-06 15:06:04,057 iteration 2754 : loss : 0.056175, loss_ce: 0.020578
 40%|██████████▉                | 162/400 [1:01:04<1:30:10, 22.73s/it]2022-01-06 15:06:05,322 iteration 2755 : loss : 0.039878, loss_ce: 0.014341
2022-01-06 15:06:06,550 iteration 2756 : loss : 0.047937, loss_ce: 0.020718
2022-01-06 15:06:07,721 iteration 2757 : loss : 0.045771, loss_ce: 0.018221
2022-01-06 15:06:08,926 iteration 2758 : loss : 0.048699, loss_ce: 0.020907
2022-01-06 15:06:10,130 iteration 2759 : loss : 0.038736, loss_ce: 0.014693
2022-01-06 15:06:11,396 iteration 2760 : loss : 0.047385, loss_ce: 0.012781
2022-01-06 15:06:12,591 iteration 2761 : loss : 0.033882, loss_ce: 0.012650
2022-01-06 15:06:13,771 iteration 2762 : loss : 0.048990, loss_ce: 0.017932
2022-01-06 15:06:14,953 iteration 2763 : loss : 0.037479, loss_ce: 0.015766
2022-01-06 15:06:16,077 iteration 2764 : loss : 0.032476, loss_ce: 0.013847
2022-01-06 15:06:17,270 iteration 2765 : loss : 0.072878, loss_ce: 0.031066
2022-01-06 15:06:18,527 iteration 2766 : loss : 0.054513, loss_ce: 0.019721
2022-01-06 15:06:19,638 iteration 2767 : loss : 0.030990, loss_ce: 0.009763
2022-01-06 15:06:20,959 iteration 2768 : loss : 0.066002, loss_ce: 0.023811
2022-01-06 15:06:22,114 iteration 2769 : loss : 0.037317, loss_ce: 0.012978
2022-01-06 15:06:23,228 iteration 2770 : loss : 0.041710, loss_ce: 0.020114
2022-01-06 15:06:24,403 iteration 2771 : loss : 0.042589, loss_ce: 0.019345
 41%|███████████                | 163/400 [1:01:24<1:26:58, 22.02s/it]2022-01-06 15:06:25,746 iteration 2772 : loss : 0.044402, loss_ce: 0.014756
2022-01-06 15:06:26,850 iteration 2773 : loss : 0.042679, loss_ce: 0.015364
2022-01-06 15:06:28,010 iteration 2774 : loss : 0.043650, loss_ce: 0.021959
2022-01-06 15:06:29,201 iteration 2775 : loss : 0.043430, loss_ce: 0.018282
2022-01-06 15:06:30,390 iteration 2776 : loss : 0.059576, loss_ce: 0.032796
2022-01-06 15:06:31,617 iteration 2777 : loss : 0.045946, loss_ce: 0.018869
2022-01-06 15:06:32,717 iteration 2778 : loss : 0.037438, loss_ce: 0.016559
2022-01-06 15:06:33,945 iteration 2779 : loss : 0.056045, loss_ce: 0.024151
2022-01-06 15:06:35,213 iteration 2780 : loss : 0.049752, loss_ce: 0.020683
2022-01-06 15:06:36,300 iteration 2781 : loss : 0.041265, loss_ce: 0.015558
2022-01-06 15:06:37,525 iteration 2782 : loss : 0.040193, loss_ce: 0.014800
2022-01-06 15:06:38,795 iteration 2783 : loss : 0.054237, loss_ce: 0.018756
2022-01-06 15:06:40,105 iteration 2784 : loss : 0.043260, loss_ce: 0.018823
2022-01-06 15:06:41,212 iteration 2785 : loss : 0.042487, loss_ce: 0.015828
2022-01-06 15:06:42,420 iteration 2786 : loss : 0.062298, loss_ce: 0.022050
2022-01-06 15:06:43,707 iteration 2787 : loss : 0.059177, loss_ce: 0.014344
2022-01-06 15:06:44,909 iteration 2788 : loss : 0.037133, loss_ce: 0.012283
 41%|███████████                | 164/400 [1:01:45<1:24:48, 21.56s/it]2022-01-06 15:06:46,161 iteration 2789 : loss : 0.029417, loss_ce: 0.014874
2022-01-06 15:06:47,302 iteration 2790 : loss : 0.051506, loss_ce: 0.013099
2022-01-06 15:06:48,515 iteration 2791 : loss : 0.048650, loss_ce: 0.012962
2022-01-06 15:06:49,766 iteration 2792 : loss : 0.067837, loss_ce: 0.021003
2022-01-06 15:06:50,963 iteration 2793 : loss : 0.066692, loss_ce: 0.032528
2022-01-06 15:06:52,173 iteration 2794 : loss : 0.048817, loss_ce: 0.021383
2022-01-06 15:06:53,434 iteration 2795 : loss : 0.049811, loss_ce: 0.019520
2022-01-06 15:06:54,739 iteration 2796 : loss : 0.049974, loss_ce: 0.018831
2022-01-06 15:06:55,986 iteration 2797 : loss : 0.050088, loss_ce: 0.021315
2022-01-06 15:06:57,292 iteration 2798 : loss : 0.054074, loss_ce: 0.019512
2022-01-06 15:06:58,560 iteration 2799 : loss : 0.049537, loss_ce: 0.018829
2022-01-06 15:06:59,809 iteration 2800 : loss : 0.040577, loss_ce: 0.019441
2022-01-06 15:07:01,012 iteration 2801 : loss : 0.051822, loss_ce: 0.023691
2022-01-06 15:07:02,169 iteration 2802 : loss : 0.043701, loss_ce: 0.019634
2022-01-06 15:07:03,513 iteration 2803 : loss : 0.063275, loss_ce: 0.022214
2022-01-06 15:07:04,741 iteration 2804 : loss : 0.055393, loss_ce: 0.023065
2022-01-06 15:07:04,741 Training Data Eval:
2022-01-06 15:07:10,659   Average segmentation loss on training set: 0.7868
2022-01-06 15:07:10,659 Validation Data Eval:
2022-01-06 15:07:12,682   Average segmentation loss on validation set: 0.7481
2022-01-06 15:07:13,853 iteration 2805 : loss : 0.072216, loss_ce: 0.025510
 41%|███████████▏               | 165/400 [1:02:14<1:33:07, 23.78s/it]2022-01-06 15:07:15,147 iteration 2806 : loss : 0.065398, loss_ce: 0.022124
2022-01-06 15:07:16,498 iteration 2807 : loss : 0.054111, loss_ce: 0.019091
2022-01-06 15:07:17,566 iteration 2808 : loss : 0.042288, loss_ce: 0.020333
2022-01-06 15:07:18,751 iteration 2809 : loss : 0.049556, loss_ce: 0.018768
2022-01-06 15:07:19,988 iteration 2810 : loss : 0.045650, loss_ce: 0.015142
2022-01-06 15:07:21,139 iteration 2811 : loss : 0.047668, loss_ce: 0.017206
2022-01-06 15:07:22,338 iteration 2812 : loss : 0.055037, loss_ce: 0.024431
2022-01-06 15:07:23,577 iteration 2813 : loss : 0.048477, loss_ce: 0.015230
2022-01-06 15:07:24,778 iteration 2814 : loss : 0.067742, loss_ce: 0.027454
2022-01-06 15:07:25,921 iteration 2815 : loss : 0.039983, loss_ce: 0.018916
2022-01-06 15:07:27,065 iteration 2816 : loss : 0.045132, loss_ce: 0.021337
2022-01-06 15:07:28,231 iteration 2817 : loss : 0.035557, loss_ce: 0.011988
2022-01-06 15:07:29,520 iteration 2818 : loss : 0.067001, loss_ce: 0.018476
2022-01-06 15:07:30,776 iteration 2819 : loss : 0.040447, loss_ce: 0.018199
2022-01-06 15:07:32,106 iteration 2820 : loss : 0.044988, loss_ce: 0.019177
2022-01-06 15:07:33,286 iteration 2821 : loss : 0.052839, loss_ce: 0.025112
2022-01-06 15:07:34,424 iteration 2822 : loss : 0.038305, loss_ce: 0.012758
 42%|███████████▏               | 166/400 [1:02:34<1:28:58, 22.81s/it]2022-01-06 15:07:35,761 iteration 2823 : loss : 0.072511, loss_ce: 0.025677
2022-01-06 15:07:36,885 iteration 2824 : loss : 0.033501, loss_ce: 0.013361
2022-01-06 15:07:38,107 iteration 2825 : loss : 0.070085, loss_ce: 0.030532
2022-01-06 15:07:39,255 iteration 2826 : loss : 0.037958, loss_ce: 0.016062
2022-01-06 15:07:40,485 iteration 2827 : loss : 0.054661, loss_ce: 0.025217
2022-01-06 15:07:41,692 iteration 2828 : loss : 0.060009, loss_ce: 0.025227
2022-01-06 15:07:43,011 iteration 2829 : loss : 0.045413, loss_ce: 0.019459
2022-01-06 15:07:44,129 iteration 2830 : loss : 0.035083, loss_ce: 0.015422
2022-01-06 15:07:45,324 iteration 2831 : loss : 0.053774, loss_ce: 0.026943
2022-01-06 15:07:46,560 iteration 2832 : loss : 0.059465, loss_ce: 0.019472
2022-01-06 15:07:47,823 iteration 2833 : loss : 0.043788, loss_ce: 0.017488
2022-01-06 15:07:48,995 iteration 2834 : loss : 0.057285, loss_ce: 0.021284
2022-01-06 15:07:50,227 iteration 2835 : loss : 0.086360, loss_ce: 0.022557
2022-01-06 15:07:51,480 iteration 2836 : loss : 0.051622, loss_ce: 0.021423
2022-01-06 15:07:52,663 iteration 2837 : loss : 0.055448, loss_ce: 0.014132
2022-01-06 15:07:53,781 iteration 2838 : loss : 0.035743, loss_ce: 0.011500
2022-01-06 15:07:55,011 iteration 2839 : loss : 0.056451, loss_ce: 0.019874
 42%|███████████▎               | 167/400 [1:02:55<1:26:00, 22.15s/it]2022-01-06 15:07:56,229 iteration 2840 : loss : 0.092988, loss_ce: 0.048641
2022-01-06 15:07:57,424 iteration 2841 : loss : 0.041754, loss_ce: 0.019990
2022-01-06 15:07:58,614 iteration 2842 : loss : 0.062367, loss_ce: 0.023759
2022-01-06 15:07:59,709 iteration 2843 : loss : 0.039552, loss_ce: 0.015176
2022-01-06 15:08:00,795 iteration 2844 : loss : 0.034499, loss_ce: 0.012331
2022-01-06 15:08:01,951 iteration 2845 : loss : 0.061536, loss_ce: 0.022574
2022-01-06 15:08:03,258 iteration 2846 : loss : 0.059987, loss_ce: 0.021257
2022-01-06 15:08:04,482 iteration 2847 : loss : 0.061796, loss_ce: 0.023991
2022-01-06 15:08:05,656 iteration 2848 : loss : 0.051129, loss_ce: 0.017374
2022-01-06 15:08:06,865 iteration 2849 : loss : 0.051218, loss_ce: 0.018996
2022-01-06 15:08:08,046 iteration 2850 : loss : 0.048916, loss_ce: 0.022909
2022-01-06 15:08:09,327 iteration 2851 : loss : 0.046318, loss_ce: 0.020763
2022-01-06 15:08:10,461 iteration 2852 : loss : 0.047184, loss_ce: 0.015533
2022-01-06 15:08:11,675 iteration 2853 : loss : 0.068839, loss_ce: 0.020856
2022-01-06 15:08:12,793 iteration 2854 : loss : 0.057622, loss_ce: 0.016148
2022-01-06 15:08:13,942 iteration 2855 : loss : 0.046680, loss_ce: 0.020021
2022-01-06 15:08:15,033 iteration 2856 : loss : 0.046301, loss_ce: 0.021566
 42%|███████████▎               | 168/400 [1:03:15<1:23:11, 21.51s/it]2022-01-06 15:08:16,297 iteration 2857 : loss : 0.053502, loss_ce: 0.024367
2022-01-06 15:08:17,614 iteration 2858 : loss : 0.100260, loss_ce: 0.034429
2022-01-06 15:08:18,839 iteration 2859 : loss : 0.038737, loss_ce: 0.014741
2022-01-06 15:08:20,052 iteration 2860 : loss : 0.038535, loss_ce: 0.014306
2022-01-06 15:08:21,244 iteration 2861 : loss : 0.044012, loss_ce: 0.013083
2022-01-06 15:08:22,470 iteration 2862 : loss : 0.085977, loss_ce: 0.021035
2022-01-06 15:08:23,595 iteration 2863 : loss : 0.039836, loss_ce: 0.014063
2022-01-06 15:08:24,764 iteration 2864 : loss : 0.047098, loss_ce: 0.017763
2022-01-06 15:08:25,937 iteration 2865 : loss : 0.050201, loss_ce: 0.020833
2022-01-06 15:08:27,105 iteration 2866 : loss : 0.031558, loss_ce: 0.011604
2022-01-06 15:08:28,403 iteration 2867 : loss : 0.056425, loss_ce: 0.021267
2022-01-06 15:08:29,631 iteration 2868 : loss : 0.047170, loss_ce: 0.019865
2022-01-06 15:08:30,865 iteration 2869 : loss : 0.043939, loss_ce: 0.016491
2022-01-06 15:08:32,146 iteration 2870 : loss : 0.052032, loss_ce: 0.018264
2022-01-06 15:08:33,248 iteration 2871 : loss : 0.069061, loss_ce: 0.041391
2022-01-06 15:08:34,394 iteration 2872 : loss : 0.045586, loss_ce: 0.021356
2022-01-06 15:08:35,554 iteration 2873 : loss : 0.057871, loss_ce: 0.025931
 42%|███████████▍               | 169/400 [1:03:35<1:21:40, 21.22s/it]2022-01-06 15:08:36,783 iteration 2874 : loss : 0.036745, loss_ce: 0.013317
2022-01-06 15:08:38,037 iteration 2875 : loss : 0.043788, loss_ce: 0.018260
2022-01-06 15:08:39,170 iteration 2876 : loss : 0.066060, loss_ce: 0.024414
2022-01-06 15:08:40,274 iteration 2877 : loss : 0.059795, loss_ce: 0.019054
2022-01-06 15:08:41,455 iteration 2878 : loss : 0.039733, loss_ce: 0.013199
2022-01-06 15:08:42,559 iteration 2879 : loss : 0.038918, loss_ce: 0.017170
2022-01-06 15:08:43,781 iteration 2880 : loss : 0.043012, loss_ce: 0.022610
2022-01-06 15:08:44,930 iteration 2881 : loss : 0.031223, loss_ce: 0.013767
2022-01-06 15:08:46,082 iteration 2882 : loss : 0.057008, loss_ce: 0.020842
2022-01-06 15:08:47,316 iteration 2883 : loss : 0.054000, loss_ce: 0.015750
2022-01-06 15:08:48,548 iteration 2884 : loss : 0.061867, loss_ce: 0.026012
2022-01-06 15:08:49,729 iteration 2885 : loss : 0.053434, loss_ce: 0.022067
2022-01-06 15:08:50,862 iteration 2886 : loss : 0.040543, loss_ce: 0.014019
2022-01-06 15:08:52,002 iteration 2887 : loss : 0.056727, loss_ce: 0.020188
2022-01-06 15:08:53,227 iteration 2888 : loss : 0.054761, loss_ce: 0.026568
2022-01-06 15:08:54,399 iteration 2889 : loss : 0.057171, loss_ce: 0.022351
2022-01-06 15:08:54,400 Training Data Eval:
2022-01-06 15:09:00,229   Average segmentation loss on training set: 0.1580
2022-01-06 15:09:00,230 Validation Data Eval:
2022-01-06 15:09:02,232   Average segmentation loss on validation set: 0.3171
2022-01-06 15:09:03,392 iteration 2890 : loss : 0.051986, loss_ce: 0.021957
 42%|███████████▍               | 170/400 [1:04:03<1:28:56, 23.20s/it]2022-01-06 15:09:04,625 iteration 2891 : loss : 0.039799, loss_ce: 0.016780
2022-01-06 15:09:05,820 iteration 2892 : loss : 0.038928, loss_ce: 0.015616
2022-01-06 15:09:06,901 iteration 2893 : loss : 0.033217, loss_ce: 0.013112
2022-01-06 15:09:08,025 iteration 2894 : loss : 0.036519, loss_ce: 0.014256
2022-01-06 15:09:09,165 iteration 2895 : loss : 0.057252, loss_ce: 0.017798
2022-01-06 15:09:10,339 iteration 2896 : loss : 0.067713, loss_ce: 0.025115
2022-01-06 15:09:11,656 iteration 2897 : loss : 0.092833, loss_ce: 0.024323
2022-01-06 15:09:12,854 iteration 2898 : loss : 0.050004, loss_ce: 0.018083
2022-01-06 15:09:14,029 iteration 2899 : loss : 0.060358, loss_ce: 0.024351
2022-01-06 15:09:15,351 iteration 2900 : loss : 0.062581, loss_ce: 0.025431
2022-01-06 15:09:16,568 iteration 2901 : loss : 0.033771, loss_ce: 0.012456
2022-01-06 15:09:17,719 iteration 2902 : loss : 0.093145, loss_ce: 0.068354
2022-01-06 15:09:18,917 iteration 2903 : loss : 0.050298, loss_ce: 0.020041
2022-01-06 15:09:20,094 iteration 2904 : loss : 0.048275, loss_ce: 0.021323
2022-01-06 15:09:21,316 iteration 2905 : loss : 0.038600, loss_ce: 0.016603
2022-01-06 15:09:22,444 iteration 2906 : loss : 0.039196, loss_ce: 0.018397
2022-01-06 15:09:23,630 iteration 2907 : loss : 0.032499, loss_ce: 0.012558
 43%|███████████▌               | 171/400 [1:04:23<1:25:08, 22.31s/it]2022-01-06 15:09:24,905 iteration 2908 : loss : 0.044270, loss_ce: 0.017392
2022-01-06 15:09:26,146 iteration 2909 : loss : 0.047926, loss_ce: 0.014428
2022-01-06 15:09:27,367 iteration 2910 : loss : 0.032837, loss_ce: 0.012495
2022-01-06 15:09:28,603 iteration 2911 : loss : 0.042527, loss_ce: 0.015486
2022-01-06 15:09:29,733 iteration 2912 : loss : 0.035479, loss_ce: 0.014054
2022-01-06 15:09:30,934 iteration 2913 : loss : 0.050785, loss_ce: 0.017858
2022-01-06 15:09:32,166 iteration 2914 : loss : 0.050708, loss_ce: 0.022086
2022-01-06 15:09:33,363 iteration 2915 : loss : 0.041641, loss_ce: 0.020920
2022-01-06 15:09:34,540 iteration 2916 : loss : 0.047536, loss_ce: 0.021251
2022-01-06 15:09:35,725 iteration 2917 : loss : 0.044307, loss_ce: 0.017454
2022-01-06 15:09:36,824 iteration 2918 : loss : 0.052680, loss_ce: 0.022262
2022-01-06 15:09:38,015 iteration 2919 : loss : 0.052905, loss_ce: 0.020366
2022-01-06 15:09:39,173 iteration 2920 : loss : 0.040234, loss_ce: 0.018099
2022-01-06 15:09:40,351 iteration 2921 : loss : 0.042744, loss_ce: 0.019342
2022-01-06 15:09:41,512 iteration 2922 : loss : 0.067418, loss_ce: 0.023784
2022-01-06 15:09:42,784 iteration 2923 : loss : 0.055524, loss_ce: 0.014379
2022-01-06 15:09:44,009 iteration 2924 : loss : 0.033660, loss_ce: 0.014039
 43%|███████████▌               | 172/400 [1:04:44<1:22:35, 21.73s/it]2022-01-06 15:09:45,263 iteration 2925 : loss : 0.046978, loss_ce: 0.022541
2022-01-06 15:09:46,373 iteration 2926 : loss : 0.047393, loss_ce: 0.019398
2022-01-06 15:09:47,564 iteration 2927 : loss : 0.027970, loss_ce: 0.009859
2022-01-06 15:09:48,794 iteration 2928 : loss : 0.044462, loss_ce: 0.015626
2022-01-06 15:09:49,934 iteration 2929 : loss : 0.060331, loss_ce: 0.018165
2022-01-06 15:09:51,066 iteration 2930 : loss : 0.049560, loss_ce: 0.024989
2022-01-06 15:09:52,250 iteration 2931 : loss : 0.122711, loss_ce: 0.019064
2022-01-06 15:09:53,527 iteration 2932 : loss : 0.056325, loss_ce: 0.024562
2022-01-06 15:09:54,755 iteration 2933 : loss : 0.053752, loss_ce: 0.022561
2022-01-06 15:09:56,013 iteration 2934 : loss : 0.045932, loss_ce: 0.020825
2022-01-06 15:09:57,162 iteration 2935 : loss : 0.040531, loss_ce: 0.011765
2022-01-06 15:09:58,342 iteration 2936 : loss : 0.030068, loss_ce: 0.013526
2022-01-06 15:09:59,544 iteration 2937 : loss : 0.067435, loss_ce: 0.034740
2022-01-06 15:10:00,753 iteration 2938 : loss : 0.049187, loss_ce: 0.015641
2022-01-06 15:10:01,820 iteration 2939 : loss : 0.048007, loss_ce: 0.015609
2022-01-06 15:10:02,978 iteration 2940 : loss : 0.040058, loss_ce: 0.015082
2022-01-06 15:10:04,144 iteration 2941 : loss : 0.062986, loss_ce: 0.025367
 43%|███████████▋               | 173/400 [1:05:04<1:20:24, 21.25s/it]2022-01-06 15:10:05,494 iteration 2942 : loss : 0.066121, loss_ce: 0.029496
2022-01-06 15:10:06,713 iteration 2943 : loss : 0.040981, loss_ce: 0.019041
2022-01-06 15:10:07,898 iteration 2944 : loss : 0.076800, loss_ce: 0.023372
2022-01-06 15:10:09,068 iteration 2945 : loss : 0.052006, loss_ce: 0.022184
2022-01-06 15:10:10,325 iteration 2946 : loss : 0.034717, loss_ce: 0.014658
2022-01-06 15:10:11,470 iteration 2947 : loss : 0.050175, loss_ce: 0.017099
2022-01-06 15:10:12,672 iteration 2948 : loss : 0.044896, loss_ce: 0.018555
2022-01-06 15:10:13,814 iteration 2949 : loss : 0.050369, loss_ce: 0.018379
2022-01-06 15:10:14,899 iteration 2950 : loss : 0.032106, loss_ce: 0.013402
2022-01-06 15:10:16,080 iteration 2951 : loss : 0.037964, loss_ce: 0.013407
2022-01-06 15:10:17,180 iteration 2952 : loss : 0.115018, loss_ce: 0.027210
2022-01-06 15:10:18,447 iteration 2953 : loss : 0.043161, loss_ce: 0.017291
2022-01-06 15:10:19,630 iteration 2954 : loss : 0.052182, loss_ce: 0.022490
2022-01-06 15:10:20,809 iteration 2955 : loss : 0.067504, loss_ce: 0.017510
2022-01-06 15:10:21,917 iteration 2956 : loss : 0.073922, loss_ce: 0.039163
2022-01-06 15:10:23,029 iteration 2957 : loss : 0.048692, loss_ce: 0.016196
2022-01-06 15:10:24,228 iteration 2958 : loss : 0.062725, loss_ce: 0.023579
 44%|███████████▋               | 174/400 [1:05:24<1:18:44, 20.90s/it]2022-01-06 15:10:25,523 iteration 2959 : loss : 0.049078, loss_ce: 0.023090
2022-01-06 15:10:26,697 iteration 2960 : loss : 0.048718, loss_ce: 0.022851
2022-01-06 15:10:27,920 iteration 2961 : loss : 0.041841, loss_ce: 0.019052
2022-01-06 15:10:29,139 iteration 2962 : loss : 0.041745, loss_ce: 0.016205
2022-01-06 15:10:30,282 iteration 2963 : loss : 0.041033, loss_ce: 0.016571
2022-01-06 15:10:31,415 iteration 2964 : loss : 0.044498, loss_ce: 0.017732
2022-01-06 15:10:32,519 iteration 2965 : loss : 0.083722, loss_ce: 0.031458
2022-01-06 15:10:33,610 iteration 2966 : loss : 0.051597, loss_ce: 0.019979
2022-01-06 15:10:34,818 iteration 2967 : loss : 0.044126, loss_ce: 0.020530
2022-01-06 15:10:36,111 iteration 2968 : loss : 0.110492, loss_ce: 0.040110
2022-01-06 15:10:37,343 iteration 2969 : loss : 0.034221, loss_ce: 0.013890
2022-01-06 15:10:38,502 iteration 2970 : loss : 0.046467, loss_ce: 0.015478
2022-01-06 15:10:39,606 iteration 2971 : loss : 0.048510, loss_ce: 0.021410
2022-01-06 15:10:40,832 iteration 2972 : loss : 0.076929, loss_ce: 0.028529
2022-01-06 15:10:41,968 iteration 2973 : loss : 0.039744, loss_ce: 0.016507
2022-01-06 15:10:43,215 iteration 2974 : loss : 0.028732, loss_ce: 0.010269
2022-01-06 15:10:43,215 Training Data Eval:
2022-01-06 15:10:49,029   Average segmentation loss on training set: 0.2033
2022-01-06 15:10:49,029 Validation Data Eval:
2022-01-06 15:10:51,033   Average segmentation loss on validation set: 0.3422
2022-01-06 15:10:52,262 iteration 2975 : loss : 0.054625, loss_ce: 0.015526
 44%|███████████▊               | 175/400 [1:05:52<1:26:24, 23.04s/it]2022-01-06 15:10:53,616 iteration 2976 : loss : 0.055172, loss_ce: 0.027485
2022-01-06 15:10:54,703 iteration 2977 : loss : 0.042766, loss_ce: 0.016820
2022-01-06 15:10:55,790 iteration 2978 : loss : 0.040024, loss_ce: 0.014011
2022-01-06 15:10:57,022 iteration 2979 : loss : 0.030230, loss_ce: 0.011397
2022-01-06 15:10:58,352 iteration 2980 : loss : 0.045439, loss_ce: 0.018605
2022-01-06 15:10:59,536 iteration 2981 : loss : 0.060880, loss_ce: 0.020006
2022-01-06 15:11:00,796 iteration 2982 : loss : 0.040205, loss_ce: 0.017649
2022-01-06 15:11:01,944 iteration 2983 : loss : 0.046560, loss_ce: 0.021032
2022-01-06 15:11:03,189 iteration 2984 : loss : 0.036692, loss_ce: 0.015620
2022-01-06 15:11:04,394 iteration 2985 : loss : 0.056091, loss_ce: 0.016970
2022-01-06 15:11:05,637 iteration 2986 : loss : 0.042707, loss_ce: 0.017335
2022-01-06 15:11:06,760 iteration 2987 : loss : 0.055420, loss_ce: 0.027159
2022-01-06 15:11:07,917 iteration 2988 : loss : 0.034259, loss_ce: 0.012573
2022-01-06 15:11:09,129 iteration 2989 : loss : 0.043683, loss_ce: 0.011666
2022-01-06 15:11:10,352 iteration 2990 : loss : 0.062235, loss_ce: 0.022016
2022-01-06 15:11:11,642 iteration 2991 : loss : 0.039418, loss_ce: 0.012103
2022-01-06 15:11:12,996 iteration 2992 : loss : 0.051011, loss_ce: 0.021353
 44%|███████████▉               | 176/400 [1:06:13<1:23:25, 22.35s/it]2022-01-06 15:11:14,172 iteration 2993 : loss : 0.035029, loss_ce: 0.009977
2022-01-06 15:11:15,359 iteration 2994 : loss : 0.038478, loss_ce: 0.013448
2022-01-06 15:11:16,661 iteration 2995 : loss : 0.048624, loss_ce: 0.020382
2022-01-06 15:11:17,853 iteration 2996 : loss : 0.047979, loss_ce: 0.017113
2022-01-06 15:11:19,039 iteration 2997 : loss : 0.039513, loss_ce: 0.014022
2022-01-06 15:11:20,218 iteration 2998 : loss : 0.047845, loss_ce: 0.016941
2022-01-06 15:11:21,467 iteration 2999 : loss : 0.052115, loss_ce: 0.019864
2022-01-06 15:11:22,649 iteration 3000 : loss : 0.056717, loss_ce: 0.018881
2022-01-06 15:11:23,879 iteration 3001 : loss : 0.039416, loss_ce: 0.014569
2022-01-06 15:11:24,972 iteration 3002 : loss : 0.043976, loss_ce: 0.017793
2022-01-06 15:11:26,223 iteration 3003 : loss : 0.044226, loss_ce: 0.019220
2022-01-06 15:11:27,313 iteration 3004 : loss : 0.032809, loss_ce: 0.013064
2022-01-06 15:11:28,534 iteration 3005 : loss : 0.066656, loss_ce: 0.024474
2022-01-06 15:11:29,768 iteration 3006 : loss : 0.042306, loss_ce: 0.015957
2022-01-06 15:11:30,933 iteration 3007 : loss : 0.038619, loss_ce: 0.014628
2022-01-06 15:11:32,255 iteration 3008 : loss : 0.047916, loss_ce: 0.023618
2022-01-06 15:11:33,421 iteration 3009 : loss : 0.048144, loss_ce: 0.023815
 44%|███████████▉               | 177/400 [1:06:33<1:20:55, 21.77s/it]2022-01-06 15:11:34,660 iteration 3010 : loss : 0.044802, loss_ce: 0.017079
2022-01-06 15:11:35,839 iteration 3011 : loss : 0.055264, loss_ce: 0.020499
2022-01-06 15:11:36,976 iteration 3012 : loss : 0.044275, loss_ce: 0.013634
2022-01-06 15:11:38,106 iteration 3013 : loss : 0.033074, loss_ce: 0.014449
2022-01-06 15:11:39,364 iteration 3014 : loss : 0.066146, loss_ce: 0.024144
2022-01-06 15:11:40,568 iteration 3015 : loss : 0.041221, loss_ce: 0.014536
2022-01-06 15:11:41,742 iteration 3016 : loss : 0.045654, loss_ce: 0.014111
2022-01-06 15:11:42,910 iteration 3017 : loss : 0.059398, loss_ce: 0.018317
2022-01-06 15:11:44,042 iteration 3018 : loss : 0.040239, loss_ce: 0.011533
2022-01-06 15:11:45,209 iteration 3019 : loss : 0.039513, loss_ce: 0.015633
2022-01-06 15:11:46,263 iteration 3020 : loss : 0.040088, loss_ce: 0.020792
2022-01-06 15:11:47,345 iteration 3021 : loss : 0.034807, loss_ce: 0.012016
2022-01-06 15:11:48,503 iteration 3022 : loss : 0.057522, loss_ce: 0.016541
2022-01-06 15:11:49,661 iteration 3023 : loss : 0.045695, loss_ce: 0.022941
2022-01-06 15:11:50,840 iteration 3024 : loss : 0.048101, loss_ce: 0.018999
2022-01-06 15:11:51,914 iteration 3025 : loss : 0.049640, loss_ce: 0.019706
2022-01-06 15:11:53,145 iteration 3026 : loss : 0.041133, loss_ce: 0.019285
 44%|████████████               | 178/400 [1:06:53<1:18:17, 21.16s/it]2022-01-06 15:11:54,393 iteration 3027 : loss : 0.037889, loss_ce: 0.017423
2022-01-06 15:11:55,556 iteration 3028 : loss : 0.043049, loss_ce: 0.017834
2022-01-06 15:11:56,744 iteration 3029 : loss : 0.029784, loss_ce: 0.013500
2022-01-06 15:11:58,029 iteration 3030 : loss : 0.051229, loss_ce: 0.020059
2022-01-06 15:11:59,103 iteration 3031 : loss : 0.034480, loss_ce: 0.013352
2022-01-06 15:12:00,276 iteration 3032 : loss : 0.039977, loss_ce: 0.014256
2022-01-06 15:12:01,483 iteration 3033 : loss : 0.040692, loss_ce: 0.021237
2022-01-06 15:12:02,631 iteration 3034 : loss : 0.063812, loss_ce: 0.026457
2022-01-06 15:12:03,891 iteration 3035 : loss : 0.051297, loss_ce: 0.017987
2022-01-06 15:12:05,073 iteration 3036 : loss : 0.035428, loss_ce: 0.016261
2022-01-06 15:12:06,300 iteration 3037 : loss : 0.043947, loss_ce: 0.018568
2022-01-06 15:12:07,385 iteration 3038 : loss : 0.053708, loss_ce: 0.013427
2022-01-06 15:12:08,750 iteration 3039 : loss : 0.061025, loss_ce: 0.025309
2022-01-06 15:12:09,865 iteration 3040 : loss : 0.040171, loss_ce: 0.015818
2022-01-06 15:12:11,005 iteration 3041 : loss : 0.043710, loss_ce: 0.017205
2022-01-06 15:12:12,140 iteration 3042 : loss : 0.039769, loss_ce: 0.018904
2022-01-06 15:12:13,469 iteration 3043 : loss : 0.048365, loss_ce: 0.020626
 45%|████████████               | 179/400 [1:07:13<1:17:00, 20.91s/it]2022-01-06 15:12:14,746 iteration 3044 : loss : 0.045470, loss_ce: 0.016823
2022-01-06 15:12:15,893 iteration 3045 : loss : 0.036257, loss_ce: 0.016862
2022-01-06 15:12:17,026 iteration 3046 : loss : 0.054970, loss_ce: 0.021526
2022-01-06 15:12:18,336 iteration 3047 : loss : 0.051062, loss_ce: 0.023134
2022-01-06 15:12:19,586 iteration 3048 : loss : 0.042555, loss_ce: 0.021318
2022-01-06 15:12:20,723 iteration 3049 : loss : 0.038692, loss_ce: 0.015633
2022-01-06 15:12:22,026 iteration 3050 : loss : 0.055760, loss_ce: 0.025300
2022-01-06 15:12:23,254 iteration 3051 : loss : 0.074438, loss_ce: 0.019834
2022-01-06 15:12:24,531 iteration 3052 : loss : 0.067608, loss_ce: 0.028177
2022-01-06 15:12:25,733 iteration 3053 : loss : 0.044915, loss_ce: 0.017481
2022-01-06 15:12:26,932 iteration 3054 : loss : 0.068638, loss_ce: 0.024582
2022-01-06 15:12:28,153 iteration 3055 : loss : 0.048986, loss_ce: 0.019657
2022-01-06 15:12:29,343 iteration 3056 : loss : 0.062302, loss_ce: 0.022688
2022-01-06 15:12:30,509 iteration 3057 : loss : 0.032546, loss_ce: 0.012226
2022-01-06 15:12:31,780 iteration 3058 : loss : 0.057186, loss_ce: 0.018871
2022-01-06 15:12:33,062 iteration 3059 : loss : 0.072844, loss_ce: 0.030416
2022-01-06 15:12:33,062 Training Data Eval:
2022-01-06 15:12:38,914   Average segmentation loss on training set: 0.0741
2022-01-06 15:12:38,914 Validation Data Eval:
2022-01-06 15:12:40,912   Average segmentation loss on validation set: 0.1700
2022-01-06 15:12:42,093 iteration 3060 : loss : 0.067342, loss_ce: 0.028870
 45%|████████████▏              | 180/400 [1:07:42<1:25:09, 23.22s/it]2022-01-06 15:12:43,310 iteration 3061 : loss : 0.047857, loss_ce: 0.016671
2022-01-06 15:12:44,524 iteration 3062 : loss : 0.045476, loss_ce: 0.016350
2022-01-06 15:12:45,677 iteration 3063 : loss : 0.066038, loss_ce: 0.019010
2022-01-06 15:12:46,921 iteration 3064 : loss : 0.070653, loss_ce: 0.040953
2022-01-06 15:12:48,087 iteration 3065 : loss : 0.069376, loss_ce: 0.017729
2022-01-06 15:12:49,244 iteration 3066 : loss : 0.035817, loss_ce: 0.009344
2022-01-06 15:12:50,443 iteration 3067 : loss : 0.048682, loss_ce: 0.021773
2022-01-06 15:12:51,713 iteration 3068 : loss : 0.031138, loss_ce: 0.011220
2022-01-06 15:12:52,967 iteration 3069 : loss : 0.074099, loss_ce: 0.017694
2022-01-06 15:12:54,069 iteration 3070 : loss : 0.039419, loss_ce: 0.014301
2022-01-06 15:12:55,329 iteration 3071 : loss : 0.062323, loss_ce: 0.025736
2022-01-06 15:12:56,527 iteration 3072 : loss : 0.037388, loss_ce: 0.014436
2022-01-06 15:12:57,657 iteration 3073 : loss : 0.061313, loss_ce: 0.024578
2022-01-06 15:12:58,961 iteration 3074 : loss : 0.055301, loss_ce: 0.026843
2022-01-06 15:13:00,194 iteration 3075 : loss : 0.044528, loss_ce: 0.016451
2022-01-06 15:13:01,351 iteration 3076 : loss : 0.037921, loss_ce: 0.017449
2022-01-06 15:13:02,630 iteration 3077 : loss : 0.054990, loss_ce: 0.031370
 45%|████████████▏              | 181/400 [1:08:02<1:21:49, 22.42s/it]2022-01-06 15:13:03,759 iteration 3078 : loss : 0.027193, loss_ce: 0.011366
2022-01-06 15:13:05,022 iteration 3079 : loss : 0.043580, loss_ce: 0.015088
2022-01-06 15:13:06,223 iteration 3080 : loss : 0.038570, loss_ce: 0.015313
2022-01-06 15:13:07,393 iteration 3081 : loss : 0.046430, loss_ce: 0.013230
2022-01-06 15:13:08,592 iteration 3082 : loss : 0.035552, loss_ce: 0.014646
2022-01-06 15:13:09,787 iteration 3083 : loss : 0.052474, loss_ce: 0.022058
2022-01-06 15:13:10,954 iteration 3084 : loss : 0.032847, loss_ce: 0.013895
2022-01-06 15:13:12,146 iteration 3085 : loss : 0.113639, loss_ce: 0.021433
2022-01-06 15:13:13,289 iteration 3086 : loss : 0.041983, loss_ce: 0.017412
2022-01-06 15:13:14,419 iteration 3087 : loss : 0.044184, loss_ce: 0.013518
2022-01-06 15:13:15,548 iteration 3088 : loss : 0.031422, loss_ce: 0.011723
2022-01-06 15:13:16,673 iteration 3089 : loss : 0.040768, loss_ce: 0.016447
2022-01-06 15:13:17,880 iteration 3090 : loss : 0.056011, loss_ce: 0.019837
2022-01-06 15:13:19,018 iteration 3091 : loss : 0.053859, loss_ce: 0.019856
2022-01-06 15:13:20,103 iteration 3092 : loss : 0.043141, loss_ce: 0.019135
2022-01-06 15:13:21,271 iteration 3093 : loss : 0.033604, loss_ce: 0.010189
2022-01-06 15:13:22,413 iteration 3094 : loss : 0.060633, loss_ce: 0.033189
 46%|████████████▎              | 182/400 [1:08:22<1:18:33, 21.62s/it]2022-01-06 15:13:23,732 iteration 3095 : loss : 0.032811, loss_ce: 0.011198
2022-01-06 15:13:24,877 iteration 3096 : loss : 0.028975, loss_ce: 0.011982
2022-01-06 15:13:26,034 iteration 3097 : loss : 0.068258, loss_ce: 0.024776
2022-01-06 15:13:27,159 iteration 3098 : loss : 0.032574, loss_ce: 0.012376
2022-01-06 15:13:28,416 iteration 3099 : loss : 0.056893, loss_ce: 0.019593
2022-01-06 15:13:29,531 iteration 3100 : loss : 0.045287, loss_ce: 0.016218
2022-01-06 15:13:30,699 iteration 3101 : loss : 0.037427, loss_ce: 0.016862
2022-01-06 15:13:31,920 iteration 3102 : loss : 0.035233, loss_ce: 0.012826
2022-01-06 15:13:33,067 iteration 3103 : loss : 0.042363, loss_ce: 0.012547
2022-01-06 15:13:34,247 iteration 3104 : loss : 0.044043, loss_ce: 0.013575
2022-01-06 15:13:35,399 iteration 3105 : loss : 0.035250, loss_ce: 0.011346
2022-01-06 15:13:36,746 iteration 3106 : loss : 0.045497, loss_ce: 0.017165
2022-01-06 15:13:37,915 iteration 3107 : loss : 0.049837, loss_ce: 0.019874
2022-01-06 15:13:39,012 iteration 3108 : loss : 0.035853, loss_ce: 0.013166
2022-01-06 15:13:40,263 iteration 3109 : loss : 0.062770, loss_ce: 0.032828
2022-01-06 15:13:41,508 iteration 3110 : loss : 0.064738, loss_ce: 0.029677
2022-01-06 15:13:42,634 iteration 3111 : loss : 0.042483, loss_ce: 0.014255
 46%|████████████▎              | 183/400 [1:08:42<1:16:41, 21.20s/it]2022-01-06 15:13:43,819 iteration 3112 : loss : 0.029537, loss_ce: 0.011652
2022-01-06 15:13:44,984 iteration 3113 : loss : 0.033272, loss_ce: 0.014183
2022-01-06 15:13:46,300 iteration 3114 : loss : 0.082306, loss_ce: 0.024122
2022-01-06 15:13:47,537 iteration 3115 : loss : 0.037682, loss_ce: 0.017106
2022-01-06 15:13:48,729 iteration 3116 : loss : 0.041838, loss_ce: 0.016165
2022-01-06 15:13:49,882 iteration 3117 : loss : 0.042173, loss_ce: 0.018405
2022-01-06 15:13:51,125 iteration 3118 : loss : 0.050755, loss_ce: 0.021746
2022-01-06 15:13:52,344 iteration 3119 : loss : 0.040871, loss_ce: 0.015853
2022-01-06 15:13:53,541 iteration 3120 : loss : 0.034135, loss_ce: 0.011988
2022-01-06 15:13:54,760 iteration 3121 : loss : 0.053307, loss_ce: 0.015872
2022-01-06 15:13:56,036 iteration 3122 : loss : 0.058244, loss_ce: 0.017771
2022-01-06 15:13:57,232 iteration 3123 : loss : 0.036632, loss_ce: 0.016298
2022-01-06 15:13:58,446 iteration 3124 : loss : 0.042122, loss_ce: 0.016814
2022-01-06 15:13:59,657 iteration 3125 : loss : 0.060619, loss_ce: 0.026416
2022-01-06 15:14:00,769 iteration 3126 : loss : 0.036478, loss_ce: 0.014930
2022-01-06 15:14:01,970 iteration 3127 : loss : 0.038803, loss_ce: 0.015847
2022-01-06 15:14:03,068 iteration 3128 : loss : 0.034765, loss_ce: 0.012367
 46%|████████████▍              | 184/400 [1:09:03<1:15:29, 20.97s/it]2022-01-06 15:14:04,359 iteration 3129 : loss : 0.040066, loss_ce: 0.016161
2022-01-06 15:14:05,571 iteration 3130 : loss : 0.041248, loss_ce: 0.016440
2022-01-06 15:14:06,832 iteration 3131 : loss : 0.055355, loss_ce: 0.021261
2022-01-06 15:14:07,997 iteration 3132 : loss : 0.031327, loss_ce: 0.012326
2022-01-06 15:14:09,104 iteration 3133 : loss : 0.042722, loss_ce: 0.014991
2022-01-06 15:14:10,291 iteration 3134 : loss : 0.030670, loss_ce: 0.010441
2022-01-06 15:14:11,472 iteration 3135 : loss : 0.045115, loss_ce: 0.015722
2022-01-06 15:14:12,718 iteration 3136 : loss : 0.082663, loss_ce: 0.027373
2022-01-06 15:14:13,910 iteration 3137 : loss : 0.044356, loss_ce: 0.020128
2022-01-06 15:14:15,127 iteration 3138 : loss : 0.038656, loss_ce: 0.017865
2022-01-06 15:14:16,387 iteration 3139 : loss : 0.064765, loss_ce: 0.021741
2022-01-06 15:14:17,610 iteration 3140 : loss : 0.045562, loss_ce: 0.017234
2022-01-06 15:14:18,833 iteration 3141 : loss : 0.033966, loss_ce: 0.013166
2022-01-06 15:14:19,998 iteration 3142 : loss : 0.041899, loss_ce: 0.014259
2022-01-06 15:14:21,249 iteration 3143 : loss : 0.045021, loss_ce: 0.020657
2022-01-06 15:14:22,515 iteration 3144 : loss : 0.041996, loss_ce: 0.019902
2022-01-06 15:14:22,516 Training Data Eval:
2022-01-06 15:14:28,314   Average segmentation loss on training set: 0.0422
2022-01-06 15:14:28,315 Validation Data Eval:
2022-01-06 15:14:30,302   Average segmentation loss on validation set: 0.1278
2022-01-06 15:14:31,366 iteration 3145 : loss : 0.041309, loss_ce: 0.016964
 46%|████████████▍              | 185/400 [1:09:31<1:23:01, 23.17s/it]2022-01-06 15:14:32,511 iteration 3146 : loss : 0.036032, loss_ce: 0.012541
2022-01-06 15:14:33,664 iteration 3147 : loss : 0.036790, loss_ce: 0.016091
2022-01-06 15:14:34,834 iteration 3148 : loss : 0.037195, loss_ce: 0.011470
2022-01-06 15:14:35,975 iteration 3149 : loss : 0.039810, loss_ce: 0.016690
2022-01-06 15:14:37,136 iteration 3150 : loss : 0.047945, loss_ce: 0.021704
2022-01-06 15:14:38,283 iteration 3151 : loss : 0.047702, loss_ce: 0.013654
2022-01-06 15:14:39,443 iteration 3152 : loss : 0.038007, loss_ce: 0.015044
2022-01-06 15:14:40,641 iteration 3153 : loss : 0.044539, loss_ce: 0.016190
2022-01-06 15:14:41,778 iteration 3154 : loss : 0.041264, loss_ce: 0.019182
2022-01-06 15:14:42,930 iteration 3155 : loss : 0.038431, loss_ce: 0.012595
2022-01-06 15:14:44,184 iteration 3156 : loss : 0.053992, loss_ce: 0.020787
2022-01-06 15:14:45,397 iteration 3157 : loss : 0.037164, loss_ce: 0.013701
2022-01-06 15:14:46,546 iteration 3158 : loss : 0.047301, loss_ce: 0.019580
2022-01-06 15:14:47,715 iteration 3159 : loss : 0.030593, loss_ce: 0.012286
2022-01-06 15:14:48,892 iteration 3160 : loss : 0.062956, loss_ce: 0.031006
2022-01-06 15:14:50,019 iteration 3161 : loss : 0.040946, loss_ce: 0.015142
2022-01-06 15:14:51,292 iteration 3162 : loss : 0.040377, loss_ce: 0.014090
 46%|████████████▌              | 186/400 [1:09:51<1:19:10, 22.20s/it]2022-01-06 15:14:52,625 iteration 3163 : loss : 0.047869, loss_ce: 0.023728
2022-01-06 15:14:53,838 iteration 3164 : loss : 0.038005, loss_ce: 0.012887
2022-01-06 15:14:54,987 iteration 3165 : loss : 0.030350, loss_ce: 0.011579
2022-01-06 15:14:56,235 iteration 3166 : loss : 0.043779, loss_ce: 0.014503
2022-01-06 15:14:57,410 iteration 3167 : loss : 0.029812, loss_ce: 0.012083
2022-01-06 15:14:58,744 iteration 3168 : loss : 0.050484, loss_ce: 0.024627
2022-01-06 15:15:00,032 iteration 3169 : loss : 0.047694, loss_ce: 0.023837
2022-01-06 15:15:01,177 iteration 3170 : loss : 0.038471, loss_ce: 0.016747
2022-01-06 15:15:02,282 iteration 3171 : loss : 0.034692, loss_ce: 0.015613
2022-01-06 15:15:03,501 iteration 3172 : loss : 0.043814, loss_ce: 0.015827
2022-01-06 15:15:04,745 iteration 3173 : loss : 0.045199, loss_ce: 0.016393
2022-01-06 15:15:05,924 iteration 3174 : loss : 0.050466, loss_ce: 0.019735
2022-01-06 15:15:07,217 iteration 3175 : loss : 0.058568, loss_ce: 0.018630
2022-01-06 15:15:08,385 iteration 3176 : loss : 0.044282, loss_ce: 0.015548
2022-01-06 15:15:09,617 iteration 3177 : loss : 0.061081, loss_ce: 0.022085
2022-01-06 15:15:10,723 iteration 3178 : loss : 0.039581, loss_ce: 0.013880
2022-01-06 15:15:11,876 iteration 3179 : loss : 0.041231, loss_ce: 0.012620
 47%|████████████▌              | 187/400 [1:10:12<1:17:05, 21.71s/it]2022-01-06 15:15:13,186 iteration 3180 : loss : 0.065411, loss_ce: 0.034387
2022-01-06 15:15:14,329 iteration 3181 : loss : 0.062452, loss_ce: 0.023862
2022-01-06 15:15:15,530 iteration 3182 : loss : 0.053849, loss_ce: 0.021569
2022-01-06 15:15:16,680 iteration 3183 : loss : 0.036028, loss_ce: 0.010728
2022-01-06 15:15:17,879 iteration 3184 : loss : 0.043877, loss_ce: 0.015027
2022-01-06 15:15:19,117 iteration 3185 : loss : 0.040202, loss_ce: 0.015090
2022-01-06 15:15:20,212 iteration 3186 : loss : 0.039124, loss_ce: 0.019242
2022-01-06 15:15:21,408 iteration 3187 : loss : 0.032850, loss_ce: 0.013185
2022-01-06 15:15:22,577 iteration 3188 : loss : 0.046492, loss_ce: 0.020350
2022-01-06 15:15:23,794 iteration 3189 : loss : 0.045671, loss_ce: 0.019968
2022-01-06 15:15:24,966 iteration 3190 : loss : 0.037803, loss_ce: 0.014202
2022-01-06 15:15:26,279 iteration 3191 : loss : 0.081533, loss_ce: 0.029351
2022-01-06 15:15:27,418 iteration 3192 : loss : 0.046312, loss_ce: 0.016188
2022-01-06 15:15:28,659 iteration 3193 : loss : 0.042407, loss_ce: 0.021027
2022-01-06 15:15:29,903 iteration 3194 : loss : 0.073587, loss_ce: 0.025266
2022-01-06 15:15:31,094 iteration 3195 : loss : 0.047268, loss_ce: 0.020332
2022-01-06 15:15:32,276 iteration 3196 : loss : 0.048340, loss_ce: 0.022956
 47%|████████████▋              | 188/400 [1:10:32<1:15:19, 21.32s/it]2022-01-06 15:15:33,448 iteration 3197 : loss : 0.038576, loss_ce: 0.016454
2022-01-06 15:15:34,625 iteration 3198 : loss : 0.053971, loss_ce: 0.022874
2022-01-06 15:15:35,817 iteration 3199 : loss : 0.058158, loss_ce: 0.025520
2022-01-06 15:15:37,112 iteration 3200 : loss : 0.050274, loss_ce: 0.023988
2022-01-06 15:15:38,311 iteration 3201 : loss : 0.084284, loss_ce: 0.031912
2022-01-06 15:15:39,476 iteration 3202 : loss : 0.044670, loss_ce: 0.016711
2022-01-06 15:15:40,773 iteration 3203 : loss : 0.033261, loss_ce: 0.014431
2022-01-06 15:15:41,935 iteration 3204 : loss : 0.037633, loss_ce: 0.011305
2022-01-06 15:15:43,181 iteration 3205 : loss : 0.039752, loss_ce: 0.012184
2022-01-06 15:15:44,374 iteration 3206 : loss : 0.033242, loss_ce: 0.013291
2022-01-06 15:15:45,670 iteration 3207 : loss : 0.052294, loss_ce: 0.024109
2022-01-06 15:15:46,787 iteration 3208 : loss : 0.033631, loss_ce: 0.010401
2022-01-06 15:15:47,929 iteration 3209 : loss : 0.052667, loss_ce: 0.031383
2022-01-06 15:15:49,008 iteration 3210 : loss : 0.037539, loss_ce: 0.017736
2022-01-06 15:15:50,182 iteration 3211 : loss : 0.039652, loss_ce: 0.012473
2022-01-06 15:15:51,337 iteration 3212 : loss : 0.035465, loss_ce: 0.013077
2022-01-06 15:15:52,529 iteration 3213 : loss : 0.046134, loss_ce: 0.013985
 47%|████████████▊              | 189/400 [1:10:52<1:13:51, 21.00s/it]2022-01-06 15:15:53,792 iteration 3214 : loss : 0.037076, loss_ce: 0.018220
2022-01-06 15:15:55,087 iteration 3215 : loss : 0.047176, loss_ce: 0.017915
2022-01-06 15:15:56,334 iteration 3216 : loss : 0.044691, loss_ce: 0.017186
2022-01-06 15:15:57,586 iteration 3217 : loss : 0.067475, loss_ce: 0.017305
2022-01-06 15:15:58,801 iteration 3218 : loss : 0.047335, loss_ce: 0.020621
2022-01-06 15:15:59,973 iteration 3219 : loss : 0.031779, loss_ce: 0.012517
2022-01-06 15:16:01,221 iteration 3220 : loss : 0.039916, loss_ce: 0.013949
2022-01-06 15:16:02,381 iteration 3221 : loss : 0.034011, loss_ce: 0.014392
2022-01-06 15:16:03,596 iteration 3222 : loss : 0.049316, loss_ce: 0.029191
2022-01-06 15:16:04,839 iteration 3223 : loss : 0.037721, loss_ce: 0.010806
2022-01-06 15:16:06,098 iteration 3224 : loss : 0.044514, loss_ce: 0.017335
2022-01-06 15:16:07,275 iteration 3225 : loss : 0.050515, loss_ce: 0.017640
2022-01-06 15:16:08,579 iteration 3226 : loss : 0.054593, loss_ce: 0.017542
2022-01-06 15:16:09,797 iteration 3227 : loss : 0.039208, loss_ce: 0.017575
2022-01-06 15:16:10,910 iteration 3228 : loss : 0.036704, loss_ce: 0.018250
2022-01-06 15:16:12,199 iteration 3229 : loss : 0.031135, loss_ce: 0.014150
2022-01-06 15:16:12,199 Training Data Eval:
2022-01-06 15:16:18,071   Average segmentation loss on training set: 0.0437
2022-01-06 15:16:18,071 Validation Data Eval:
2022-01-06 15:16:20,068   Average segmentation loss on validation set: 0.1394
2022-01-06 15:16:21,375 iteration 3230 : loss : 0.046738, loss_ce: 0.017731
 48%|████████████▊              | 190/400 [1:11:21<1:21:43, 23.35s/it]2022-01-06 15:16:22,606 iteration 3231 : loss : 0.046399, loss_ce: 0.016543
2022-01-06 15:16:23,793 iteration 3232 : loss : 0.043251, loss_ce: 0.020239
2022-01-06 15:16:24,958 iteration 3233 : loss : 0.046612, loss_ce: 0.014911
2022-01-06 15:16:26,082 iteration 3234 : loss : 0.047334, loss_ce: 0.016007
2022-01-06 15:16:27,225 iteration 3235 : loss : 0.035452, loss_ce: 0.010642
2022-01-06 15:16:28,448 iteration 3236 : loss : 0.032741, loss_ce: 0.014113
2022-01-06 15:16:29,686 iteration 3237 : loss : 0.040493, loss_ce: 0.015472
2022-01-06 15:16:30,882 iteration 3238 : loss : 0.043868, loss_ce: 0.012639
2022-01-06 15:16:32,056 iteration 3239 : loss : 0.033237, loss_ce: 0.015012
2022-01-06 15:16:33,265 iteration 3240 : loss : 0.049054, loss_ce: 0.018471
2022-01-06 15:16:34,515 iteration 3241 : loss : 0.040717, loss_ce: 0.015817
2022-01-06 15:16:35,809 iteration 3242 : loss : 0.039884, loss_ce: 0.016456
2022-01-06 15:16:36,967 iteration 3243 : loss : 0.041050, loss_ce: 0.020286
2022-01-06 15:16:38,150 iteration 3244 : loss : 0.054060, loss_ce: 0.014535
2022-01-06 15:16:39,312 iteration 3245 : loss : 0.036501, loss_ce: 0.016304
2022-01-06 15:16:40,579 iteration 3246 : loss : 0.042271, loss_ce: 0.016619
2022-01-06 15:16:41,786 iteration 3247 : loss : 0.045227, loss_ce: 0.016449
 48%|████████████▉              | 191/400 [1:11:42<1:18:16, 22.47s/it]2022-01-06 15:16:43,220 iteration 3248 : loss : 0.049814, loss_ce: 0.023962
2022-01-06 15:16:44,301 iteration 3249 : loss : 0.033989, loss_ce: 0.010817
2022-01-06 15:16:45,514 iteration 3250 : loss : 0.061089, loss_ce: 0.023860
2022-01-06 15:16:46,700 iteration 3251 : loss : 0.050820, loss_ce: 0.012687
2022-01-06 15:16:47,827 iteration 3252 : loss : 0.054773, loss_ce: 0.024330
2022-01-06 15:16:49,051 iteration 3253 : loss : 0.047137, loss_ce: 0.021364
2022-01-06 15:16:50,250 iteration 3254 : loss : 0.035861, loss_ce: 0.011705
2022-01-06 15:16:51,497 iteration 3255 : loss : 0.034486, loss_ce: 0.011890
2022-01-06 15:16:52,623 iteration 3256 : loss : 0.039083, loss_ce: 0.021808
2022-01-06 15:16:53,797 iteration 3257 : loss : 0.048965, loss_ce: 0.017549
2022-01-06 15:16:55,076 iteration 3258 : loss : 0.037546, loss_ce: 0.015747
2022-01-06 15:16:56,255 iteration 3259 : loss : 0.046948, loss_ce: 0.018953
2022-01-06 15:16:57,410 iteration 3260 : loss : 0.039476, loss_ce: 0.014667
2022-01-06 15:16:58,667 iteration 3261 : loss : 0.044257, loss_ce: 0.016145
2022-01-06 15:16:59,846 iteration 3262 : loss : 0.038916, loss_ce: 0.017147
2022-01-06 15:17:01,006 iteration 3263 : loss : 0.058317, loss_ce: 0.019519
2022-01-06 15:17:02,167 iteration 3264 : loss : 0.043901, loss_ce: 0.017711
 48%|████████████▉              | 192/400 [1:12:02<1:15:44, 21.85s/it]2022-01-06 15:17:03,300 iteration 3265 : loss : 0.026968, loss_ce: 0.010165
2022-01-06 15:17:04,527 iteration 3266 : loss : 0.029403, loss_ce: 0.011999
2022-01-06 15:17:05,668 iteration 3267 : loss : 0.034712, loss_ce: 0.014278
2022-01-06 15:17:07,000 iteration 3268 : loss : 0.043295, loss_ce: 0.017920
2022-01-06 15:17:08,134 iteration 3269 : loss : 0.042963, loss_ce: 0.015128
2022-01-06 15:17:09,281 iteration 3270 : loss : 0.039071, loss_ce: 0.012870
2022-01-06 15:17:10,571 iteration 3271 : loss : 0.047445, loss_ce: 0.016421
2022-01-06 15:17:11,750 iteration 3272 : loss : 0.038874, loss_ce: 0.013639
2022-01-06 15:17:12,912 iteration 3273 : loss : 0.051249, loss_ce: 0.021808
2022-01-06 15:17:14,158 iteration 3274 : loss : 0.044084, loss_ce: 0.015328
2022-01-06 15:17:15,297 iteration 3275 : loss : 0.031532, loss_ce: 0.012454
2022-01-06 15:17:16,495 iteration 3276 : loss : 0.042203, loss_ce: 0.014001
2022-01-06 15:17:17,774 iteration 3277 : loss : 0.035115, loss_ce: 0.014918
2022-01-06 15:17:18,968 iteration 3278 : loss : 0.040110, loss_ce: 0.014512
2022-01-06 15:17:20,225 iteration 3279 : loss : 0.073859, loss_ce: 0.031217
2022-01-06 15:17:21,441 iteration 3280 : loss : 0.049993, loss_ce: 0.022468
2022-01-06 15:17:22,637 iteration 3281 : loss : 0.031611, loss_ce: 0.009645
 48%|█████████████              | 193/400 [1:12:22<1:13:56, 21.43s/it]2022-01-06 15:17:23,859 iteration 3282 : loss : 0.038056, loss_ce: 0.016973
2022-01-06 15:17:25,179 iteration 3283 : loss : 0.048961, loss_ce: 0.018346
2022-01-06 15:17:26,343 iteration 3284 : loss : 0.031955, loss_ce: 0.015116
2022-01-06 15:17:27,447 iteration 3285 : loss : 0.032074, loss_ce: 0.013305
2022-01-06 15:17:28,714 iteration 3286 : loss : 0.039739, loss_ce: 0.012872
2022-01-06 15:17:29,937 iteration 3287 : loss : 0.050760, loss_ce: 0.020938
2022-01-06 15:17:31,129 iteration 3288 : loss : 0.045062, loss_ce: 0.014733
2022-01-06 15:17:32,336 iteration 3289 : loss : 0.059988, loss_ce: 0.017375
2022-01-06 15:17:33,478 iteration 3290 : loss : 0.033775, loss_ce: 0.008738
2022-01-06 15:17:34,753 iteration 3291 : loss : 0.049245, loss_ce: 0.014548
2022-01-06 15:17:35,938 iteration 3292 : loss : 0.041845, loss_ce: 0.018737
2022-01-06 15:17:37,075 iteration 3293 : loss : 0.027882, loss_ce: 0.010208
2022-01-06 15:17:38,232 iteration 3294 : loss : 0.032398, loss_ce: 0.013079
2022-01-06 15:17:39,443 iteration 3295 : loss : 0.038604, loss_ce: 0.015987
2022-01-06 15:17:40,676 iteration 3296 : loss : 0.032988, loss_ce: 0.013043
2022-01-06 15:17:41,854 iteration 3297 : loss : 0.066909, loss_ce: 0.026479
2022-01-06 15:17:42,975 iteration 3298 : loss : 0.035837, loss_ce: 0.013949
 48%|█████████████              | 194/400 [1:12:43<1:12:27, 21.11s/it]2022-01-06 15:17:44,193 iteration 3299 : loss : 0.027805, loss_ce: 0.011392
2022-01-06 15:17:45,363 iteration 3300 : loss : 0.041780, loss_ce: 0.017938
2022-01-06 15:17:46,612 iteration 3301 : loss : 0.048495, loss_ce: 0.014184
2022-01-06 15:17:47,734 iteration 3302 : loss : 0.036608, loss_ce: 0.011931
2022-01-06 15:17:48,984 iteration 3303 : loss : 0.034467, loss_ce: 0.013128
2022-01-06 15:17:50,202 iteration 3304 : loss : 0.032480, loss_ce: 0.017408
2022-01-06 15:17:51,427 iteration 3305 : loss : 0.041501, loss_ce: 0.014022
2022-01-06 15:17:52,569 iteration 3306 : loss : 0.029566, loss_ce: 0.011161
2022-01-06 15:17:53,852 iteration 3307 : loss : 0.044274, loss_ce: 0.016764
2022-01-06 15:17:55,122 iteration 3308 : loss : 0.048535, loss_ce: 0.016168
2022-01-06 15:17:56,319 iteration 3309 : loss : 0.031967, loss_ce: 0.012212
2022-01-06 15:17:57,576 iteration 3310 : loss : 0.044252, loss_ce: 0.012937
2022-01-06 15:17:58,732 iteration 3311 : loss : 0.049947, loss_ce: 0.014371
2022-01-06 15:17:59,856 iteration 3312 : loss : 0.025101, loss_ce: 0.010894
2022-01-06 15:18:01,108 iteration 3313 : loss : 0.053897, loss_ce: 0.020765
2022-01-06 15:18:02,177 iteration 3314 : loss : 0.033203, loss_ce: 0.014409
2022-01-06 15:18:02,177 Training Data Eval:
2022-01-06 15:18:08,053   Average segmentation loss on training set: 0.1429
2022-01-06 15:18:08,054 Validation Data Eval:
2022-01-06 15:18:10,088   Average segmentation loss on validation set: 0.3215
2022-01-06 15:18:11,264 iteration 3315 : loss : 0.037282, loss_ce: 0.014690
 49%|█████████████▏             | 195/400 [1:13:11<1:19:27, 23.26s/it]2022-01-06 15:18:12,528 iteration 3316 : loss : 0.033789, loss_ce: 0.008445
2022-01-06 15:18:13,751 iteration 3317 : loss : 0.043094, loss_ce: 0.014577
2022-01-06 15:18:14,970 iteration 3318 : loss : 0.037252, loss_ce: 0.015293
2022-01-06 15:18:16,120 iteration 3319 : loss : 0.041228, loss_ce: 0.021849
2022-01-06 15:18:17,456 iteration 3320 : loss : 0.044567, loss_ce: 0.015388
2022-01-06 15:18:18,578 iteration 3321 : loss : 0.028926, loss_ce: 0.015357
2022-01-06 15:18:19,683 iteration 3322 : loss : 0.036512, loss_ce: 0.011900
2022-01-06 15:18:20,908 iteration 3323 : loss : 0.051968, loss_ce: 0.025799
2022-01-06 15:18:22,117 iteration 3324 : loss : 0.032673, loss_ce: 0.011190
2022-01-06 15:18:23,388 iteration 3325 : loss : 0.038458, loss_ce: 0.013163
2022-01-06 15:18:24,591 iteration 3326 : loss : 0.035940, loss_ce: 0.012584
2022-01-06 15:18:25,947 iteration 3327 : loss : 0.055154, loss_ce: 0.017871
2022-01-06 15:18:27,084 iteration 3328 : loss : 0.034230, loss_ce: 0.016204
2022-01-06 15:18:28,229 iteration 3329 : loss : 0.034458, loss_ce: 0.010416
2022-01-06 15:18:29,442 iteration 3330 : loss : 0.033984, loss_ce: 0.015882
2022-01-06 15:18:30,533 iteration 3331 : loss : 0.033605, loss_ce: 0.013607
2022-01-06 15:18:31,615 iteration 3332 : loss : 0.036597, loss_ce: 0.012174
 49%|█████████████▏             | 196/400 [1:13:31<1:16:07, 22.39s/it]2022-01-06 15:18:32,871 iteration 3333 : loss : 0.031265, loss_ce: 0.011947
2022-01-06 15:18:34,078 iteration 3334 : loss : 0.029841, loss_ce: 0.012977
2022-01-06 15:18:35,344 iteration 3335 : loss : 0.057951, loss_ce: 0.027022
2022-01-06 15:18:36,477 iteration 3336 : loss : 0.036734, loss_ce: 0.013385
2022-01-06 15:18:37,599 iteration 3337 : loss : 0.035907, loss_ce: 0.013573
2022-01-06 15:18:38,835 iteration 3338 : loss : 0.034787, loss_ce: 0.012573
2022-01-06 15:18:40,075 iteration 3339 : loss : 0.043940, loss_ce: 0.015122
2022-01-06 15:18:41,271 iteration 3340 : loss : 0.040216, loss_ce: 0.017695
2022-01-06 15:18:42,412 iteration 3341 : loss : 0.035391, loss_ce: 0.015477
2022-01-06 15:18:43,548 iteration 3342 : loss : 0.042460, loss_ce: 0.022960
2022-01-06 15:18:44,717 iteration 3343 : loss : 0.045196, loss_ce: 0.020074
2022-01-06 15:18:45,960 iteration 3344 : loss : 0.028783, loss_ce: 0.007994
2022-01-06 15:18:47,210 iteration 3345 : loss : 0.040794, loss_ce: 0.020339
2022-01-06 15:18:48,405 iteration 3346 : loss : 0.042226, loss_ce: 0.013999
2022-01-06 15:18:49,563 iteration 3347 : loss : 0.028249, loss_ce: 0.010392
2022-01-06 15:18:50,831 iteration 3348 : loss : 0.038665, loss_ce: 0.015626
2022-01-06 15:18:51,949 iteration 3349 : loss : 0.054986, loss_ce: 0.017109
 49%|█████████████▎             | 197/400 [1:13:52<1:13:39, 21.77s/it]2022-01-06 15:18:53,141 iteration 3350 : loss : 0.033536, loss_ce: 0.014720
2022-01-06 15:18:54,362 iteration 3351 : loss : 0.038743, loss_ce: 0.014510
2022-01-06 15:18:55,499 iteration 3352 : loss : 0.039226, loss_ce: 0.014362
2022-01-06 15:18:56,685 iteration 3353 : loss : 0.036751, loss_ce: 0.011510
2022-01-06 15:18:57,827 iteration 3354 : loss : 0.038923, loss_ce: 0.012394
2022-01-06 15:18:59,017 iteration 3355 : loss : 0.039603, loss_ce: 0.017909
2022-01-06 15:19:00,144 iteration 3356 : loss : 0.052579, loss_ce: 0.022448
2022-01-06 15:19:01,277 iteration 3357 : loss : 0.030253, loss_ce: 0.012388
2022-01-06 15:19:02,447 iteration 3358 : loss : 0.046371, loss_ce: 0.016464
2022-01-06 15:19:03,599 iteration 3359 : loss : 0.039656, loss_ce: 0.016984
2022-01-06 15:19:04,834 iteration 3360 : loss : 0.031270, loss_ce: 0.010862
2022-01-06 15:19:06,023 iteration 3361 : loss : 0.039072, loss_ce: 0.014409
2022-01-06 15:19:07,171 iteration 3362 : loss : 0.048865, loss_ce: 0.019128
2022-01-06 15:19:08,250 iteration 3363 : loss : 0.056290, loss_ce: 0.022942
2022-01-06 15:19:09,499 iteration 3364 : loss : 0.036569, loss_ce: 0.013592
2022-01-06 15:19:10,664 iteration 3365 : loss : 0.038627, loss_ce: 0.016812
2022-01-06 15:19:11,830 iteration 3366 : loss : 0.031863, loss_ce: 0.013096
 50%|█████████████▎             | 198/400 [1:14:12<1:11:23, 21.20s/it]2022-01-06 15:19:13,202 iteration 3367 : loss : 0.050617, loss_ce: 0.018692
2022-01-06 15:19:14,371 iteration 3368 : loss : 0.049945, loss_ce: 0.024955
2022-01-06 15:19:15,563 iteration 3369 : loss : 0.037163, loss_ce: 0.015950
2022-01-06 15:19:16,684 iteration 3370 : loss : 0.044390, loss_ce: 0.011549
2022-01-06 15:19:17,860 iteration 3371 : loss : 0.027818, loss_ce: 0.010904
2022-01-06 15:19:19,107 iteration 3372 : loss : 0.051618, loss_ce: 0.018079
2022-01-06 15:19:20,291 iteration 3373 : loss : 0.046832, loss_ce: 0.019940
2022-01-06 15:19:21,547 iteration 3374 : loss : 0.037243, loss_ce: 0.014410
2022-01-06 15:19:22,834 iteration 3375 : loss : 0.052733, loss_ce: 0.015387
2022-01-06 15:19:23,916 iteration 3376 : loss : 0.031933, loss_ce: 0.011506
2022-01-06 15:19:25,159 iteration 3377 : loss : 0.030069, loss_ce: 0.011268
2022-01-06 15:19:26,308 iteration 3378 : loss : 0.037503, loss_ce: 0.010303
2022-01-06 15:19:27,540 iteration 3379 : loss : 0.031555, loss_ce: 0.011940
2022-01-06 15:19:28,678 iteration 3380 : loss : 0.030810, loss_ce: 0.009794
2022-01-06 15:19:29,890 iteration 3381 : loss : 0.037185, loss_ce: 0.020868
2022-01-06 15:19:31,117 iteration 3382 : loss : 0.036273, loss_ce: 0.014329
2022-01-06 15:19:32,293 iteration 3383 : loss : 0.039731, loss_ce: 0.017412
 50%|█████████████▍             | 199/400 [1:14:32<1:10:17, 20.98s/it]2022-01-06 15:19:33,550 iteration 3384 : loss : 0.038934, loss_ce: 0.016861
2022-01-06 15:19:34,700 iteration 3385 : loss : 0.042838, loss_ce: 0.016924
2022-01-06 15:19:35,838 iteration 3386 : loss : 0.031534, loss_ce: 0.010881
2022-01-06 15:19:36,966 iteration 3387 : loss : 0.041244, loss_ce: 0.012873
2022-01-06 15:19:38,221 iteration 3388 : loss : 0.054297, loss_ce: 0.018104
2022-01-06 15:19:39,356 iteration 3389 : loss : 0.041281, loss_ce: 0.012629
2022-01-06 15:19:40,559 iteration 3390 : loss : 0.052383, loss_ce: 0.019578
2022-01-06 15:19:41,853 iteration 3391 : loss : 0.042731, loss_ce: 0.024945
2022-01-06 15:19:43,081 iteration 3392 : loss : 0.040880, loss_ce: 0.018762
2022-01-06 15:19:44,190 iteration 3393 : loss : 0.033626, loss_ce: 0.014454
2022-01-06 15:19:45,366 iteration 3394 : loss : 0.043783, loss_ce: 0.015233
2022-01-06 15:19:46,566 iteration 3395 : loss : 0.039111, loss_ce: 0.017488
2022-01-06 15:19:47,754 iteration 3396 : loss : 0.048353, loss_ce: 0.016999
2022-01-06 15:19:48,857 iteration 3397 : loss : 0.033624, loss_ce: 0.013281
2022-01-06 15:19:50,201 iteration 3398 : loss : 0.063286, loss_ce: 0.026113
2022-01-06 15:19:51,390 iteration 3399 : loss : 0.044304, loss_ce: 0.020079
2022-01-06 15:19:51,390 Training Data Eval:
2022-01-06 15:19:57,216   Average segmentation loss on training set: 0.0838
2022-01-06 15:19:57,216 Validation Data Eval:
2022-01-06 15:19:59,206   Average segmentation loss on validation set: 0.1872
2022-01-06 15:20:00,412 iteration 3400 : loss : 0.044987, loss_ce: 0.013581
 50%|█████████████▌             | 200/400 [1:15:00<1:17:03, 23.12s/it]2022-01-06 15:20:01,710 iteration 3401 : loss : 0.050664, loss_ce: 0.021154
2022-01-06 15:20:02,905 iteration 3402 : loss : 0.035689, loss_ce: 0.013544
2022-01-06 15:20:04,176 iteration 3403 : loss : 0.054235, loss_ce: 0.021130
2022-01-06 15:20:05,295 iteration 3404 : loss : 0.039512, loss_ce: 0.012594
2022-01-06 15:20:06,448 iteration 3405 : loss : 0.031249, loss_ce: 0.009212
2022-01-06 15:20:07,559 iteration 3406 : loss : 0.032039, loss_ce: 0.016106
2022-01-06 15:20:08,715 iteration 3407 : loss : 0.036025, loss_ce: 0.011976
2022-01-06 15:20:09,915 iteration 3408 : loss : 0.027348, loss_ce: 0.009498
2022-01-06 15:20:11,136 iteration 3409 : loss : 0.032110, loss_ce: 0.006355
2022-01-06 15:20:12,308 iteration 3410 : loss : 0.031419, loss_ce: 0.011730
2022-01-06 15:20:13,528 iteration 3411 : loss : 0.036493, loss_ce: 0.011996
2022-01-06 15:20:14,771 iteration 3412 : loss : 0.035778, loss_ce: 0.017518
2022-01-06 15:20:16,044 iteration 3413 : loss : 0.034413, loss_ce: 0.015083
2022-01-06 15:20:17,278 iteration 3414 : loss : 0.058156, loss_ce: 0.022049
2022-01-06 15:20:18,339 iteration 3415 : loss : 0.029392, loss_ce: 0.012611
2022-01-06 15:20:19,550 iteration 3416 : loss : 0.035786, loss_ce: 0.015677
2022-01-06 15:20:20,709 iteration 3417 : loss : 0.028125, loss_ce: 0.012411
 50%|█████████████▌             | 201/400 [1:15:21<1:13:53, 22.28s/it]2022-01-06 15:20:22,064 iteration 3418 : loss : 0.052642, loss_ce: 0.017187
2022-01-06 15:20:23,269 iteration 3419 : loss : 0.059454, loss_ce: 0.031285
2022-01-06 15:20:24,396 iteration 3420 : loss : 0.030922, loss_ce: 0.016140
2022-01-06 15:20:25,594 iteration 3421 : loss : 0.061827, loss_ce: 0.021060
2022-01-06 15:20:26,781 iteration 3422 : loss : 0.037540, loss_ce: 0.015760
2022-01-06 15:20:28,046 iteration 3423 : loss : 0.034812, loss_ce: 0.015954
2022-01-06 15:20:29,177 iteration 3424 : loss : 0.036834, loss_ce: 0.012108
2022-01-06 15:20:30,394 iteration 3425 : loss : 0.046686, loss_ce: 0.023097
2022-01-06 15:20:31,570 iteration 3426 : loss : 0.037147, loss_ce: 0.011616
2022-01-06 15:20:32,890 iteration 3427 : loss : 0.046942, loss_ce: 0.018051
2022-01-06 15:20:34,091 iteration 3428 : loss : 0.027190, loss_ce: 0.010412
2022-01-06 15:20:35,301 iteration 3429 : loss : 0.043565, loss_ce: 0.020005
2022-01-06 15:20:36,516 iteration 3430 : loss : 0.027509, loss_ce: 0.009425
2022-01-06 15:20:37,776 iteration 3431 : loss : 0.043775, loss_ce: 0.014878
2022-01-06 15:20:38,923 iteration 3432 : loss : 0.040211, loss_ce: 0.012214
2022-01-06 15:20:40,122 iteration 3433 : loss : 0.031714, loss_ce: 0.010464
2022-01-06 15:20:41,284 iteration 3434 : loss : 0.032563, loss_ce: 0.009948
 50%|█████████████▋             | 202/400 [1:15:41<1:11:49, 21.77s/it]2022-01-06 15:20:42,471 iteration 3435 : loss : 0.031955, loss_ce: 0.015601
2022-01-06 15:20:43,633 iteration 3436 : loss : 0.051743, loss_ce: 0.018279
2022-01-06 15:20:44,766 iteration 3437 : loss : 0.034065, loss_ce: 0.011175
2022-01-06 15:20:45,941 iteration 3438 : loss : 0.044144, loss_ce: 0.020763
2022-01-06 15:20:47,206 iteration 3439 : loss : 0.039810, loss_ce: 0.015275
2022-01-06 15:20:48,468 iteration 3440 : loss : 0.060822, loss_ce: 0.021567
2022-01-06 15:20:49,530 iteration 3441 : loss : 0.033236, loss_ce: 0.017134
2022-01-06 15:20:50,668 iteration 3442 : loss : 0.030367, loss_ce: 0.009429
2022-01-06 15:20:51,883 iteration 3443 : loss : 0.027451, loss_ce: 0.010513
2022-01-06 15:20:53,026 iteration 3444 : loss : 0.034949, loss_ce: 0.011746
2022-01-06 15:20:54,242 iteration 3445 : loss : 0.022233, loss_ce: 0.007587
2022-01-06 15:20:55,406 iteration 3446 : loss : 0.046361, loss_ce: 0.016252
2022-01-06 15:20:56,592 iteration 3447 : loss : 0.036380, loss_ce: 0.018496
2022-01-06 15:20:57,679 iteration 3448 : loss : 0.035352, loss_ce: 0.012678
2022-01-06 15:20:58,876 iteration 3449 : loss : 0.052318, loss_ce: 0.015946
2022-01-06 15:21:00,124 iteration 3450 : loss : 0.037566, loss_ce: 0.010991
2022-01-06 15:21:01,254 iteration 3451 : loss : 0.025865, loss_ce: 0.010890
 51%|█████████████▋             | 203/400 [1:16:01<1:09:41, 21.23s/it]2022-01-06 15:21:02,493 iteration 3452 : loss : 0.028087, loss_ce: 0.010690
2022-01-06 15:21:03,732 iteration 3453 : loss : 0.042384, loss_ce: 0.015097
2022-01-06 15:21:04,979 iteration 3454 : loss : 0.035078, loss_ce: 0.012074
2022-01-06 15:21:06,175 iteration 3455 : loss : 0.037550, loss_ce: 0.016590
2022-01-06 15:21:07,367 iteration 3456 : loss : 0.048920, loss_ce: 0.017704
2022-01-06 15:21:08,576 iteration 3457 : loss : 0.034369, loss_ce: 0.013820
2022-01-06 15:21:09,802 iteration 3458 : loss : 0.032389, loss_ce: 0.016330
2022-01-06 15:21:11,084 iteration 3459 : loss : 0.039111, loss_ce: 0.019409
2022-01-06 15:21:12,301 iteration 3460 : loss : 0.042346, loss_ce: 0.015361
2022-01-06 15:21:13,480 iteration 3461 : loss : 0.059940, loss_ce: 0.025977
2022-01-06 15:21:14,607 iteration 3462 : loss : 0.028161, loss_ce: 0.010492
2022-01-06 15:21:15,705 iteration 3463 : loss : 0.025909, loss_ce: 0.008724
2022-01-06 15:21:16,886 iteration 3464 : loss : 0.042771, loss_ce: 0.017690
2022-01-06 15:21:18,016 iteration 3465 : loss : 0.039264, loss_ce: 0.013733
2022-01-06 15:21:19,233 iteration 3466 : loss : 0.025251, loss_ce: 0.010434
2022-01-06 15:21:20,529 iteration 3467 : loss : 0.040565, loss_ce: 0.015775
2022-01-06 15:21:21,856 iteration 3468 : loss : 0.037314, loss_ce: 0.013369
 51%|█████████████▊             | 204/400 [1:16:22<1:08:43, 21.04s/it]2022-01-06 15:21:23,014 iteration 3469 : loss : 0.028655, loss_ce: 0.012202
2022-01-06 15:21:24,360 iteration 3470 : loss : 0.045339, loss_ce: 0.016202
2022-01-06 15:21:25,589 iteration 3471 : loss : 0.038421, loss_ce: 0.013502
2022-01-06 15:21:26,837 iteration 3472 : loss : 0.040307, loss_ce: 0.018280
2022-01-06 15:21:28,090 iteration 3473 : loss : 0.042569, loss_ce: 0.017502
2022-01-06 15:21:29,234 iteration 3474 : loss : 0.038836, loss_ce: 0.014861
2022-01-06 15:21:30,520 iteration 3475 : loss : 0.064099, loss_ce: 0.026998
2022-01-06 15:21:31,694 iteration 3476 : loss : 0.050960, loss_ce: 0.022914
2022-01-06 15:21:32,936 iteration 3477 : loss : 0.045893, loss_ce: 0.014992
2022-01-06 15:21:34,142 iteration 3478 : loss : 0.036067, loss_ce: 0.010358
2022-01-06 15:21:35,310 iteration 3479 : loss : 0.058210, loss_ce: 0.027402
2022-01-06 15:21:36,438 iteration 3480 : loss : 0.024924, loss_ce: 0.009951
2022-01-06 15:21:37,625 iteration 3481 : loss : 0.042196, loss_ce: 0.026829
2022-01-06 15:21:38,733 iteration 3482 : loss : 0.046224, loss_ce: 0.016086
2022-01-06 15:21:39,994 iteration 3483 : loss : 0.028610, loss_ce: 0.011576
2022-01-06 15:21:41,172 iteration 3484 : loss : 0.039224, loss_ce: 0.012926
2022-01-06 15:21:41,172 Training Data Eval:
2022-01-06 15:21:46,988   Average segmentation loss on training set: 0.0420
2022-01-06 15:21:46,989 Validation Data Eval:
2022-01-06 15:21:48,972   Average segmentation loss on validation set: 0.0809
2022-01-06 15:21:54,874 Found new lowest validation loss at iteration 3484! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_MLP_best_val_loss_seed100.pth
2022-01-06 15:21:56,057 iteration 3485 : loss : 0.032848, loss_ce: 0.013037
 51%|█████████████▊             | 205/400 [1:16:56<1:21:12, 24.99s/it]2022-01-06 15:21:57,227 iteration 3486 : loss : 0.029548, loss_ce: 0.011936
2022-01-06 15:21:58,381 iteration 3487 : loss : 0.030540, loss_ce: 0.012663
2022-01-06 15:21:59,591 iteration 3488 : loss : 0.032232, loss_ce: 0.018723
2022-01-06 15:22:00,692 iteration 3489 : loss : 0.061083, loss_ce: 0.020587
2022-01-06 15:22:01,932 iteration 3490 : loss : 0.028251, loss_ce: 0.011766
2022-01-06 15:22:03,137 iteration 3491 : loss : 0.033191, loss_ce: 0.014407
2022-01-06 15:22:04,191 iteration 3492 : loss : 0.026925, loss_ce: 0.008068
2022-01-06 15:22:05,444 iteration 3493 : loss : 0.034293, loss_ce: 0.009493
2022-01-06 15:22:06,628 iteration 3494 : loss : 0.034854, loss_ce: 0.018136
2022-01-06 15:22:07,677 iteration 3495 : loss : 0.024557, loss_ce: 0.010314
2022-01-06 15:22:08,860 iteration 3496 : loss : 0.063255, loss_ce: 0.019978
2022-01-06 15:22:10,096 iteration 3497 : loss : 0.051276, loss_ce: 0.031343
2022-01-06 15:22:11,260 iteration 3498 : loss : 0.035758, loss_ce: 0.016011
2022-01-06 15:22:12,491 iteration 3499 : loss : 0.044780, loss_ce: 0.013650
2022-01-06 15:22:13,625 iteration 3500 : loss : 0.036420, loss_ce: 0.010275
2022-01-06 15:22:14,805 iteration 3501 : loss : 0.031969, loss_ce: 0.011130
2022-01-06 15:22:15,970 iteration 3502 : loss : 0.042323, loss_ce: 0.014909
 52%|█████████████▉             | 206/400 [1:17:16<1:15:52, 23.46s/it]2022-01-06 15:22:17,106 iteration 3503 : loss : 0.028746, loss_ce: 0.013462
2022-01-06 15:22:18,285 iteration 3504 : loss : 0.027285, loss_ce: 0.010136
2022-01-06 15:22:19,474 iteration 3505 : loss : 0.040050, loss_ce: 0.020114
2022-01-06 15:22:20,773 iteration 3506 : loss : 0.036441, loss_ce: 0.012924
2022-01-06 15:22:21,940 iteration 3507 : loss : 0.035424, loss_ce: 0.013629
2022-01-06 15:22:23,051 iteration 3508 : loss : 0.026608, loss_ce: 0.008380
2022-01-06 15:22:24,229 iteration 3509 : loss : 0.039181, loss_ce: 0.019531
2022-01-06 15:22:25,537 iteration 3510 : loss : 0.034572, loss_ce: 0.014577
2022-01-06 15:22:26,699 iteration 3511 : loss : 0.027905, loss_ce: 0.011788
2022-01-06 15:22:27,893 iteration 3512 : loss : 0.037579, loss_ce: 0.014008
2022-01-06 15:22:29,010 iteration 3513 : loss : 0.041757, loss_ce: 0.013174
2022-01-06 15:22:30,119 iteration 3514 : loss : 0.020366, loss_ce: 0.008621
2022-01-06 15:22:31,290 iteration 3515 : loss : 0.028338, loss_ce: 0.010160
2022-01-06 15:22:32,527 iteration 3516 : loss : 0.038627, loss_ce: 0.014156
2022-01-06 15:22:33,724 iteration 3517 : loss : 0.030290, loss_ce: 0.012394
2022-01-06 15:22:34,851 iteration 3518 : loss : 0.025115, loss_ce: 0.011334
2022-01-06 15:22:35,998 iteration 3519 : loss : 0.041226, loss_ce: 0.013030
 52%|█████████████▉             | 207/400 [1:17:36<1:12:10, 22.44s/it]2022-01-06 15:22:37,238 iteration 3520 : loss : 0.027796, loss_ce: 0.008967
2022-01-06 15:22:38,454 iteration 3521 : loss : 0.034938, loss_ce: 0.014195
2022-01-06 15:22:39,668 iteration 3522 : loss : 0.039763, loss_ce: 0.018482
2022-01-06 15:22:40,980 iteration 3523 : loss : 0.041652, loss_ce: 0.015225
2022-01-06 15:22:42,139 iteration 3524 : loss : 0.048184, loss_ce: 0.012309
2022-01-06 15:22:43,357 iteration 3525 : loss : 0.041460, loss_ce: 0.017593
2022-01-06 15:22:44,579 iteration 3526 : loss : 0.039845, loss_ce: 0.019534
2022-01-06 15:22:45,802 iteration 3527 : loss : 0.028182, loss_ce: 0.010854
2022-01-06 15:22:46,931 iteration 3528 : loss : 0.028095, loss_ce: 0.014093
2022-01-06 15:22:48,120 iteration 3529 : loss : 0.037319, loss_ce: 0.013469
2022-01-06 15:22:49,285 iteration 3530 : loss : 0.050732, loss_ce: 0.019199
2022-01-06 15:22:50,499 iteration 3531 : loss : 0.029645, loss_ce: 0.011213
2022-01-06 15:22:51,658 iteration 3532 : loss : 0.030350, loss_ce: 0.011488
2022-01-06 15:22:52,796 iteration 3533 : loss : 0.035899, loss_ce: 0.014661
2022-01-06 15:22:53,952 iteration 3534 : loss : 0.033219, loss_ce: 0.018321
2022-01-06 15:22:55,072 iteration 3535 : loss : 0.029741, loss_ce: 0.010991
2022-01-06 15:22:56,209 iteration 3536 : loss : 0.035605, loss_ce: 0.011921
 52%|██████████████             | 208/400 [1:17:56<1:09:39, 21.77s/it]2022-01-06 15:22:57,437 iteration 3537 : loss : 0.037218, loss_ce: 0.013882
2022-01-06 15:22:58,626 iteration 3538 : loss : 0.039675, loss_ce: 0.011727
2022-01-06 15:22:59,845 iteration 3539 : loss : 0.039498, loss_ce: 0.018686
2022-01-06 15:23:00,957 iteration 3540 : loss : 0.040097, loss_ce: 0.007978
2022-01-06 15:23:02,212 iteration 3541 : loss : 0.028430, loss_ce: 0.011863
2022-01-06 15:23:03,365 iteration 3542 : loss : 0.032324, loss_ce: 0.009660
2022-01-06 15:23:04,567 iteration 3543 : loss : 0.032209, loss_ce: 0.017118
2022-01-06 15:23:05,792 iteration 3544 : loss : 0.039073, loss_ce: 0.014593
2022-01-06 15:23:06,926 iteration 3545 : loss : 0.038213, loss_ce: 0.014930
2022-01-06 15:23:08,017 iteration 3546 : loss : 0.034507, loss_ce: 0.016870
2022-01-06 15:23:09,257 iteration 3547 : loss : 0.037871, loss_ce: 0.014317
2022-01-06 15:23:10,479 iteration 3548 : loss : 0.031610, loss_ce: 0.011311
2022-01-06 15:23:11,667 iteration 3549 : loss : 0.036152, loss_ce: 0.015688
2022-01-06 15:23:12,800 iteration 3550 : loss : 0.029709, loss_ce: 0.010596
2022-01-06 15:23:14,066 iteration 3551 : loss : 0.058913, loss_ce: 0.017973
2022-01-06 15:23:15,329 iteration 3552 : loss : 0.046024, loss_ce: 0.018822
2022-01-06 15:23:16,576 iteration 3553 : loss : 0.039633, loss_ce: 0.013793
 52%|██████████████             | 209/400 [1:18:16<1:07:56, 21.34s/it]2022-01-06 15:23:17,792 iteration 3554 : loss : 0.035210, loss_ce: 0.014630
2022-01-06 15:23:19,027 iteration 3555 : loss : 0.050254, loss_ce: 0.024401
2022-01-06 15:23:20,199 iteration 3556 : loss : 0.037721, loss_ce: 0.013350
2022-01-06 15:23:21,360 iteration 3557 : loss : 0.030831, loss_ce: 0.012868
2022-01-06 15:23:22,462 iteration 3558 : loss : 0.028368, loss_ce: 0.009336
2022-01-06 15:23:23,719 iteration 3559 : loss : 0.033167, loss_ce: 0.011564
2022-01-06 15:23:24,912 iteration 3560 : loss : 0.050520, loss_ce: 0.015561
2022-01-06 15:23:26,130 iteration 3561 : loss : 0.027728, loss_ce: 0.011934
2022-01-06 15:23:27,302 iteration 3562 : loss : 0.039874, loss_ce: 0.014388
2022-01-06 15:23:28,478 iteration 3563 : loss : 0.026064, loss_ce: 0.009609
2022-01-06 15:23:29,688 iteration 3564 : loss : 0.030476, loss_ce: 0.009977
2022-01-06 15:23:30,803 iteration 3565 : loss : 0.029244, loss_ce: 0.012070
2022-01-06 15:23:31,858 iteration 3566 : loss : 0.024315, loss_ce: 0.009263
2022-01-06 15:23:32,963 iteration 3567 : loss : 0.042881, loss_ce: 0.012657
2022-01-06 15:23:34,190 iteration 3568 : loss : 0.049298, loss_ce: 0.018833
2022-01-06 15:23:35,317 iteration 3569 : loss : 0.034418, loss_ce: 0.014160
2022-01-06 15:23:35,317 Training Data Eval:
2022-01-06 15:23:41,122   Average segmentation loss on training set: 0.0948
2022-01-06 15:23:41,123 Validation Data Eval:
2022-01-06 15:23:43,121   Average segmentation loss on validation set: 0.1483
2022-01-06 15:23:44,281 iteration 3570 : loss : 0.030208, loss_ce: 0.009354
 52%|██████████████▏            | 210/400 [1:18:44<1:13:38, 23.25s/it]2022-01-06 15:23:45,558 iteration 3571 : loss : 0.047810, loss_ce: 0.018732
2022-01-06 15:23:46,810 iteration 3572 : loss : 0.028953, loss_ce: 0.011011
2022-01-06 15:23:48,015 iteration 3573 : loss : 0.045846, loss_ce: 0.015268
2022-01-06 15:23:49,155 iteration 3574 : loss : 0.026494, loss_ce: 0.012329
2022-01-06 15:23:50,283 iteration 3575 : loss : 0.043916, loss_ce: 0.020163
2022-01-06 15:23:51,432 iteration 3576 : loss : 0.044628, loss_ce: 0.016029
2022-01-06 15:23:52,651 iteration 3577 : loss : 0.037309, loss_ce: 0.015055
2022-01-06 15:23:53,808 iteration 3578 : loss : 0.032424, loss_ce: 0.015658
2022-01-06 15:23:55,006 iteration 3579 : loss : 0.032566, loss_ce: 0.012220
2022-01-06 15:23:56,233 iteration 3580 : loss : 0.025556, loss_ce: 0.012750
2022-01-06 15:23:57,458 iteration 3581 : loss : 0.029301, loss_ce: 0.011408
2022-01-06 15:23:58,658 iteration 3582 : loss : 0.037455, loss_ce: 0.012286
2022-01-06 15:23:59,816 iteration 3583 : loss : 0.055932, loss_ce: 0.015065
2022-01-06 15:24:00,910 iteration 3584 : loss : 0.027452, loss_ce: 0.009515
2022-01-06 15:24:02,061 iteration 3585 : loss : 0.022746, loss_ce: 0.009390
2022-01-06 15:24:03,180 iteration 3586 : loss : 0.022415, loss_ce: 0.006641
2022-01-06 15:24:04,360 iteration 3587 : loss : 0.034218, loss_ce: 0.015661
 53%|██████████████▏            | 211/400 [1:19:04<1:10:14, 22.30s/it]2022-01-06 15:24:05,651 iteration 3588 : loss : 0.024118, loss_ce: 0.010199
2022-01-06 15:24:06,918 iteration 3589 : loss : 0.056538, loss_ce: 0.018177
2022-01-06 15:24:08,108 iteration 3590 : loss : 0.049675, loss_ce: 0.013399
2022-01-06 15:24:09,227 iteration 3591 : loss : 0.031315, loss_ce: 0.010933
2022-01-06 15:24:10,402 iteration 3592 : loss : 0.024732, loss_ce: 0.009624
2022-01-06 15:24:11,626 iteration 3593 : loss : 0.036096, loss_ce: 0.019603
2022-01-06 15:24:12,799 iteration 3594 : loss : 0.038686, loss_ce: 0.012199
2022-01-06 15:24:13,962 iteration 3595 : loss : 0.053957, loss_ce: 0.027177
2022-01-06 15:24:15,095 iteration 3596 : loss : 0.035990, loss_ce: 0.017397
2022-01-06 15:24:16,290 iteration 3597 : loss : 0.030436, loss_ce: 0.012591
2022-01-06 15:24:17,510 iteration 3598 : loss : 0.031906, loss_ce: 0.014171
2022-01-06 15:24:18,726 iteration 3599 : loss : 0.054407, loss_ce: 0.021027
2022-01-06 15:24:19,850 iteration 3600 : loss : 0.031733, loss_ce: 0.018631
2022-01-06 15:24:21,081 iteration 3601 : loss : 0.030698, loss_ce: 0.011133
2022-01-06 15:24:22,333 iteration 3602 : loss : 0.035965, loss_ce: 0.013840
2022-01-06 15:24:23,505 iteration 3603 : loss : 0.033627, loss_ce: 0.014868
2022-01-06 15:24:24,697 iteration 3604 : loss : 0.046611, loss_ce: 0.011838
 53%|██████████████▎            | 212/400 [1:19:25<1:08:02, 21.71s/it]2022-01-06 15:24:26,077 iteration 3605 : loss : 0.049870, loss_ce: 0.018804
2022-01-06 15:24:27,189 iteration 3606 : loss : 0.040113, loss_ce: 0.011132
2022-01-06 15:24:28,273 iteration 3607 : loss : 0.018404, loss_ce: 0.006376
2022-01-06 15:24:29,532 iteration 3608 : loss : 0.029714, loss_ce: 0.010082
2022-01-06 15:24:30,770 iteration 3609 : loss : 0.075523, loss_ce: 0.033147
2022-01-06 15:24:31,876 iteration 3610 : loss : 0.023218, loss_ce: 0.010398
2022-01-06 15:24:33,049 iteration 3611 : loss : 0.044012, loss_ce: 0.020033
2022-01-06 15:24:34,184 iteration 3612 : loss : 0.030256, loss_ce: 0.013572
2022-01-06 15:24:35,379 iteration 3613 : loss : 0.026248, loss_ce: 0.010160
2022-01-06 15:24:36,521 iteration 3614 : loss : 0.033438, loss_ce: 0.013093
2022-01-06 15:24:37,686 iteration 3615 : loss : 0.046875, loss_ce: 0.015488
2022-01-06 15:24:38,853 iteration 3616 : loss : 0.031455, loss_ce: 0.014549
2022-01-06 15:24:40,071 iteration 3617 : loss : 0.031590, loss_ce: 0.013184
2022-01-06 15:24:41,286 iteration 3618 : loss : 0.039522, loss_ce: 0.015522
2022-01-06 15:24:42,439 iteration 3619 : loss : 0.030391, loss_ce: 0.012929
2022-01-06 15:24:43,632 iteration 3620 : loss : 0.055903, loss_ce: 0.017572
2022-01-06 15:24:44,801 iteration 3621 : loss : 0.030611, loss_ce: 0.014431
 53%|██████████████▍            | 213/400 [1:19:45<1:06:09, 21.23s/it]2022-01-06 15:24:46,058 iteration 3622 : loss : 0.029547, loss_ce: 0.009614
2022-01-06 15:24:47,248 iteration 3623 : loss : 0.035566, loss_ce: 0.014466
2022-01-06 15:24:48,448 iteration 3624 : loss : 0.029180, loss_ce: 0.012005
2022-01-06 15:24:49,532 iteration 3625 : loss : 0.030911, loss_ce: 0.012636
2022-01-06 15:24:50,732 iteration 3626 : loss : 0.033830, loss_ce: 0.011450
2022-01-06 15:24:51,956 iteration 3627 : loss : 0.036180, loss_ce: 0.016036
2022-01-06 15:24:53,185 iteration 3628 : loss : 0.051103, loss_ce: 0.017517
2022-01-06 15:24:54,447 iteration 3629 : loss : 0.045031, loss_ce: 0.017959
2022-01-06 15:24:55,646 iteration 3630 : loss : 0.029004, loss_ce: 0.012355
2022-01-06 15:24:56,826 iteration 3631 : loss : 0.028058, loss_ce: 0.010302
2022-01-06 15:24:58,038 iteration 3632 : loss : 0.029223, loss_ce: 0.015263
2022-01-06 15:24:59,253 iteration 3633 : loss : 0.043284, loss_ce: 0.013654
2022-01-06 15:25:00,370 iteration 3634 : loss : 0.029606, loss_ce: 0.008832
2022-01-06 15:25:01,679 iteration 3635 : loss : 0.024726, loss_ce: 0.011942
2022-01-06 15:25:02,762 iteration 3636 : loss : 0.030516, loss_ce: 0.015333
2022-01-06 15:25:03,951 iteration 3637 : loss : 0.029058, loss_ce: 0.011688
2022-01-06 15:25:05,174 iteration 3638 : loss : 0.031320, loss_ce: 0.011107
 54%|██████████████▍            | 214/400 [1:20:05<1:05:01, 20.97s/it]2022-01-06 15:25:06,441 iteration 3639 : loss : 0.053039, loss_ce: 0.015042
2022-01-06 15:25:07,657 iteration 3640 : loss : 0.054042, loss_ce: 0.019042
2022-01-06 15:25:08,865 iteration 3641 : loss : 0.027013, loss_ce: 0.009792
2022-01-06 15:25:09,995 iteration 3642 : loss : 0.034745, loss_ce: 0.014146
2022-01-06 15:25:11,204 iteration 3643 : loss : 0.049119, loss_ce: 0.016622
2022-01-06 15:25:12,425 iteration 3644 : loss : 0.028773, loss_ce: 0.010687
2022-01-06 15:25:13,713 iteration 3645 : loss : 0.047709, loss_ce: 0.012821
2022-01-06 15:25:14,930 iteration 3646 : loss : 0.042874, loss_ce: 0.018786
2022-01-06 15:25:16,096 iteration 3647 : loss : 0.035225, loss_ce: 0.014682
2022-01-06 15:25:17,312 iteration 3648 : loss : 0.033945, loss_ce: 0.012465
2022-01-06 15:25:18,380 iteration 3649 : loss : 0.033330, loss_ce: 0.014617
2022-01-06 15:25:19,580 iteration 3650 : loss : 0.029656, loss_ce: 0.012833
2022-01-06 15:25:20,718 iteration 3651 : loss : 0.032024, loss_ce: 0.012389
2022-01-06 15:25:21,833 iteration 3652 : loss : 0.034104, loss_ce: 0.013301
2022-01-06 15:25:23,076 iteration 3653 : loss : 0.055569, loss_ce: 0.026845
2022-01-06 15:25:24,229 iteration 3654 : loss : 0.031603, loss_ce: 0.012021
2022-01-06 15:25:24,229 Training Data Eval:
2022-01-06 15:25:30,044   Average segmentation loss on training set: 0.0303
2022-01-06 15:25:30,044 Validation Data Eval:
2022-01-06 15:25:32,026   Average segmentation loss on validation set: 0.0754
2022-01-06 15:25:37,267 Found new lowest validation loss at iteration 3654! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_MLP_best_val_loss_seed100.pth
2022-01-06 15:25:38,630 iteration 3655 : loss : 0.047080, loss_ce: 0.016066
 54%|██████████████▌            | 215/400 [1:20:38<1:16:12, 24.71s/it]2022-01-06 15:25:39,834 iteration 3656 : loss : 0.032024, loss_ce: 0.010436
2022-01-06 15:25:41,038 iteration 3657 : loss : 0.025814, loss_ce: 0.011166
2022-01-06 15:25:42,187 iteration 3658 : loss : 0.029868, loss_ce: 0.012738
2022-01-06 15:25:43,255 iteration 3659 : loss : 0.049261, loss_ce: 0.016577
2022-01-06 15:25:44,495 iteration 3660 : loss : 0.037226, loss_ce: 0.015956
2022-01-06 15:25:45,628 iteration 3661 : loss : 0.036955, loss_ce: 0.017711
2022-01-06 15:25:46,708 iteration 3662 : loss : 0.038839, loss_ce: 0.020524
2022-01-06 15:25:47,842 iteration 3663 : loss : 0.026060, loss_ce: 0.010002
2022-01-06 15:25:49,021 iteration 3664 : loss : 0.042912, loss_ce: 0.014050
2022-01-06 15:25:50,305 iteration 3665 : loss : 0.043821, loss_ce: 0.013721
2022-01-06 15:25:51,524 iteration 3666 : loss : 0.036000, loss_ce: 0.015555
2022-01-06 15:25:52,646 iteration 3667 : loss : 0.029725, loss_ce: 0.011553
2022-01-06 15:25:53,783 iteration 3668 : loss : 0.043444, loss_ce: 0.014944
2022-01-06 15:25:54,979 iteration 3669 : loss : 0.057291, loss_ce: 0.017414
2022-01-06 15:25:56,087 iteration 3670 : loss : 0.027677, loss_ce: 0.012590
2022-01-06 15:25:57,267 iteration 3671 : loss : 0.041942, loss_ce: 0.012314
2022-01-06 15:25:58,440 iteration 3672 : loss : 0.038131, loss_ce: 0.011242
 54%|██████████████▌            | 216/400 [1:20:58<1:11:17, 23.25s/it]2022-01-06 15:25:59,675 iteration 3673 : loss : 0.034580, loss_ce: 0.015289
2022-01-06 15:26:00,947 iteration 3674 : loss : 0.038009, loss_ce: 0.015706
2022-01-06 15:26:02,210 iteration 3675 : loss : 0.036531, loss_ce: 0.015320
2022-01-06 15:26:03,461 iteration 3676 : loss : 0.039906, loss_ce: 0.010186
2022-01-06 15:26:04,687 iteration 3677 : loss : 0.032065, loss_ce: 0.014248
2022-01-06 15:26:05,970 iteration 3678 : loss : 0.046801, loss_ce: 0.017678
2022-01-06 15:26:07,070 iteration 3679 : loss : 0.028595, loss_ce: 0.009551
2022-01-06 15:26:08,198 iteration 3680 : loss : 0.030269, loss_ce: 0.012397
2022-01-06 15:26:09,395 iteration 3681 : loss : 0.065247, loss_ce: 0.018991
2022-01-06 15:26:10,492 iteration 3682 : loss : 0.027877, loss_ce: 0.008956
2022-01-06 15:26:11,676 iteration 3683 : loss : 0.045869, loss_ce: 0.019687
2022-01-06 15:26:12,795 iteration 3684 : loss : 0.037904, loss_ce: 0.012302
2022-01-06 15:26:13,960 iteration 3685 : loss : 0.028557, loss_ce: 0.009160
2022-01-06 15:26:15,232 iteration 3686 : loss : 0.035243, loss_ce: 0.014004
2022-01-06 15:26:16,498 iteration 3687 : loss : 0.051975, loss_ce: 0.022496
2022-01-06 15:26:17,696 iteration 3688 : loss : 0.034171, loss_ce: 0.012988
2022-01-06 15:26:18,906 iteration 3689 : loss : 0.043101, loss_ce: 0.012320
 54%|██████████████▋            | 217/400 [1:21:19<1:08:21, 22.41s/it]2022-01-06 15:26:20,284 iteration 3690 : loss : 0.037137, loss_ce: 0.017199
2022-01-06 15:26:21,426 iteration 3691 : loss : 0.042121, loss_ce: 0.011786
2022-01-06 15:26:22,630 iteration 3692 : loss : 0.037189, loss_ce: 0.011246
2022-01-06 15:26:23,758 iteration 3693 : loss : 0.023620, loss_ce: 0.006766
2022-01-06 15:26:24,990 iteration 3694 : loss : 0.030333, loss_ce: 0.010952
2022-01-06 15:26:26,216 iteration 3695 : loss : 0.026131, loss_ce: 0.008762
2022-01-06 15:26:27,363 iteration 3696 : loss : 0.030710, loss_ce: 0.012301
2022-01-06 15:26:28,502 iteration 3697 : loss : 0.058278, loss_ce: 0.019365
2022-01-06 15:26:29,792 iteration 3698 : loss : 0.036118, loss_ce: 0.012450
2022-01-06 15:26:30,973 iteration 3699 : loss : 0.034725, loss_ce: 0.015904
2022-01-06 15:26:32,149 iteration 3700 : loss : 0.033084, loss_ce: 0.011360
2022-01-06 15:26:33,261 iteration 3701 : loss : 0.037320, loss_ce: 0.017910
2022-01-06 15:26:34,493 iteration 3702 : loss : 0.037740, loss_ce: 0.013877
2022-01-06 15:26:35,671 iteration 3703 : loss : 0.029337, loss_ce: 0.011714
2022-01-06 15:26:36,840 iteration 3704 : loss : 0.037484, loss_ce: 0.013533
2022-01-06 15:26:38,039 iteration 3705 : loss : 0.034333, loss_ce: 0.017224
2022-01-06 15:26:39,156 iteration 3706 : loss : 0.030875, loss_ce: 0.010151
 55%|██████████████▋            | 218/400 [1:21:39<1:06:00, 21.76s/it]2022-01-06 15:26:40,429 iteration 3707 : loss : 0.099987, loss_ce: 0.022843
2022-01-06 15:26:41,740 iteration 3708 : loss : 0.065587, loss_ce: 0.016452
2022-01-06 15:26:42,915 iteration 3709 : loss : 0.036705, loss_ce: 0.012290
2022-01-06 15:26:43,991 iteration 3710 : loss : 0.026026, loss_ce: 0.010315
2022-01-06 15:26:45,312 iteration 3711 : loss : 0.040050, loss_ce: 0.017190
2022-01-06 15:26:46,442 iteration 3712 : loss : 0.028839, loss_ce: 0.011101
2022-01-06 15:26:47,558 iteration 3713 : loss : 0.023396, loss_ce: 0.008366
2022-01-06 15:26:48,788 iteration 3714 : loss : 0.037139, loss_ce: 0.017465
2022-01-06 15:26:49,933 iteration 3715 : loss : 0.025433, loss_ce: 0.009633
2022-01-06 15:26:51,120 iteration 3716 : loss : 0.062121, loss_ce: 0.027189
2022-01-06 15:26:52,324 iteration 3717 : loss : 0.029185, loss_ce: 0.014310
2022-01-06 15:26:53,552 iteration 3718 : loss : 0.032337, loss_ce: 0.012343
2022-01-06 15:26:54,847 iteration 3719 : loss : 0.033141, loss_ce: 0.011484
2022-01-06 15:26:56,128 iteration 3720 : loss : 0.035997, loss_ce: 0.014367
2022-01-06 15:26:57,317 iteration 3721 : loss : 0.041695, loss_ce: 0.016128
2022-01-06 15:26:58,581 iteration 3722 : loss : 0.039652, loss_ce: 0.012206
2022-01-06 15:26:59,739 iteration 3723 : loss : 0.028780, loss_ce: 0.012206
 55%|██████████████▊            | 219/400 [1:22:00<1:04:34, 21.41s/it]2022-01-06 15:27:00,978 iteration 3724 : loss : 0.038503, loss_ce: 0.021718
2022-01-06 15:27:02,215 iteration 3725 : loss : 0.047475, loss_ce: 0.022342
2022-01-06 15:27:03,388 iteration 3726 : loss : 0.027466, loss_ce: 0.011999
2022-01-06 15:27:04,574 iteration 3727 : loss : 0.024892, loss_ce: 0.009762
2022-01-06 15:27:05,655 iteration 3728 : loss : 0.029019, loss_ce: 0.014455
2022-01-06 15:27:06,877 iteration 3729 : loss : 0.039324, loss_ce: 0.011761
2022-01-06 15:27:08,007 iteration 3730 : loss : 0.029364, loss_ce: 0.011933
2022-01-06 15:27:09,249 iteration 3731 : loss : 0.044337, loss_ce: 0.019545
2022-01-06 15:27:10,434 iteration 3732 : loss : 0.035217, loss_ce: 0.017472
2022-01-06 15:27:11,553 iteration 3733 : loss : 0.023530, loss_ce: 0.006956
2022-01-06 15:27:12,783 iteration 3734 : loss : 0.034857, loss_ce: 0.012840
2022-01-06 15:27:14,037 iteration 3735 : loss : 0.037314, loss_ce: 0.012770
2022-01-06 15:27:15,229 iteration 3736 : loss : 0.035620, loss_ce: 0.014759
2022-01-06 15:27:16,464 iteration 3737 : loss : 0.044166, loss_ce: 0.019425
2022-01-06 15:27:17,625 iteration 3738 : loss : 0.050646, loss_ce: 0.016019
2022-01-06 15:27:18,787 iteration 3739 : loss : 0.032370, loss_ce: 0.009468
2022-01-06 15:27:18,787 Training Data Eval:
2022-01-06 15:27:24,595   Average segmentation loss on training set: 0.0285
2022-01-06 15:27:24,595 Validation Data Eval:
2022-01-06 15:27:26,577   Average segmentation loss on validation set: 0.1194
2022-01-06 15:27:27,706 iteration 3740 : loss : 0.033097, loss_ce: 0.009700
 55%|██████████████▊            | 220/400 [1:22:28<1:10:08, 23.38s/it]2022-01-06 15:27:28,965 iteration 3741 : loss : 0.037723, loss_ce: 0.016074
2022-01-06 15:27:30,122 iteration 3742 : loss : 0.035388, loss_ce: 0.012013
2022-01-06 15:27:31,273 iteration 3743 : loss : 0.028008, loss_ce: 0.008076
2022-01-06 15:27:32,486 iteration 3744 : loss : 0.024675, loss_ce: 0.009656
2022-01-06 15:27:33,768 iteration 3745 : loss : 0.038886, loss_ce: 0.021282
2022-01-06 15:27:34,928 iteration 3746 : loss : 0.028561, loss_ce: 0.008400
2022-01-06 15:27:36,149 iteration 3747 : loss : 0.044212, loss_ce: 0.021725
2022-01-06 15:27:37,383 iteration 3748 : loss : 0.037896, loss_ce: 0.013532
2022-01-06 15:27:38,563 iteration 3749 : loss : 0.037263, loss_ce: 0.011313
2022-01-06 15:27:39,857 iteration 3750 : loss : 0.044511, loss_ce: 0.018072
2022-01-06 15:27:41,022 iteration 3751 : loss : 0.032836, loss_ce: 0.015815
2022-01-06 15:27:42,172 iteration 3752 : loss : 0.027091, loss_ce: 0.010299
2022-01-06 15:27:43,347 iteration 3753 : loss : 0.029606, loss_ce: 0.009762
2022-01-06 15:27:44,453 iteration 3754 : loss : 0.026636, loss_ce: 0.012619
2022-01-06 15:27:45,659 iteration 3755 : loss : 0.035120, loss_ce: 0.013740
2022-01-06 15:27:46,810 iteration 3756 : loss : 0.026145, loss_ce: 0.010373
2022-01-06 15:27:47,989 iteration 3757 : loss : 0.042447, loss_ce: 0.016321
 55%|██████████████▉            | 221/400 [1:22:48<1:06:58, 22.45s/it]2022-01-06 15:27:49,276 iteration 3758 : loss : 0.046352, loss_ce: 0.019512
2022-01-06 15:27:50,497 iteration 3759 : loss : 0.034468, loss_ce: 0.014657
2022-01-06 15:27:51,713 iteration 3760 : loss : 0.049846, loss_ce: 0.021957
2022-01-06 15:27:52,899 iteration 3761 : loss : 0.026049, loss_ce: 0.012366
2022-01-06 15:27:54,139 iteration 3762 : loss : 0.039323, loss_ce: 0.016373
2022-01-06 15:27:55,383 iteration 3763 : loss : 0.042652, loss_ce: 0.020475
2022-01-06 15:27:56,517 iteration 3764 : loss : 0.043034, loss_ce: 0.012858
2022-01-06 15:27:57,614 iteration 3765 : loss : 0.057174, loss_ce: 0.025401
2022-01-06 15:27:58,759 iteration 3766 : loss : 0.028311, loss_ce: 0.009189
2022-01-06 15:27:59,955 iteration 3767 : loss : 0.040584, loss_ce: 0.014266
2022-01-06 15:28:01,214 iteration 3768 : loss : 0.031819, loss_ce: 0.011344
2022-01-06 15:28:02,396 iteration 3769 : loss : 0.038091, loss_ce: 0.013544
2022-01-06 15:28:03,632 iteration 3770 : loss : 0.022369, loss_ce: 0.008877
2022-01-06 15:28:04,835 iteration 3771 : loss : 0.033334, loss_ce: 0.014980
2022-01-06 15:28:06,059 iteration 3772 : loss : 0.047041, loss_ce: 0.015404
2022-01-06 15:28:07,244 iteration 3773 : loss : 0.035320, loss_ce: 0.012811
2022-01-06 15:28:08,437 iteration 3774 : loss : 0.045306, loss_ce: 0.021003
 56%|██████████████▉            | 222/400 [1:23:08<1:04:49, 21.85s/it]2022-01-06 15:28:09,736 iteration 3775 : loss : 0.034664, loss_ce: 0.012987
2022-01-06 15:28:10,915 iteration 3776 : loss : 0.028637, loss_ce: 0.011730
2022-01-06 15:28:12,099 iteration 3777 : loss : 0.039348, loss_ce: 0.012269
2022-01-06 15:28:13,346 iteration 3778 : loss : 0.050754, loss_ce: 0.023011
2022-01-06 15:28:14,475 iteration 3779 : loss : 0.035469, loss_ce: 0.014431
2022-01-06 15:28:15,730 iteration 3780 : loss : 0.036709, loss_ce: 0.016451
2022-01-06 15:28:16,967 iteration 3781 : loss : 0.037956, loss_ce: 0.015348
2022-01-06 15:28:18,231 iteration 3782 : loss : 0.047631, loss_ce: 0.018033
2022-01-06 15:28:19,486 iteration 3783 : loss : 0.032493, loss_ce: 0.015606
2022-01-06 15:28:20,612 iteration 3784 : loss : 0.044011, loss_ce: 0.014082
2022-01-06 15:28:21,803 iteration 3785 : loss : 0.044700, loss_ce: 0.015271
2022-01-06 15:28:22,982 iteration 3786 : loss : 0.038172, loss_ce: 0.015943
2022-01-06 15:28:24,110 iteration 3787 : loss : 0.055592, loss_ce: 0.024618
2022-01-06 15:28:25,326 iteration 3788 : loss : 0.038098, loss_ce: 0.014057
2022-01-06 15:28:26,520 iteration 3789 : loss : 0.032213, loss_ce: 0.013706
2022-01-06 15:28:27,670 iteration 3790 : loss : 0.031404, loss_ce: 0.011631
2022-01-06 15:28:28,928 iteration 3791 : loss : 0.029362, loss_ce: 0.010703
 56%|███████████████            | 223/400 [1:23:29<1:03:15, 21.44s/it]2022-01-06 15:28:30,137 iteration 3792 : loss : 0.037706, loss_ce: 0.015156
2022-01-06 15:28:31,242 iteration 3793 : loss : 0.034831, loss_ce: 0.008544
2022-01-06 15:28:32,409 iteration 3794 : loss : 0.028803, loss_ce: 0.013245
2022-01-06 15:28:33,583 iteration 3795 : loss : 0.028499, loss_ce: 0.008709
2022-01-06 15:28:34,870 iteration 3796 : loss : 0.035079, loss_ce: 0.011724
2022-01-06 15:28:36,104 iteration 3797 : loss : 0.024294, loss_ce: 0.007769
2022-01-06 15:28:37,242 iteration 3798 : loss : 0.026313, loss_ce: 0.010050
2022-01-06 15:28:38,551 iteration 3799 : loss : 0.042751, loss_ce: 0.014436
2022-01-06 15:28:39,920 iteration 3800 : loss : 0.063521, loss_ce: 0.029308
2022-01-06 15:28:41,215 iteration 3801 : loss : 0.048574, loss_ce: 0.015216
2022-01-06 15:28:42,473 iteration 3802 : loss : 0.061266, loss_ce: 0.021440
2022-01-06 15:28:43,654 iteration 3803 : loss : 0.050123, loss_ce: 0.021530
2022-01-06 15:28:44,857 iteration 3804 : loss : 0.035061, loss_ce: 0.015044
2022-01-06 15:28:46,071 iteration 3805 : loss : 0.031493, loss_ce: 0.012706
2022-01-06 15:28:47,200 iteration 3806 : loss : 0.028407, loss_ce: 0.012988
2022-01-06 15:28:48,442 iteration 3807 : loss : 0.037124, loss_ce: 0.012296
2022-01-06 15:28:49,622 iteration 3808 : loss : 0.045723, loss_ce: 0.023287
 56%|███████████████            | 224/400 [1:23:49<1:02:14, 21.22s/it]2022-01-06 15:28:50,911 iteration 3809 : loss : 0.030845, loss_ce: 0.013511
2022-01-06 15:28:52,143 iteration 3810 : loss : 0.028824, loss_ce: 0.010467
2022-01-06 15:28:53,304 iteration 3811 : loss : 0.026768, loss_ce: 0.010031
2022-01-06 15:28:54,527 iteration 3812 : loss : 0.028856, loss_ce: 0.011066
2022-01-06 15:28:55,641 iteration 3813 : loss : 0.028677, loss_ce: 0.013050
2022-01-06 15:28:56,920 iteration 3814 : loss : 0.033519, loss_ce: 0.014958
2022-01-06 15:28:58,249 iteration 3815 : loss : 0.035175, loss_ce: 0.013022
2022-01-06 15:28:59,403 iteration 3816 : loss : 0.028499, loss_ce: 0.011641
2022-01-06 15:29:00,489 iteration 3817 : loss : 0.023306, loss_ce: 0.008878
2022-01-06 15:29:01,665 iteration 3818 : loss : 0.034659, loss_ce: 0.008925
2022-01-06 15:29:02,922 iteration 3819 : loss : 0.048143, loss_ce: 0.018898
2022-01-06 15:29:04,033 iteration 3820 : loss : 0.023062, loss_ce: 0.009757
2022-01-06 15:29:05,300 iteration 3821 : loss : 0.038968, loss_ce: 0.017675
2022-01-06 15:29:06,470 iteration 3822 : loss : 0.039625, loss_ce: 0.012004
2022-01-06 15:29:07,612 iteration 3823 : loss : 0.044580, loss_ce: 0.015011
2022-01-06 15:29:08,837 iteration 3824 : loss : 0.032644, loss_ce: 0.012956
2022-01-06 15:29:08,837 Training Data Eval:
2022-01-06 15:29:14,698   Average segmentation loss on training set: 0.0836
2022-01-06 15:29:14,698 Validation Data Eval:
2022-01-06 15:29:16,695   Average segmentation loss on validation set: 0.1819
2022-01-06 15:29:17,790 iteration 3825 : loss : 0.034363, loss_ce: 0.010537
 56%|███████████████▏           | 225/400 [1:24:18<1:07:58, 23.30s/it]2022-01-06 15:29:19,134 iteration 3826 : loss : 0.032498, loss_ce: 0.013181
2022-01-06 15:29:20,395 iteration 3827 : loss : 0.042504, loss_ce: 0.019532
2022-01-06 15:29:21,521 iteration 3828 : loss : 0.025916, loss_ce: 0.013957
2022-01-06 15:29:22,667 iteration 3829 : loss : 0.022644, loss_ce: 0.008397
2022-01-06 15:29:23,946 iteration 3830 : loss : 0.041131, loss_ce: 0.013415
2022-01-06 15:29:25,192 iteration 3831 : loss : 0.032635, loss_ce: 0.013589
2022-01-06 15:29:26,409 iteration 3832 : loss : 0.042498, loss_ce: 0.019172
2022-01-06 15:29:27,625 iteration 3833 : loss : 0.052248, loss_ce: 0.015440
2022-01-06 15:29:28,765 iteration 3834 : loss : 0.035277, loss_ce: 0.013178
2022-01-06 15:29:29,989 iteration 3835 : loss : 0.053373, loss_ce: 0.012288
2022-01-06 15:29:31,103 iteration 3836 : loss : 0.025232, loss_ce: 0.010783
2022-01-06 15:29:32,199 iteration 3837 : loss : 0.027038, loss_ce: 0.011549
2022-01-06 15:29:33,449 iteration 3838 : loss : 0.033105, loss_ce: 0.015465
2022-01-06 15:29:34,575 iteration 3839 : loss : 0.048999, loss_ce: 0.014328
2022-01-06 15:29:35,768 iteration 3840 : loss : 0.029817, loss_ce: 0.011766
2022-01-06 15:29:36,854 iteration 3841 : loss : 0.032357, loss_ce: 0.012874
2022-01-06 15:29:38,083 iteration 3842 : loss : 0.077903, loss_ce: 0.018485
 56%|███████████████▎           | 226/400 [1:24:38<1:04:57, 22.40s/it]2022-01-06 15:29:39,322 iteration 3843 : loss : 0.027927, loss_ce: 0.008961
2022-01-06 15:29:40,568 iteration 3844 : loss : 0.060567, loss_ce: 0.029197
2022-01-06 15:29:41,649 iteration 3845 : loss : 0.044986, loss_ce: 0.014738
2022-01-06 15:29:42,811 iteration 3846 : loss : 0.040163, loss_ce: 0.014939
2022-01-06 15:29:44,114 iteration 3847 : loss : 0.044263, loss_ce: 0.014957
2022-01-06 15:29:45,255 iteration 3848 : loss : 0.046979, loss_ce: 0.016864
2022-01-06 15:29:46,394 iteration 3849 : loss : 0.027237, loss_ce: 0.010356
2022-01-06 15:29:47,644 iteration 3850 : loss : 0.045269, loss_ce: 0.016576
2022-01-06 15:29:48,916 iteration 3851 : loss : 0.054329, loss_ce: 0.017342
2022-01-06 15:29:50,140 iteration 3852 : loss : 0.033711, loss_ce: 0.013712
2022-01-06 15:29:51,309 iteration 3853 : loss : 0.029496, loss_ce: 0.011707
2022-01-06 15:29:52,380 iteration 3854 : loss : 0.026314, loss_ce: 0.011332
2022-01-06 15:29:53,541 iteration 3855 : loss : 0.040270, loss_ce: 0.011337
2022-01-06 15:29:54,849 iteration 3856 : loss : 0.040045, loss_ce: 0.021097
2022-01-06 15:29:56,003 iteration 3857 : loss : 0.083159, loss_ce: 0.023783
2022-01-06 15:29:57,164 iteration 3858 : loss : 0.042103, loss_ce: 0.020910
2022-01-06 15:29:58,298 iteration 3859 : loss : 0.044445, loss_ce: 0.020356
 57%|███████████████▎           | 227/400 [1:24:58<1:02:41, 21.75s/it]2022-01-06 15:29:59,483 iteration 3860 : loss : 0.042428, loss_ce: 0.021367
2022-01-06 15:30:00,614 iteration 3861 : loss : 0.028438, loss_ce: 0.012335
2022-01-06 15:30:01,733 iteration 3862 : loss : 0.036820, loss_ce: 0.011642
2022-01-06 15:30:02,926 iteration 3863 : loss : 0.046603, loss_ce: 0.026425
2022-01-06 15:30:04,005 iteration 3864 : loss : 0.037288, loss_ce: 0.013830
2022-01-06 15:30:05,208 iteration 3865 : loss : 0.031523, loss_ce: 0.013476
2022-01-06 15:30:06,374 iteration 3866 : loss : 0.038196, loss_ce: 0.015778
2022-01-06 15:30:07,604 iteration 3867 : loss : 0.059406, loss_ce: 0.024791
2022-01-06 15:30:08,702 iteration 3868 : loss : 0.032680, loss_ce: 0.014677
2022-01-06 15:30:09,973 iteration 3869 : loss : 0.050510, loss_ce: 0.016334
2022-01-06 15:30:11,120 iteration 3870 : loss : 0.026851, loss_ce: 0.010127
2022-01-06 15:30:12,348 iteration 3871 : loss : 0.053391, loss_ce: 0.018806
2022-01-06 15:30:13,510 iteration 3872 : loss : 0.030899, loss_ce: 0.011829
2022-01-06 15:30:14,664 iteration 3873 : loss : 0.026817, loss_ce: 0.008594
2022-01-06 15:30:15,900 iteration 3874 : loss : 0.042183, loss_ce: 0.015284
2022-01-06 15:30:17,103 iteration 3875 : loss : 0.048624, loss_ce: 0.013205
2022-01-06 15:30:18,378 iteration 3876 : loss : 0.036788, loss_ce: 0.015553
 57%|███████████████▍           | 228/400 [1:25:18<1:00:54, 21.25s/it]2022-01-06 15:30:19,571 iteration 3877 : loss : 0.040473, loss_ce: 0.016468
2022-01-06 15:30:20,773 iteration 3878 : loss : 0.037245, loss_ce: 0.013626
2022-01-06 15:30:22,015 iteration 3879 : loss : 0.040383, loss_ce: 0.025168
2022-01-06 15:30:23,186 iteration 3880 : loss : 0.028705, loss_ce: 0.010102
2022-01-06 15:30:24,386 iteration 3881 : loss : 0.047129, loss_ce: 0.016519
2022-01-06 15:30:25,623 iteration 3882 : loss : 0.030514, loss_ce: 0.014542
2022-01-06 15:30:26,820 iteration 3883 : loss : 0.032466, loss_ce: 0.011586
2022-01-06 15:30:28,017 iteration 3884 : loss : 0.027815, loss_ce: 0.010599
2022-01-06 15:30:29,152 iteration 3885 : loss : 0.030001, loss_ce: 0.009359
2022-01-06 15:30:30,383 iteration 3886 : loss : 0.030680, loss_ce: 0.012946
2022-01-06 15:30:31,583 iteration 3887 : loss : 0.029354, loss_ce: 0.011410
2022-01-06 15:30:32,836 iteration 3888 : loss : 0.034949, loss_ce: 0.018671
2022-01-06 15:30:33,976 iteration 3889 : loss : 0.044130, loss_ce: 0.014269
2022-01-06 15:30:35,134 iteration 3890 : loss : 0.032754, loss_ce: 0.012232
2022-01-06 15:30:36,356 iteration 3891 : loss : 0.041021, loss_ce: 0.018776
2022-01-06 15:30:37,530 iteration 3892 : loss : 0.031316, loss_ce: 0.011597
2022-01-06 15:30:38,659 iteration 3893 : loss : 0.035284, loss_ce: 0.009737
 57%|████████████████▌            | 229/400 [1:25:38<59:43, 20.95s/it]2022-01-06 15:30:39,893 iteration 3894 : loss : 0.031003, loss_ce: 0.012089
2022-01-06 15:30:41,113 iteration 3895 : loss : 0.031528, loss_ce: 0.013806
2022-01-06 15:30:42,223 iteration 3896 : loss : 0.028870, loss_ce: 0.011305
2022-01-06 15:30:43,470 iteration 3897 : loss : 0.040464, loss_ce: 0.017222
2022-01-06 15:30:44,681 iteration 3898 : loss : 0.048055, loss_ce: 0.014277
2022-01-06 15:30:45,883 iteration 3899 : loss : 0.022361, loss_ce: 0.007457
2022-01-06 15:30:47,099 iteration 3900 : loss : 0.030251, loss_ce: 0.013503
2022-01-06 15:30:48,185 iteration 3901 : loss : 0.032323, loss_ce: 0.016780
2022-01-06 15:30:49,379 iteration 3902 : loss : 0.023127, loss_ce: 0.007784
2022-01-06 15:30:50,510 iteration 3903 : loss : 0.026104, loss_ce: 0.008234
2022-01-06 15:30:51,702 iteration 3904 : loss : 0.023332, loss_ce: 0.010121
2022-01-06 15:30:52,891 iteration 3905 : loss : 0.028324, loss_ce: 0.015138
2022-01-06 15:30:54,088 iteration 3906 : loss : 0.024094, loss_ce: 0.008574
2022-01-06 15:30:55,277 iteration 3907 : loss : 0.053374, loss_ce: 0.017960
2022-01-06 15:30:56,368 iteration 3908 : loss : 0.030954, loss_ce: 0.011741
2022-01-06 15:30:57,482 iteration 3909 : loss : 0.033672, loss_ce: 0.014958
2022-01-06 15:30:57,482 Training Data Eval:
2022-01-06 15:31:03,366   Average segmentation loss on training set: 0.0366
2022-01-06 15:31:03,366 Validation Data Eval:
2022-01-06 15:31:05,374   Average segmentation loss on validation set: 0.0760
2022-01-06 15:31:06,574 iteration 3910 : loss : 0.029025, loss_ce: 0.012326
 57%|███████████████▌           | 230/400 [1:26:06<1:05:17, 23.04s/it]2022-01-06 15:31:07,726 iteration 3911 : loss : 0.027242, loss_ce: 0.012088
2022-01-06 15:31:08,876 iteration 3912 : loss : 0.054176, loss_ce: 0.018038
2022-01-06 15:31:10,077 iteration 3913 : loss : 0.035552, loss_ce: 0.016936
2022-01-06 15:31:11,164 iteration 3914 : loss : 0.020100, loss_ce: 0.009112
2022-01-06 15:31:12,386 iteration 3915 : loss : 0.039512, loss_ce: 0.013076
2022-01-06 15:31:13,584 iteration 3916 : loss : 0.029812, loss_ce: 0.012209
2022-01-06 15:31:14,805 iteration 3917 : loss : 0.049237, loss_ce: 0.015280
2022-01-06 15:31:15,983 iteration 3918 : loss : 0.029341, loss_ce: 0.010011
2022-01-06 15:31:17,167 iteration 3919 : loss : 0.029511, loss_ce: 0.010699
2022-01-06 15:31:18,390 iteration 3920 : loss : 0.077482, loss_ce: 0.012603
2022-01-06 15:31:19,566 iteration 3921 : loss : 0.038801, loss_ce: 0.010306
2022-01-06 15:31:20,774 iteration 3922 : loss : 0.049759, loss_ce: 0.012942
2022-01-06 15:31:22,052 iteration 3923 : loss : 0.064333, loss_ce: 0.027148
2022-01-06 15:31:23,184 iteration 3924 : loss : 0.047231, loss_ce: 0.017215
2022-01-06 15:31:24,278 iteration 3925 : loss : 0.039627, loss_ce: 0.017213
2022-01-06 15:31:25,616 iteration 3926 : loss : 0.076077, loss_ce: 0.028321
2022-01-06 15:31:26,823 iteration 3927 : loss : 0.050774, loss_ce: 0.023432
 58%|███████████████▌           | 231/400 [1:26:27<1:02:32, 22.20s/it]2022-01-06 15:31:28,048 iteration 3928 : loss : 0.031981, loss_ce: 0.011637
2022-01-06 15:31:29,180 iteration 3929 : loss : 0.039479, loss_ce: 0.021987
2022-01-06 15:31:30,357 iteration 3930 : loss : 0.052752, loss_ce: 0.023490
2022-01-06 15:31:31,445 iteration 3931 : loss : 0.037389, loss_ce: 0.010802
2022-01-06 15:31:32,632 iteration 3932 : loss : 0.060814, loss_ce: 0.021037
2022-01-06 15:31:33,785 iteration 3933 : loss : 0.039022, loss_ce: 0.018499
2022-01-06 15:31:34,996 iteration 3934 : loss : 0.047684, loss_ce: 0.018467
2022-01-06 15:31:36,161 iteration 3935 : loss : 0.055131, loss_ce: 0.020079
2022-01-06 15:31:37,278 iteration 3936 : loss : 0.043931, loss_ce: 0.014967
2022-01-06 15:31:38,460 iteration 3937 : loss : 0.033144, loss_ce: 0.010899
2022-01-06 15:31:39,634 iteration 3938 : loss : 0.032393, loss_ce: 0.009441
2022-01-06 15:31:40,879 iteration 3939 : loss : 0.060653, loss_ce: 0.015565
2022-01-06 15:31:42,042 iteration 3940 : loss : 0.047316, loss_ce: 0.020860
2022-01-06 15:31:43,210 iteration 3941 : loss : 0.039713, loss_ce: 0.013405
2022-01-06 15:31:44,351 iteration 3942 : loss : 0.035154, loss_ce: 0.018609
2022-01-06 15:31:45,485 iteration 3943 : loss : 0.045676, loss_ce: 0.019359
2022-01-06 15:31:46,639 iteration 3944 : loss : 0.060544, loss_ce: 0.017683
 58%|███████████████▋           | 232/400 [1:26:46<1:00:10, 21.49s/it]2022-01-06 15:31:47,938 iteration 3945 : loss : 0.047238, loss_ce: 0.015908
2022-01-06 15:31:49,081 iteration 3946 : loss : 0.030883, loss_ce: 0.012505
2022-01-06 15:31:50,202 iteration 3947 : loss : 0.039946, loss_ce: 0.016413
2022-01-06 15:31:51,430 iteration 3948 : loss : 0.049596, loss_ce: 0.021414
2022-01-06 15:31:52,684 iteration 3949 : loss : 0.047530, loss_ce: 0.015127
2022-01-06 15:31:53,834 iteration 3950 : loss : 0.030300, loss_ce: 0.015088
2022-01-06 15:31:54,951 iteration 3951 : loss : 0.048912, loss_ce: 0.013479
2022-01-06 15:31:56,248 iteration 3952 : loss : 0.050889, loss_ce: 0.023270
2022-01-06 15:31:57,417 iteration 3953 : loss : 0.025531, loss_ce: 0.010899
2022-01-06 15:31:58,533 iteration 3954 : loss : 0.025198, loss_ce: 0.009630
2022-01-06 15:31:59,741 iteration 3955 : loss : 0.037729, loss_ce: 0.015276
2022-01-06 15:32:00,964 iteration 3956 : loss : 0.033940, loss_ce: 0.017147
2022-01-06 15:32:02,196 iteration 3957 : loss : 0.066424, loss_ce: 0.018963
2022-01-06 15:32:03,427 iteration 3958 : loss : 0.050922, loss_ce: 0.024883
2022-01-06 15:32:04,502 iteration 3959 : loss : 0.027509, loss_ce: 0.007334
2022-01-06 15:32:05,738 iteration 3960 : loss : 0.054540, loss_ce: 0.021848
2022-01-06 15:32:06,891 iteration 3961 : loss : 0.034437, loss_ce: 0.009352
 58%|████████████████▉            | 233/400 [1:27:07<58:46, 21.12s/it]2022-01-06 15:32:08,148 iteration 3962 : loss : 0.034725, loss_ce: 0.014015
2022-01-06 15:32:09,384 iteration 3963 : loss : 0.047037, loss_ce: 0.019716
2022-01-06 15:32:10,493 iteration 3964 : loss : 0.029452, loss_ce: 0.009832
2022-01-06 15:32:11,691 iteration 3965 : loss : 0.028812, loss_ce: 0.012805
2022-01-06 15:32:12,985 iteration 3966 : loss : 0.034402, loss_ce: 0.016644
2022-01-06 15:32:14,171 iteration 3967 : loss : 0.050266, loss_ce: 0.013201
2022-01-06 15:32:15,353 iteration 3968 : loss : 0.032395, loss_ce: 0.011896
2022-01-06 15:32:16,502 iteration 3969 : loss : 0.028480, loss_ce: 0.010033
2022-01-06 15:32:17,688 iteration 3970 : loss : 0.048313, loss_ce: 0.014720
2022-01-06 15:32:18,858 iteration 3971 : loss : 0.029143, loss_ce: 0.009563
2022-01-06 15:32:20,048 iteration 3972 : loss : 0.047716, loss_ce: 0.020345
2022-01-06 15:32:21,305 iteration 3973 : loss : 0.035492, loss_ce: 0.016162
2022-01-06 15:32:22,552 iteration 3974 : loss : 0.033232, loss_ce: 0.014718
2022-01-06 15:32:23,724 iteration 3975 : loss : 0.035828, loss_ce: 0.010882
2022-01-06 15:32:24,878 iteration 3976 : loss : 0.030344, loss_ce: 0.010931
2022-01-06 15:32:26,081 iteration 3977 : loss : 0.038836, loss_ce: 0.019270
2022-01-06 15:32:27,259 iteration 3978 : loss : 0.037514, loss_ce: 0.016814
 58%|████████████████▉            | 234/400 [1:27:27<57:47, 20.89s/it]2022-01-06 15:32:28,583 iteration 3979 : loss : 0.043531, loss_ce: 0.022107
2022-01-06 15:32:29,664 iteration 3980 : loss : 0.032513, loss_ce: 0.013016
2022-01-06 15:32:30,846 iteration 3981 : loss : 0.041820, loss_ce: 0.013818
2022-01-06 15:32:32,039 iteration 3982 : loss : 0.030226, loss_ce: 0.013543
2022-01-06 15:32:33,235 iteration 3983 : loss : 0.078542, loss_ce: 0.022783
2022-01-06 15:32:34,450 iteration 3984 : loss : 0.041435, loss_ce: 0.018686
2022-01-06 15:32:35,674 iteration 3985 : loss : 0.030032, loss_ce: 0.009706
2022-01-06 15:32:36,922 iteration 3986 : loss : 0.040537, loss_ce: 0.013211
2022-01-06 15:32:38,050 iteration 3987 : loss : 0.054247, loss_ce: 0.026462
2022-01-06 15:32:39,215 iteration 3988 : loss : 0.031715, loss_ce: 0.011849
2022-01-06 15:32:40,416 iteration 3989 : loss : 0.027412, loss_ce: 0.010621
2022-01-06 15:32:41,613 iteration 3990 : loss : 0.034438, loss_ce: 0.014095
2022-01-06 15:32:42,839 iteration 3991 : loss : 0.035758, loss_ce: 0.016007
2022-01-06 15:32:44,068 iteration 3992 : loss : 0.033599, loss_ce: 0.013982
2022-01-06 15:32:45,258 iteration 3993 : loss : 0.030906, loss_ce: 0.011876
2022-01-06 15:32:46,495 iteration 3994 : loss : 0.031207, loss_ce: 0.010105
2022-01-06 15:32:46,496 Training Data Eval:
2022-01-06 15:32:52,325   Average segmentation loss on training set: 0.0991
2022-01-06 15:32:52,326 Validation Data Eval:
2022-01-06 15:32:54,316   Average segmentation loss on validation set: 0.1864
2022-01-06 15:32:55,619 iteration 3995 : loss : 0.028715, loss_ce: 0.009497
 59%|███████████████▊           | 235/400 [1:27:55<1:03:36, 23.13s/it]2022-01-06 15:32:56,837 iteration 3996 : loss : 0.029472, loss_ce: 0.013714
2022-01-06 15:32:57,944 iteration 3997 : loss : 0.028229, loss_ce: 0.013962
2022-01-06 15:32:59,048 iteration 3998 : loss : 0.029816, loss_ce: 0.009334
2022-01-06 15:33:00,302 iteration 3999 : loss : 0.042082, loss_ce: 0.012905
2022-01-06 15:33:01,490 iteration 4000 : loss : 0.027866, loss_ce: 0.011575
2022-01-06 15:33:02,717 iteration 4001 : loss : 0.034734, loss_ce: 0.017598
2022-01-06 15:33:03,798 iteration 4002 : loss : 0.031577, loss_ce: 0.010792
2022-01-06 15:33:05,038 iteration 4003 : loss : 0.040864, loss_ce: 0.013283
2022-01-06 15:33:06,255 iteration 4004 : loss : 0.041293, loss_ce: 0.017558
2022-01-06 15:33:07,446 iteration 4005 : loss : 0.031510, loss_ce: 0.012271
2022-01-06 15:33:08,567 iteration 4006 : loss : 0.032608, loss_ce: 0.011415
2022-01-06 15:33:09,759 iteration 4007 : loss : 0.031332, loss_ce: 0.010547
2022-01-06 15:33:10,929 iteration 4008 : loss : 0.027268, loss_ce: 0.011226
2022-01-06 15:33:12,122 iteration 4009 : loss : 0.025716, loss_ce: 0.009005
2022-01-06 15:33:13,326 iteration 4010 : loss : 0.030991, loss_ce: 0.012174
2022-01-06 15:33:14,497 iteration 4011 : loss : 0.030657, loss_ce: 0.010569
2022-01-06 15:33:15,694 iteration 4012 : loss : 0.030687, loss_ce: 0.010871
 59%|███████████████▉           | 236/400 [1:28:16<1:00:43, 22.22s/it]2022-01-06 15:33:16,801 iteration 4013 : loss : 0.023565, loss_ce: 0.011189
2022-01-06 15:33:18,058 iteration 4014 : loss : 0.027723, loss_ce: 0.011841
2022-01-06 15:33:19,189 iteration 4015 : loss : 0.029878, loss_ce: 0.013819
2022-01-06 15:33:20,449 iteration 4016 : loss : 0.030355, loss_ce: 0.010737
2022-01-06 15:33:21,652 iteration 4017 : loss : 0.037576, loss_ce: 0.010727
2022-01-06 15:33:22,751 iteration 4018 : loss : 0.030334, loss_ce: 0.010109
2022-01-06 15:33:23,959 iteration 4019 : loss : 0.045697, loss_ce: 0.015231
2022-01-06 15:33:25,127 iteration 4020 : loss : 0.025895, loss_ce: 0.010660
2022-01-06 15:33:26,347 iteration 4021 : loss : 0.046099, loss_ce: 0.013782
2022-01-06 15:33:27,632 iteration 4022 : loss : 0.040479, loss_ce: 0.014549
2022-01-06 15:33:28,755 iteration 4023 : loss : 0.025132, loss_ce: 0.011885
2022-01-06 15:33:30,008 iteration 4024 : loss : 0.027752, loss_ce: 0.007827
2022-01-06 15:33:31,216 iteration 4025 : loss : 0.034219, loss_ce: 0.011709
2022-01-06 15:33:32,481 iteration 4026 : loss : 0.036746, loss_ce: 0.014463
2022-01-06 15:33:33,590 iteration 4027 : loss : 0.024699, loss_ce: 0.007956
2022-01-06 15:33:34,950 iteration 4028 : loss : 0.031373, loss_ce: 0.014782
2022-01-06 15:33:36,273 iteration 4029 : loss : 0.037212, loss_ce: 0.014634
 59%|█████████████████▏           | 237/400 [1:28:36<59:01, 21.73s/it]2022-01-06 15:33:37,524 iteration 4030 : loss : 0.033448, loss_ce: 0.012869
2022-01-06 15:33:38,727 iteration 4031 : loss : 0.030877, loss_ce: 0.015422
2022-01-06 15:33:39,863 iteration 4032 : loss : 0.019269, loss_ce: 0.006068
2022-01-06 15:33:41,029 iteration 4033 : loss : 0.026224, loss_ce: 0.010111
2022-01-06 15:33:42,287 iteration 4034 : loss : 0.027431, loss_ce: 0.011539
2022-01-06 15:33:43,533 iteration 4035 : loss : 0.037192, loss_ce: 0.013145
2022-01-06 15:33:44,737 iteration 4036 : loss : 0.028155, loss_ce: 0.012421
2022-01-06 15:33:45,873 iteration 4037 : loss : 0.026652, loss_ce: 0.013499
2022-01-06 15:33:47,063 iteration 4038 : loss : 0.042871, loss_ce: 0.016840
2022-01-06 15:33:48,268 iteration 4039 : loss : 0.049377, loss_ce: 0.015006
2022-01-06 15:33:49,439 iteration 4040 : loss : 0.036295, loss_ce: 0.011332
2022-01-06 15:33:50,683 iteration 4041 : loss : 0.032705, loss_ce: 0.012337
2022-01-06 15:33:51,904 iteration 4042 : loss : 0.038682, loss_ce: 0.013862
2022-01-06 15:33:53,103 iteration 4043 : loss : 0.032121, loss_ce: 0.013449
2022-01-06 15:33:54,284 iteration 4044 : loss : 0.027701, loss_ce: 0.011585
2022-01-06 15:33:55,486 iteration 4045 : loss : 0.025599, loss_ce: 0.007595
2022-01-06 15:33:56,732 iteration 4046 : loss : 0.032648, loss_ce: 0.011134
 60%|█████████████████▎           | 238/400 [1:28:57<57:37, 21.34s/it]2022-01-06 15:33:58,008 iteration 4047 : loss : 0.023692, loss_ce: 0.009804
2022-01-06 15:33:59,184 iteration 4048 : loss : 0.033700, loss_ce: 0.011176
2022-01-06 15:34:00,358 iteration 4049 : loss : 0.025310, loss_ce: 0.007424
2022-01-06 15:34:01,468 iteration 4050 : loss : 0.039144, loss_ce: 0.012993
2022-01-06 15:34:02,654 iteration 4051 : loss : 0.031041, loss_ce: 0.013289
2022-01-06 15:34:03,781 iteration 4052 : loss : 0.027030, loss_ce: 0.010358
2022-01-06 15:34:04,991 iteration 4053 : loss : 0.048166, loss_ce: 0.011827
2022-01-06 15:34:06,218 iteration 4054 : loss : 0.035979, loss_ce: 0.013897
2022-01-06 15:34:07,415 iteration 4055 : loss : 0.034160, loss_ce: 0.014743
2022-01-06 15:34:08,597 iteration 4056 : loss : 0.039185, loss_ce: 0.019099
2022-01-06 15:34:09,706 iteration 4057 : loss : 0.043025, loss_ce: 0.011194
2022-01-06 15:34:10,846 iteration 4058 : loss : 0.021066, loss_ce: 0.009044
2022-01-06 15:34:12,067 iteration 4059 : loss : 0.044943, loss_ce: 0.016033
2022-01-06 15:34:13,196 iteration 4060 : loss : 0.028417, loss_ce: 0.010593
2022-01-06 15:34:14,271 iteration 4061 : loss : 0.027628, loss_ce: 0.011443
2022-01-06 15:34:15,421 iteration 4062 : loss : 0.024428, loss_ce: 0.011088
2022-01-06 15:34:16,668 iteration 4063 : loss : 0.030831, loss_ce: 0.011771
 60%|█████████████████▎           | 239/400 [1:29:16<56:08, 20.92s/it]2022-01-06 15:34:17,906 iteration 4064 : loss : 0.025063, loss_ce: 0.008808
2022-01-06 15:34:19,039 iteration 4065 : loss : 0.033698, loss_ce: 0.009396
2022-01-06 15:34:20,213 iteration 4066 : loss : 0.027498, loss_ce: 0.009897
2022-01-06 15:34:21,383 iteration 4067 : loss : 0.029622, loss_ce: 0.009494
2022-01-06 15:34:22,597 iteration 4068 : loss : 0.039027, loss_ce: 0.016364
2022-01-06 15:34:23,809 iteration 4069 : loss : 0.047126, loss_ce: 0.021671
2022-01-06 15:34:24,940 iteration 4070 : loss : 0.036167, loss_ce: 0.010690
2022-01-06 15:34:26,120 iteration 4071 : loss : 0.033766, loss_ce: 0.010579
2022-01-06 15:34:27,305 iteration 4072 : loss : 0.030904, loss_ce: 0.012405
2022-01-06 15:34:28,525 iteration 4073 : loss : 0.029317, loss_ce: 0.012291
2022-01-06 15:34:29,711 iteration 4074 : loss : 0.033171, loss_ce: 0.013098
2022-01-06 15:34:30,997 iteration 4075 : loss : 0.047857, loss_ce: 0.016500
2022-01-06 15:34:32,196 iteration 4076 : loss : 0.023434, loss_ce: 0.009514
2022-01-06 15:34:33,442 iteration 4077 : loss : 0.027366, loss_ce: 0.010775
2022-01-06 15:34:34,690 iteration 4078 : loss : 0.033555, loss_ce: 0.011141
2022-01-06 15:34:35,824 iteration 4079 : loss : 0.025784, loss_ce: 0.009795
2022-01-06 15:34:35,824 Training Data Eval:
2022-01-06 15:34:41,640   Average segmentation loss on training set: 0.0257
2022-01-06 15:34:41,641 Validation Data Eval:
2022-01-06 15:34:43,656   Average segmentation loss on validation set: 0.0906
2022-01-06 15:34:44,930 iteration 4080 : loss : 0.038666, loss_ce: 0.015596
 60%|████████████████▏          | 240/400 [1:29:45<1:01:40, 23.13s/it]2022-01-06 15:34:46,135 iteration 4081 : loss : 0.029612, loss_ce: 0.012267
2022-01-06 15:34:47,299 iteration 4082 : loss : 0.031157, loss_ce: 0.013005
2022-01-06 15:34:48,451 iteration 4083 : loss : 0.028929, loss_ce: 0.013066
2022-01-06 15:34:49,669 iteration 4084 : loss : 0.042524, loss_ce: 0.019124
2022-01-06 15:34:50,940 iteration 4085 : loss : 0.043053, loss_ce: 0.014773
2022-01-06 15:34:52,176 iteration 4086 : loss : 0.043240, loss_ce: 0.019530
2022-01-06 15:34:53,325 iteration 4087 : loss : 0.033383, loss_ce: 0.009523
2022-01-06 15:34:54,492 iteration 4088 : loss : 0.032810, loss_ce: 0.012785
2022-01-06 15:34:55,717 iteration 4089 : loss : 0.034059, loss_ce: 0.010306
2022-01-06 15:34:56,945 iteration 4090 : loss : 0.021287, loss_ce: 0.009053
2022-01-06 15:34:58,105 iteration 4091 : loss : 0.047286, loss_ce: 0.017397
2022-01-06 15:34:59,285 iteration 4092 : loss : 0.031195, loss_ce: 0.012987
2022-01-06 15:35:00,465 iteration 4093 : loss : 0.024925, loss_ce: 0.010390
2022-01-06 15:35:01,614 iteration 4094 : loss : 0.027176, loss_ce: 0.009431
2022-01-06 15:35:02,784 iteration 4095 : loss : 0.030096, loss_ce: 0.011600
2022-01-06 15:35:03,976 iteration 4096 : loss : 0.029228, loss_ce: 0.011508
2022-01-06 15:35:05,191 iteration 4097 : loss : 0.051970, loss_ce: 0.025088
 60%|█████████████████▍           | 241/400 [1:30:05<58:59, 22.26s/it]2022-01-06 15:35:06,491 iteration 4098 : loss : 0.034385, loss_ce: 0.015256
2022-01-06 15:35:07,643 iteration 4099 : loss : 0.052770, loss_ce: 0.014519
2022-01-06 15:35:08,769 iteration 4100 : loss : 0.029215, loss_ce: 0.012812
2022-01-06 15:35:09,922 iteration 4101 : loss : 0.027390, loss_ce: 0.011082
2022-01-06 15:35:11,136 iteration 4102 : loss : 0.030056, loss_ce: 0.012170
2022-01-06 15:35:12,304 iteration 4103 : loss : 0.025365, loss_ce: 0.010062
2022-01-06 15:35:13,449 iteration 4104 : loss : 0.025167, loss_ce: 0.009887
2022-01-06 15:35:14,671 iteration 4105 : loss : 0.034308, loss_ce: 0.013611
2022-01-06 15:35:15,821 iteration 4106 : loss : 0.041143, loss_ce: 0.006877
2022-01-06 15:35:17,150 iteration 4107 : loss : 0.042163, loss_ce: 0.028573
2022-01-06 15:35:18,394 iteration 4108 : loss : 0.040926, loss_ce: 0.015964
2022-01-06 15:35:19,520 iteration 4109 : loss : 0.027142, loss_ce: 0.006714
2022-01-06 15:35:20,734 iteration 4110 : loss : 0.031325, loss_ce: 0.014606
2022-01-06 15:35:22,010 iteration 4111 : loss : 0.033440, loss_ce: 0.013246
2022-01-06 15:35:23,172 iteration 4112 : loss : 0.041235, loss_ce: 0.015717
2022-01-06 15:35:24,387 iteration 4113 : loss : 0.030081, loss_ce: 0.012041
2022-01-06 15:35:25,672 iteration 4114 : loss : 0.028083, loss_ce: 0.010506
 60%|█████████████████▌           | 242/400 [1:30:25<57:13, 21.73s/it]2022-01-06 15:35:26,883 iteration 4115 : loss : 0.038178, loss_ce: 0.010804
2022-01-06 15:35:28,006 iteration 4116 : loss : 0.024904, loss_ce: 0.010085
2022-01-06 15:35:29,354 iteration 4117 : loss : 0.039696, loss_ce: 0.012677
2022-01-06 15:35:30,665 iteration 4118 : loss : 0.035582, loss_ce: 0.019898
2022-01-06 15:35:31,875 iteration 4119 : loss : 0.031290, loss_ce: 0.015914
2022-01-06 15:35:33,071 iteration 4120 : loss : 0.076098, loss_ce: 0.016041
2022-01-06 15:35:34,274 iteration 4121 : loss : 0.027006, loss_ce: 0.012349
2022-01-06 15:35:35,435 iteration 4122 : loss : 0.029554, loss_ce: 0.010499
2022-01-06 15:35:36,715 iteration 4123 : loss : 0.042039, loss_ce: 0.015046
2022-01-06 15:35:37,842 iteration 4124 : loss : 0.034857, loss_ce: 0.012977
2022-01-06 15:35:39,110 iteration 4125 : loss : 0.037793, loss_ce: 0.010181
2022-01-06 15:35:40,309 iteration 4126 : loss : 0.029922, loss_ce: 0.013645
2022-01-06 15:35:41,536 iteration 4127 : loss : 0.037803, loss_ce: 0.015752
2022-01-06 15:35:42,680 iteration 4128 : loss : 0.031497, loss_ce: 0.015122
2022-01-06 15:35:43,942 iteration 4129 : loss : 0.034689, loss_ce: 0.013064
2022-01-06 15:35:45,128 iteration 4130 : loss : 0.025925, loss_ce: 0.013767
2022-01-06 15:35:46,233 iteration 4131 : loss : 0.027010, loss_ce: 0.007915
 61%|█████████████████▌           | 243/400 [1:30:46<55:56, 21.38s/it]2022-01-06 15:35:47,550 iteration 4132 : loss : 0.037058, loss_ce: 0.014782
2022-01-06 15:35:48,801 iteration 4133 : loss : 0.042566, loss_ce: 0.017730
2022-01-06 15:35:49,898 iteration 4134 : loss : 0.026344, loss_ce: 0.011304
2022-01-06 15:35:51,146 iteration 4135 : loss : 0.030449, loss_ce: 0.011832
2022-01-06 15:35:52,332 iteration 4136 : loss : 0.031004, loss_ce: 0.015053
2022-01-06 15:35:53,491 iteration 4137 : loss : 0.029581, loss_ce: 0.017713
2022-01-06 15:35:54,643 iteration 4138 : loss : 0.024769, loss_ce: 0.011320
2022-01-06 15:35:55,873 iteration 4139 : loss : 0.039236, loss_ce: 0.011341
2022-01-06 15:35:57,122 iteration 4140 : loss : 0.050368, loss_ce: 0.015579
2022-01-06 15:35:58,306 iteration 4141 : loss : 0.041375, loss_ce: 0.009605
2022-01-06 15:35:59,517 iteration 4142 : loss : 0.046155, loss_ce: 0.015351
2022-01-06 15:36:00,749 iteration 4143 : loss : 0.036078, loss_ce: 0.010075
2022-01-06 15:36:01,906 iteration 4144 : loss : 0.040435, loss_ce: 0.014465
2022-01-06 15:36:03,066 iteration 4145 : loss : 0.021301, loss_ce: 0.005904
2022-01-06 15:36:04,254 iteration 4146 : loss : 0.050763, loss_ce: 0.015934
2022-01-06 15:36:05,410 iteration 4147 : loss : 0.033881, loss_ce: 0.013403
2022-01-06 15:36:06,492 iteration 4148 : loss : 0.025111, loss_ce: 0.009696
 61%|█████████████████▋           | 244/400 [1:31:06<54:42, 21.04s/it]2022-01-06 15:36:07,875 iteration 4149 : loss : 0.045918, loss_ce: 0.016910
2022-01-06 15:36:08,963 iteration 4150 : loss : 0.025806, loss_ce: 0.007473
2022-01-06 15:36:10,171 iteration 4151 : loss : 0.026475, loss_ce: 0.012864
2022-01-06 15:36:11,244 iteration 4152 : loss : 0.024733, loss_ce: 0.011427
2022-01-06 15:36:12,460 iteration 4153 : loss : 0.036318, loss_ce: 0.010376
2022-01-06 15:36:13,662 iteration 4154 : loss : 0.044402, loss_ce: 0.019174
2022-01-06 15:36:14,915 iteration 4155 : loss : 0.029451, loss_ce: 0.010841
2022-01-06 15:36:15,981 iteration 4156 : loss : 0.018911, loss_ce: 0.007409
2022-01-06 15:36:17,194 iteration 4157 : loss : 0.038501, loss_ce: 0.012459
2022-01-06 15:36:18,372 iteration 4158 : loss : 0.035394, loss_ce: 0.013485
2022-01-06 15:36:19,583 iteration 4159 : loss : 0.024134, loss_ce: 0.010376
2022-01-06 15:36:20,759 iteration 4160 : loss : 0.030459, loss_ce: 0.013063
2022-01-06 15:36:21,942 iteration 4161 : loss : 0.025081, loss_ce: 0.010762
2022-01-06 15:36:23,107 iteration 4162 : loss : 0.027722, loss_ce: 0.011235
2022-01-06 15:36:24,341 iteration 4163 : loss : 0.045457, loss_ce: 0.013782
2022-01-06 15:36:25,573 iteration 4164 : loss : 0.033473, loss_ce: 0.014423
2022-01-06 15:36:25,573 Training Data Eval:
2022-01-06 15:36:31,423   Average segmentation loss on training set: 0.0267
2022-01-06 15:36:31,423 Validation Data Eval:
2022-01-06 15:36:33,440   Average segmentation loss on validation set: 0.0646
2022-01-06 15:36:40,627 Found new lowest validation loss at iteration 4164! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_MLP_best_val_loss_seed100.pth
2022-01-06 15:36:41,818 iteration 4165 : loss : 0.041412, loss_ce: 0.017998
 61%|████████████████▌          | 245/400 [1:31:42<1:05:26, 25.33s/it]2022-01-06 15:36:42,960 iteration 4166 : loss : 0.024267, loss_ce: 0.010697
2022-01-06 15:36:44,133 iteration 4167 : loss : 0.040118, loss_ce: 0.010469
2022-01-06 15:36:45,330 iteration 4168 : loss : 0.035525, loss_ce: 0.015581
2022-01-06 15:36:46,502 iteration 4169 : loss : 0.044585, loss_ce: 0.017718
2022-01-06 15:36:47,698 iteration 4170 : loss : 0.041838, loss_ce: 0.017426
2022-01-06 15:36:48,804 iteration 4171 : loss : 0.027175, loss_ce: 0.012006
2022-01-06 15:36:49,938 iteration 4172 : loss : 0.037973, loss_ce: 0.017800
2022-01-06 15:36:50,986 iteration 4173 : loss : 0.028598, loss_ce: 0.009437
2022-01-06 15:36:52,183 iteration 4174 : loss : 0.035100, loss_ce: 0.012921
2022-01-06 15:36:53,353 iteration 4175 : loss : 0.036107, loss_ce: 0.011233
2022-01-06 15:36:54,553 iteration 4176 : loss : 0.048641, loss_ce: 0.014966
2022-01-06 15:36:55,628 iteration 4177 : loss : 0.027697, loss_ce: 0.012429
2022-01-06 15:36:56,764 iteration 4178 : loss : 0.023352, loss_ce: 0.009831
2022-01-06 15:36:57,960 iteration 4179 : loss : 0.030222, loss_ce: 0.011041
2022-01-06 15:36:59,205 iteration 4180 : loss : 0.032852, loss_ce: 0.010069
2022-01-06 15:37:00,385 iteration 4181 : loss : 0.033136, loss_ce: 0.014316
2022-01-06 15:37:01,637 iteration 4182 : loss : 0.031553, loss_ce: 0.014401
 62%|████████████████▌          | 246/400 [1:32:01<1:00:45, 23.67s/it]2022-01-06 15:37:02,879 iteration 4183 : loss : 0.036133, loss_ce: 0.010855
2022-01-06 15:37:04,050 iteration 4184 : loss : 0.030630, loss_ce: 0.010661
2022-01-06 15:37:05,278 iteration 4185 : loss : 0.034472, loss_ce: 0.011296
2022-01-06 15:37:06,436 iteration 4186 : loss : 0.030440, loss_ce: 0.011605
2022-01-06 15:37:07,568 iteration 4187 : loss : 0.027255, loss_ce: 0.011271
2022-01-06 15:37:08,730 iteration 4188 : loss : 0.033257, loss_ce: 0.013460
2022-01-06 15:37:09,833 iteration 4189 : loss : 0.035352, loss_ce: 0.011869
2022-01-06 15:37:11,128 iteration 4190 : loss : 0.044625, loss_ce: 0.019045
2022-01-06 15:37:12,368 iteration 4191 : loss : 0.026593, loss_ce: 0.007155
2022-01-06 15:37:13,487 iteration 4192 : loss : 0.026392, loss_ce: 0.014393
2022-01-06 15:37:14,580 iteration 4193 : loss : 0.021958, loss_ce: 0.007110
2022-01-06 15:37:15,768 iteration 4194 : loss : 0.031675, loss_ce: 0.010936
2022-01-06 15:37:16,905 iteration 4195 : loss : 0.024825, loss_ce: 0.011252
2022-01-06 15:37:18,037 iteration 4196 : loss : 0.026304, loss_ce: 0.010344
2022-01-06 15:37:19,173 iteration 4197 : loss : 0.028102, loss_ce: 0.011218
2022-01-06 15:37:20,291 iteration 4198 : loss : 0.054292, loss_ce: 0.012451
2022-01-06 15:37:21,526 iteration 4199 : loss : 0.027436, loss_ce: 0.012348
 62%|█████████████████▉           | 247/400 [1:32:21<57:28, 22.54s/it]2022-01-06 15:37:22,776 iteration 4200 : loss : 0.026837, loss_ce: 0.008903
2022-01-06 15:37:24,023 iteration 4201 : loss : 0.043692, loss_ce: 0.022496
2022-01-06 15:37:25,303 iteration 4202 : loss : 0.024865, loss_ce: 0.011641
2022-01-06 15:37:26,581 iteration 4203 : loss : 0.036704, loss_ce: 0.013901
2022-01-06 15:37:27,803 iteration 4204 : loss : 0.021251, loss_ce: 0.009061
2022-01-06 15:37:28,814 iteration 4205 : loss : 0.019247, loss_ce: 0.007026
2022-01-06 15:37:29,970 iteration 4206 : loss : 0.027176, loss_ce: 0.008041
2022-01-06 15:37:31,126 iteration 4207 : loss : 0.020898, loss_ce: 0.009866
2022-01-06 15:37:32,354 iteration 4208 : loss : 0.033928, loss_ce: 0.012215
2022-01-06 15:37:33,511 iteration 4209 : loss : 0.028562, loss_ce: 0.008420
2022-01-06 15:37:34,668 iteration 4210 : loss : 0.026645, loss_ce: 0.010218
2022-01-06 15:37:35,879 iteration 4211 : loss : 0.026862, loss_ce: 0.011890
2022-01-06 15:37:37,034 iteration 4212 : loss : 0.019954, loss_ce: 0.008236
2022-01-06 15:37:38,295 iteration 4213 : loss : 0.026058, loss_ce: 0.008740
2022-01-06 15:37:39,533 iteration 4214 : loss : 0.042307, loss_ce: 0.017350
2022-01-06 15:37:40,740 iteration 4215 : loss : 0.039865, loss_ce: 0.010396
2022-01-06 15:37:41,910 iteration 4216 : loss : 0.032540, loss_ce: 0.013398
 62%|█████████████████▉           | 248/400 [1:32:42<55:27, 21.89s/it]2022-01-06 15:37:43,242 iteration 4217 : loss : 0.073556, loss_ce: 0.019502
2022-01-06 15:37:44,516 iteration 4218 : loss : 0.025248, loss_ce: 0.012546
2022-01-06 15:37:45,725 iteration 4219 : loss : 0.029068, loss_ce: 0.011363
2022-01-06 15:37:46,909 iteration 4220 : loss : 0.030106, loss_ce: 0.010605
2022-01-06 15:37:48,128 iteration 4221 : loss : 0.034334, loss_ce: 0.010964
2022-01-06 15:37:49,364 iteration 4222 : loss : 0.026867, loss_ce: 0.011830
2022-01-06 15:37:50,687 iteration 4223 : loss : 0.058903, loss_ce: 0.015283
2022-01-06 15:37:51,860 iteration 4224 : loss : 0.032241, loss_ce: 0.010636
2022-01-06 15:37:53,105 iteration 4225 : loss : 0.025909, loss_ce: 0.012133
2022-01-06 15:37:54,400 iteration 4226 : loss : 0.025279, loss_ce: 0.009384
2022-01-06 15:37:55,564 iteration 4227 : loss : 0.028018, loss_ce: 0.008514
2022-01-06 15:37:56,790 iteration 4228 : loss : 0.034663, loss_ce: 0.016789
2022-01-06 15:37:57,922 iteration 4229 : loss : 0.025829, loss_ce: 0.010260
2022-01-06 15:37:59,160 iteration 4230 : loss : 0.028481, loss_ce: 0.010101
2022-01-06 15:38:00,367 iteration 4231 : loss : 0.027483, loss_ce: 0.009283
2022-01-06 15:38:01,593 iteration 4232 : loss : 0.060234, loss_ce: 0.032358
2022-01-06 15:38:02,700 iteration 4233 : loss : 0.028511, loss_ce: 0.010314
 62%|██████████████████           | 249/400 [1:33:03<54:16, 21.56s/it]2022-01-06 15:38:04,002 iteration 4234 : loss : 0.034748, loss_ce: 0.013579
2022-01-06 15:38:05,133 iteration 4235 : loss : 0.026577, loss_ce: 0.010238
2022-01-06 15:38:06,377 iteration 4236 : loss : 0.044137, loss_ce: 0.023586
2022-01-06 15:38:07,555 iteration 4237 : loss : 0.039134, loss_ce: 0.019017
2022-01-06 15:38:08,825 iteration 4238 : loss : 0.039004, loss_ce: 0.012845
2022-01-06 15:38:09,933 iteration 4239 : loss : 0.036913, loss_ce: 0.010272
2022-01-06 15:38:11,225 iteration 4240 : loss : 0.040910, loss_ce: 0.014701
2022-01-06 15:38:12,393 iteration 4241 : loss : 0.059335, loss_ce: 0.030350
2022-01-06 15:38:13,538 iteration 4242 : loss : 0.037628, loss_ce: 0.013656
2022-01-06 15:38:14,714 iteration 4243 : loss : 0.039230, loss_ce: 0.017870
2022-01-06 15:38:15,953 iteration 4244 : loss : 0.036784, loss_ce: 0.014671
2022-01-06 15:38:17,143 iteration 4245 : loss : 0.079461, loss_ce: 0.027501
2022-01-06 15:38:18,290 iteration 4246 : loss : 0.030309, loss_ce: 0.013687
2022-01-06 15:38:19,441 iteration 4247 : loss : 0.030784, loss_ce: 0.008150
2022-01-06 15:38:20,658 iteration 4248 : loss : 0.030007, loss_ce: 0.010780
2022-01-06 15:38:21,842 iteration 4249 : loss : 0.036106, loss_ce: 0.017475
2022-01-06 15:38:21,842 Training Data Eval:
2022-01-06 15:38:27,696   Average segmentation loss on training set: 0.0390
2022-01-06 15:38:27,696 Validation Data Eval:
2022-01-06 15:38:29,689   Average segmentation loss on validation set: 0.1054
2022-01-06 15:38:30,913 iteration 4250 : loss : 0.021466, loss_ce: 0.007950
 62%|██████████████████▏          | 250/400 [1:33:31<58:53, 23.56s/it]2022-01-06 15:38:32,221 iteration 4251 : loss : 0.028568, loss_ce: 0.011259
2022-01-06 15:38:33,444 iteration 4252 : loss : 0.030253, loss_ce: 0.012566
2022-01-06 15:38:34,691 iteration 4253 : loss : 0.037752, loss_ce: 0.017464
2022-01-06 15:38:35,985 iteration 4254 : loss : 0.036373, loss_ce: 0.015318
2022-01-06 15:38:37,066 iteration 4255 : loss : 0.022681, loss_ce: 0.008628
2022-01-06 15:38:38,240 iteration 4256 : loss : 0.043287, loss_ce: 0.012083
2022-01-06 15:38:39,507 iteration 4257 : loss : 0.041222, loss_ce: 0.013361
2022-01-06 15:38:40,645 iteration 4258 : loss : 0.029380, loss_ce: 0.011870
2022-01-06 15:38:41,843 iteration 4259 : loss : 0.030051, loss_ce: 0.010951
2022-01-06 15:38:43,047 iteration 4260 : loss : 0.025006, loss_ce: 0.008941
2022-01-06 15:38:44,261 iteration 4261 : loss : 0.024546, loss_ce: 0.009551
2022-01-06 15:38:45,555 iteration 4262 : loss : 0.024698, loss_ce: 0.011593
2022-01-06 15:38:46,738 iteration 4263 : loss : 0.027946, loss_ce: 0.012138
2022-01-06 15:38:47,850 iteration 4264 : loss : 0.028950, loss_ce: 0.009656
2022-01-06 15:38:49,016 iteration 4265 : loss : 0.047440, loss_ce: 0.016604
2022-01-06 15:38:50,204 iteration 4266 : loss : 0.036380, loss_ce: 0.013980
2022-01-06 15:38:51,391 iteration 4267 : loss : 0.026151, loss_ce: 0.011861
 63%|██████████████████▏          | 251/400 [1:33:51<56:12, 22.63s/it]2022-01-06 15:38:52,631 iteration 4268 : loss : 0.027951, loss_ce: 0.011728
2022-01-06 15:38:53,752 iteration 4269 : loss : 0.031007, loss_ce: 0.011496
2022-01-06 15:38:54,901 iteration 4270 : loss : 0.038111, loss_ce: 0.006223
2022-01-06 15:38:56,034 iteration 4271 : loss : 0.026408, loss_ce: 0.012214
2022-01-06 15:38:57,274 iteration 4272 : loss : 0.029410, loss_ce: 0.009806
2022-01-06 15:38:58,479 iteration 4273 : loss : 0.029658, loss_ce: 0.012978
2022-01-06 15:38:59,705 iteration 4274 : loss : 0.037497, loss_ce: 0.017001
2022-01-06 15:39:00,749 iteration 4275 : loss : 0.026064, loss_ce: 0.009826
2022-01-06 15:39:01,881 iteration 4276 : loss : 0.032003, loss_ce: 0.011514
2022-01-06 15:39:03,027 iteration 4277 : loss : 0.019866, loss_ce: 0.008272
2022-01-06 15:39:04,184 iteration 4278 : loss : 0.035115, loss_ce: 0.009473
2022-01-06 15:39:05,292 iteration 4279 : loss : 0.025037, loss_ce: 0.007910
2022-01-06 15:39:06,418 iteration 4280 : loss : 0.047554, loss_ce: 0.013256
2022-01-06 15:39:07,746 iteration 4281 : loss : 0.027686, loss_ce: 0.011746
2022-01-06 15:39:08,933 iteration 4282 : loss : 0.032079, loss_ce: 0.013364
2022-01-06 15:39:10,008 iteration 4283 : loss : 0.024210, loss_ce: 0.010051
2022-01-06 15:39:11,172 iteration 4284 : loss : 0.030309, loss_ce: 0.014200
 63%|██████████████████▎          | 252/400 [1:34:11<53:43, 21.78s/it]2022-01-06 15:39:12,475 iteration 4285 : loss : 0.037385, loss_ce: 0.016709
2022-01-06 15:39:13,682 iteration 4286 : loss : 0.032238, loss_ce: 0.009642
2022-01-06 15:39:14,906 iteration 4287 : loss : 0.055395, loss_ce: 0.024662
2022-01-06 15:39:16,069 iteration 4288 : loss : 0.030359, loss_ce: 0.009740
2022-01-06 15:39:17,269 iteration 4289 : loss : 0.033258, loss_ce: 0.009719
2022-01-06 15:39:18,513 iteration 4290 : loss : 0.038750, loss_ce: 0.013453
2022-01-06 15:39:19,628 iteration 4291 : loss : 0.023432, loss_ce: 0.008454
2022-01-06 15:39:20,894 iteration 4292 : loss : 0.030362, loss_ce: 0.011364
2022-01-06 15:39:22,095 iteration 4293 : loss : 0.043426, loss_ce: 0.019948
2022-01-06 15:39:23,280 iteration 4294 : loss : 0.036675, loss_ce: 0.017642
2022-01-06 15:39:24,516 iteration 4295 : loss : 0.028713, loss_ce: 0.012143
2022-01-06 15:39:25,621 iteration 4296 : loss : 0.027510, loss_ce: 0.011730
2022-01-06 15:39:26,824 iteration 4297 : loss : 0.032201, loss_ce: 0.010703
2022-01-06 15:39:27,969 iteration 4298 : loss : 0.032946, loss_ce: 0.011428
2022-01-06 15:39:29,256 iteration 4299 : loss : 0.042153, loss_ce: 0.015693
2022-01-06 15:39:30,503 iteration 4300 : loss : 0.038444, loss_ce: 0.011375
2022-01-06 15:39:31,669 iteration 4301 : loss : 0.027055, loss_ce: 0.010159
 63%|██████████████████▎          | 253/400 [1:34:31<52:24, 21.39s/it]2022-01-06 15:39:32,931 iteration 4302 : loss : 0.031826, loss_ce: 0.013439
2022-01-06 15:39:34,161 iteration 4303 : loss : 0.032690, loss_ce: 0.012620
2022-01-06 15:39:35,264 iteration 4304 : loss : 0.026618, loss_ce: 0.009934
2022-01-06 15:39:36,484 iteration 4305 : loss : 0.033374, loss_ce: 0.009338
2022-01-06 15:39:37,648 iteration 4306 : loss : 0.042133, loss_ce: 0.013261
2022-01-06 15:39:38,806 iteration 4307 : loss : 0.028732, loss_ce: 0.010620
2022-01-06 15:39:40,045 iteration 4308 : loss : 0.037007, loss_ce: 0.012528
2022-01-06 15:39:41,183 iteration 4309 : loss : 0.032011, loss_ce: 0.011042
2022-01-06 15:39:42,387 iteration 4310 : loss : 0.045445, loss_ce: 0.027934
2022-01-06 15:39:43,572 iteration 4311 : loss : 0.038069, loss_ce: 0.020485
2022-01-06 15:39:44,869 iteration 4312 : loss : 0.047150, loss_ce: 0.018756
2022-01-06 15:39:46,044 iteration 4313 : loss : 0.029535, loss_ce: 0.011287
2022-01-06 15:39:47,213 iteration 4314 : loss : 0.039713, loss_ce: 0.014398
2022-01-06 15:39:48,362 iteration 4315 : loss : 0.028094, loss_ce: 0.012117
2022-01-06 15:39:49,622 iteration 4316 : loss : 0.027532, loss_ce: 0.010174
2022-01-06 15:39:50,796 iteration 4317 : loss : 0.033207, loss_ce: 0.013585
2022-01-06 15:39:51,969 iteration 4318 : loss : 0.027989, loss_ce: 0.014155
 64%|██████████████████▍          | 254/400 [1:34:52<51:15, 21.06s/it]2022-01-06 15:39:53,287 iteration 4319 : loss : 0.039466, loss_ce: 0.013020
2022-01-06 15:39:54,541 iteration 4320 : loss : 0.039732, loss_ce: 0.015947
2022-01-06 15:39:55,804 iteration 4321 : loss : 0.025202, loss_ce: 0.010805
2022-01-06 15:39:57,009 iteration 4322 : loss : 0.032405, loss_ce: 0.010435
2022-01-06 15:39:58,159 iteration 4323 : loss : 0.021642, loss_ce: 0.006450
2022-01-06 15:39:59,374 iteration 4324 : loss : 0.031905, loss_ce: 0.012008
2022-01-06 15:40:00,684 iteration 4325 : loss : 0.039725, loss_ce: 0.009036
2022-01-06 15:40:01,872 iteration 4326 : loss : 0.033438, loss_ce: 0.018216
2022-01-06 15:40:03,020 iteration 4327 : loss : 0.028344, loss_ce: 0.011096
2022-01-06 15:40:04,191 iteration 4328 : loss : 0.041514, loss_ce: 0.015460
2022-01-06 15:40:05,434 iteration 4329 : loss : 0.031638, loss_ce: 0.012238
2022-01-06 15:40:06,586 iteration 4330 : loss : 0.021856, loss_ce: 0.007529
2022-01-06 15:40:07,940 iteration 4331 : loss : 0.041910, loss_ce: 0.011366
2022-01-06 15:40:09,227 iteration 4332 : loss : 0.028889, loss_ce: 0.012246
2022-01-06 15:40:10,331 iteration 4333 : loss : 0.031659, loss_ce: 0.009897
2022-01-06 15:40:11,472 iteration 4334 : loss : 0.022892, loss_ce: 0.009164
2022-01-06 15:40:11,472 Training Data Eval:
2022-01-06 15:40:17,327   Average segmentation loss on training set: 0.0354
2022-01-06 15:40:17,327 Validation Data Eval:
2022-01-06 15:40:19,338   Average segmentation loss on validation set: 0.1120
2022-01-06 15:40:20,614 iteration 4335 : loss : 0.031175, loss_ce: 0.016466
 64%|██████████████████▍          | 255/400 [1:35:20<56:24, 23.34s/it]2022-01-06 15:40:21,857 iteration 4336 : loss : 0.028118, loss_ce: 0.010884
2022-01-06 15:40:23,071 iteration 4337 : loss : 0.035657, loss_ce: 0.013762
2022-01-06 15:40:24,358 iteration 4338 : loss : 0.029096, loss_ce: 0.012182
2022-01-06 15:40:25,497 iteration 4339 : loss : 0.034160, loss_ce: 0.010970
2022-01-06 15:40:26,704 iteration 4340 : loss : 0.040226, loss_ce: 0.014891
2022-01-06 15:40:27,934 iteration 4341 : loss : 0.037219, loss_ce: 0.016814
2022-01-06 15:40:29,114 iteration 4342 : loss : 0.050330, loss_ce: 0.016516
2022-01-06 15:40:30,318 iteration 4343 : loss : 0.023759, loss_ce: 0.007737
2022-01-06 15:40:31,457 iteration 4344 : loss : 0.028941, loss_ce: 0.010066
2022-01-06 15:40:32,637 iteration 4345 : loss : 0.037401, loss_ce: 0.011884
2022-01-06 15:40:33,787 iteration 4346 : loss : 0.024394, loss_ce: 0.011187
2022-01-06 15:40:35,024 iteration 4347 : loss : 0.031117, loss_ce: 0.010685
2022-01-06 15:40:36,222 iteration 4348 : loss : 0.028879, loss_ce: 0.014826
2022-01-06 15:40:37,399 iteration 4349 : loss : 0.043811, loss_ce: 0.015749
2022-01-06 15:40:38,539 iteration 4350 : loss : 0.023338, loss_ce: 0.008818
2022-01-06 15:40:39,643 iteration 4351 : loss : 0.023525, loss_ce: 0.010990
2022-01-06 15:40:40,802 iteration 4352 : loss : 0.020363, loss_ce: 0.008525
 64%|██████████████████▌          | 256/400 [1:35:41<53:45, 22.40s/it]2022-01-06 15:40:42,102 iteration 4353 : loss : 0.024271, loss_ce: 0.008942
2022-01-06 15:40:43,379 iteration 4354 : loss : 0.038043, loss_ce: 0.012130
2022-01-06 15:40:44,493 iteration 4355 : loss : 0.029465, loss_ce: 0.011522
2022-01-06 15:40:45,721 iteration 4356 : loss : 0.029276, loss_ce: 0.013615
2022-01-06 15:40:46,922 iteration 4357 : loss : 0.028411, loss_ce: 0.011821
2022-01-06 15:40:48,067 iteration 4358 : loss : 0.028492, loss_ce: 0.012236
2022-01-06 15:40:49,234 iteration 4359 : loss : 0.020823, loss_ce: 0.007567
2022-01-06 15:40:50,407 iteration 4360 : loss : 0.038201, loss_ce: 0.013749
2022-01-06 15:40:51,651 iteration 4361 : loss : 0.026034, loss_ce: 0.011056
2022-01-06 15:40:52,795 iteration 4362 : loss : 0.028273, loss_ce: 0.011170
2022-01-06 15:40:53,885 iteration 4363 : loss : 0.027576, loss_ce: 0.013636
2022-01-06 15:40:55,116 iteration 4364 : loss : 0.035366, loss_ce: 0.012587
2022-01-06 15:40:56,279 iteration 4365 : loss : 0.030464, loss_ce: 0.012287
2022-01-06 15:40:57,327 iteration 4366 : loss : 0.021453, loss_ce: 0.009369
2022-01-06 15:40:58,496 iteration 4367 : loss : 0.027599, loss_ce: 0.009557
2022-01-06 15:40:59,761 iteration 4368 : loss : 0.031164, loss_ce: 0.011989
2022-01-06 15:41:00,877 iteration 4369 : loss : 0.035335, loss_ce: 0.009986
 64%|██████████████████▋          | 257/400 [1:36:01<51:42, 21.70s/it]2022-01-06 15:41:02,157 iteration 4370 : loss : 0.044810, loss_ce: 0.009187
2022-01-06 15:41:03,350 iteration 4371 : loss : 0.033191, loss_ce: 0.017708
2022-01-06 15:41:04,455 iteration 4372 : loss : 0.023797, loss_ce: 0.010351
2022-01-06 15:41:05,695 iteration 4373 : loss : 0.018284, loss_ce: 0.006804
2022-01-06 15:41:06,866 iteration 4374 : loss : 0.026268, loss_ce: 0.011925
2022-01-06 15:41:07,980 iteration 4375 : loss : 0.031924, loss_ce: 0.014772
2022-01-06 15:41:09,144 iteration 4376 : loss : 0.027369, loss_ce: 0.009387
2022-01-06 15:41:10,442 iteration 4377 : loss : 0.023840, loss_ce: 0.008217
2022-01-06 15:41:11,667 iteration 4378 : loss : 0.028337, loss_ce: 0.011524
2022-01-06 15:41:12,960 iteration 4379 : loss : 0.047072, loss_ce: 0.021093
2022-01-06 15:41:14,168 iteration 4380 : loss : 0.020363, loss_ce: 0.008176
2022-01-06 15:41:15,395 iteration 4381 : loss : 0.034004, loss_ce: 0.012863
2022-01-06 15:41:16,583 iteration 4382 : loss : 0.045214, loss_ce: 0.016318
2022-01-06 15:41:17,781 iteration 4383 : loss : 0.033733, loss_ce: 0.008672
2022-01-06 15:41:19,000 iteration 4384 : loss : 0.044100, loss_ce: 0.015682
2022-01-06 15:41:20,111 iteration 4385 : loss : 0.021722, loss_ce: 0.009620
2022-01-06 15:41:21,336 iteration 4386 : loss : 0.028369, loss_ce: 0.011045
 64%|██████████████████▋          | 258/400 [1:36:21<50:28, 21.33s/it]2022-01-06 15:41:22,588 iteration 4387 : loss : 0.024637, loss_ce: 0.009337
2022-01-06 15:41:23,822 iteration 4388 : loss : 0.030770, loss_ce: 0.014423
2022-01-06 15:41:24,948 iteration 4389 : loss : 0.022225, loss_ce: 0.009892
2022-01-06 15:41:26,068 iteration 4390 : loss : 0.027198, loss_ce: 0.010691
2022-01-06 15:41:27,256 iteration 4391 : loss : 0.025286, loss_ce: 0.010476
2022-01-06 15:41:28,481 iteration 4392 : loss : 0.028390, loss_ce: 0.013296
2022-01-06 15:41:29,570 iteration 4393 : loss : 0.024841, loss_ce: 0.008442
2022-01-06 15:41:30,844 iteration 4394 : loss : 0.028970, loss_ce: 0.012844
2022-01-06 15:41:32,071 iteration 4395 : loss : 0.024209, loss_ce: 0.008961
2022-01-06 15:41:33,330 iteration 4396 : loss : 0.029060, loss_ce: 0.011566
2022-01-06 15:41:34,495 iteration 4397 : loss : 0.040332, loss_ce: 0.013609
2022-01-06 15:41:35,654 iteration 4398 : loss : 0.042220, loss_ce: 0.011556
2022-01-06 15:41:36,840 iteration 4399 : loss : 0.025249, loss_ce: 0.009521
2022-01-06 15:41:37,991 iteration 4400 : loss : 0.023939, loss_ce: 0.009849
2022-01-06 15:41:39,168 iteration 4401 : loss : 0.033413, loss_ce: 0.019519
2022-01-06 15:41:40,296 iteration 4402 : loss : 0.023105, loss_ce: 0.007465
2022-01-06 15:41:41,491 iteration 4403 : loss : 0.033437, loss_ce: 0.010085
 65%|██████████████████▊          | 259/400 [1:36:41<49:17, 20.97s/it]2022-01-06 15:41:42,688 iteration 4404 : loss : 0.019381, loss_ce: 0.009536
2022-01-06 15:41:43,912 iteration 4405 : loss : 0.025983, loss_ce: 0.008227
2022-01-06 15:41:45,118 iteration 4406 : loss : 0.034516, loss_ce: 0.010760
2022-01-06 15:41:46,248 iteration 4407 : loss : 0.039085, loss_ce: 0.016784
2022-01-06 15:41:47,360 iteration 4408 : loss : 0.032586, loss_ce: 0.008229
2022-01-06 15:41:48,567 iteration 4409 : loss : 0.018868, loss_ce: 0.007474
2022-01-06 15:41:49,762 iteration 4410 : loss : 0.035693, loss_ce: 0.010344
2022-01-06 15:41:51,015 iteration 4411 : loss : 0.024904, loss_ce: 0.011995
2022-01-06 15:41:52,126 iteration 4412 : loss : 0.029961, loss_ce: 0.013127
2022-01-06 15:41:53,390 iteration 4413 : loss : 0.027071, loss_ce: 0.009115
2022-01-06 15:41:54,468 iteration 4414 : loss : 0.018833, loss_ce: 0.008199
2022-01-06 15:41:55,667 iteration 4415 : loss : 0.030101, loss_ce: 0.012011
2022-01-06 15:41:56,814 iteration 4416 : loss : 0.027231, loss_ce: 0.007981
2022-01-06 15:41:58,010 iteration 4417 : loss : 0.034607, loss_ce: 0.012105
2022-01-06 15:41:59,220 iteration 4418 : loss : 0.041056, loss_ce: 0.018582
2022-01-06 15:42:00,430 iteration 4419 : loss : 0.041926, loss_ce: 0.015985
2022-01-06 15:42:00,430 Training Data Eval:
2022-01-06 15:42:06,286   Average segmentation loss on training set: 0.0202
2022-01-06 15:42:06,287 Validation Data Eval:
2022-01-06 15:42:08,281   Average segmentation loss on validation set: 0.0700
2022-01-06 15:42:09,414 iteration 4420 : loss : 0.020517, loss_ce: 0.009192
 65%|██████████████████▊          | 260/400 [1:37:09<53:47, 23.06s/it]2022-01-06 15:42:10,682 iteration 4421 : loss : 0.026549, loss_ce: 0.009004
2022-01-06 15:42:11,906 iteration 4422 : loss : 0.025047, loss_ce: 0.009308
2022-01-06 15:42:13,195 iteration 4423 : loss : 0.035644, loss_ce: 0.015061
2022-01-06 15:42:14,440 iteration 4424 : loss : 0.025511, loss_ce: 0.009529
2022-01-06 15:42:15,552 iteration 4425 : loss : 0.021743, loss_ce: 0.007846
2022-01-06 15:42:16,790 iteration 4426 : loss : 0.033347, loss_ce: 0.014387
2022-01-06 15:42:18,006 iteration 4427 : loss : 0.041932, loss_ce: 0.010739
2022-01-06 15:42:19,231 iteration 4428 : loss : 0.025995, loss_ce: 0.011559
2022-01-06 15:42:20,468 iteration 4429 : loss : 0.034192, loss_ce: 0.011623
2022-01-06 15:42:21,616 iteration 4430 : loss : 0.027491, loss_ce: 0.011290
2022-01-06 15:42:22,766 iteration 4431 : loss : 0.031726, loss_ce: 0.014135
2022-01-06 15:42:23,935 iteration 4432 : loss : 0.021490, loss_ce: 0.008230
2022-01-06 15:42:25,103 iteration 4433 : loss : 0.035032, loss_ce: 0.012882
2022-01-06 15:42:26,269 iteration 4434 : loss : 0.030384, loss_ce: 0.014800
2022-01-06 15:42:27,492 iteration 4435 : loss : 0.057032, loss_ce: 0.020495
2022-01-06 15:42:28,812 iteration 4436 : loss : 0.041321, loss_ce: 0.018535
2022-01-06 15:42:29,924 iteration 4437 : loss : 0.024496, loss_ce: 0.008920
 65%|██████████████████▉          | 261/400 [1:37:30<51:38, 22.29s/it]2022-01-06 15:42:31,241 iteration 4438 : loss : 0.037241, loss_ce: 0.013451
2022-01-06 15:42:32,385 iteration 4439 : loss : 0.045983, loss_ce: 0.016139
2022-01-06 15:42:33,617 iteration 4440 : loss : 0.031880, loss_ce: 0.011849
2022-01-06 15:42:34,836 iteration 4441 : loss : 0.026833, loss_ce: 0.011667
2022-01-06 15:42:35,956 iteration 4442 : loss : 0.023485, loss_ce: 0.011420
2022-01-06 15:42:37,162 iteration 4443 : loss : 0.037009, loss_ce: 0.015728
2022-01-06 15:42:38,318 iteration 4444 : loss : 0.043508, loss_ce: 0.016278
2022-01-06 15:42:39,522 iteration 4445 : loss : 0.027887, loss_ce: 0.010998
2022-01-06 15:42:40,741 iteration 4446 : loss : 0.047276, loss_ce: 0.020594
2022-01-06 15:42:41,952 iteration 4447 : loss : 0.033443, loss_ce: 0.013095
2022-01-06 15:42:43,120 iteration 4448 : loss : 0.046822, loss_ce: 0.014372
2022-01-06 15:42:44,328 iteration 4449 : loss : 0.032897, loss_ce: 0.014720
2022-01-06 15:42:45,584 iteration 4450 : loss : 0.026485, loss_ce: 0.009373
2022-01-06 15:42:46,683 iteration 4451 : loss : 0.022777, loss_ce: 0.008124
2022-01-06 15:42:47,871 iteration 4452 : loss : 0.026998, loss_ce: 0.011002
2022-01-06 15:42:49,153 iteration 4453 : loss : 0.034549, loss_ce: 0.014483
2022-01-06 15:42:50,355 iteration 4454 : loss : 0.031775, loss_ce: 0.015290
 66%|██████████████████▉          | 262/400 [1:37:50<49:59, 21.74s/it]2022-01-06 15:42:51,679 iteration 4455 : loss : 0.045975, loss_ce: 0.017714
2022-01-06 15:42:52,854 iteration 4456 : loss : 0.040341, loss_ce: 0.018668
2022-01-06 15:42:53,934 iteration 4457 : loss : 0.022428, loss_ce: 0.011987
2022-01-06 15:42:55,161 iteration 4458 : loss : 0.037107, loss_ce: 0.017674
2022-01-06 15:42:56,285 iteration 4459 : loss : 0.025584, loss_ce: 0.009670
2022-01-06 15:42:57,470 iteration 4460 : loss : 0.021916, loss_ce: 0.007944
2022-01-06 15:42:58,584 iteration 4461 : loss : 0.029388, loss_ce: 0.013687
2022-01-06 15:42:59,711 iteration 4462 : loss : 0.022946, loss_ce: 0.008954
2022-01-06 15:43:00,843 iteration 4463 : loss : 0.023614, loss_ce: 0.009541
2022-01-06 15:43:01,966 iteration 4464 : loss : 0.032229, loss_ce: 0.014359
2022-01-06 15:43:03,188 iteration 4465 : loss : 0.033419, loss_ce: 0.015114
2022-01-06 15:43:04,402 iteration 4466 : loss : 0.036178, loss_ce: 0.014612
2022-01-06 15:43:05,583 iteration 4467 : loss : 0.026054, loss_ce: 0.009047
2022-01-06 15:43:06,734 iteration 4468 : loss : 0.028125, loss_ce: 0.009203
2022-01-06 15:43:07,839 iteration 4469 : loss : 0.030304, loss_ce: 0.009360
2022-01-06 15:43:09,051 iteration 4470 : loss : 0.034452, loss_ce: 0.013499
2022-01-06 15:43:10,276 iteration 4471 : loss : 0.025607, loss_ce: 0.008792
 66%|███████████████████          | 263/400 [1:38:10<48:23, 21.19s/it]2022-01-06 15:43:11,466 iteration 4472 : loss : 0.033792, loss_ce: 0.014795
2022-01-06 15:43:12,615 iteration 4473 : loss : 0.026187, loss_ce: 0.010568
2022-01-06 15:43:13,826 iteration 4474 : loss : 0.024609, loss_ce: 0.008043
2022-01-06 15:43:14,969 iteration 4475 : loss : 0.021534, loss_ce: 0.008377
2022-01-06 15:43:16,049 iteration 4476 : loss : 0.024701, loss_ce: 0.008203
2022-01-06 15:43:17,246 iteration 4477 : loss : 0.028307, loss_ce: 0.010439
2022-01-06 15:43:18,438 iteration 4478 : loss : 0.042601, loss_ce: 0.013214
2022-01-06 15:43:19,618 iteration 4479 : loss : 0.035209, loss_ce: 0.008019
2022-01-06 15:43:20,738 iteration 4480 : loss : 0.022853, loss_ce: 0.009236
2022-01-06 15:43:21,859 iteration 4481 : loss : 0.021906, loss_ce: 0.008959
2022-01-06 15:43:23,117 iteration 4482 : loss : 0.029415, loss_ce: 0.011297
2022-01-06 15:43:24,406 iteration 4483 : loss : 0.037570, loss_ce: 0.016926
2022-01-06 15:43:25,638 iteration 4484 : loss : 0.030048, loss_ce: 0.010129
2022-01-06 15:43:26,774 iteration 4485 : loss : 0.022242, loss_ce: 0.007137
2022-01-06 15:43:28,076 iteration 4486 : loss : 0.041364, loss_ce: 0.017146
2022-01-06 15:43:29,210 iteration 4487 : loss : 0.042811, loss_ce: 0.028032
2022-01-06 15:43:30,383 iteration 4488 : loss : 0.025453, loss_ce: 0.011147
 66%|███████████████████▏         | 264/400 [1:38:30<47:18, 20.87s/it]2022-01-06 15:43:31,678 iteration 4489 : loss : 0.038486, loss_ce: 0.015693
2022-01-06 15:43:32,874 iteration 4490 : loss : 0.021605, loss_ce: 0.008747
2022-01-06 15:43:34,155 iteration 4491 : loss : 0.032430, loss_ce: 0.011361
2022-01-06 15:43:35,359 iteration 4492 : loss : 0.036935, loss_ce: 0.013521
2022-01-06 15:43:36,614 iteration 4493 : loss : 0.065467, loss_ce: 0.022771
2022-01-06 15:43:37,688 iteration 4494 : loss : 0.026417, loss_ce: 0.011136
2022-01-06 15:43:38,905 iteration 4495 : loss : 0.028263, loss_ce: 0.012871
2022-01-06 15:43:40,131 iteration 4496 : loss : 0.030245, loss_ce: 0.008917
2022-01-06 15:43:41,239 iteration 4497 : loss : 0.024444, loss_ce: 0.007141
2022-01-06 15:43:42,517 iteration 4498 : loss : 0.031681, loss_ce: 0.013540
2022-01-06 15:43:43,852 iteration 4499 : loss : 0.035723, loss_ce: 0.016343
2022-01-06 15:43:45,013 iteration 4500 : loss : 0.025148, loss_ce: 0.010662
2022-01-06 15:43:46,140 iteration 4501 : loss : 0.031494, loss_ce: 0.011387
2022-01-06 15:43:47,272 iteration 4502 : loss : 0.022159, loss_ce: 0.008734
2022-01-06 15:43:48,584 iteration 4503 : loss : 0.028252, loss_ce: 0.011857
2022-01-06 15:43:49,761 iteration 4504 : loss : 0.029108, loss_ce: 0.009061
2022-01-06 15:43:49,761 Training Data Eval:
2022-01-06 15:43:55,663   Average segmentation loss on training set: 0.0730
2022-01-06 15:43:55,664 Validation Data Eval:
2022-01-06 15:43:57,684   Average segmentation loss on validation set: 0.2368
2022-01-06 15:43:58,804 iteration 4505 : loss : 0.040499, loss_ce: 0.027462
 66%|███████████████████▏         | 265/400 [1:38:59<52:02, 23.13s/it]2022-01-06 15:44:00,125 iteration 4506 : loss : 0.038909, loss_ce: 0.011548
2022-01-06 15:44:01,281 iteration 4507 : loss : 0.020055, loss_ce: 0.006572
2022-01-06 15:44:02,533 iteration 4508 : loss : 0.070034, loss_ce: 0.035115
2022-01-06 15:44:03,756 iteration 4509 : loss : 0.034874, loss_ce: 0.010634
2022-01-06 15:44:04,961 iteration 4510 : loss : 0.024222, loss_ce: 0.009259
2022-01-06 15:44:06,116 iteration 4511 : loss : 0.029161, loss_ce: 0.010983
2022-01-06 15:44:07,298 iteration 4512 : loss : 0.019337, loss_ce: 0.008513
2022-01-06 15:44:08,508 iteration 4513 : loss : 0.020107, loss_ce: 0.009816
2022-01-06 15:44:09,624 iteration 4514 : loss : 0.026829, loss_ce: 0.011068
2022-01-06 15:44:10,794 iteration 4515 : loss : 0.033100, loss_ce: 0.011166
2022-01-06 15:44:12,113 iteration 4516 : loss : 0.031438, loss_ce: 0.013971
2022-01-06 15:44:13,280 iteration 4517 : loss : 0.028708, loss_ce: 0.010542
2022-01-06 15:44:14,492 iteration 4518 : loss : 0.038683, loss_ce: 0.012810
2022-01-06 15:44:15,684 iteration 4519 : loss : 0.029020, loss_ce: 0.013752
2022-01-06 15:44:16,824 iteration 4520 : loss : 0.029651, loss_ce: 0.009547
2022-01-06 15:44:18,039 iteration 4521 : loss : 0.026793, loss_ce: 0.008976
2022-01-06 15:44:19,163 iteration 4522 : loss : 0.025269, loss_ce: 0.011816
 66%|███████████████████▎         | 266/400 [1:39:19<49:48, 22.30s/it]2022-01-06 15:44:20,488 iteration 4523 : loss : 0.023101, loss_ce: 0.008656
2022-01-06 15:44:21,579 iteration 4524 : loss : 0.020741, loss_ce: 0.008802
2022-01-06 15:44:22,845 iteration 4525 : loss : 0.040010, loss_ce: 0.012028
2022-01-06 15:44:24,052 iteration 4526 : loss : 0.023723, loss_ce: 0.009705
2022-01-06 15:44:25,248 iteration 4527 : loss : 0.026869, loss_ce: 0.011435
2022-01-06 15:44:26,494 iteration 4528 : loss : 0.024221, loss_ce: 0.008588
2022-01-06 15:44:27,652 iteration 4529 : loss : 0.035217, loss_ce: 0.014457
2022-01-06 15:44:28,851 iteration 4530 : loss : 0.032490, loss_ce: 0.014740
2022-01-06 15:44:29,966 iteration 4531 : loss : 0.025552, loss_ce: 0.011031
2022-01-06 15:44:31,191 iteration 4532 : loss : 0.022422, loss_ce: 0.009315
2022-01-06 15:44:32,457 iteration 4533 : loss : 0.032318, loss_ce: 0.013158
2022-01-06 15:44:33,785 iteration 4534 : loss : 0.032548, loss_ce: 0.011756
2022-01-06 15:44:34,873 iteration 4535 : loss : 0.044355, loss_ce: 0.013570
2022-01-06 15:44:36,078 iteration 4536 : loss : 0.024820, loss_ce: 0.012963
2022-01-06 15:44:37,346 iteration 4537 : loss : 0.033993, loss_ce: 0.012775
2022-01-06 15:44:38,503 iteration 4538 : loss : 0.044745, loss_ce: 0.019690
2022-01-06 15:44:39,715 iteration 4539 : loss : 0.033775, loss_ce: 0.013701
 67%|███████████████████▎         | 267/400 [1:39:40<48:15, 21.77s/it]2022-01-06 15:44:41,009 iteration 4540 : loss : 0.037273, loss_ce: 0.010828
2022-01-06 15:44:42,101 iteration 4541 : loss : 0.025322, loss_ce: 0.010100
2022-01-06 15:44:43,291 iteration 4542 : loss : 0.031573, loss_ce: 0.008556
2022-01-06 15:44:44,372 iteration 4543 : loss : 0.023308, loss_ce: 0.009404
2022-01-06 15:44:45,597 iteration 4544 : loss : 0.038317, loss_ce: 0.018496
2022-01-06 15:44:46,862 iteration 4545 : loss : 0.033479, loss_ce: 0.016946
2022-01-06 15:44:48,072 iteration 4546 : loss : 0.030743, loss_ce: 0.010634
2022-01-06 15:44:49,276 iteration 4547 : loss : 0.018497, loss_ce: 0.008665
2022-01-06 15:44:50,459 iteration 4548 : loss : 0.036827, loss_ce: 0.014416
2022-01-06 15:44:51,747 iteration 4549 : loss : 0.056842, loss_ce: 0.020452
2022-01-06 15:44:53,008 iteration 4550 : loss : 0.036437, loss_ce: 0.012099
2022-01-06 15:44:54,246 iteration 4551 : loss : 0.044378, loss_ce: 0.010233
2022-01-06 15:44:55,324 iteration 4552 : loss : 0.019417, loss_ce: 0.007544
2022-01-06 15:44:56,445 iteration 4553 : loss : 0.035561, loss_ce: 0.013123
2022-01-06 15:44:57,613 iteration 4554 : loss : 0.022105, loss_ce: 0.008835
2022-01-06 15:44:58,850 iteration 4555 : loss : 0.047466, loss_ce: 0.019799
2022-01-06 15:45:00,019 iteration 4556 : loss : 0.024637, loss_ce: 0.011570
 67%|███████████████████▍         | 268/400 [1:40:00<46:56, 21.33s/it]2022-01-06 15:45:01,291 iteration 4557 : loss : 0.029135, loss_ce: 0.015154
2022-01-06 15:45:02,545 iteration 4558 : loss : 0.043057, loss_ce: 0.019188
2022-01-06 15:45:03,701 iteration 4559 : loss : 0.054132, loss_ce: 0.015893
2022-01-06 15:45:04,851 iteration 4560 : loss : 0.024479, loss_ce: 0.008467
2022-01-06 15:45:06,038 iteration 4561 : loss : 0.036731, loss_ce: 0.010181
2022-01-06 15:45:07,228 iteration 4562 : loss : 0.033553, loss_ce: 0.011160
2022-01-06 15:45:08,370 iteration 4563 : loss : 0.028171, loss_ce: 0.012505
2022-01-06 15:45:09,567 iteration 4564 : loss : 0.041583, loss_ce: 0.014342
2022-01-06 15:45:10,730 iteration 4565 : loss : 0.022666, loss_ce: 0.009383
2022-01-06 15:45:11,862 iteration 4566 : loss : 0.031683, loss_ce: 0.015507
2022-01-06 15:45:13,079 iteration 4567 : loss : 0.033625, loss_ce: 0.013494
2022-01-06 15:45:14,267 iteration 4568 : loss : 0.029031, loss_ce: 0.008368
2022-01-06 15:45:15,472 iteration 4569 : loss : 0.027736, loss_ce: 0.010853
2022-01-06 15:45:16,617 iteration 4570 : loss : 0.023617, loss_ce: 0.009530
2022-01-06 15:45:17,770 iteration 4571 : loss : 0.025627, loss_ce: 0.012325
2022-01-06 15:45:18,941 iteration 4572 : loss : 0.020831, loss_ce: 0.007443
2022-01-06 15:45:20,131 iteration 4573 : loss : 0.025979, loss_ce: 0.010981
 67%|███████████████████▌         | 269/400 [1:40:20<45:46, 20.97s/it]2022-01-06 15:45:21,362 iteration 4574 : loss : 0.022764, loss_ce: 0.007677
2022-01-06 15:45:22,491 iteration 4575 : loss : 0.020774, loss_ce: 0.008304
2022-01-06 15:45:23,714 iteration 4576 : loss : 0.021848, loss_ce: 0.013279
2022-01-06 15:45:24,959 iteration 4577 : loss : 0.033570, loss_ce: 0.012424
2022-01-06 15:45:26,149 iteration 4578 : loss : 0.025926, loss_ce: 0.010832
2022-01-06 15:45:27,323 iteration 4579 : loss : 0.028541, loss_ce: 0.012101
2022-01-06 15:45:28,490 iteration 4580 : loss : 0.021966, loss_ce: 0.009673
2022-01-06 15:45:29,782 iteration 4581 : loss : 0.026208, loss_ce: 0.008565
2022-01-06 15:45:30,978 iteration 4582 : loss : 0.020770, loss_ce: 0.009128
2022-01-06 15:45:32,177 iteration 4583 : loss : 0.023358, loss_ce: 0.009276
2022-01-06 15:45:33,334 iteration 4584 : loss : 0.029680, loss_ce: 0.011774
2022-01-06 15:45:34,556 iteration 4585 : loss : 0.033288, loss_ce: 0.010159
2022-01-06 15:45:35,719 iteration 4586 : loss : 0.022769, loss_ce: 0.009626
2022-01-06 15:45:37,013 iteration 4587 : loss : 0.032865, loss_ce: 0.011400
2022-01-06 15:45:38,169 iteration 4588 : loss : 0.029578, loss_ce: 0.008834
2022-01-06 15:45:39,360 iteration 4589 : loss : 0.025666, loss_ce: 0.010564
2022-01-06 15:45:39,360 Training Data Eval:
2022-01-06 15:45:45,285   Average segmentation loss on training set: 0.0185
2022-01-06 15:45:45,286 Validation Data Eval:
2022-01-06 15:45:47,320   Average segmentation loss on validation set: 0.0663
2022-01-06 15:45:48,662 iteration 4590 : loss : 0.030580, loss_ce: 0.011581
 68%|███████████████████▌         | 270/400 [1:40:48<50:20, 23.23s/it]2022-01-06 15:45:49,837 iteration 4591 : loss : 0.023821, loss_ce: 0.007191
2022-01-06 15:45:51,026 iteration 4592 : loss : 0.027887, loss_ce: 0.010038
2022-01-06 15:45:52,114 iteration 4593 : loss : 0.028817, loss_ce: 0.009402
2022-01-06 15:45:53,275 iteration 4594 : loss : 0.029734, loss_ce: 0.013641
2022-01-06 15:45:54,478 iteration 4595 : loss : 0.031097, loss_ce: 0.009529
2022-01-06 15:45:55,743 iteration 4596 : loss : 0.039572, loss_ce: 0.014734
2022-01-06 15:45:56,909 iteration 4597 : loss : 0.020914, loss_ce: 0.008602
2022-01-06 15:45:57,996 iteration 4598 : loss : 0.052180, loss_ce: 0.026493
2022-01-06 15:45:59,144 iteration 4599 : loss : 0.028345, loss_ce: 0.007580
2022-01-06 15:46:00,412 iteration 4600 : loss : 0.022924, loss_ce: 0.011055
2022-01-06 15:46:01,718 iteration 4601 : loss : 0.026193, loss_ce: 0.012459
2022-01-06 15:46:02,802 iteration 4602 : loss : 0.022069, loss_ce: 0.008775
2022-01-06 15:46:03,932 iteration 4603 : loss : 0.022707, loss_ce: 0.011050
2022-01-06 15:46:05,165 iteration 4604 : loss : 0.035380, loss_ce: 0.013440
2022-01-06 15:46:06,384 iteration 4605 : loss : 0.046095, loss_ce: 0.013786
2022-01-06 15:46:07,647 iteration 4606 : loss : 0.031679, loss_ce: 0.010105
2022-01-06 15:46:08,760 iteration 4607 : loss : 0.025896, loss_ce: 0.015459
 68%|███████████████████▋         | 271/400 [1:41:09<47:56, 22.30s/it]2022-01-06 15:46:10,059 iteration 4608 : loss : 0.032086, loss_ce: 0.010740
2022-01-06 15:46:11,315 iteration 4609 : loss : 0.040669, loss_ce: 0.020374
2022-01-06 15:46:12,540 iteration 4610 : loss : 0.039533, loss_ce: 0.010062
2022-01-06 15:46:13,724 iteration 4611 : loss : 0.023907, loss_ce: 0.010461
2022-01-06 15:46:14,839 iteration 4612 : loss : 0.022345, loss_ce: 0.008621
2022-01-06 15:46:16,060 iteration 4613 : loss : 0.025318, loss_ce: 0.011439
2022-01-06 15:46:17,280 iteration 4614 : loss : 0.055878, loss_ce: 0.019161
2022-01-06 15:46:18,501 iteration 4615 : loss : 0.031862, loss_ce: 0.012723
2022-01-06 15:46:19,761 iteration 4616 : loss : 0.023063, loss_ce: 0.009891
2022-01-06 15:46:20,874 iteration 4617 : loss : 0.026966, loss_ce: 0.013182
2022-01-06 15:46:22,112 iteration 4618 : loss : 0.032749, loss_ce: 0.010716
2022-01-06 15:46:23,347 iteration 4619 : loss : 0.030432, loss_ce: 0.012333
2022-01-06 15:46:24,581 iteration 4620 : loss : 0.021302, loss_ce: 0.006753
2022-01-06 15:46:25,835 iteration 4621 : loss : 0.029355, loss_ce: 0.010745
2022-01-06 15:46:27,010 iteration 4622 : loss : 0.025526, loss_ce: 0.008515
2022-01-06 15:46:28,171 iteration 4623 : loss : 0.042942, loss_ce: 0.015275
2022-01-06 15:46:29,368 iteration 4624 : loss : 0.027098, loss_ce: 0.010830
 68%|███████████████████▋         | 272/400 [1:41:29<46:28, 21.79s/it]2022-01-06 15:46:30,618 iteration 4625 : loss : 0.021673, loss_ce: 0.009348
2022-01-06 15:46:31,789 iteration 4626 : loss : 0.024506, loss_ce: 0.008124
2022-01-06 15:46:32,986 iteration 4627 : loss : 0.021960, loss_ce: 0.007769
2022-01-06 15:46:34,123 iteration 4628 : loss : 0.027084, loss_ce: 0.010262
2022-01-06 15:46:35,270 iteration 4629 : loss : 0.026426, loss_ce: 0.011361
2022-01-06 15:46:36,441 iteration 4630 : loss : 0.036786, loss_ce: 0.011893
2022-01-06 15:46:37,728 iteration 4631 : loss : 0.024153, loss_ce: 0.009389
2022-01-06 15:46:38,923 iteration 4632 : loss : 0.017965, loss_ce: 0.005660
2022-01-06 15:46:40,115 iteration 4633 : loss : 0.025236, loss_ce: 0.010107
2022-01-06 15:46:41,203 iteration 4634 : loss : 0.019968, loss_ce: 0.008895
2022-01-06 15:46:42,432 iteration 4635 : loss : 0.020604, loss_ce: 0.006945
2022-01-06 15:46:43,614 iteration 4636 : loss : 0.026988, loss_ce: 0.009723
2022-01-06 15:46:44,789 iteration 4637 : loss : 0.024913, loss_ce: 0.008780
2022-01-06 15:46:46,021 iteration 4638 : loss : 0.031501, loss_ce: 0.013063
2022-01-06 15:46:47,298 iteration 4639 : loss : 0.036072, loss_ce: 0.013356
2022-01-06 15:46:48,559 iteration 4640 : loss : 0.027030, loss_ce: 0.010339
2022-01-06 15:46:49,698 iteration 4641 : loss : 0.021641, loss_ce: 0.009562
 68%|███████████████████▊         | 273/400 [1:41:50<45:11, 21.35s/it]2022-01-06 15:46:51,003 iteration 4642 : loss : 0.027950, loss_ce: 0.008249
2022-01-06 15:46:52,221 iteration 4643 : loss : 0.039578, loss_ce: 0.015785
2022-01-06 15:46:53,423 iteration 4644 : loss : 0.026190, loss_ce: 0.010813
2022-01-06 15:46:54,789 iteration 4645 : loss : 0.022885, loss_ce: 0.009896
2022-01-06 15:46:55,944 iteration 4646 : loss : 0.021946, loss_ce: 0.009853
2022-01-06 15:46:57,095 iteration 4647 : loss : 0.023449, loss_ce: 0.009022
2022-01-06 15:46:58,181 iteration 4648 : loss : 0.021426, loss_ce: 0.008344
2022-01-06 15:46:59,393 iteration 4649 : loss : 0.031269, loss_ce: 0.012717
2022-01-06 15:47:00,553 iteration 4650 : loss : 0.022521, loss_ce: 0.008370
2022-01-06 15:47:01,753 iteration 4651 : loss : 0.023764, loss_ce: 0.008906
2022-01-06 15:47:02,954 iteration 4652 : loss : 0.025940, loss_ce: 0.010185
2022-01-06 15:47:04,137 iteration 4653 : loss : 0.022443, loss_ce: 0.008054
2022-01-06 15:47:05,327 iteration 4654 : loss : 0.026096, loss_ce: 0.011978
2022-01-06 15:47:06,530 iteration 4655 : loss : 0.029875, loss_ce: 0.011891
2022-01-06 15:47:07,797 iteration 4656 : loss : 0.067956, loss_ce: 0.014713
2022-01-06 15:47:08,965 iteration 4657 : loss : 0.068803, loss_ce: 0.009579
2022-01-06 15:47:10,100 iteration 4658 : loss : 0.043003, loss_ce: 0.010004
 68%|███████████████████▊         | 274/400 [1:42:10<44:14, 21.07s/it]2022-01-06 15:47:11,336 iteration 4659 : loss : 0.038576, loss_ce: 0.014966
2022-01-06 15:47:12,549 iteration 4660 : loss : 0.020915, loss_ce: 0.004919
2022-01-06 15:47:13,723 iteration 4661 : loss : 0.028964, loss_ce: 0.017056
2022-01-06 15:47:15,026 iteration 4662 : loss : 0.038440, loss_ce: 0.021181
2022-01-06 15:47:16,130 iteration 4663 : loss : 0.028254, loss_ce: 0.007862
2022-01-06 15:47:17,319 iteration 4664 : loss : 0.036077, loss_ce: 0.011583
2022-01-06 15:47:18,679 iteration 4665 : loss : 0.041823, loss_ce: 0.014982
2022-01-06 15:47:19,793 iteration 4666 : loss : 0.023642, loss_ce: 0.009065
2022-01-06 15:47:20,964 iteration 4667 : loss : 0.030074, loss_ce: 0.012002
2022-01-06 15:47:22,126 iteration 4668 : loss : 0.033658, loss_ce: 0.008563
2022-01-06 15:47:23,387 iteration 4669 : loss : 0.026797, loss_ce: 0.011251
2022-01-06 15:47:24,644 iteration 4670 : loss : 0.039470, loss_ce: 0.020010
2022-01-06 15:47:25,744 iteration 4671 : loss : 0.021094, loss_ce: 0.007152
2022-01-06 15:47:27,074 iteration 4672 : loss : 0.034052, loss_ce: 0.012999
2022-01-06 15:47:28,295 iteration 4673 : loss : 0.029544, loss_ce: 0.009560
2022-01-06 15:47:29,554 iteration 4674 : loss : 0.029171, loss_ce: 0.015626
2022-01-06 15:47:29,554 Training Data Eval:
2022-01-06 15:47:35,363   Average segmentation loss on training set: 0.0258
2022-01-06 15:47:35,363 Validation Data Eval:
2022-01-06 15:47:37,357   Average segmentation loss on validation set: 0.0771
2022-01-06 15:47:38,516 iteration 4675 : loss : 0.024166, loss_ce: 0.009702
 69%|███████████████████▉         | 275/400 [1:42:38<48:29, 23.27s/it]2022-01-06 15:47:39,856 iteration 4676 : loss : 0.037890, loss_ce: 0.012158
2022-01-06 15:47:41,020 iteration 4677 : loss : 0.033449, loss_ce: 0.012690
2022-01-06 15:47:42,194 iteration 4678 : loss : 0.042769, loss_ce: 0.013169
2022-01-06 15:47:43,386 iteration 4679 : loss : 0.024219, loss_ce: 0.012950
2022-01-06 15:47:44,532 iteration 4680 : loss : 0.025491, loss_ce: 0.009063
2022-01-06 15:47:45,738 iteration 4681 : loss : 0.026596, loss_ce: 0.008194
2022-01-06 15:47:46,845 iteration 4682 : loss : 0.032654, loss_ce: 0.008347
2022-01-06 15:47:48,020 iteration 4683 : loss : 0.033180, loss_ce: 0.013659
2022-01-06 15:47:49,242 iteration 4684 : loss : 0.032249, loss_ce: 0.013827
2022-01-06 15:47:50,432 iteration 4685 : loss : 0.023586, loss_ce: 0.010827
2022-01-06 15:47:51,516 iteration 4686 : loss : 0.033381, loss_ce: 0.010880
2022-01-06 15:47:52,769 iteration 4687 : loss : 0.027642, loss_ce: 0.011630
2022-01-06 15:47:54,043 iteration 4688 : loss : 0.028308, loss_ce: 0.009835
2022-01-06 15:47:55,321 iteration 4689 : loss : 0.026990, loss_ce: 0.011457
2022-01-06 15:47:56,474 iteration 4690 : loss : 0.026503, loss_ce: 0.006888
2022-01-06 15:47:57,602 iteration 4691 : loss : 0.026197, loss_ce: 0.008401
2022-01-06 15:47:58,775 iteration 4692 : loss : 0.024732, loss_ce: 0.010526
 69%|████████████████████         | 276/400 [1:42:59<46:13, 22.37s/it]2022-01-06 15:48:00,060 iteration 4693 : loss : 0.031135, loss_ce: 0.012778
2022-01-06 15:48:01,197 iteration 4694 : loss : 0.015974, loss_ce: 0.004993
2022-01-06 15:48:02,449 iteration 4695 : loss : 0.032406, loss_ce: 0.012033
2022-01-06 15:48:03,651 iteration 4696 : loss : 0.034829, loss_ce: 0.014521
2022-01-06 15:48:04,901 iteration 4697 : loss : 0.031275, loss_ce: 0.014101
2022-01-06 15:48:05,971 iteration 4698 : loss : 0.019870, loss_ce: 0.007357
2022-01-06 15:48:07,149 iteration 4699 : loss : 0.036481, loss_ce: 0.014592
2022-01-06 15:48:08,344 iteration 4700 : loss : 0.031129, loss_ce: 0.012553
2022-01-06 15:48:09,537 iteration 4701 : loss : 0.032256, loss_ce: 0.014375
2022-01-06 15:48:10,632 iteration 4702 : loss : 0.020860, loss_ce: 0.007956
2022-01-06 15:48:11,858 iteration 4703 : loss : 0.027181, loss_ce: 0.010094
2022-01-06 15:48:13,045 iteration 4704 : loss : 0.029604, loss_ce: 0.007216
2022-01-06 15:48:14,164 iteration 4705 : loss : 0.020294, loss_ce: 0.007189
2022-01-06 15:48:15,305 iteration 4706 : loss : 0.020982, loss_ce: 0.009106
2022-01-06 15:48:16,572 iteration 4707 : loss : 0.029560, loss_ce: 0.012898
2022-01-06 15:48:17,743 iteration 4708 : loss : 0.027450, loss_ce: 0.006548
2022-01-06 15:48:18,906 iteration 4709 : loss : 0.036531, loss_ce: 0.014187
 69%|████████████████████         | 277/400 [1:43:19<44:28, 21.69s/it]2022-01-06 15:48:20,239 iteration 4710 : loss : 0.026136, loss_ce: 0.009958
2022-01-06 15:48:21,377 iteration 4711 : loss : 0.030214, loss_ce: 0.013978
2022-01-06 15:48:22,563 iteration 4712 : loss : 0.021659, loss_ce: 0.008275
2022-01-06 15:48:23,706 iteration 4713 : loss : 0.020636, loss_ce: 0.007328
2022-01-06 15:48:24,856 iteration 4714 : loss : 0.019566, loss_ce: 0.008335
2022-01-06 15:48:26,134 iteration 4715 : loss : 0.031218, loss_ce: 0.009463
2022-01-06 15:48:27,319 iteration 4716 : loss : 0.019686, loss_ce: 0.007977
2022-01-06 15:48:28,504 iteration 4717 : loss : 0.032329, loss_ce: 0.009236
2022-01-06 15:48:29,668 iteration 4718 : loss : 0.025989, loss_ce: 0.009058
2022-01-06 15:48:30,820 iteration 4719 : loss : 0.016315, loss_ce: 0.005840
2022-01-06 15:48:32,076 iteration 4720 : loss : 0.026823, loss_ce: 0.010491
2022-01-06 15:48:33,252 iteration 4721 : loss : 0.027367, loss_ce: 0.008895
2022-01-06 15:48:34,426 iteration 4722 : loss : 0.030702, loss_ce: 0.013936
2022-01-06 15:48:35,652 iteration 4723 : loss : 0.053322, loss_ce: 0.017073
2022-01-06 15:48:36,778 iteration 4724 : loss : 0.022404, loss_ce: 0.011460
2022-01-06 15:48:37,952 iteration 4725 : loss : 0.026973, loss_ce: 0.011153
2022-01-06 15:48:39,098 iteration 4726 : loss : 0.023964, loss_ce: 0.008691
 70%|████████████████████▏        | 278/400 [1:43:39<43:12, 21.25s/it]2022-01-06 15:48:40,358 iteration 4727 : loss : 0.030432, loss_ce: 0.011569
2022-01-06 15:48:41,597 iteration 4728 : loss : 0.032954, loss_ce: 0.013415
2022-01-06 15:48:42,815 iteration 4729 : loss : 0.018090, loss_ce: 0.005566
2022-01-06 15:48:44,013 iteration 4730 : loss : 0.027679, loss_ce: 0.010388
2022-01-06 15:48:45,205 iteration 4731 : loss : 0.032290, loss_ce: 0.009182
2022-01-06 15:48:46,336 iteration 4732 : loss : 0.024734, loss_ce: 0.011059
2022-01-06 15:48:47,649 iteration 4733 : loss : 0.065151, loss_ce: 0.032317
2022-01-06 15:48:48,847 iteration 4734 : loss : 0.021085, loss_ce: 0.008128
2022-01-06 15:48:49,961 iteration 4735 : loss : 0.022202, loss_ce: 0.008848
2022-01-06 15:48:51,217 iteration 4736 : loss : 0.034443, loss_ce: 0.014138
2022-01-06 15:48:52,435 iteration 4737 : loss : 0.024535, loss_ce: 0.011775
2022-01-06 15:48:53,766 iteration 4738 : loss : 0.052655, loss_ce: 0.015125
2022-01-06 15:48:54,992 iteration 4739 : loss : 0.018950, loss_ce: 0.006517
2022-01-06 15:48:56,200 iteration 4740 : loss : 0.020940, loss_ce: 0.006062
2022-01-06 15:48:57,388 iteration 4741 : loss : 0.043563, loss_ce: 0.021645
2022-01-06 15:48:58,594 iteration 4742 : loss : 0.034397, loss_ce: 0.015841
2022-01-06 15:48:59,973 iteration 4743 : loss : 0.049567, loss_ce: 0.027647
 70%|████████████████████▏        | 279/400 [1:44:00<42:37, 21.13s/it]2022-01-06 15:49:01,272 iteration 4744 : loss : 0.030682, loss_ce: 0.010950
2022-01-06 15:49:02,465 iteration 4745 : loss : 0.029491, loss_ce: 0.014195
2022-01-06 15:49:03,642 iteration 4746 : loss : 0.028622, loss_ce: 0.008609
2022-01-06 15:49:04,754 iteration 4747 : loss : 0.023807, loss_ce: 0.012214
2022-01-06 15:49:06,054 iteration 4748 : loss : 0.037919, loss_ce: 0.013425
2022-01-06 15:49:07,172 iteration 4749 : loss : 0.029059, loss_ce: 0.012729
2022-01-06 15:49:08,381 iteration 4750 : loss : 0.035130, loss_ce: 0.010496
2022-01-06 15:49:09,594 iteration 4751 : loss : 0.027532, loss_ce: 0.011350
2022-01-06 15:49:10,768 iteration 4752 : loss : 0.032827, loss_ce: 0.009535
2022-01-06 15:49:11,925 iteration 4753 : loss : 0.018564, loss_ce: 0.007126
2022-01-06 15:49:13,129 iteration 4754 : loss : 0.024516, loss_ce: 0.011629
2022-01-06 15:49:14,403 iteration 4755 : loss : 0.041539, loss_ce: 0.011739
2022-01-06 15:49:15,607 iteration 4756 : loss : 0.040966, loss_ce: 0.013593
2022-01-06 15:49:16,786 iteration 4757 : loss : 0.026375, loss_ce: 0.012492
2022-01-06 15:49:18,062 iteration 4758 : loss : 0.032295, loss_ce: 0.012015
2022-01-06 15:49:19,243 iteration 4759 : loss : 0.050894, loss_ce: 0.031006
2022-01-06 15:49:19,244 Training Data Eval:
2022-01-06 15:49:25,181   Average segmentation loss on training set: 0.0266
2022-01-06 15:49:25,181 Validation Data Eval:
2022-01-06 15:49:27,208   Average segmentation loss on validation set: 0.1053
2022-01-06 15:49:28,442 iteration 4760 : loss : 0.032079, loss_ce: 0.010682
 70%|████████████████████▎        | 280/400 [1:44:28<46:40, 23.33s/it]2022-01-06 15:49:29,643 iteration 4761 : loss : 0.022511, loss_ce: 0.007970
2022-01-06 15:49:30,949 iteration 4762 : loss : 0.043780, loss_ce: 0.012590
2022-01-06 15:49:32,171 iteration 4763 : loss : 0.023882, loss_ce: 0.010467
2022-01-06 15:49:33,298 iteration 4764 : loss : 0.021013, loss_ce: 0.008050
2022-01-06 15:49:34,473 iteration 4765 : loss : 0.027905, loss_ce: 0.012326
2022-01-06 15:49:35,630 iteration 4766 : loss : 0.024853, loss_ce: 0.008456
2022-01-06 15:49:36,897 iteration 4767 : loss : 0.046506, loss_ce: 0.020973
2022-01-06 15:49:38,206 iteration 4768 : loss : 0.029311, loss_ce: 0.013011
2022-01-06 15:49:39,492 iteration 4769 : loss : 0.053752, loss_ce: 0.014076
2022-01-06 15:49:40,697 iteration 4770 : loss : 0.026115, loss_ce: 0.011988
2022-01-06 15:49:41,860 iteration 4771 : loss : 0.026041, loss_ce: 0.010808
2022-01-06 15:49:43,043 iteration 4772 : loss : 0.020156, loss_ce: 0.006535
2022-01-06 15:49:44,200 iteration 4773 : loss : 0.025593, loss_ce: 0.010814
2022-01-06 15:49:45,353 iteration 4774 : loss : 0.018962, loss_ce: 0.007159
2022-01-06 15:49:46,565 iteration 4775 : loss : 0.035842, loss_ce: 0.013781
2022-01-06 15:49:47,747 iteration 4776 : loss : 0.040459, loss_ce: 0.017624
2022-01-06 15:49:48,918 iteration 4777 : loss : 0.023600, loss_ce: 0.007943
 70%|████████████████████▎        | 281/400 [1:44:49<44:34, 22.48s/it]2022-01-06 15:49:50,248 iteration 4778 : loss : 0.026562, loss_ce: 0.009996
2022-01-06 15:49:51,382 iteration 4779 : loss : 0.021813, loss_ce: 0.007760
2022-01-06 15:49:52,530 iteration 4780 : loss : 0.040710, loss_ce: 0.009594
2022-01-06 15:49:53,685 iteration 4781 : loss : 0.023069, loss_ce: 0.010773
2022-01-06 15:49:54,938 iteration 4782 : loss : 0.040214, loss_ce: 0.016234
2022-01-06 15:49:56,047 iteration 4783 : loss : 0.026341, loss_ce: 0.007661
2022-01-06 15:49:57,316 iteration 4784 : loss : 0.035509, loss_ce: 0.015798
2022-01-06 15:49:58,600 iteration 4785 : loss : 0.045378, loss_ce: 0.023237
2022-01-06 15:49:59,775 iteration 4786 : loss : 0.024993, loss_ce: 0.010447
2022-01-06 15:50:01,001 iteration 4787 : loss : 0.029645, loss_ce: 0.007045
2022-01-06 15:50:02,205 iteration 4788 : loss : 0.021728, loss_ce: 0.008888
2022-01-06 15:50:03,406 iteration 4789 : loss : 0.017974, loss_ce: 0.007187
2022-01-06 15:50:04,590 iteration 4790 : loss : 0.023077, loss_ce: 0.009111
2022-01-06 15:50:05,737 iteration 4791 : loss : 0.018957, loss_ce: 0.006861
2022-01-06 15:50:06,908 iteration 4792 : loss : 0.027331, loss_ce: 0.012926
2022-01-06 15:50:08,144 iteration 4793 : loss : 0.020608, loss_ce: 0.009184
2022-01-06 15:50:09,326 iteration 4794 : loss : 0.060509, loss_ce: 0.018063
 70%|████████████████████▍        | 282/400 [1:45:09<42:58, 21.85s/it]2022-01-06 15:50:10,503 iteration 4795 : loss : 0.017442, loss_ce: 0.005374
2022-01-06 15:50:11,615 iteration 4796 : loss : 0.022090, loss_ce: 0.008690
2022-01-06 15:50:12,757 iteration 4797 : loss : 0.030557, loss_ce: 0.011271
2022-01-06 15:50:14,019 iteration 4798 : loss : 0.043403, loss_ce: 0.020002
2022-01-06 15:50:15,195 iteration 4799 : loss : 0.039753, loss_ce: 0.008786
2022-01-06 15:50:16,396 iteration 4800 : loss : 0.052483, loss_ce: 0.022708
2022-01-06 15:50:17,598 iteration 4801 : loss : 0.022046, loss_ce: 0.008515
2022-01-06 15:50:18,690 iteration 4802 : loss : 0.019844, loss_ce: 0.006088
2022-01-06 15:50:19,889 iteration 4803 : loss : 0.027293, loss_ce: 0.012449
2022-01-06 15:50:21,037 iteration 4804 : loss : 0.042203, loss_ce: 0.010961
2022-01-06 15:50:22,263 iteration 4805 : loss : 0.033382, loss_ce: 0.011513
2022-01-06 15:50:23,410 iteration 4806 : loss : 0.025920, loss_ce: 0.007404
2022-01-06 15:50:24,691 iteration 4807 : loss : 0.033073, loss_ce: 0.012193
2022-01-06 15:50:26,114 iteration 4808 : loss : 0.039318, loss_ce: 0.015132
2022-01-06 15:50:27,181 iteration 4809 : loss : 0.021257, loss_ce: 0.010334
2022-01-06 15:50:28,426 iteration 4810 : loss : 0.025718, loss_ce: 0.010456
2022-01-06 15:50:29,601 iteration 4811 : loss : 0.022021, loss_ce: 0.008646
 71%|████████████████████▌        | 283/400 [1:45:29<41:41, 21.38s/it]2022-01-06 15:50:30,816 iteration 4812 : loss : 0.031478, loss_ce: 0.015068
2022-01-06 15:50:32,012 iteration 4813 : loss : 0.024560, loss_ce: 0.008430
2022-01-06 15:50:33,137 iteration 4814 : loss : 0.027759, loss_ce: 0.012439
2022-01-06 15:50:34,334 iteration 4815 : loss : 0.024296, loss_ce: 0.008684
2022-01-06 15:50:35,478 iteration 4816 : loss : 0.025256, loss_ce: 0.008575
2022-01-06 15:50:36,527 iteration 4817 : loss : 0.024942, loss_ce: 0.008736
2022-01-06 15:50:37,762 iteration 4818 : loss : 0.036480, loss_ce: 0.016502
2022-01-06 15:50:38,960 iteration 4819 : loss : 0.030846, loss_ce: 0.010033
2022-01-06 15:50:40,201 iteration 4820 : loss : 0.023000, loss_ce: 0.010252
2022-01-06 15:50:41,352 iteration 4821 : loss : 0.026649, loss_ce: 0.012725
2022-01-06 15:50:42,560 iteration 4822 : loss : 0.031294, loss_ce: 0.008278
2022-01-06 15:50:43,710 iteration 4823 : loss : 0.025323, loss_ce: 0.010738
2022-01-06 15:50:44,955 iteration 4824 : loss : 0.024926, loss_ce: 0.008815
2022-01-06 15:50:46,121 iteration 4825 : loss : 0.020910, loss_ce: 0.005886
2022-01-06 15:50:47,268 iteration 4826 : loss : 0.035714, loss_ce: 0.010702
2022-01-06 15:50:48,584 iteration 4827 : loss : 0.037153, loss_ce: 0.012282
2022-01-06 15:50:49,843 iteration 4828 : loss : 0.029502, loss_ce: 0.011028
 71%|████████████████████▌        | 284/400 [1:45:50<40:41, 21.04s/it]2022-01-06 15:50:51,119 iteration 4829 : loss : 0.024100, loss_ce: 0.008249
2022-01-06 15:50:52,232 iteration 4830 : loss : 0.021656, loss_ce: 0.008076
2022-01-06 15:50:53,517 iteration 4831 : loss : 0.035942, loss_ce: 0.012350
2022-01-06 15:50:54,688 iteration 4832 : loss : 0.042179, loss_ce: 0.021639
2022-01-06 15:50:55,944 iteration 4833 : loss : 0.026679, loss_ce: 0.007452
2022-01-06 15:50:57,233 iteration 4834 : loss : 0.025165, loss_ce: 0.009015
2022-01-06 15:50:58,401 iteration 4835 : loss : 0.026551, loss_ce: 0.010292
2022-01-06 15:50:59,587 iteration 4836 : loss : 0.049798, loss_ce: 0.018328
2022-01-06 15:51:00,716 iteration 4837 : loss : 0.027707, loss_ce: 0.006317
2022-01-06 15:51:01,936 iteration 4838 : loss : 0.026256, loss_ce: 0.011796
2022-01-06 15:51:03,148 iteration 4839 : loss : 0.022925, loss_ce: 0.011217
2022-01-06 15:51:04,336 iteration 4840 : loss : 0.027438, loss_ce: 0.010628
2022-01-06 15:51:05,600 iteration 4841 : loss : 0.041941, loss_ce: 0.013058
2022-01-06 15:51:06,808 iteration 4842 : loss : 0.027248, loss_ce: 0.011866
2022-01-06 15:51:07,993 iteration 4843 : loss : 0.023258, loss_ce: 0.011161
2022-01-06 15:51:09,109 iteration 4844 : loss : 0.022641, loss_ce: 0.010905
2022-01-06 15:51:09,110 Training Data Eval:
2022-01-06 15:51:14,955   Average segmentation loss on training set: 0.0214
2022-01-06 15:51:14,955 Validation Data Eval:
2022-01-06 15:51:16,948   Average segmentation loss on validation set: 0.0781
2022-01-06 15:51:18,096 iteration 4845 : loss : 0.022861, loss_ce: 0.007015
 71%|████████████████████▋        | 285/400 [1:46:18<44:28, 23.20s/it]2022-01-06 15:51:19,388 iteration 4846 : loss : 0.024683, loss_ce: 0.008923
2022-01-06 15:51:20,539 iteration 4847 : loss : 0.033936, loss_ce: 0.010823
2022-01-06 15:51:21,831 iteration 4848 : loss : 0.034866, loss_ce: 0.014925
2022-01-06 15:51:22,987 iteration 4849 : loss : 0.023865, loss_ce: 0.009921
2022-01-06 15:51:24,209 iteration 4850 : loss : 0.060010, loss_ce: 0.013056
2022-01-06 15:51:25,423 iteration 4851 : loss : 0.025841, loss_ce: 0.012312
2022-01-06 15:51:26,566 iteration 4852 : loss : 0.030577, loss_ce: 0.012059
2022-01-06 15:51:27,715 iteration 4853 : loss : 0.022693, loss_ce: 0.007752
2022-01-06 15:51:28,810 iteration 4854 : loss : 0.023738, loss_ce: 0.009394
2022-01-06 15:51:30,007 iteration 4855 : loss : 0.025025, loss_ce: 0.008508
2022-01-06 15:51:31,227 iteration 4856 : loss : 0.030947, loss_ce: 0.011570
2022-01-06 15:51:32,452 iteration 4857 : loss : 0.032117, loss_ce: 0.015654
2022-01-06 15:51:33,632 iteration 4858 : loss : 0.027530, loss_ce: 0.010608
2022-01-06 15:51:34,737 iteration 4859 : loss : 0.019978, loss_ce: 0.007544
2022-01-06 15:51:35,888 iteration 4860 : loss : 0.024167, loss_ce: 0.006491
2022-01-06 15:51:36,984 iteration 4861 : loss : 0.023691, loss_ce: 0.007297
2022-01-06 15:51:38,264 iteration 4862 : loss : 0.027274, loss_ce: 0.008886
 72%|████████████████████▋        | 286/400 [1:46:38<42:21, 22.29s/it]2022-01-06 15:51:39,545 iteration 4863 : loss : 0.026699, loss_ce: 0.008414
2022-01-06 15:51:40,757 iteration 4864 : loss : 0.029097, loss_ce: 0.009282
2022-01-06 15:51:41,954 iteration 4865 : loss : 0.026086, loss_ce: 0.009326
2022-01-06 15:51:43,145 iteration 4866 : loss : 0.028790, loss_ce: 0.011977
2022-01-06 15:51:44,376 iteration 4867 : loss : 0.040241, loss_ce: 0.014853
2022-01-06 15:51:45,581 iteration 4868 : loss : 0.033657, loss_ce: 0.017082
2022-01-06 15:51:46,712 iteration 4869 : loss : 0.026393, loss_ce: 0.009377
2022-01-06 15:51:47,776 iteration 4870 : loss : 0.028254, loss_ce: 0.010575
2022-01-06 15:51:48,924 iteration 4871 : loss : 0.028076, loss_ce: 0.009059
2022-01-06 15:51:50,110 iteration 4872 : loss : 0.027213, loss_ce: 0.012739
2022-01-06 15:51:51,291 iteration 4873 : loss : 0.029083, loss_ce: 0.010014
2022-01-06 15:51:52,466 iteration 4874 : loss : 0.023873, loss_ce: 0.008877
2022-01-06 15:51:53,603 iteration 4875 : loss : 0.023314, loss_ce: 0.006931
2022-01-06 15:51:54,684 iteration 4876 : loss : 0.032045, loss_ce: 0.012702
2022-01-06 15:51:55,841 iteration 4877 : loss : 0.019532, loss_ce: 0.006595
2022-01-06 15:51:57,033 iteration 4878 : loss : 0.040738, loss_ce: 0.015909
2022-01-06 15:51:58,268 iteration 4879 : loss : 0.024919, loss_ce: 0.008372
 72%|████████████████████▊        | 287/400 [1:46:58<40:41, 21.61s/it]2022-01-06 15:51:59,528 iteration 4880 : loss : 0.030201, loss_ce: 0.011053
2022-01-06 15:52:00,633 iteration 4881 : loss : 0.019565, loss_ce: 0.006164
2022-01-06 15:52:01,749 iteration 4882 : loss : 0.024117, loss_ce: 0.006524
2022-01-06 15:52:02,967 iteration 4883 : loss : 0.029983, loss_ce: 0.012465
2022-01-06 15:52:04,088 iteration 4884 : loss : 0.035130, loss_ce: 0.016208
2022-01-06 15:52:05,273 iteration 4885 : loss : 0.025438, loss_ce: 0.007395
2022-01-06 15:52:06,488 iteration 4886 : loss : 0.030190, loss_ce: 0.010505
2022-01-06 15:52:07,626 iteration 4887 : loss : 0.028223, loss_ce: 0.010659
2022-01-06 15:52:08,814 iteration 4888 : loss : 0.024314, loss_ce: 0.007026
2022-01-06 15:52:09,974 iteration 4889 : loss : 0.025032, loss_ce: 0.010228
2022-01-06 15:52:11,171 iteration 4890 : loss : 0.026906, loss_ce: 0.010346
2022-01-06 15:52:12,351 iteration 4891 : loss : 0.026177, loss_ce: 0.012910
2022-01-06 15:52:13,540 iteration 4892 : loss : 0.024280, loss_ce: 0.012062
2022-01-06 15:52:14,904 iteration 4893 : loss : 0.037858, loss_ce: 0.015074
2022-01-06 15:52:16,073 iteration 4894 : loss : 0.022982, loss_ce: 0.010264
2022-01-06 15:52:17,228 iteration 4895 : loss : 0.026076, loss_ce: 0.011173
2022-01-06 15:52:18,375 iteration 4896 : loss : 0.032069, loss_ce: 0.012506
 72%|████████████████████▉        | 288/400 [1:47:18<39:29, 21.16s/it]2022-01-06 15:52:19,545 iteration 4897 : loss : 0.023472, loss_ce: 0.010525
2022-01-06 15:52:20,710 iteration 4898 : loss : 0.020575, loss_ce: 0.007693
2022-01-06 15:52:21,835 iteration 4899 : loss : 0.035382, loss_ce: 0.016996
2022-01-06 15:52:23,087 iteration 4900 : loss : 0.036602, loss_ce: 0.014125
2022-01-06 15:52:24,283 iteration 4901 : loss : 0.021563, loss_ce: 0.007366
2022-01-06 15:52:25,419 iteration 4902 : loss : 0.022880, loss_ce: 0.005696
2022-01-06 15:52:26,616 iteration 4903 : loss : 0.021463, loss_ce: 0.008990
2022-01-06 15:52:27,793 iteration 4904 : loss : 0.025535, loss_ce: 0.012309
2022-01-06 15:52:28,949 iteration 4905 : loss : 0.020169, loss_ce: 0.008905
2022-01-06 15:52:30,102 iteration 4906 : loss : 0.026630, loss_ce: 0.009372
2022-01-06 15:52:31,294 iteration 4907 : loss : 0.033353, loss_ce: 0.012647
2022-01-06 15:52:32,369 iteration 4908 : loss : 0.030525, loss_ce: 0.008671
2022-01-06 15:52:33,549 iteration 4909 : loss : 0.036680, loss_ce: 0.010742
2022-01-06 15:52:34,695 iteration 4910 : loss : 0.032392, loss_ce: 0.009042
2022-01-06 15:52:35,952 iteration 4911 : loss : 0.022952, loss_ce: 0.009419
2022-01-06 15:52:37,183 iteration 4912 : loss : 0.035091, loss_ce: 0.012444
2022-01-06 15:52:38,310 iteration 4913 : loss : 0.021961, loss_ce: 0.008468
 72%|████████████████████▉        | 289/400 [1:47:38<38:27, 20.79s/it]2022-01-06 15:52:39,464 iteration 4914 : loss : 0.027148, loss_ce: 0.008606
2022-01-06 15:52:40,680 iteration 4915 : loss : 0.027259, loss_ce: 0.012193
2022-01-06 15:52:41,884 iteration 4916 : loss : 0.028465, loss_ce: 0.009454
2022-01-06 15:52:43,060 iteration 4917 : loss : 0.021360, loss_ce: 0.008126
2022-01-06 15:52:44,188 iteration 4918 : loss : 0.024438, loss_ce: 0.011040
2022-01-06 15:52:45,317 iteration 4919 : loss : 0.026695, loss_ce: 0.009996
2022-01-06 15:52:46,527 iteration 4920 : loss : 0.033117, loss_ce: 0.016338
2022-01-06 15:52:47,686 iteration 4921 : loss : 0.023458, loss_ce: 0.005036
2022-01-06 15:52:48,857 iteration 4922 : loss : 0.033733, loss_ce: 0.006508
2022-01-06 15:52:50,031 iteration 4923 : loss : 0.025114, loss_ce: 0.009927
2022-01-06 15:52:51,227 iteration 4924 : loss : 0.028283, loss_ce: 0.009620
2022-01-06 15:52:52,546 iteration 4925 : loss : 0.040134, loss_ce: 0.016361
2022-01-06 15:52:53,683 iteration 4926 : loss : 0.023133, loss_ce: 0.008384
2022-01-06 15:52:54,928 iteration 4927 : loss : 0.024190, loss_ce: 0.009808
2022-01-06 15:52:56,188 iteration 4928 : loss : 0.029517, loss_ce: 0.014307
2022-01-06 15:52:57,417 iteration 4929 : loss : 0.059656, loss_ce: 0.024007
2022-01-06 15:52:57,417 Training Data Eval:
2022-01-06 15:53:03,259   Average segmentation loss on training set: 0.0184
2022-01-06 15:53:03,259 Validation Data Eval:
2022-01-06 15:53:05,290   Average segmentation loss on validation set: 0.0759
2022-01-06 15:53:06,510 iteration 4930 : loss : 0.020618, loss_ce: 0.008960
 72%|█████████████████████        | 290/400 [1:48:06<42:11, 23.02s/it]2022-01-06 15:53:07,824 iteration 4931 : loss : 0.020853, loss_ce: 0.008861
2022-01-06 15:53:08,980 iteration 4932 : loss : 0.021532, loss_ce: 0.008386
2022-01-06 15:53:10,125 iteration 4933 : loss : 0.029509, loss_ce: 0.010085
2022-01-06 15:53:11,328 iteration 4934 : loss : 0.028088, loss_ce: 0.013306
2022-01-06 15:53:12,555 iteration 4935 : loss : 0.033593, loss_ce: 0.011339
2022-01-06 15:53:13,752 iteration 4936 : loss : 0.025750, loss_ce: 0.010608
2022-01-06 15:53:14,967 iteration 4937 : loss : 0.032267, loss_ce: 0.010894
2022-01-06 15:53:16,151 iteration 4938 : loss : 0.032408, loss_ce: 0.012540
2022-01-06 15:53:17,414 iteration 4939 : loss : 0.028212, loss_ce: 0.010642
2022-01-06 15:53:18,558 iteration 4940 : loss : 0.037276, loss_ce: 0.010833
2022-01-06 15:53:19,743 iteration 4941 : loss : 0.023437, loss_ce: 0.009086
2022-01-06 15:53:20,971 iteration 4942 : loss : 0.025147, loss_ce: 0.010007
2022-01-06 15:53:22,100 iteration 4943 : loss : 0.016951, loss_ce: 0.005745
2022-01-06 15:53:23,285 iteration 4944 : loss : 0.032186, loss_ce: 0.014482
2022-01-06 15:53:24,571 iteration 4945 : loss : 0.023921, loss_ce: 0.006710
2022-01-06 15:53:25,818 iteration 4946 : loss : 0.048676, loss_ce: 0.023701
2022-01-06 15:53:27,115 iteration 4947 : loss : 0.034027, loss_ce: 0.012283
 73%|█████████████████████        | 291/400 [1:48:27<40:29, 22.29s/it]2022-01-06 15:53:28,423 iteration 4948 : loss : 0.025176, loss_ce: 0.009259
2022-01-06 15:53:29,570 iteration 4949 : loss : 0.020462, loss_ce: 0.007783
2022-01-06 15:53:30,695 iteration 4950 : loss : 0.025349, loss_ce: 0.009367
2022-01-06 15:53:31,860 iteration 4951 : loss : 0.036270, loss_ce: 0.013837
2022-01-06 15:53:32,944 iteration 4952 : loss : 0.028069, loss_ce: 0.012437
2022-01-06 15:53:34,177 iteration 4953 : loss : 0.020266, loss_ce: 0.007695
2022-01-06 15:53:35,296 iteration 4954 : loss : 0.039104, loss_ce: 0.015136
2022-01-06 15:53:36,458 iteration 4955 : loss : 0.017558, loss_ce: 0.006749
2022-01-06 15:53:37,624 iteration 4956 : loss : 0.027603, loss_ce: 0.009503
2022-01-06 15:53:38,796 iteration 4957 : loss : 0.022217, loss_ce: 0.009107
2022-01-06 15:53:39,980 iteration 4958 : loss : 0.034434, loss_ce: 0.016219
2022-01-06 15:53:41,060 iteration 4959 : loss : 0.023208, loss_ce: 0.007733
2022-01-06 15:53:42,238 iteration 4960 : loss : 0.027617, loss_ce: 0.011049
2022-01-06 15:53:43,370 iteration 4961 : loss : 0.027994, loss_ce: 0.010885
2022-01-06 15:53:44,606 iteration 4962 : loss : 0.024227, loss_ce: 0.009849
2022-01-06 15:53:45,724 iteration 4963 : loss : 0.023469, loss_ce: 0.009179
2022-01-06 15:53:46,997 iteration 4964 : loss : 0.026095, loss_ce: 0.010125
 73%|█████████████████████▏       | 292/400 [1:48:47<38:49, 21.57s/it]2022-01-06 15:53:48,314 iteration 4965 : loss : 0.027879, loss_ce: 0.008346
2022-01-06 15:53:49,620 iteration 4966 : loss : 0.034655, loss_ce: 0.010032
2022-01-06 15:53:50,727 iteration 4967 : loss : 0.017776, loss_ce: 0.007213
2022-01-06 15:53:51,916 iteration 4968 : loss : 0.031058, loss_ce: 0.012741
2022-01-06 15:53:53,068 iteration 4969 : loss : 0.023088, loss_ce: 0.005577
2022-01-06 15:53:54,246 iteration 4970 : loss : 0.018174, loss_ce: 0.006859
2022-01-06 15:53:55,485 iteration 4971 : loss : 0.025298, loss_ce: 0.010561
2022-01-06 15:53:56,749 iteration 4972 : loss : 0.025652, loss_ce: 0.008341
2022-01-06 15:53:57,903 iteration 4973 : loss : 0.022304, loss_ce: 0.009249
2022-01-06 15:53:59,056 iteration 4974 : loss : 0.022174, loss_ce: 0.008290
2022-01-06 15:54:00,210 iteration 4975 : loss : 0.041775, loss_ce: 0.014954
2022-01-06 15:54:01,366 iteration 4976 : loss : 0.033480, loss_ce: 0.013015
2022-01-06 15:54:02,656 iteration 4977 : loss : 0.028679, loss_ce: 0.015092
2022-01-06 15:54:03,836 iteration 4978 : loss : 0.041523, loss_ce: 0.012873
2022-01-06 15:54:05,027 iteration 4979 : loss : 0.026376, loss_ce: 0.013147
2022-01-06 15:54:06,097 iteration 4980 : loss : 0.019413, loss_ce: 0.007459
2022-01-06 15:54:07,279 iteration 4981 : loss : 0.028318, loss_ce: 0.009921
 73%|█████████████████████▏       | 293/400 [1:49:07<37:46, 21.18s/it]2022-01-06 15:54:08,502 iteration 4982 : loss : 0.029896, loss_ce: 0.013985
2022-01-06 15:54:09,637 iteration 4983 : loss : 0.025821, loss_ce: 0.008908
2022-01-06 15:54:10,889 iteration 4984 : loss : 0.028921, loss_ce: 0.011787
2022-01-06 15:54:12,088 iteration 4985 : loss : 0.021350, loss_ce: 0.006655
2022-01-06 15:54:13,374 iteration 4986 : loss : 0.032224, loss_ce: 0.014594
2022-01-06 15:54:14,524 iteration 4987 : loss : 0.024534, loss_ce: 0.010959
2022-01-06 15:54:15,691 iteration 4988 : loss : 0.026927, loss_ce: 0.011113
2022-01-06 15:54:16,845 iteration 4989 : loss : 0.023347, loss_ce: 0.008527
2022-01-06 15:54:18,129 iteration 4990 : loss : 0.019935, loss_ce: 0.006498
2022-01-06 15:54:19,322 iteration 4991 : loss : 0.029795, loss_ce: 0.008146
2022-01-06 15:54:20,513 iteration 4992 : loss : 0.030699, loss_ce: 0.015024
2022-01-06 15:54:21,777 iteration 4993 : loss : 0.025693, loss_ce: 0.011999
2022-01-06 15:54:23,067 iteration 4994 : loss : 0.036926, loss_ce: 0.008409
2022-01-06 15:54:24,246 iteration 4995 : loss : 0.027401, loss_ce: 0.009399
2022-01-06 15:54:25,424 iteration 4996 : loss : 0.039125, loss_ce: 0.014375
2022-01-06 15:54:26,704 iteration 4997 : loss : 0.045354, loss_ce: 0.019171
2022-01-06 15:54:27,830 iteration 4998 : loss : 0.022129, loss_ce: 0.008553
 74%|█████████████████████▎       | 294/400 [1:49:28<37:05, 20.99s/it]2022-01-06 15:54:29,136 iteration 4999 : loss : 0.028088, loss_ce: 0.010391
2022-01-06 15:54:30,257 iteration 5000 : loss : 0.023739, loss_ce: 0.009028
2022-01-06 15:54:31,414 iteration 5001 : loss : 0.050921, loss_ce: 0.015645
2022-01-06 15:54:32,636 iteration 5002 : loss : 0.028854, loss_ce: 0.013286
2022-01-06 15:54:33,809 iteration 5003 : loss : 0.037155, loss_ce: 0.010817
2022-01-06 15:54:35,080 iteration 5004 : loss : 0.033080, loss_ce: 0.012971
2022-01-06 15:54:36,227 iteration 5005 : loss : 0.027584, loss_ce: 0.010180
2022-01-06 15:54:37,461 iteration 5006 : loss : 0.024175, loss_ce: 0.006666
2022-01-06 15:54:38,613 iteration 5007 : loss : 0.019903, loss_ce: 0.007291
2022-01-06 15:54:39,726 iteration 5008 : loss : 0.031699, loss_ce: 0.011696
2022-01-06 15:54:40,914 iteration 5009 : loss : 0.030301, loss_ce: 0.010277
2022-01-06 15:54:42,224 iteration 5010 : loss : 0.046501, loss_ce: 0.017122
2022-01-06 15:54:43,428 iteration 5011 : loss : 0.023255, loss_ce: 0.013459
2022-01-06 15:54:44,685 iteration 5012 : loss : 0.037685, loss_ce: 0.015265
2022-01-06 15:54:46,056 iteration 5013 : loss : 0.032332, loss_ce: 0.013447
2022-01-06 15:54:47,141 iteration 5014 : loss : 0.024513, loss_ce: 0.009117
2022-01-06 15:54:47,141 Training Data Eval:
2022-01-06 15:54:52,984   Average segmentation loss on training set: 0.0187
2022-01-06 15:54:52,984 Validation Data Eval:
2022-01-06 15:54:54,980   Average segmentation loss on validation set: 0.0777
2022-01-06 15:54:56,150 iteration 5015 : loss : 0.019491, loss_ce: 0.004279
 74%|█████████████████████▍       | 295/400 [1:49:56<40:34, 23.19s/it]2022-01-06 15:54:57,344 iteration 5016 : loss : 0.031672, loss_ce: 0.012496
2022-01-06 15:54:58,502 iteration 5017 : loss : 0.025734, loss_ce: 0.007763
2022-01-06 15:54:59,867 iteration 5018 : loss : 0.030871, loss_ce: 0.011518
2022-01-06 15:55:00,985 iteration 5019 : loss : 0.023211, loss_ce: 0.006617
2022-01-06 15:55:02,189 iteration 5020 : loss : 0.022401, loss_ce: 0.009095
2022-01-06 15:55:03,451 iteration 5021 : loss : 0.032260, loss_ce: 0.011520
2022-01-06 15:55:04,702 iteration 5022 : loss : 0.031058, loss_ce: 0.014034
2022-01-06 15:55:05,925 iteration 5023 : loss : 0.025834, loss_ce: 0.010091
2022-01-06 15:55:07,091 iteration 5024 : loss : 0.020936, loss_ce: 0.008306
2022-01-06 15:55:08,234 iteration 5025 : loss : 0.021092, loss_ce: 0.008515
2022-01-06 15:55:09,434 iteration 5026 : loss : 0.031197, loss_ce: 0.012872
2022-01-06 15:55:10,627 iteration 5027 : loss : 0.017891, loss_ce: 0.005673
2022-01-06 15:55:11,833 iteration 5028 : loss : 0.024379, loss_ce: 0.010116
2022-01-06 15:55:12,964 iteration 5029 : loss : 0.022545, loss_ce: 0.009042
2022-01-06 15:55:14,169 iteration 5030 : loss : 0.031627, loss_ce: 0.010730
2022-01-06 15:55:15,425 iteration 5031 : loss : 0.029242, loss_ce: 0.011723
2022-01-06 15:55:16,683 iteration 5032 : loss : 0.039462, loss_ce: 0.010547
 74%|█████████████████████▍       | 296/400 [1:50:17<38:49, 22.40s/it]2022-01-06 15:55:17,927 iteration 5033 : loss : 0.025420, loss_ce: 0.013028
2022-01-06 15:55:19,087 iteration 5034 : loss : 0.028899, loss_ce: 0.007409
2022-01-06 15:55:20,244 iteration 5035 : loss : 0.021521, loss_ce: 0.007536
2022-01-06 15:55:21,362 iteration 5036 : loss : 0.019710, loss_ce: 0.007210
2022-01-06 15:55:22,525 iteration 5037 : loss : 0.020377, loss_ce: 0.008276
2022-01-06 15:55:23,876 iteration 5038 : loss : 0.027492, loss_ce: 0.012314
2022-01-06 15:55:25,028 iteration 5039 : loss : 0.030457, loss_ce: 0.010028
2022-01-06 15:55:26,255 iteration 5040 : loss : 0.031615, loss_ce: 0.012695
2022-01-06 15:55:27,439 iteration 5041 : loss : 0.024082, loss_ce: 0.008270
2022-01-06 15:55:28,625 iteration 5042 : loss : 0.022035, loss_ce: 0.008625
2022-01-06 15:55:29,783 iteration 5043 : loss : 0.023014, loss_ce: 0.008726
2022-01-06 15:55:30,983 iteration 5044 : loss : 0.019940, loss_ce: 0.006841
2022-01-06 15:55:32,064 iteration 5045 : loss : 0.018970, loss_ce: 0.006022
2022-01-06 15:55:33,257 iteration 5046 : loss : 0.022032, loss_ce: 0.007899
2022-01-06 15:55:34,402 iteration 5047 : loss : 0.022819, loss_ce: 0.008807
2022-01-06 15:55:35,532 iteration 5048 : loss : 0.018581, loss_ce: 0.007533
2022-01-06 15:55:36,810 iteration 5049 : loss : 0.027898, loss_ce: 0.013870
 74%|█████████████████████▌       | 297/400 [1:50:37<37:16, 21.71s/it]2022-01-06 15:55:38,041 iteration 5050 : loss : 0.024441, loss_ce: 0.006694
2022-01-06 15:55:39,149 iteration 5051 : loss : 0.020028, loss_ce: 0.007815
2022-01-06 15:55:40,368 iteration 5052 : loss : 0.023148, loss_ce: 0.011083
2022-01-06 15:55:41,480 iteration 5053 : loss : 0.021767, loss_ce: 0.009107
2022-01-06 15:55:42,676 iteration 5054 : loss : 0.018122, loss_ce: 0.009080
2022-01-06 15:55:43,816 iteration 5055 : loss : 0.022310, loss_ce: 0.008624
2022-01-06 15:55:44,978 iteration 5056 : loss : 0.022130, loss_ce: 0.007260
2022-01-06 15:55:46,148 iteration 5057 : loss : 0.022472, loss_ce: 0.010011
2022-01-06 15:55:47,288 iteration 5058 : loss : 0.034983, loss_ce: 0.014976
2022-01-06 15:55:48,468 iteration 5059 : loss : 0.023752, loss_ce: 0.010770
2022-01-06 15:55:49,705 iteration 5060 : loss : 0.052222, loss_ce: 0.012849
2022-01-06 15:55:50,880 iteration 5061 : loss : 0.019764, loss_ce: 0.009311
2022-01-06 15:55:52,119 iteration 5062 : loss : 0.028246, loss_ce: 0.009353
2022-01-06 15:55:53,251 iteration 5063 : loss : 0.023559, loss_ce: 0.009496
2022-01-06 15:55:54,447 iteration 5064 : loss : 0.039079, loss_ce: 0.010731
2022-01-06 15:55:55,715 iteration 5065 : loss : 0.044587, loss_ce: 0.015740
2022-01-06 15:55:56,776 iteration 5066 : loss : 0.021640, loss_ce: 0.010003
 74%|█████████████████████▌       | 298/400 [1:50:57<36:00, 21.19s/it]2022-01-06 15:55:58,154 iteration 5067 : loss : 0.038750, loss_ce: 0.018588
2022-01-06 15:55:59,319 iteration 5068 : loss : 0.019072, loss_ce: 0.006786
2022-01-06 15:56:00,458 iteration 5069 : loss : 0.028989, loss_ce: 0.010294
2022-01-06 15:56:01,627 iteration 5070 : loss : 0.026624, loss_ce: 0.013409
2022-01-06 15:56:02,884 iteration 5071 : loss : 0.030774, loss_ce: 0.014242
2022-01-06 15:56:04,103 iteration 5072 : loss : 0.021822, loss_ce: 0.007078
2022-01-06 15:56:05,362 iteration 5073 : loss : 0.038581, loss_ce: 0.010116
2022-01-06 15:56:06,603 iteration 5074 : loss : 0.022341, loss_ce: 0.008458
2022-01-06 15:56:07,697 iteration 5075 : loss : 0.025956, loss_ce: 0.011929
2022-01-06 15:56:08,797 iteration 5076 : loss : 0.024494, loss_ce: 0.010412
2022-01-06 15:56:10,051 iteration 5077 : loss : 0.064366, loss_ce: 0.022708
2022-01-06 15:56:11,248 iteration 5078 : loss : 0.021157, loss_ce: 0.007698
2022-01-06 15:56:12,412 iteration 5079 : loss : 0.017031, loss_ce: 0.006157
2022-01-06 15:56:13,615 iteration 5080 : loss : 0.022154, loss_ce: 0.009177
2022-01-06 15:56:14,813 iteration 5081 : loss : 0.023460, loss_ce: 0.010567
2022-01-06 15:56:15,954 iteration 5082 : loss : 0.028857, loss_ce: 0.009241
2022-01-06 15:56:17,102 iteration 5083 : loss : 0.028763, loss_ce: 0.011138
 75%|█████████████████████▋       | 299/400 [1:51:17<35:14, 20.93s/it]2022-01-06 15:56:18,359 iteration 5084 : loss : 0.044146, loss_ce: 0.026102
2022-01-06 15:56:19,649 iteration 5085 : loss : 0.030163, loss_ce: 0.011100
2022-01-06 15:56:20,742 iteration 5086 : loss : 0.017428, loss_ce: 0.005462
2022-01-06 15:56:21,942 iteration 5087 : loss : 0.028728, loss_ce: 0.007536
2022-01-06 15:56:23,050 iteration 5088 : loss : 0.015976, loss_ce: 0.007485
2022-01-06 15:56:24,209 iteration 5089 : loss : 0.022922, loss_ce: 0.008720
2022-01-06 15:56:25,419 iteration 5090 : loss : 0.027420, loss_ce: 0.010959
2022-01-06 15:56:26,640 iteration 5091 : loss : 0.037182, loss_ce: 0.012734
2022-01-06 15:56:27,809 iteration 5092 : loss : 0.031199, loss_ce: 0.007816
2022-01-06 15:56:28,990 iteration 5093 : loss : 0.019180, loss_ce: 0.006928
2022-01-06 15:56:30,183 iteration 5094 : loss : 0.020595, loss_ce: 0.006857
2022-01-06 15:56:31,377 iteration 5095 : loss : 0.025031, loss_ce: 0.012848
2022-01-06 15:56:32,641 iteration 5096 : loss : 0.030153, loss_ce: 0.008074
2022-01-06 15:56:33,868 iteration 5097 : loss : 0.031626, loss_ce: 0.013070
2022-01-06 15:56:35,045 iteration 5098 : loss : 0.025748, loss_ce: 0.010821
2022-01-06 15:56:36,285 iteration 5099 : loss : 0.025448, loss_ce: 0.009668
2022-01-06 15:56:36,285 Training Data Eval:
2022-01-06 15:56:42,166   Average segmentation loss on training set: 0.0157
2022-01-06 15:56:42,166 Validation Data Eval:
2022-01-06 15:56:44,192   Average segmentation loss on validation set: 0.0785
2022-01-06 15:56:45,362 iteration 5100 : loss : 0.026969, loss_ce: 0.008635
 75%|█████████████████████▊       | 300/400 [1:51:45<38:33, 23.13s/it]2022-01-06 15:56:46,652 iteration 5101 : loss : 0.025879, loss_ce: 0.006835
2022-01-06 15:56:47,931 iteration 5102 : loss : 0.024456, loss_ce: 0.012614
2022-01-06 15:56:49,069 iteration 5103 : loss : 0.021086, loss_ce: 0.007762
2022-01-06 15:56:50,254 iteration 5104 : loss : 0.018463, loss_ce: 0.006845
2022-01-06 15:56:51,416 iteration 5105 : loss : 0.025082, loss_ce: 0.008763
2022-01-06 15:56:52,606 iteration 5106 : loss : 0.041141, loss_ce: 0.019289
2022-01-06 15:56:53,810 iteration 5107 : loss : 0.023245, loss_ce: 0.009360
2022-01-06 15:56:55,023 iteration 5108 : loss : 0.026165, loss_ce: 0.011748
2022-01-06 15:56:56,201 iteration 5109 : loss : 0.021735, loss_ce: 0.008210
2022-01-06 15:56:57,273 iteration 5110 : loss : 0.024123, loss_ce: 0.009021
2022-01-06 15:56:58,498 iteration 5111 : loss : 0.031810, loss_ce: 0.009727
2022-01-06 15:56:59,635 iteration 5112 : loss : 0.019038, loss_ce: 0.008098
2022-01-06 15:57:00,933 iteration 5113 : loss : 0.032445, loss_ce: 0.011705
2022-01-06 15:57:02,081 iteration 5114 : loss : 0.024297, loss_ce: 0.010151
2022-01-06 15:57:03,251 iteration 5115 : loss : 0.025044, loss_ce: 0.013292
2022-01-06 15:57:04,476 iteration 5116 : loss : 0.029512, loss_ce: 0.011434
2022-01-06 15:57:05,692 iteration 5117 : loss : 0.025519, loss_ce: 0.009018
 75%|█████████████████████▊       | 301/400 [1:52:06<36:46, 22.29s/it]2022-01-06 15:57:06,972 iteration 5118 : loss : 0.022985, loss_ce: 0.008929
2022-01-06 15:57:08,173 iteration 5119 : loss : 0.032101, loss_ce: 0.012030
2022-01-06 15:57:09,470 iteration 5120 : loss : 0.024767, loss_ce: 0.007710
2022-01-06 15:57:10,611 iteration 5121 : loss : 0.022684, loss_ce: 0.008460
2022-01-06 15:57:11,776 iteration 5122 : loss : 0.032941, loss_ce: 0.009827
2022-01-06 15:57:12,971 iteration 5123 : loss : 0.021363, loss_ce: 0.010970
2022-01-06 15:57:14,123 iteration 5124 : loss : 0.024923, loss_ce: 0.006071
2022-01-06 15:57:15,341 iteration 5125 : loss : 0.024956, loss_ce: 0.008831
2022-01-06 15:57:16,503 iteration 5126 : loss : 0.027105, loss_ce: 0.009703
2022-01-06 15:57:17,716 iteration 5127 : loss : 0.031642, loss_ce: 0.010999
2022-01-06 15:57:19,026 iteration 5128 : loss : 0.032749, loss_ce: 0.011428
2022-01-06 15:57:20,295 iteration 5129 : loss : 0.034104, loss_ce: 0.011273
2022-01-06 15:57:21,414 iteration 5130 : loss : 0.019225, loss_ce: 0.007815
2022-01-06 15:57:22,638 iteration 5131 : loss : 0.027137, loss_ce: 0.008910
2022-01-06 15:57:23,846 iteration 5132 : loss : 0.025078, loss_ce: 0.011010
2022-01-06 15:57:24,981 iteration 5133 : loss : 0.018991, loss_ce: 0.007774
2022-01-06 15:57:26,114 iteration 5134 : loss : 0.018212, loss_ce: 0.008198
 76%|█████████████████████▉       | 302/400 [1:52:26<35:29, 21.73s/it]2022-01-06 15:57:27,361 iteration 5135 : loss : 0.022958, loss_ce: 0.008728
2022-01-06 15:57:28,645 iteration 5136 : loss : 0.027300, loss_ce: 0.011139
2022-01-06 15:57:29,925 iteration 5137 : loss : 0.025832, loss_ce: 0.008003
2022-01-06 15:57:31,222 iteration 5138 : loss : 0.025257, loss_ce: 0.012361
2022-01-06 15:57:32,327 iteration 5139 : loss : 0.015145, loss_ce: 0.004975
2022-01-06 15:57:33,586 iteration 5140 : loss : 0.023399, loss_ce: 0.010725
2022-01-06 15:57:34,708 iteration 5141 : loss : 0.022537, loss_ce: 0.011368
2022-01-06 15:57:36,011 iteration 5142 : loss : 0.024872, loss_ce: 0.008632
2022-01-06 15:57:37,137 iteration 5143 : loss : 0.018376, loss_ce: 0.007242
2022-01-06 15:57:38,255 iteration 5144 : loss : 0.020268, loss_ce: 0.008996
2022-01-06 15:57:39,470 iteration 5145 : loss : 0.031557, loss_ce: 0.011055
2022-01-06 15:57:40,806 iteration 5146 : loss : 0.040768, loss_ce: 0.012178
2022-01-06 15:57:41,990 iteration 5147 : loss : 0.020990, loss_ce: 0.012224
2022-01-06 15:57:43,210 iteration 5148 : loss : 0.025331, loss_ce: 0.008480
2022-01-06 15:57:44,365 iteration 5149 : loss : 0.024252, loss_ce: 0.009156
2022-01-06 15:57:45,511 iteration 5150 : loss : 0.023256, loss_ce: 0.008540
2022-01-06 15:57:46,617 iteration 5151 : loss : 0.017152, loss_ce: 0.006076
 76%|█████████████████████▉       | 303/400 [1:52:46<34:31, 21.36s/it]2022-01-06 15:57:47,906 iteration 5152 : loss : 0.031495, loss_ce: 0.009477
2022-01-06 15:57:49,019 iteration 5153 : loss : 0.017257, loss_ce: 0.008495
2022-01-06 15:57:50,263 iteration 5154 : loss : 0.026758, loss_ce: 0.010446
2022-01-06 15:57:51,373 iteration 5155 : loss : 0.015277, loss_ce: 0.004996
2022-01-06 15:57:52,534 iteration 5156 : loss : 0.025166, loss_ce: 0.010333
2022-01-06 15:57:53,782 iteration 5157 : loss : 0.028917, loss_ce: 0.009781
2022-01-06 15:57:54,982 iteration 5158 : loss : 0.024512, loss_ce: 0.010069
2022-01-06 15:57:56,209 iteration 5159 : loss : 0.034777, loss_ce: 0.010671
2022-01-06 15:57:57,450 iteration 5160 : loss : 0.025112, loss_ce: 0.010249
2022-01-06 15:57:58,682 iteration 5161 : loss : 0.048910, loss_ce: 0.018376
2022-01-06 15:57:59,808 iteration 5162 : loss : 0.023561, loss_ce: 0.007284
2022-01-06 15:58:00,971 iteration 5163 : loss : 0.032370, loss_ce: 0.012393
2022-01-06 15:58:02,098 iteration 5164 : loss : 0.023096, loss_ce: 0.012061
2022-01-06 15:58:03,317 iteration 5165 : loss : 0.020658, loss_ce: 0.006725
2022-01-06 15:58:04,536 iteration 5166 : loss : 0.024678, loss_ce: 0.010051
2022-01-06 15:58:05,674 iteration 5167 : loss : 0.019223, loss_ce: 0.008085
2022-01-06 15:58:06,925 iteration 5168 : loss : 0.027336, loss_ce: 0.011146
 76%|██████████████████████       | 304/400 [1:53:07<33:40, 21.04s/it]2022-01-06 15:58:08,114 iteration 5169 : loss : 0.026108, loss_ce: 0.010327
2022-01-06 15:58:09,295 iteration 5170 : loss : 0.024890, loss_ce: 0.011348
2022-01-06 15:58:10,439 iteration 5171 : loss : 0.033279, loss_ce: 0.008080
2022-01-06 15:58:11,642 iteration 5172 : loss : 0.025268, loss_ce: 0.008387
2022-01-06 15:58:12,795 iteration 5173 : loss : 0.022526, loss_ce: 0.010486
2022-01-06 15:58:14,017 iteration 5174 : loss : 0.025125, loss_ce: 0.009550
2022-01-06 15:58:15,298 iteration 5175 : loss : 0.028393, loss_ce: 0.009947
2022-01-06 15:58:16,475 iteration 5176 : loss : 0.029975, loss_ce: 0.010642
2022-01-06 15:58:17,800 iteration 5177 : loss : 0.036479, loss_ce: 0.015364
2022-01-06 15:58:18,968 iteration 5178 : loss : 0.026984, loss_ce: 0.005865
2022-01-06 15:58:20,222 iteration 5179 : loss : 0.026041, loss_ce: 0.011577
2022-01-06 15:58:21,418 iteration 5180 : loss : 0.025615, loss_ce: 0.010093
2022-01-06 15:58:22,658 iteration 5181 : loss : 0.028041, loss_ce: 0.011434
2022-01-06 15:58:23,899 iteration 5182 : loss : 0.036831, loss_ce: 0.013792
2022-01-06 15:58:25,142 iteration 5183 : loss : 0.024783, loss_ce: 0.007369
2022-01-06 15:58:26,443 iteration 5184 : loss : 0.021684, loss_ce: 0.011149
2022-01-06 15:58:26,443 Training Data Eval:
2022-01-06 15:58:32,359   Average segmentation loss on training set: 0.0170
2022-01-06 15:58:32,359 Validation Data Eval:
2022-01-06 15:58:34,373   Average segmentation loss on validation set: 0.0970
2022-01-06 15:58:35,599 iteration 5185 : loss : 0.022519, loss_ce: 0.010596
 76%|██████████████████████       | 305/400 [1:53:35<36:56, 23.34s/it]2022-01-06 15:58:36,865 iteration 5186 : loss : 0.028657, loss_ce: 0.012985
2022-01-06 15:58:38,123 iteration 5187 : loss : 0.022816, loss_ce: 0.008829
2022-01-06 15:58:39,241 iteration 5188 : loss : 0.027235, loss_ce: 0.008451
2022-01-06 15:58:40,440 iteration 5189 : loss : 0.026909, loss_ce: 0.012317
2022-01-06 15:58:41,547 iteration 5190 : loss : 0.019333, loss_ce: 0.008487
2022-01-06 15:58:42,688 iteration 5191 : loss : 0.030317, loss_ce: 0.012601
2022-01-06 15:58:43,937 iteration 5192 : loss : 0.026536, loss_ce: 0.008510
2022-01-06 15:58:45,141 iteration 5193 : loss : 0.030257, loss_ce: 0.015763
2022-01-06 15:58:46,323 iteration 5194 : loss : 0.021992, loss_ce: 0.008972
2022-01-06 15:58:47,536 iteration 5195 : loss : 0.023734, loss_ce: 0.007676
2022-01-06 15:58:48,878 iteration 5196 : loss : 0.027429, loss_ce: 0.008834
2022-01-06 15:58:49,949 iteration 5197 : loss : 0.022033, loss_ce: 0.006506
2022-01-06 15:58:51,192 iteration 5198 : loss : 0.035006, loss_ce: 0.012180
2022-01-06 15:58:52,442 iteration 5199 : loss : 0.021393, loss_ce: 0.008567
2022-01-06 15:58:53,672 iteration 5200 : loss : 0.032039, loss_ce: 0.010638
2022-01-06 15:58:54,844 iteration 5201 : loss : 0.018284, loss_ce: 0.005120
2022-01-06 15:58:56,038 iteration 5202 : loss : 0.021704, loss_ce: 0.008252
 76%|██████████████████████▏      | 306/400 [1:53:56<35:11, 22.47s/it]2022-01-06 15:58:57,188 iteration 5203 : loss : 0.017829, loss_ce: 0.005283
2022-01-06 15:58:58,413 iteration 5204 : loss : 0.022180, loss_ce: 0.008029
2022-01-06 15:58:59,627 iteration 5205 : loss : 0.023630, loss_ce: 0.008519
2022-01-06 15:59:00,758 iteration 5206 : loss : 0.019062, loss_ce: 0.008012
2022-01-06 15:59:01,937 iteration 5207 : loss : 0.018003, loss_ce: 0.006923
2022-01-06 15:59:03,215 iteration 5208 : loss : 0.030862, loss_ce: 0.011877
2022-01-06 15:59:04,417 iteration 5209 : loss : 0.023188, loss_ce: 0.009138
2022-01-06 15:59:05,578 iteration 5210 : loss : 0.019111, loss_ce: 0.006416
2022-01-06 15:59:06,719 iteration 5211 : loss : 0.021213, loss_ce: 0.006086
2022-01-06 15:59:07,918 iteration 5212 : loss : 0.020620, loss_ce: 0.006113
2022-01-06 15:59:09,140 iteration 5213 : loss : 0.027190, loss_ce: 0.010001
2022-01-06 15:59:10,314 iteration 5214 : loss : 0.022335, loss_ce: 0.012863
2022-01-06 15:59:11,474 iteration 5215 : loss : 0.026637, loss_ce: 0.009483
2022-01-06 15:59:12,650 iteration 5216 : loss : 0.021577, loss_ce: 0.008772
2022-01-06 15:59:13,774 iteration 5217 : loss : 0.019047, loss_ce: 0.006471
2022-01-06 15:59:14,943 iteration 5218 : loss : 0.019430, loss_ce: 0.006065
2022-01-06 15:59:16,154 iteration 5219 : loss : 0.026968, loss_ce: 0.012463
 77%|██████████████████████▎      | 307/400 [1:54:16<33:43, 21.76s/it]2022-01-06 15:59:17,487 iteration 5220 : loss : 0.031395, loss_ce: 0.013650
2022-01-06 15:59:18,629 iteration 5221 : loss : 0.018640, loss_ce: 0.007816
2022-01-06 15:59:19,716 iteration 5222 : loss : 0.024479, loss_ce: 0.007266
2022-01-06 15:59:20,883 iteration 5223 : loss : 0.019431, loss_ce: 0.007838
2022-01-06 15:59:22,159 iteration 5224 : loss : 0.030042, loss_ce: 0.011280
2022-01-06 15:59:23,262 iteration 5225 : loss : 0.016184, loss_ce: 0.004553
2022-01-06 15:59:24,589 iteration 5226 : loss : 0.032094, loss_ce: 0.011836
2022-01-06 15:59:25,666 iteration 5227 : loss : 0.018629, loss_ce: 0.007015
2022-01-06 15:59:26,760 iteration 5228 : loss : 0.017170, loss_ce: 0.007103
2022-01-06 15:59:27,891 iteration 5229 : loss : 0.023524, loss_ce: 0.009780
2022-01-06 15:59:29,101 iteration 5230 : loss : 0.022494, loss_ce: 0.011272
2022-01-06 15:59:30,322 iteration 5231 : loss : 0.029199, loss_ce: 0.008514
2022-01-06 15:59:31,455 iteration 5232 : loss : 0.022607, loss_ce: 0.009771
2022-01-06 15:59:32,671 iteration 5233 : loss : 0.026314, loss_ce: 0.011776
2022-01-06 15:59:33,738 iteration 5234 : loss : 0.014341, loss_ce: 0.005196
2022-01-06 15:59:35,020 iteration 5235 : loss : 0.030234, loss_ce: 0.012660
2022-01-06 15:59:36,187 iteration 5236 : loss : 0.024808, loss_ce: 0.009867
 77%|██████████████████████▎      | 308/400 [1:54:36<32:34, 21.24s/it]2022-01-06 15:59:37,456 iteration 5237 : loss : 0.026207, loss_ce: 0.009842
2022-01-06 15:59:38,629 iteration 5238 : loss : 0.020450, loss_ce: 0.008754
2022-01-06 15:59:39,803 iteration 5239 : loss : 0.031137, loss_ce: 0.008395
2022-01-06 15:59:41,091 iteration 5240 : loss : 0.025453, loss_ce: 0.008616
2022-01-06 15:59:42,305 iteration 5241 : loss : 0.024828, loss_ce: 0.010073
2022-01-06 15:59:43,512 iteration 5242 : loss : 0.029103, loss_ce: 0.011172
2022-01-06 15:59:44,704 iteration 5243 : loss : 0.022666, loss_ce: 0.007778
2022-01-06 15:59:45,918 iteration 5244 : loss : 0.037446, loss_ce: 0.013688
2022-01-06 15:59:47,115 iteration 5245 : loss : 0.030404, loss_ce: 0.012112
2022-01-06 15:59:48,190 iteration 5246 : loss : 0.016658, loss_ce: 0.007301
2022-01-06 15:59:49,379 iteration 5247 : loss : 0.028219, loss_ce: 0.009592
2022-01-06 15:59:50,609 iteration 5248 : loss : 0.020809, loss_ce: 0.007269
2022-01-06 15:59:51,770 iteration 5249 : loss : 0.022643, loss_ce: 0.009233
2022-01-06 15:59:52,946 iteration 5250 : loss : 0.023497, loss_ce: 0.011552
2022-01-06 15:59:54,187 iteration 5251 : loss : 0.018870, loss_ce: 0.007513
2022-01-06 15:59:55,345 iteration 5252 : loss : 0.022527, loss_ce: 0.007785
2022-01-06 15:59:56,564 iteration 5253 : loss : 0.020419, loss_ce: 0.007744
 77%|██████████████████████▍      | 309/400 [1:54:56<31:49, 20.98s/it]2022-01-06 15:59:57,727 iteration 5254 : loss : 0.022979, loss_ce: 0.008425
2022-01-06 15:59:58,978 iteration 5255 : loss : 0.024662, loss_ce: 0.007757
2022-01-06 16:00:00,195 iteration 5256 : loss : 0.018484, loss_ce: 0.007197
2022-01-06 16:00:01,477 iteration 5257 : loss : 0.030217, loss_ce: 0.007974
2022-01-06 16:00:02,599 iteration 5258 : loss : 0.028914, loss_ce: 0.013244
2022-01-06 16:00:03,810 iteration 5259 : loss : 0.033478, loss_ce: 0.008865
2022-01-06 16:00:04,958 iteration 5260 : loss : 0.020520, loss_ce: 0.008481
2022-01-06 16:00:06,138 iteration 5261 : loss : 0.022983, loss_ce: 0.009874
2022-01-06 16:00:07,261 iteration 5262 : loss : 0.014656, loss_ce: 0.006811
2022-01-06 16:00:08,465 iteration 5263 : loss : 0.020231, loss_ce: 0.007530
2022-01-06 16:00:09,630 iteration 5264 : loss : 0.020658, loss_ce: 0.006466
2022-01-06 16:00:10,746 iteration 5265 : loss : 0.019837, loss_ce: 0.006503
2022-01-06 16:00:11,959 iteration 5266 : loss : 0.025574, loss_ce: 0.010459
2022-01-06 16:00:13,092 iteration 5267 : loss : 0.023210, loss_ce: 0.008790
2022-01-06 16:00:14,373 iteration 5268 : loss : 0.029892, loss_ce: 0.011448
2022-01-06 16:00:15,679 iteration 5269 : loss : 0.028194, loss_ce: 0.014011
2022-01-06 16:00:15,679 Training Data Eval:
2022-01-06 16:00:21,628   Average segmentation loss on training set: 0.0156
2022-01-06 16:00:21,629 Validation Data Eval:
2022-01-06 16:00:23,653   Average segmentation loss on validation set: 0.0727
2022-01-06 16:00:24,911 iteration 5270 : loss : 0.034136, loss_ce: 0.015485
 78%|██████████████████████▍      | 310/400 [1:55:25<34:46, 23.19s/it]2022-01-06 16:00:26,208 iteration 5271 : loss : 0.023396, loss_ce: 0.010424
2022-01-06 16:00:27,401 iteration 5272 : loss : 0.025481, loss_ce: 0.010105
2022-01-06 16:00:28,581 iteration 5273 : loss : 0.026576, loss_ce: 0.011940
2022-01-06 16:00:29,718 iteration 5274 : loss : 0.025600, loss_ce: 0.009518
2022-01-06 16:00:30,946 iteration 5275 : loss : 0.030107, loss_ce: 0.009113
2022-01-06 16:00:32,165 iteration 5276 : loss : 0.028464, loss_ce: 0.009221
2022-01-06 16:00:33,266 iteration 5277 : loss : 0.025700, loss_ce: 0.007169
2022-01-06 16:00:34,602 iteration 5278 : loss : 0.026529, loss_ce: 0.012204
2022-01-06 16:00:35,771 iteration 5279 : loss : 0.021560, loss_ce: 0.006409
2022-01-06 16:00:37,032 iteration 5280 : loss : 0.028399, loss_ce: 0.011961
2022-01-06 16:00:38,224 iteration 5281 : loss : 0.021438, loss_ce: 0.011452
2022-01-06 16:00:39,452 iteration 5282 : loss : 0.025436, loss_ce: 0.009416
2022-01-06 16:00:40,655 iteration 5283 : loss : 0.026598, loss_ce: 0.008901
2022-01-06 16:00:41,894 iteration 5284 : loss : 0.020931, loss_ce: 0.009005
2022-01-06 16:00:43,123 iteration 5285 : loss : 0.030434, loss_ce: 0.016865
2022-01-06 16:00:44,389 iteration 5286 : loss : 0.017758, loss_ce: 0.006561
2022-01-06 16:00:45,714 iteration 5287 : loss : 0.029202, loss_ce: 0.013670
 78%|██████████████████████▌      | 311/400 [1:55:46<33:20, 22.48s/it]2022-01-06 16:00:46,924 iteration 5288 : loss : 0.021320, loss_ce: 0.008126
2022-01-06 16:00:48,138 iteration 5289 : loss : 0.021480, loss_ce: 0.008002
2022-01-06 16:00:49,401 iteration 5290 : loss : 0.027669, loss_ce: 0.011298
2022-01-06 16:00:50,664 iteration 5291 : loss : 0.020123, loss_ce: 0.008697
2022-01-06 16:00:51,894 iteration 5292 : loss : 0.017533, loss_ce: 0.006151
2022-01-06 16:00:53,052 iteration 5293 : loss : 0.028686, loss_ce: 0.008451
2022-01-06 16:00:54,325 iteration 5294 : loss : 0.025228, loss_ce: 0.011517
2022-01-06 16:00:55,466 iteration 5295 : loss : 0.014663, loss_ce: 0.005777
2022-01-06 16:00:56,659 iteration 5296 : loss : 0.025654, loss_ce: 0.008394
2022-01-06 16:00:57,940 iteration 5297 : loss : 0.028935, loss_ce: 0.009483
2022-01-06 16:00:59,061 iteration 5298 : loss : 0.018624, loss_ce: 0.007857
2022-01-06 16:01:00,322 iteration 5299 : loss : 0.027311, loss_ce: 0.011817
2022-01-06 16:01:01,419 iteration 5300 : loss : 0.020945, loss_ce: 0.008144
2022-01-06 16:01:02,616 iteration 5301 : loss : 0.026200, loss_ce: 0.008943
2022-01-06 16:01:03,762 iteration 5302 : loss : 0.020351, loss_ce: 0.008778
2022-01-06 16:01:04,827 iteration 5303 : loss : 0.019770, loss_ce: 0.007274
2022-01-06 16:01:06,081 iteration 5304 : loss : 0.025206, loss_ce: 0.010734
 78%|██████████████████████▌      | 312/400 [1:56:06<32:02, 21.84s/it]2022-01-06 16:01:07,334 iteration 5305 : loss : 0.020989, loss_ce: 0.007389
2022-01-06 16:01:08,541 iteration 5306 : loss : 0.035546, loss_ce: 0.011283
2022-01-06 16:01:09,716 iteration 5307 : loss : 0.020963, loss_ce: 0.008684
2022-01-06 16:01:10,942 iteration 5308 : loss : 0.024868, loss_ce: 0.010106
2022-01-06 16:01:12,126 iteration 5309 : loss : 0.019573, loss_ce: 0.009518
2022-01-06 16:01:13,318 iteration 5310 : loss : 0.021131, loss_ce: 0.009439
2022-01-06 16:01:14,518 iteration 5311 : loss : 0.019763, loss_ce: 0.007122
2022-01-06 16:01:15,689 iteration 5312 : loss : 0.022864, loss_ce: 0.008538
2022-01-06 16:01:16,913 iteration 5313 : loss : 0.033443, loss_ce: 0.011218
2022-01-06 16:01:18,043 iteration 5314 : loss : 0.018998, loss_ce: 0.008003
2022-01-06 16:01:19,275 iteration 5315 : loss : 0.024339, loss_ce: 0.009914
2022-01-06 16:01:20,356 iteration 5316 : loss : 0.016154, loss_ce: 0.007767
2022-01-06 16:01:21,563 iteration 5317 : loss : 0.024102, loss_ce: 0.008395
2022-01-06 16:01:22,789 iteration 5318 : loss : 0.028243, loss_ce: 0.010222
2022-01-06 16:01:24,024 iteration 5319 : loss : 0.031300, loss_ce: 0.010995
2022-01-06 16:01:25,262 iteration 5320 : loss : 0.027706, loss_ce: 0.011259
2022-01-06 16:01:26,386 iteration 5321 : loss : 0.022841, loss_ce: 0.004632
 78%|██████████████████████▋      | 313/400 [1:56:26<31:00, 21.38s/it]2022-01-06 16:01:27,693 iteration 5322 : loss : 0.025987, loss_ce: 0.010849
2022-01-06 16:01:28,960 iteration 5323 : loss : 0.025414, loss_ce: 0.009128
2022-01-06 16:01:30,152 iteration 5324 : loss : 0.020417, loss_ce: 0.008620
2022-01-06 16:01:31,349 iteration 5325 : loss : 0.034253, loss_ce: 0.011337
2022-01-06 16:01:32,640 iteration 5326 : loss : 0.025416, loss_ce: 0.006747
2022-01-06 16:01:33,851 iteration 5327 : loss : 0.023819, loss_ce: 0.009343
2022-01-06 16:01:35,023 iteration 5328 : loss : 0.021783, loss_ce: 0.008235
2022-01-06 16:01:36,208 iteration 5329 : loss : 0.020036, loss_ce: 0.007282
2022-01-06 16:01:37,384 iteration 5330 : loss : 0.024443, loss_ce: 0.010652
2022-01-06 16:01:38,492 iteration 5331 : loss : 0.018257, loss_ce: 0.007336
2022-01-06 16:01:39,889 iteration 5332 : loss : 0.039359, loss_ce: 0.017364
2022-01-06 16:01:41,054 iteration 5333 : loss : 0.029321, loss_ce: 0.010668
2022-01-06 16:01:42,124 iteration 5334 : loss : 0.026071, loss_ce: 0.007258
2022-01-06 16:01:43,325 iteration 5335 : loss : 0.024233, loss_ce: 0.007725
2022-01-06 16:01:44,497 iteration 5336 : loss : 0.025899, loss_ce: 0.009437
2022-01-06 16:01:45,694 iteration 5337 : loss : 0.022299, loss_ce: 0.008525
2022-01-06 16:01:46,902 iteration 5338 : loss : 0.032649, loss_ce: 0.011238
 78%|██████████████████████▊      | 314/400 [1:56:47<30:16, 21.12s/it]2022-01-06 16:01:48,134 iteration 5339 : loss : 0.023686, loss_ce: 0.008946
2022-01-06 16:01:49,285 iteration 5340 : loss : 0.018126, loss_ce: 0.006701
2022-01-06 16:01:50,551 iteration 5341 : loss : 0.022733, loss_ce: 0.005141
2022-01-06 16:01:51,776 iteration 5342 : loss : 0.022091, loss_ce: 0.006616
2022-01-06 16:01:52,974 iteration 5343 : loss : 0.022007, loss_ce: 0.009387
2022-01-06 16:01:54,102 iteration 5344 : loss : 0.019861, loss_ce: 0.008385
2022-01-06 16:01:55,258 iteration 5345 : loss : 0.016983, loss_ce: 0.006681
2022-01-06 16:01:56,398 iteration 5346 : loss : 0.032910, loss_ce: 0.013075
2022-01-06 16:01:57,603 iteration 5347 : loss : 0.017014, loss_ce: 0.007144
2022-01-06 16:01:58,737 iteration 5348 : loss : 0.025907, loss_ce: 0.009299
2022-01-06 16:01:59,885 iteration 5349 : loss : 0.023434, loss_ce: 0.007749
2022-01-06 16:02:01,058 iteration 5350 : loss : 0.024006, loss_ce: 0.007948
2022-01-06 16:02:02,178 iteration 5351 : loss : 0.018108, loss_ce: 0.006360
2022-01-06 16:02:03,430 iteration 5352 : loss : 0.023817, loss_ce: 0.009813
2022-01-06 16:02:04,616 iteration 5353 : loss : 0.024897, loss_ce: 0.009844
2022-01-06 16:02:05,728 iteration 5354 : loss : 0.020931, loss_ce: 0.009056
2022-01-06 16:02:05,728 Training Data Eval:
2022-01-06 16:02:11,529   Average segmentation loss on training set: 0.0156
2022-01-06 16:02:11,530 Validation Data Eval:
2022-01-06 16:02:13,520   Average segmentation loss on validation set: 0.0791
2022-01-06 16:02:14,718 iteration 5355 : loss : 0.023253, loss_ce: 0.010717
 79%|██████████████████████▊      | 315/400 [1:57:15<32:46, 23.13s/it]2022-01-06 16:02:16,015 iteration 5356 : loss : 0.025686, loss_ce: 0.010223
2022-01-06 16:02:17,141 iteration 5357 : loss : 0.014958, loss_ce: 0.005415
2022-01-06 16:02:18,255 iteration 5358 : loss : 0.020908, loss_ce: 0.008438
2022-01-06 16:02:19,385 iteration 5359 : loss : 0.017385, loss_ce: 0.005898
2022-01-06 16:02:20,620 iteration 5360 : loss : 0.031309, loss_ce: 0.013641
2022-01-06 16:02:21,777 iteration 5361 : loss : 0.023514, loss_ce: 0.010584
2022-01-06 16:02:22,990 iteration 5362 : loss : 0.019418, loss_ce: 0.007316
2022-01-06 16:02:24,352 iteration 5363 : loss : 0.028822, loss_ce: 0.012198
2022-01-06 16:02:25,517 iteration 5364 : loss : 0.025770, loss_ce: 0.007220
2022-01-06 16:02:26,715 iteration 5365 : loss : 0.026832, loss_ce: 0.007959
2022-01-06 16:02:27,874 iteration 5366 : loss : 0.022017, loss_ce: 0.008151
2022-01-06 16:02:28,961 iteration 5367 : loss : 0.020342, loss_ce: 0.007591
2022-01-06 16:02:30,286 iteration 5368 : loss : 0.026249, loss_ce: 0.009716
2022-01-06 16:02:31,434 iteration 5369 : loss : 0.015522, loss_ce: 0.006429
2022-01-06 16:02:32,585 iteration 5370 : loss : 0.019021, loss_ce: 0.007873
2022-01-06 16:02:33,755 iteration 5371 : loss : 0.015786, loss_ce: 0.004569
2022-01-06 16:02:34,846 iteration 5372 : loss : 0.019575, loss_ce: 0.007891
 79%|██████████████████████▉      | 316/400 [1:57:35<31:07, 22.23s/it]2022-01-06 16:02:36,082 iteration 5373 : loss : 0.022935, loss_ce: 0.009252
2022-01-06 16:02:37,235 iteration 5374 : loss : 0.026016, loss_ce: 0.007360
2022-01-06 16:02:38,427 iteration 5375 : loss : 0.018432, loss_ce: 0.007374
2022-01-06 16:02:39,611 iteration 5376 : loss : 0.021509, loss_ce: 0.007929
2022-01-06 16:02:40,808 iteration 5377 : loss : 0.028525, loss_ce: 0.007474
2022-01-06 16:02:42,009 iteration 5378 : loss : 0.027424, loss_ce: 0.008071
2022-01-06 16:02:43,177 iteration 5379 : loss : 0.018020, loss_ce: 0.006429
2022-01-06 16:02:44,282 iteration 5380 : loss : 0.019315, loss_ce: 0.009054
2022-01-06 16:02:45,550 iteration 5381 : loss : 0.020419, loss_ce: 0.008395
2022-01-06 16:02:46,728 iteration 5382 : loss : 0.018999, loss_ce: 0.007522
2022-01-06 16:02:47,888 iteration 5383 : loss : 0.034538, loss_ce: 0.017531
2022-01-06 16:02:49,031 iteration 5384 : loss : 0.016056, loss_ce: 0.008680
2022-01-06 16:02:50,232 iteration 5385 : loss : 0.025675, loss_ce: 0.009147
2022-01-06 16:02:51,373 iteration 5386 : loss : 0.025932, loss_ce: 0.009183
2022-01-06 16:02:52,659 iteration 5387 : loss : 0.022912, loss_ce: 0.007690
2022-01-06 16:02:53,879 iteration 5388 : loss : 0.017658, loss_ce: 0.005163
2022-01-06 16:02:55,152 iteration 5389 : loss : 0.025830, loss_ce: 0.010929
 79%|██████████████████████▉      | 317/400 [1:57:55<29:57, 21.65s/it]2022-01-06 16:02:56,358 iteration 5390 : loss : 0.017218, loss_ce: 0.005815
2022-01-06 16:02:57,644 iteration 5391 : loss : 0.027787, loss_ce: 0.009874
2022-01-06 16:02:58,804 iteration 5392 : loss : 0.028074, loss_ce: 0.008683
2022-01-06 16:02:59,931 iteration 5393 : loss : 0.019245, loss_ce: 0.006890
2022-01-06 16:03:01,096 iteration 5394 : loss : 0.022201, loss_ce: 0.006016
2022-01-06 16:03:02,265 iteration 5395 : loss : 0.019790, loss_ce: 0.008161
2022-01-06 16:03:03,399 iteration 5396 : loss : 0.023170, loss_ce: 0.008550
2022-01-06 16:03:04,623 iteration 5397 : loss : 0.021719, loss_ce: 0.008948
2022-01-06 16:03:05,964 iteration 5398 : loss : 0.028744, loss_ce: 0.009539
2022-01-06 16:03:07,188 iteration 5399 : loss : 0.028330, loss_ce: 0.010796
2022-01-06 16:03:08,326 iteration 5400 : loss : 0.026133, loss_ce: 0.013059
2022-01-06 16:03:09,534 iteration 5401 : loss : 0.029670, loss_ce: 0.015755
2022-01-06 16:03:10,678 iteration 5402 : loss : 0.019813, loss_ce: 0.009667
2022-01-06 16:03:11,863 iteration 5403 : loss : 0.019868, loss_ce: 0.008057
2022-01-06 16:03:13,074 iteration 5404 : loss : 0.019448, loss_ce: 0.008162
2022-01-06 16:03:14,380 iteration 5405 : loss : 0.029773, loss_ce: 0.011686
2022-01-06 16:03:15,502 iteration 5406 : loss : 0.017403, loss_ce: 0.008113
 80%|███████████████████████      | 318/400 [1:58:15<29:03, 21.26s/it]2022-01-06 16:03:16,708 iteration 5407 : loss : 0.018057, loss_ce: 0.007499
2022-01-06 16:03:17,859 iteration 5408 : loss : 0.021237, loss_ce: 0.008243
2022-01-06 16:03:18,952 iteration 5409 : loss : 0.019575, loss_ce: 0.006801
2022-01-06 16:03:20,157 iteration 5410 : loss : 0.019360, loss_ce: 0.008117
2022-01-06 16:03:21,284 iteration 5411 : loss : 0.026583, loss_ce: 0.009523
2022-01-06 16:03:22,481 iteration 5412 : loss : 0.020390, loss_ce: 0.007808
2022-01-06 16:03:23,624 iteration 5413 : loss : 0.030894, loss_ce: 0.011050
2022-01-06 16:03:24,817 iteration 5414 : loss : 0.016361, loss_ce: 0.006198
2022-01-06 16:03:26,123 iteration 5415 : loss : 0.029972, loss_ce: 0.011311
2022-01-06 16:03:27,260 iteration 5416 : loss : 0.024779, loss_ce: 0.011045
2022-01-06 16:03:28,434 iteration 5417 : loss : 0.020572, loss_ce: 0.006233
2022-01-06 16:03:29,700 iteration 5418 : loss : 0.027787, loss_ce: 0.012152
2022-01-06 16:03:30,800 iteration 5419 : loss : 0.028161, loss_ce: 0.013110
2022-01-06 16:03:31,912 iteration 5420 : loss : 0.017779, loss_ce: 0.007381
2022-01-06 16:03:33,140 iteration 5421 : loss : 0.038157, loss_ce: 0.014883
2022-01-06 16:03:34,387 iteration 5422 : loss : 0.041759, loss_ce: 0.011597
2022-01-06 16:03:35,614 iteration 5423 : loss : 0.022005, loss_ce: 0.009321
 80%|███████████████████████▏     | 319/400 [1:58:35<28:14, 20.92s/it]2022-01-06 16:03:36,843 iteration 5424 : loss : 0.037785, loss_ce: 0.012795
2022-01-06 16:03:37,983 iteration 5425 : loss : 0.027676, loss_ce: 0.007229
2022-01-06 16:03:39,242 iteration 5426 : loss : 0.025784, loss_ce: 0.012534
2022-01-06 16:03:40,334 iteration 5427 : loss : 0.021534, loss_ce: 0.006686
2022-01-06 16:03:41,476 iteration 5428 : loss : 0.015654, loss_ce: 0.006045
2022-01-06 16:03:42,632 iteration 5429 : loss : 0.027027, loss_ce: 0.010780
2022-01-06 16:03:43,938 iteration 5430 : loss : 0.027918, loss_ce: 0.010720
2022-01-06 16:03:45,095 iteration 5431 : loss : 0.031134, loss_ce: 0.011962
2022-01-06 16:03:46,247 iteration 5432 : loss : 0.023414, loss_ce: 0.011861
2022-01-06 16:03:47,489 iteration 5433 : loss : 0.018199, loss_ce: 0.007093
2022-01-06 16:03:48,639 iteration 5434 : loss : 0.018981, loss_ce: 0.006907
2022-01-06 16:03:49,942 iteration 5435 : loss : 0.028105, loss_ce: 0.014489
2022-01-06 16:03:51,109 iteration 5436 : loss : 0.017923, loss_ce: 0.007137
2022-01-06 16:03:52,152 iteration 5437 : loss : 0.018538, loss_ce: 0.007499
2022-01-06 16:03:53,452 iteration 5438 : loss : 0.025583, loss_ce: 0.009486
2022-01-06 16:03:54,588 iteration 5439 : loss : 0.014603, loss_ce: 0.005498
2022-01-06 16:03:54,588 Training Data Eval:
2022-01-06 16:04:00,409   Average segmentation loss on training set: 0.0179
2022-01-06 16:04:00,409 Validation Data Eval:
2022-01-06 16:04:02,396   Average segmentation loss on validation set: 0.0826
2022-01-06 16:04:03,634 iteration 5440 : loss : 0.032663, loss_ce: 0.009385
 80%|███████████████████████▏     | 320/400 [1:59:03<30:43, 23.05s/it]2022-01-06 16:04:04,746 iteration 5441 : loss : 0.018989, loss_ce: 0.006214
2022-01-06 16:04:05,926 iteration 5442 : loss : 0.026470, loss_ce: 0.008640
2022-01-06 16:04:07,135 iteration 5443 : loss : 0.020370, loss_ce: 0.007400
2022-01-06 16:04:08,335 iteration 5444 : loss : 0.021975, loss_ce: 0.008275
2022-01-06 16:04:09,619 iteration 5445 : loss : 0.024130, loss_ce: 0.010471
2022-01-06 16:04:10,762 iteration 5446 : loss : 0.027226, loss_ce: 0.010558
2022-01-06 16:04:11,935 iteration 5447 : loss : 0.022263, loss_ce: 0.009125
2022-01-06 16:04:13,141 iteration 5448 : loss : 0.026561, loss_ce: 0.008952
2022-01-06 16:04:14,316 iteration 5449 : loss : 0.023005, loss_ce: 0.007701
2022-01-06 16:04:15,562 iteration 5450 : loss : 0.028853, loss_ce: 0.015606
2022-01-06 16:04:16,726 iteration 5451 : loss : 0.025878, loss_ce: 0.011518
2022-01-06 16:04:17,962 iteration 5452 : loss : 0.024754, loss_ce: 0.010288
2022-01-06 16:04:19,148 iteration 5453 : loss : 0.021609, loss_ce: 0.008176
2022-01-06 16:04:20,216 iteration 5454 : loss : 0.014918, loss_ce: 0.007093
2022-01-06 16:04:21,531 iteration 5455 : loss : 0.034452, loss_ce: 0.012036
2022-01-06 16:04:22,795 iteration 5456 : loss : 0.023691, loss_ce: 0.010212
2022-01-06 16:04:23,980 iteration 5457 : loss : 0.021392, loss_ce: 0.008468
 80%|███████████████████████▎     | 321/400 [1:59:24<29:16, 22.24s/it]2022-01-06 16:04:25,190 iteration 5458 : loss : 0.019216, loss_ce: 0.007513
2022-01-06 16:04:26,318 iteration 5459 : loss : 0.018881, loss_ce: 0.006926
2022-01-06 16:04:27,528 iteration 5460 : loss : 0.039206, loss_ce: 0.014299
2022-01-06 16:04:28,751 iteration 5461 : loss : 0.028002, loss_ce: 0.011437
2022-01-06 16:04:29,954 iteration 5462 : loss : 0.020327, loss_ce: 0.005702
2022-01-06 16:04:31,125 iteration 5463 : loss : 0.022115, loss_ce: 0.007950
2022-01-06 16:04:32,292 iteration 5464 : loss : 0.022973, loss_ce: 0.009266
2022-01-06 16:04:33,429 iteration 5465 : loss : 0.022365, loss_ce: 0.007330
2022-01-06 16:04:34,708 iteration 5466 : loss : 0.023452, loss_ce: 0.009804
2022-01-06 16:04:35,877 iteration 5467 : loss : 0.026653, loss_ce: 0.014658
2022-01-06 16:04:37,084 iteration 5468 : loss : 0.027315, loss_ce: 0.011423
2022-01-06 16:04:38,205 iteration 5469 : loss : 0.018380, loss_ce: 0.006135
2022-01-06 16:04:39,439 iteration 5470 : loss : 0.030703, loss_ce: 0.011114
2022-01-06 16:04:40,624 iteration 5471 : loss : 0.020426, loss_ce: 0.006618
2022-01-06 16:04:41,868 iteration 5472 : loss : 0.029316, loss_ce: 0.009885
2022-01-06 16:04:43,096 iteration 5473 : loss : 0.020175, loss_ce: 0.006727
2022-01-06 16:04:44,327 iteration 5474 : loss : 0.024585, loss_ce: 0.011489
 80%|███████████████████████▎     | 322/400 [1:59:44<28:10, 21.67s/it]2022-01-06 16:04:45,554 iteration 5475 : loss : 0.023328, loss_ce: 0.006258
2022-01-06 16:04:46,707 iteration 5476 : loss : 0.029929, loss_ce: 0.008899
2022-01-06 16:04:47,856 iteration 5477 : loss : 0.020301, loss_ce: 0.007769
2022-01-06 16:04:49,035 iteration 5478 : loss : 0.021419, loss_ce: 0.009749
2022-01-06 16:04:50,324 iteration 5479 : loss : 0.025147, loss_ce: 0.011715
2022-01-06 16:04:51,580 iteration 5480 : loss : 0.030701, loss_ce: 0.013565
2022-01-06 16:04:52,748 iteration 5481 : loss : 0.021435, loss_ce: 0.006762
2022-01-06 16:04:53,949 iteration 5482 : loss : 0.019012, loss_ce: 0.006761
2022-01-06 16:04:55,146 iteration 5483 : loss : 0.024752, loss_ce: 0.009691
2022-01-06 16:04:56,296 iteration 5484 : loss : 0.019760, loss_ce: 0.007943
2022-01-06 16:04:57,366 iteration 5485 : loss : 0.018671, loss_ce: 0.006453
2022-01-06 16:04:58,650 iteration 5486 : loss : 0.028436, loss_ce: 0.013499
2022-01-06 16:04:59,996 iteration 5487 : loss : 0.048262, loss_ce: 0.022518
2022-01-06 16:05:01,164 iteration 5488 : loss : 0.027290, loss_ce: 0.010242
2022-01-06 16:05:02,329 iteration 5489 : loss : 0.019459, loss_ce: 0.007753
2022-01-06 16:05:03,471 iteration 5490 : loss : 0.048529, loss_ce: 0.014607
2022-01-06 16:05:04,628 iteration 5491 : loss : 0.025412, loss_ce: 0.009777
 81%|███████████████████████▍     | 323/400 [2:00:04<27:16, 21.26s/it]2022-01-06 16:05:05,872 iteration 5492 : loss : 0.020268, loss_ce: 0.007847
2022-01-06 16:05:07,013 iteration 5493 : loss : 0.019598, loss_ce: 0.008498
2022-01-06 16:05:08,134 iteration 5494 : loss : 0.023790, loss_ce: 0.006598
2022-01-06 16:05:09,399 iteration 5495 : loss : 0.050786, loss_ce: 0.007723
2022-01-06 16:05:10,534 iteration 5496 : loss : 0.016518, loss_ce: 0.005742
2022-01-06 16:05:11,726 iteration 5497 : loss : 0.020046, loss_ce: 0.006240
2022-01-06 16:05:12,951 iteration 5498 : loss : 0.031126, loss_ce: 0.020798
2022-01-06 16:05:14,140 iteration 5499 : loss : 0.028332, loss_ce: 0.013841
2022-01-06 16:05:15,360 iteration 5500 : loss : 0.026155, loss_ce: 0.010585
2022-01-06 16:05:16,646 iteration 5501 : loss : 0.023751, loss_ce: 0.009858
2022-01-06 16:05:17,885 iteration 5502 : loss : 0.023885, loss_ce: 0.009136
2022-01-06 16:05:19,039 iteration 5503 : loss : 0.032883, loss_ce: 0.011133
2022-01-06 16:05:20,248 iteration 5504 : loss : 0.024500, loss_ce: 0.010290
2022-01-06 16:05:21,438 iteration 5505 : loss : 0.028004, loss_ce: 0.011549
2022-01-06 16:05:22,569 iteration 5506 : loss : 0.026523, loss_ce: 0.009938
2022-01-06 16:05:23,789 iteration 5507 : loss : 0.033468, loss_ce: 0.010110
2022-01-06 16:05:24,993 iteration 5508 : loss : 0.026229, loss_ce: 0.011042
 81%|███████████████████████▍     | 324/400 [2:00:25<26:35, 20.99s/it]2022-01-06 16:05:26,222 iteration 5509 : loss : 0.025909, loss_ce: 0.009469
2022-01-06 16:05:27,549 iteration 5510 : loss : 0.031894, loss_ce: 0.010987
2022-01-06 16:05:28,731 iteration 5511 : loss : 0.028019, loss_ce: 0.014638
2022-01-06 16:05:29,987 iteration 5512 : loss : 0.022676, loss_ce: 0.004291
2022-01-06 16:05:31,130 iteration 5513 : loss : 0.026553, loss_ce: 0.005666
2022-01-06 16:05:32,362 iteration 5514 : loss : 0.022671, loss_ce: 0.008985
2022-01-06 16:05:33,624 iteration 5515 : loss : 0.032247, loss_ce: 0.010616
2022-01-06 16:05:34,708 iteration 5516 : loss : 0.017204, loss_ce: 0.007057
2022-01-06 16:05:35,935 iteration 5517 : loss : 0.029734, loss_ce: 0.010312
2022-01-06 16:05:37,100 iteration 5518 : loss : 0.041053, loss_ce: 0.019243
2022-01-06 16:05:38,252 iteration 5519 : loss : 0.030380, loss_ce: 0.009840
2022-01-06 16:05:39,459 iteration 5520 : loss : 0.037424, loss_ce: 0.015298
2022-01-06 16:05:40,786 iteration 5521 : loss : 0.029194, loss_ce: 0.010397
2022-01-06 16:05:41,893 iteration 5522 : loss : 0.018672, loss_ce: 0.008753
2022-01-06 16:05:43,102 iteration 5523 : loss : 0.023530, loss_ce: 0.009738
2022-01-06 16:05:44,321 iteration 5524 : loss : 0.027466, loss_ce: 0.013203
2022-01-06 16:05:44,321 Training Data Eval:
2022-01-06 16:05:50,121   Average segmentation loss on training set: 0.0186
2022-01-06 16:05:50,122 Validation Data Eval:
2022-01-06 16:05:52,115   Average segmentation loss on validation set: 0.0825
2022-01-06 16:05:53,376 iteration 5525 : loss : 0.026678, loss_ce: 0.011763
 81%|███████████████████████▌     | 325/400 [2:00:53<29:00, 23.21s/it]2022-01-06 16:05:54,740 iteration 5526 : loss : 0.024340, loss_ce: 0.010660
2022-01-06 16:05:55,882 iteration 5527 : loss : 0.027463, loss_ce: 0.011246
2022-01-06 16:05:57,095 iteration 5528 : loss : 0.026472, loss_ce: 0.009470
2022-01-06 16:05:58,241 iteration 5529 : loss : 0.020019, loss_ce: 0.006683
2022-01-06 16:05:59,360 iteration 5530 : loss : 0.023310, loss_ce: 0.006919
2022-01-06 16:06:00,556 iteration 5531 : loss : 0.017858, loss_ce: 0.006595
2022-01-06 16:06:01,643 iteration 5532 : loss : 0.024548, loss_ce: 0.008919
2022-01-06 16:06:02,869 iteration 5533 : loss : 0.026906, loss_ce: 0.011243
2022-01-06 16:06:04,079 iteration 5534 : loss : 0.025414, loss_ce: 0.009028
2022-01-06 16:06:05,284 iteration 5535 : loss : 0.022832, loss_ce: 0.008916
2022-01-06 16:06:06,454 iteration 5536 : loss : 0.031383, loss_ce: 0.010063
2022-01-06 16:06:07,634 iteration 5537 : loss : 0.020298, loss_ce: 0.006657
2022-01-06 16:06:08,811 iteration 5538 : loss : 0.026719, loss_ce: 0.017094
2022-01-06 16:06:10,013 iteration 5539 : loss : 0.021696, loss_ce: 0.009351
2022-01-06 16:06:11,206 iteration 5540 : loss : 0.019552, loss_ce: 0.006888
2022-01-06 16:06:12,290 iteration 5541 : loss : 0.022058, loss_ce: 0.008874
2022-01-06 16:06:13,438 iteration 5542 : loss : 0.023639, loss_ce: 0.009829
 82%|███████████████████████▋     | 326/400 [2:01:13<27:27, 22.26s/it]2022-01-06 16:06:14,757 iteration 5543 : loss : 0.032488, loss_ce: 0.013063
2022-01-06 16:06:16,058 iteration 5544 : loss : 0.029070, loss_ce: 0.010214
2022-01-06 16:06:17,311 iteration 5545 : loss : 0.022335, loss_ce: 0.009138
2022-01-06 16:06:18,403 iteration 5546 : loss : 0.016240, loss_ce: 0.007697
2022-01-06 16:06:19,621 iteration 5547 : loss : 0.018815, loss_ce: 0.008304
2022-01-06 16:06:20,763 iteration 5548 : loss : 0.023779, loss_ce: 0.008868
2022-01-06 16:06:21,942 iteration 5549 : loss : 0.024212, loss_ce: 0.009789
2022-01-06 16:06:23,152 iteration 5550 : loss : 0.026819, loss_ce: 0.010826
2022-01-06 16:06:24,350 iteration 5551 : loss : 0.031956, loss_ce: 0.010484
2022-01-06 16:06:25,640 iteration 5552 : loss : 0.036100, loss_ce: 0.010660
2022-01-06 16:06:26,848 iteration 5553 : loss : 0.025543, loss_ce: 0.010337
2022-01-06 16:06:28,134 iteration 5554 : loss : 0.023456, loss_ce: 0.005710
2022-01-06 16:06:29,401 iteration 5555 : loss : 0.034979, loss_ce: 0.015925
2022-01-06 16:06:30,643 iteration 5556 : loss : 0.035274, loss_ce: 0.015817
2022-01-06 16:06:31,879 iteration 5557 : loss : 0.032021, loss_ce: 0.011261
2022-01-06 16:06:33,042 iteration 5558 : loss : 0.028814, loss_ce: 0.010346
2022-01-06 16:06:34,156 iteration 5559 : loss : 0.021543, loss_ce: 0.007188
 82%|███████████████████████▋     | 327/400 [2:01:34<26:31, 21.80s/it]2022-01-06 16:06:35,405 iteration 5560 : loss : 0.020789, loss_ce: 0.008909
2022-01-06 16:06:36,681 iteration 5561 : loss : 0.024509, loss_ce: 0.006261
2022-01-06 16:06:37,898 iteration 5562 : loss : 0.025189, loss_ce: 0.009025
2022-01-06 16:06:39,145 iteration 5563 : loss : 0.031185, loss_ce: 0.010584
2022-01-06 16:06:40,237 iteration 5564 : loss : 0.026702, loss_ce: 0.007863
2022-01-06 16:06:41,457 iteration 5565 : loss : 0.032763, loss_ce: 0.009083
2022-01-06 16:06:42,608 iteration 5566 : loss : 0.021344, loss_ce: 0.006651
2022-01-06 16:06:43,864 iteration 5567 : loss : 0.020954, loss_ce: 0.010476
2022-01-06 16:06:45,120 iteration 5568 : loss : 0.033320, loss_ce: 0.014396
2022-01-06 16:06:46,386 iteration 5569 : loss : 0.027295, loss_ce: 0.011650
2022-01-06 16:06:47,580 iteration 5570 : loss : 0.023659, loss_ce: 0.011423
2022-01-06 16:06:48,764 iteration 5571 : loss : 0.030035, loss_ce: 0.008953
2022-01-06 16:06:49,840 iteration 5572 : loss : 0.017614, loss_ce: 0.006485
2022-01-06 16:06:51,086 iteration 5573 : loss : 0.029566, loss_ce: 0.011693
2022-01-06 16:06:52,232 iteration 5574 : loss : 0.022499, loss_ce: 0.008358
2022-01-06 16:06:53,342 iteration 5575 : loss : 0.025661, loss_ce: 0.010762
2022-01-06 16:06:54,426 iteration 5576 : loss : 0.021233, loss_ce: 0.006541
 82%|███████████████████████▊     | 328/400 [2:01:54<25:36, 21.34s/it]2022-01-06 16:06:55,737 iteration 5577 : loss : 0.025173, loss_ce: 0.008846
2022-01-06 16:06:56,921 iteration 5578 : loss : 0.018095, loss_ce: 0.006187
2022-01-06 16:06:58,216 iteration 5579 : loss : 0.025861, loss_ce: 0.006423
2022-01-06 16:06:59,458 iteration 5580 : loss : 0.022382, loss_ce: 0.006886
2022-01-06 16:07:00,669 iteration 5581 : loss : 0.029063, loss_ce: 0.013763
2022-01-06 16:07:01,836 iteration 5582 : loss : 0.023456, loss_ce: 0.010773
2022-01-06 16:07:02,957 iteration 5583 : loss : 0.014948, loss_ce: 0.004555
2022-01-06 16:07:04,114 iteration 5584 : loss : 0.022329, loss_ce: 0.008046
2022-01-06 16:07:05,213 iteration 5585 : loss : 0.022678, loss_ce: 0.008260
2022-01-06 16:07:06,349 iteration 5586 : loss : 0.016994, loss_ce: 0.006698
2022-01-06 16:07:07,633 iteration 5587 : loss : 0.044588, loss_ce: 0.016796
2022-01-06 16:07:08,813 iteration 5588 : loss : 0.029246, loss_ce: 0.012797
2022-01-06 16:07:10,067 iteration 5589 : loss : 0.030577, loss_ce: 0.008785
2022-01-06 16:07:11,180 iteration 5590 : loss : 0.016973, loss_ce: 0.007378
2022-01-06 16:07:12,401 iteration 5591 : loss : 0.029228, loss_ce: 0.011031
2022-01-06 16:07:13,616 iteration 5592 : loss : 0.023389, loss_ce: 0.009785
2022-01-06 16:07:14,711 iteration 5593 : loss : 0.021102, loss_ce: 0.010721
 82%|███████████████████████▊     | 329/400 [2:02:15<24:52, 21.02s/it]2022-01-06 16:07:15,963 iteration 5594 : loss : 0.018538, loss_ce: 0.005354
2022-01-06 16:07:17,078 iteration 5595 : loss : 0.018630, loss_ce: 0.008039
2022-01-06 16:07:18,344 iteration 5596 : loss : 0.033267, loss_ce: 0.012529
2022-01-06 16:07:19,548 iteration 5597 : loss : 0.021029, loss_ce: 0.008759
2022-01-06 16:07:20,871 iteration 5598 : loss : 0.029420, loss_ce: 0.013574
2022-01-06 16:07:22,142 iteration 5599 : loss : 0.025585, loss_ce: 0.009950
2022-01-06 16:07:23,246 iteration 5600 : loss : 0.021009, loss_ce: 0.008786
2022-01-06 16:07:24,448 iteration 5601 : loss : 0.019356, loss_ce: 0.007117
2022-01-06 16:07:25,644 iteration 5602 : loss : 0.029361, loss_ce: 0.009893
2022-01-06 16:07:26,838 iteration 5603 : loss : 0.039758, loss_ce: 0.018871
2022-01-06 16:07:28,026 iteration 5604 : loss : 0.042225, loss_ce: 0.014784
2022-01-06 16:07:29,165 iteration 5605 : loss : 0.028617, loss_ce: 0.013118
2022-01-06 16:07:30,397 iteration 5606 : loss : 0.029871, loss_ce: 0.010597
2022-01-06 16:07:31,541 iteration 5607 : loss : 0.025643, loss_ce: 0.010794
2022-01-06 16:07:32,766 iteration 5608 : loss : 0.021567, loss_ce: 0.009948
2022-01-06 16:07:33,904 iteration 5609 : loss : 0.016636, loss_ce: 0.006097
2022-01-06 16:07:33,905 Training Data Eval:
2022-01-06 16:07:39,723   Average segmentation loss on training set: 0.0162
2022-01-06 16:07:39,724 Validation Data Eval:
2022-01-06 16:07:41,723   Average segmentation loss on validation set: 0.0652
2022-01-06 16:07:42,947 iteration 5610 : loss : 0.026897, loss_ce: 0.011863
 82%|███████████████████████▉     | 330/400 [2:02:43<27:03, 23.19s/it]2022-01-06 16:07:44,129 iteration 5611 : loss : 0.016528, loss_ce: 0.006297
2022-01-06 16:07:45,382 iteration 5612 : loss : 0.032207, loss_ce: 0.011416
2022-01-06 16:07:46,529 iteration 5613 : loss : 0.031791, loss_ce: 0.006213
2022-01-06 16:07:47,683 iteration 5614 : loss : 0.023280, loss_ce: 0.007587
2022-01-06 16:07:48,875 iteration 5615 : loss : 0.024280, loss_ce: 0.011181
2022-01-06 16:07:50,053 iteration 5616 : loss : 0.019150, loss_ce: 0.005719
2022-01-06 16:07:51,414 iteration 5617 : loss : 0.040150, loss_ce: 0.012427
2022-01-06 16:07:52,645 iteration 5618 : loss : 0.020525, loss_ce: 0.006922
2022-01-06 16:07:53,828 iteration 5619 : loss : 0.020681, loss_ce: 0.010806
2022-01-06 16:07:54,942 iteration 5620 : loss : 0.018976, loss_ce: 0.007386
2022-01-06 16:07:56,054 iteration 5621 : loss : 0.031533, loss_ce: 0.012901
2022-01-06 16:07:57,136 iteration 5622 : loss : 0.018352, loss_ce: 0.008537
2022-01-06 16:07:58,392 iteration 5623 : loss : 0.016449, loss_ce: 0.005689
2022-01-06 16:07:59,549 iteration 5624 : loss : 0.018678, loss_ce: 0.006990
2022-01-06 16:08:00,753 iteration 5625 : loss : 0.025465, loss_ce: 0.011704
2022-01-06 16:08:01,952 iteration 5626 : loss : 0.023571, loss_ce: 0.009298
2022-01-06 16:08:03,119 iteration 5627 : loss : 0.022767, loss_ce: 0.009627
 83%|███████████████████████▉     | 331/400 [2:03:03<25:37, 22.28s/it]2022-01-06 16:08:04,358 iteration 5628 : loss : 0.019407, loss_ce: 0.007086
2022-01-06 16:08:05,547 iteration 5629 : loss : 0.030953, loss_ce: 0.012529
2022-01-06 16:08:06,757 iteration 5630 : loss : 0.035415, loss_ce: 0.012845
2022-01-06 16:08:07,963 iteration 5631 : loss : 0.020816, loss_ce: 0.010079
2022-01-06 16:08:09,223 iteration 5632 : loss : 0.031035, loss_ce: 0.010610
2022-01-06 16:08:10,302 iteration 5633 : loss : 0.027726, loss_ce: 0.007954
2022-01-06 16:08:11,566 iteration 5634 : loss : 0.030880, loss_ce: 0.008749
2022-01-06 16:08:12,749 iteration 5635 : loss : 0.019041, loss_ce: 0.008235
2022-01-06 16:08:13,858 iteration 5636 : loss : 0.023401, loss_ce: 0.006681
2022-01-06 16:08:15,137 iteration 5637 : loss : 0.022652, loss_ce: 0.008983
2022-01-06 16:08:16,306 iteration 5638 : loss : 0.020762, loss_ce: 0.009573
2022-01-06 16:08:17,506 iteration 5639 : loss : 0.027770, loss_ce: 0.009321
2022-01-06 16:08:18,698 iteration 5640 : loss : 0.030011, loss_ce: 0.009830
2022-01-06 16:08:19,828 iteration 5641 : loss : 0.021803, loss_ce: 0.007479
2022-01-06 16:08:21,140 iteration 5642 : loss : 0.020881, loss_ce: 0.008729
2022-01-06 16:08:22,216 iteration 5643 : loss : 0.023775, loss_ce: 0.011382
2022-01-06 16:08:23,384 iteration 5644 : loss : 0.024113, loss_ce: 0.011036
 83%|████████████████████████     | 332/400 [2:03:23<24:34, 21.68s/it]2022-01-06 16:08:24,567 iteration 5645 : loss : 0.017246, loss_ce: 0.004971
2022-01-06 16:08:25,722 iteration 5646 : loss : 0.017084, loss_ce: 0.004586
2022-01-06 16:08:26,839 iteration 5647 : loss : 0.015747, loss_ce: 0.005116
2022-01-06 16:08:28,029 iteration 5648 : loss : 0.017212, loss_ce: 0.007503
2022-01-06 16:08:29,169 iteration 5649 : loss : 0.022931, loss_ce: 0.007739
2022-01-06 16:08:30,432 iteration 5650 : loss : 0.024392, loss_ce: 0.010490
2022-01-06 16:08:31,605 iteration 5651 : loss : 0.023268, loss_ce: 0.010403
2022-01-06 16:08:32,812 iteration 5652 : loss : 0.018184, loss_ce: 0.005865
2022-01-06 16:08:33,991 iteration 5653 : loss : 0.025069, loss_ce: 0.007076
2022-01-06 16:08:35,168 iteration 5654 : loss : 0.023727, loss_ce: 0.009454
2022-01-06 16:08:36,250 iteration 5655 : loss : 0.019109, loss_ce: 0.009134
2022-01-06 16:08:37,542 iteration 5656 : loss : 0.028357, loss_ce: 0.010157
2022-01-06 16:08:38,795 iteration 5657 : loss : 0.037681, loss_ce: 0.013670
2022-01-06 16:08:40,042 iteration 5658 : loss : 0.025620, loss_ce: 0.011618
2022-01-06 16:08:41,187 iteration 5659 : loss : 0.018482, loss_ce: 0.007513
2022-01-06 16:08:42,400 iteration 5660 : loss : 0.025904, loss_ce: 0.008791
2022-01-06 16:08:43,571 iteration 5661 : loss : 0.021453, loss_ce: 0.009248
 83%|████████████████████████▏    | 333/400 [2:03:43<23:42, 21.23s/it]2022-01-06 16:08:44,799 iteration 5662 : loss : 0.022494, loss_ce: 0.008125
2022-01-06 16:08:45,935 iteration 5663 : loss : 0.021011, loss_ce: 0.006182
2022-01-06 16:08:47,136 iteration 5664 : loss : 0.021180, loss_ce: 0.010160
2022-01-06 16:08:48,264 iteration 5665 : loss : 0.013395, loss_ce: 0.005680
2022-01-06 16:08:49,475 iteration 5666 : loss : 0.021048, loss_ce: 0.006397
2022-01-06 16:08:50,716 iteration 5667 : loss : 0.027207, loss_ce: 0.013749
2022-01-06 16:08:51,912 iteration 5668 : loss : 0.018671, loss_ce: 0.007031
2022-01-06 16:08:53,026 iteration 5669 : loss : 0.022610, loss_ce: 0.009257
2022-01-06 16:08:54,185 iteration 5670 : loss : 0.021512, loss_ce: 0.007666
2022-01-06 16:08:55,321 iteration 5671 : loss : 0.017705, loss_ce: 0.006692
2022-01-06 16:08:56,616 iteration 5672 : loss : 0.029338, loss_ce: 0.014079
2022-01-06 16:08:57,768 iteration 5673 : loss : 0.026703, loss_ce: 0.009561
2022-01-06 16:08:58,939 iteration 5674 : loss : 0.025277, loss_ce: 0.007221
2022-01-06 16:09:00,075 iteration 5675 : loss : 0.016828, loss_ce: 0.008093
2022-01-06 16:09:01,360 iteration 5676 : loss : 0.024008, loss_ce: 0.009743
2022-01-06 16:09:02,531 iteration 5677 : loss : 0.035586, loss_ce: 0.010687
2022-01-06 16:09:03,816 iteration 5678 : loss : 0.022805, loss_ce: 0.008500
 84%|████████████████████████▏    | 334/400 [2:04:04<23:01, 20.93s/it]2022-01-06 16:09:05,088 iteration 5679 : loss : 0.018198, loss_ce: 0.006376
2022-01-06 16:09:06,202 iteration 5680 : loss : 0.019154, loss_ce: 0.008219
2022-01-06 16:09:07,403 iteration 5681 : loss : 0.017173, loss_ce: 0.007054
2022-01-06 16:09:08,566 iteration 5682 : loss : 0.022434, loss_ce: 0.009963
2022-01-06 16:09:09,778 iteration 5683 : loss : 0.040064, loss_ce: 0.016742
2022-01-06 16:09:11,037 iteration 5684 : loss : 0.033713, loss_ce: 0.011701
2022-01-06 16:09:12,258 iteration 5685 : loss : 0.021749, loss_ce: 0.006227
2022-01-06 16:09:13,427 iteration 5686 : loss : 0.021071, loss_ce: 0.008880
2022-01-06 16:09:14,570 iteration 5687 : loss : 0.018190, loss_ce: 0.005438
2022-01-06 16:09:15,753 iteration 5688 : loss : 0.026057, loss_ce: 0.010465
2022-01-06 16:09:16,923 iteration 5689 : loss : 0.020579, loss_ce: 0.009017
2022-01-06 16:09:18,145 iteration 5690 : loss : 0.025793, loss_ce: 0.013218
2022-01-06 16:09:19,304 iteration 5691 : loss : 0.018775, loss_ce: 0.004946
2022-01-06 16:09:20,439 iteration 5692 : loss : 0.016719, loss_ce: 0.006009
2022-01-06 16:09:21,654 iteration 5693 : loss : 0.027620, loss_ce: 0.013024
2022-01-06 16:09:22,886 iteration 5694 : loss : 0.028368, loss_ce: 0.011614
2022-01-06 16:09:22,887 Training Data Eval:
2022-01-06 16:09:28,702   Average segmentation loss on training set: 0.0136
2022-01-06 16:09:28,703 Validation Data Eval:
2022-01-06 16:09:30,693   Average segmentation loss on validation set: 0.0669
2022-01-06 16:09:31,847 iteration 5695 : loss : 0.023202, loss_ce: 0.007483
 84%|████████████████████████▎    | 335/400 [2:04:32<24:59, 23.06s/it]2022-01-06 16:09:33,077 iteration 5696 : loss : 0.019139, loss_ce: 0.005772
2022-01-06 16:09:34,329 iteration 5697 : loss : 0.021477, loss_ce: 0.007871
2022-01-06 16:09:35,519 iteration 5698 : loss : 0.021628, loss_ce: 0.007830
2022-01-06 16:09:36,791 iteration 5699 : loss : 0.026558, loss_ce: 0.009327
2022-01-06 16:09:37,974 iteration 5700 : loss : 0.023805, loss_ce: 0.008088
2022-01-06 16:09:39,137 iteration 5701 : loss : 0.022055, loss_ce: 0.011759
2022-01-06 16:09:40,330 iteration 5702 : loss : 0.014663, loss_ce: 0.005358
2022-01-06 16:09:41,495 iteration 5703 : loss : 0.017978, loss_ce: 0.007434
2022-01-06 16:09:42,724 iteration 5704 : loss : 0.021709, loss_ce: 0.010546
2022-01-06 16:09:43,939 iteration 5705 : loss : 0.025379, loss_ce: 0.008797
2022-01-06 16:09:45,081 iteration 5706 : loss : 0.022373, loss_ce: 0.004366
2022-01-06 16:09:46,402 iteration 5707 : loss : 0.026340, loss_ce: 0.009204
2022-01-06 16:09:47,553 iteration 5708 : loss : 0.018066, loss_ce: 0.007238
2022-01-06 16:09:48,747 iteration 5709 : loss : 0.024520, loss_ce: 0.010849
2022-01-06 16:09:50,020 iteration 5710 : loss : 0.024558, loss_ce: 0.009961
2022-01-06 16:09:51,159 iteration 5711 : loss : 0.029326, loss_ce: 0.014915
2022-01-06 16:09:52,320 iteration 5712 : loss : 0.020871, loss_ce: 0.009032
 84%|████████████████████████▎    | 336/400 [2:04:52<23:46, 22.29s/it]2022-01-06 16:09:53,518 iteration 5713 : loss : 0.021721, loss_ce: 0.006327
2022-01-06 16:09:54,665 iteration 5714 : loss : 0.025350, loss_ce: 0.010667
2022-01-06 16:09:55,811 iteration 5715 : loss : 0.018230, loss_ce: 0.007057
2022-01-06 16:09:57,014 iteration 5716 : loss : 0.019112, loss_ce: 0.009419
2022-01-06 16:09:58,283 iteration 5717 : loss : 0.021450, loss_ce: 0.008429
2022-01-06 16:09:59,391 iteration 5718 : loss : 0.024748, loss_ce: 0.007853
2022-01-06 16:10:00,607 iteration 5719 : loss : 0.016174, loss_ce: 0.006364
2022-01-06 16:10:01,748 iteration 5720 : loss : 0.021887, loss_ce: 0.009796
2022-01-06 16:10:02,821 iteration 5721 : loss : 0.016848, loss_ce: 0.004965
2022-01-06 16:10:03,994 iteration 5722 : loss : 0.018354, loss_ce: 0.006196
2022-01-06 16:10:05,162 iteration 5723 : loss : 0.023844, loss_ce: 0.008181
2022-01-06 16:10:06,227 iteration 5724 : loss : 0.016403, loss_ce: 0.005966
2022-01-06 16:10:07,452 iteration 5725 : loss : 0.024005, loss_ce: 0.011483
2022-01-06 16:10:08,577 iteration 5726 : loss : 0.018656, loss_ce: 0.007581
2022-01-06 16:10:09,746 iteration 5727 : loss : 0.021148, loss_ce: 0.007219
2022-01-06 16:10:10,898 iteration 5728 : loss : 0.023583, loss_ce: 0.010071
2022-01-06 16:10:12,110 iteration 5729 : loss : 0.016720, loss_ce: 0.006027
 84%|████████████████████████▍    | 337/400 [2:05:12<22:36, 21.54s/it]2022-01-06 16:10:13,356 iteration 5730 : loss : 0.029170, loss_ce: 0.007548
2022-01-06 16:10:14,575 iteration 5731 : loss : 0.016361, loss_ce: 0.006681
2022-01-06 16:10:15,728 iteration 5732 : loss : 0.026483, loss_ce: 0.008388
2022-01-06 16:10:16,863 iteration 5733 : loss : 0.018012, loss_ce: 0.005858
2022-01-06 16:10:18,059 iteration 5734 : loss : 0.023598, loss_ce: 0.008673
2022-01-06 16:10:19,246 iteration 5735 : loss : 0.033537, loss_ce: 0.015650
2022-01-06 16:10:20,407 iteration 5736 : loss : 0.021270, loss_ce: 0.011415
2022-01-06 16:10:21,563 iteration 5737 : loss : 0.021696, loss_ce: 0.008974
2022-01-06 16:10:22,729 iteration 5738 : loss : 0.013936, loss_ce: 0.006048
2022-01-06 16:10:23,859 iteration 5739 : loss : 0.018329, loss_ce: 0.007278
2022-01-06 16:10:25,099 iteration 5740 : loss : 0.023079, loss_ce: 0.008047
2022-01-06 16:10:26,318 iteration 5741 : loss : 0.028673, loss_ce: 0.014010
2022-01-06 16:10:27,537 iteration 5742 : loss : 0.025537, loss_ce: 0.009341
2022-01-06 16:10:28,696 iteration 5743 : loss : 0.017556, loss_ce: 0.007156
2022-01-06 16:10:29,947 iteration 5744 : loss : 0.024312, loss_ce: 0.009562
2022-01-06 16:10:31,130 iteration 5745 : loss : 0.025291, loss_ce: 0.007352
2022-01-06 16:10:32,232 iteration 5746 : loss : 0.021831, loss_ce: 0.006051
 84%|████████████████████████▌    | 338/400 [2:05:32<21:49, 21.11s/it]2022-01-06 16:10:33,541 iteration 5747 : loss : 0.027014, loss_ce: 0.009423
2022-01-06 16:10:34,778 iteration 5748 : loss : 0.035721, loss_ce: 0.010102
2022-01-06 16:10:36,026 iteration 5749 : loss : 0.021774, loss_ce: 0.008984
2022-01-06 16:10:37,305 iteration 5750 : loss : 0.020112, loss_ce: 0.005878
2022-01-06 16:10:38,414 iteration 5751 : loss : 0.021130, loss_ce: 0.008179
2022-01-06 16:10:39,636 iteration 5752 : loss : 0.030005, loss_ce: 0.008909
2022-01-06 16:10:40,813 iteration 5753 : loss : 0.031888, loss_ce: 0.011595
2022-01-06 16:10:41,926 iteration 5754 : loss : 0.017652, loss_ce: 0.006982
2022-01-06 16:10:43,121 iteration 5755 : loss : 0.027374, loss_ce: 0.013327
2022-01-06 16:10:44,320 iteration 5756 : loss : 0.023699, loss_ce: 0.007793
2022-01-06 16:10:45,490 iteration 5757 : loss : 0.019862, loss_ce: 0.009697
2022-01-06 16:10:46,669 iteration 5758 : loss : 0.030231, loss_ce: 0.007634
2022-01-06 16:10:47,866 iteration 5759 : loss : 0.017699, loss_ce: 0.008412
2022-01-06 16:10:49,138 iteration 5760 : loss : 0.019876, loss_ce: 0.007648
2022-01-06 16:10:50,224 iteration 5761 : loss : 0.015506, loss_ce: 0.006023
2022-01-06 16:10:51,613 iteration 5762 : loss : 0.031916, loss_ce: 0.012184
2022-01-06 16:10:52,778 iteration 5763 : loss : 0.021212, loss_ce: 0.008415
 85%|████████████████████████▌    | 339/400 [2:05:53<21:17, 20.95s/it]2022-01-06 16:10:54,037 iteration 5764 : loss : 0.025654, loss_ce: 0.010664
2022-01-06 16:10:55,251 iteration 5765 : loss : 0.037329, loss_ce: 0.009677
2022-01-06 16:10:56,451 iteration 5766 : loss : 0.019349, loss_ce: 0.007863
2022-01-06 16:10:57,659 iteration 5767 : loss : 0.019960, loss_ce: 0.007809
2022-01-06 16:10:58,975 iteration 5768 : loss : 0.024442, loss_ce: 0.007463
2022-01-06 16:11:00,137 iteration 5769 : loss : 0.019558, loss_ce: 0.006767
2022-01-06 16:11:01,307 iteration 5770 : loss : 0.017345, loss_ce: 0.007338
2022-01-06 16:11:02,523 iteration 5771 : loss : 0.029253, loss_ce: 0.015111
2022-01-06 16:11:03,715 iteration 5772 : loss : 0.026561, loss_ce: 0.011132
2022-01-06 16:11:04,985 iteration 5773 : loss : 0.020470, loss_ce: 0.008316
2022-01-06 16:11:06,249 iteration 5774 : loss : 0.033670, loss_ce: 0.015692
2022-01-06 16:11:07,544 iteration 5775 : loss : 0.023898, loss_ce: 0.007624
2022-01-06 16:11:08,691 iteration 5776 : loss : 0.025474, loss_ce: 0.008275
2022-01-06 16:11:09,849 iteration 5777 : loss : 0.023534, loss_ce: 0.008479
2022-01-06 16:11:11,078 iteration 5778 : loss : 0.022726, loss_ce: 0.007000
2022-01-06 16:11:12,249 iteration 5779 : loss : 0.023348, loss_ce: 0.009127
2022-01-06 16:11:12,249 Training Data Eval:
2022-01-06 16:11:18,063   Average segmentation loss on training set: 0.0146
2022-01-06 16:11:18,064 Validation Data Eval:
2022-01-06 16:11:20,048   Average segmentation loss on validation set: 0.0739
2022-01-06 16:11:21,288 iteration 5780 : loss : 0.018147, loss_ce: 0.006489
 85%|████████████████████████▋    | 340/400 [2:06:21<23:12, 23.21s/it]2022-01-06 16:11:22,708 iteration 5781 : loss : 0.043086, loss_ce: 0.016713
2022-01-06 16:11:23,775 iteration 5782 : loss : 0.015739, loss_ce: 0.007041
2022-01-06 16:11:24,900 iteration 5783 : loss : 0.022442, loss_ce: 0.008645
2022-01-06 16:11:26,052 iteration 5784 : loss : 0.026143, loss_ce: 0.008842
2022-01-06 16:11:27,164 iteration 5785 : loss : 0.018430, loss_ce: 0.007568
2022-01-06 16:11:28,375 iteration 5786 : loss : 0.043675, loss_ce: 0.010109
2022-01-06 16:11:29,545 iteration 5787 : loss : 0.028457, loss_ce: 0.010873
2022-01-06 16:11:30,620 iteration 5788 : loss : 0.020709, loss_ce: 0.008558
2022-01-06 16:11:31,730 iteration 5789 : loss : 0.013843, loss_ce: 0.005588
2022-01-06 16:11:32,950 iteration 5790 : loss : 0.017850, loss_ce: 0.006958
2022-01-06 16:11:34,126 iteration 5791 : loss : 0.018435, loss_ce: 0.008258
2022-01-06 16:11:35,249 iteration 5792 : loss : 0.018818, loss_ce: 0.007222
2022-01-06 16:11:36,482 iteration 5793 : loss : 0.024074, loss_ce: 0.007423
2022-01-06 16:11:37,740 iteration 5794 : loss : 0.021560, loss_ce: 0.010125
2022-01-06 16:11:38,862 iteration 5795 : loss : 0.013896, loss_ce: 0.006102
2022-01-06 16:11:40,055 iteration 5796 : loss : 0.019688, loss_ce: 0.007878
2022-01-06 16:11:41,250 iteration 5797 : loss : 0.035876, loss_ce: 0.013708
 85%|████████████████████████▋    | 341/400 [2:06:41<21:52, 22.24s/it]2022-01-06 16:11:42,601 iteration 5798 : loss : 0.020978, loss_ce: 0.007425
2022-01-06 16:11:43,766 iteration 5799 : loss : 0.019978, loss_ce: 0.005763
2022-01-06 16:11:44,927 iteration 5800 : loss : 0.020451, loss_ce: 0.005870
2022-01-06 16:11:46,005 iteration 5801 : loss : 0.016202, loss_ce: 0.005516
2022-01-06 16:11:47,220 iteration 5802 : loss : 0.020543, loss_ce: 0.010888
2022-01-06 16:11:48,325 iteration 5803 : loss : 0.020242, loss_ce: 0.007309
2022-01-06 16:11:49,621 iteration 5804 : loss : 0.026730, loss_ce: 0.009726
2022-01-06 16:11:50,756 iteration 5805 : loss : 0.026097, loss_ce: 0.012782
2022-01-06 16:11:51,881 iteration 5806 : loss : 0.021768, loss_ce: 0.008472
2022-01-06 16:11:53,081 iteration 5807 : loss : 0.019761, loss_ce: 0.007486
2022-01-06 16:11:54,234 iteration 5808 : loss : 0.024498, loss_ce: 0.010403
2022-01-06 16:11:55,532 iteration 5809 : loss : 0.031324, loss_ce: 0.010864
2022-01-06 16:11:56,630 iteration 5810 : loss : 0.018437, loss_ce: 0.005496
2022-01-06 16:11:57,804 iteration 5811 : loss : 0.017728, loss_ce: 0.007753
2022-01-06 16:11:59,023 iteration 5812 : loss : 0.021802, loss_ce: 0.009397
2022-01-06 16:12:00,151 iteration 5813 : loss : 0.018873, loss_ce: 0.006285
2022-01-06 16:12:01,337 iteration 5814 : loss : 0.025106, loss_ce: 0.009858
 86%|████████████████████████▊    | 342/400 [2:07:01<20:52, 21.59s/it]2022-01-06 16:12:02,733 iteration 5815 : loss : 0.027497, loss_ce: 0.011649
2022-01-06 16:12:03,837 iteration 5816 : loss : 0.023253, loss_ce: 0.007970
2022-01-06 16:12:05,042 iteration 5817 : loss : 0.020997, loss_ce: 0.006938
2022-01-06 16:12:06,210 iteration 5818 : loss : 0.019213, loss_ce: 0.006613
2022-01-06 16:12:07,509 iteration 5819 : loss : 0.020560, loss_ce: 0.009304
2022-01-06 16:12:08,681 iteration 5820 : loss : 0.018632, loss_ce: 0.008064
2022-01-06 16:12:09,842 iteration 5821 : loss : 0.019107, loss_ce: 0.007344
2022-01-06 16:12:11,093 iteration 5822 : loss : 0.021905, loss_ce: 0.006257
2022-01-06 16:12:12,286 iteration 5823 : loss : 0.016427, loss_ce: 0.005314
2022-01-06 16:12:13,462 iteration 5824 : loss : 0.018516, loss_ce: 0.006420
2022-01-06 16:12:14,634 iteration 5825 : loss : 0.021519, loss_ce: 0.009080
2022-01-06 16:12:15,769 iteration 5826 : loss : 0.025164, loss_ce: 0.007014
2022-01-06 16:12:16,889 iteration 5827 : loss : 0.026183, loss_ce: 0.006860
2022-01-06 16:12:17,974 iteration 5828 : loss : 0.014776, loss_ce: 0.006356
2022-01-06 16:12:19,160 iteration 5829 : loss : 0.021135, loss_ce: 0.007994
2022-01-06 16:12:20,358 iteration 5830 : loss : 0.024240, loss_ce: 0.009289
2022-01-06 16:12:21,590 iteration 5831 : loss : 0.018718, loss_ce: 0.007517
 86%|████████████████████████▊    | 343/400 [2:07:21<20:07, 21.19s/it]2022-01-06 16:12:22,931 iteration 5832 : loss : 0.022840, loss_ce: 0.009019
2022-01-06 16:12:24,075 iteration 5833 : loss : 0.014802, loss_ce: 0.005201
2022-01-06 16:12:25,350 iteration 5834 : loss : 0.021565, loss_ce: 0.008089
2022-01-06 16:12:26,707 iteration 5835 : loss : 0.045181, loss_ce: 0.020806
2022-01-06 16:12:27,868 iteration 5836 : loss : 0.020899, loss_ce: 0.009699
2022-01-06 16:12:29,145 iteration 5837 : loss : 0.034833, loss_ce: 0.014493
2022-01-06 16:12:30,418 iteration 5838 : loss : 0.024535, loss_ce: 0.007389
2022-01-06 16:12:31,523 iteration 5839 : loss : 0.020946, loss_ce: 0.006951
2022-01-06 16:12:32,940 iteration 5840 : loss : 0.024662, loss_ce: 0.011183
2022-01-06 16:12:34,108 iteration 5841 : loss : 0.016018, loss_ce: 0.006452
2022-01-06 16:12:35,372 iteration 5842 : loss : 0.033132, loss_ce: 0.013984
2022-01-06 16:12:36,647 iteration 5843 : loss : 0.027800, loss_ce: 0.009193
2022-01-06 16:12:37,856 iteration 5844 : loss : 0.034540, loss_ce: 0.011910
2022-01-06 16:12:39,090 iteration 5845 : loss : 0.023233, loss_ce: 0.008363
2022-01-06 16:12:40,275 iteration 5846 : loss : 0.019445, loss_ce: 0.008866
2022-01-06 16:12:41,442 iteration 5847 : loss : 0.014313, loss_ce: 0.004953
2022-01-06 16:12:42,541 iteration 5848 : loss : 0.016912, loss_ce: 0.005160
 86%|████████████████████████▉    | 344/400 [2:07:42<19:42, 21.12s/it]2022-01-06 16:12:43,733 iteration 5849 : loss : 0.017407, loss_ce: 0.006146
2022-01-06 16:12:44,996 iteration 5850 : loss : 0.025429, loss_ce: 0.006377
2022-01-06 16:12:46,125 iteration 5851 : loss : 0.022878, loss_ce: 0.011672
2022-01-06 16:12:47,258 iteration 5852 : loss : 0.018478, loss_ce: 0.006105
2022-01-06 16:12:48,357 iteration 5853 : loss : 0.025218, loss_ce: 0.006411
2022-01-06 16:12:49,527 iteration 5854 : loss : 0.026601, loss_ce: 0.010739
2022-01-06 16:12:50,682 iteration 5855 : loss : 0.023466, loss_ce: 0.005616
2022-01-06 16:12:51,800 iteration 5856 : loss : 0.018377, loss_ce: 0.007275
2022-01-06 16:12:53,023 iteration 5857 : loss : 0.022771, loss_ce: 0.010439
2022-01-06 16:12:54,186 iteration 5858 : loss : 0.022932, loss_ce: 0.005840
2022-01-06 16:12:55,407 iteration 5859 : loss : 0.018851, loss_ce: 0.008975
2022-01-06 16:12:56,629 iteration 5860 : loss : 0.015067, loss_ce: 0.005617
2022-01-06 16:12:57,842 iteration 5861 : loss : 0.020687, loss_ce: 0.010665
2022-01-06 16:12:59,007 iteration 5862 : loss : 0.015743, loss_ce: 0.006051
2022-01-06 16:13:00,163 iteration 5863 : loss : 0.018688, loss_ce: 0.006739
2022-01-06 16:13:01,322 iteration 5864 : loss : 0.023099, loss_ce: 0.008527
2022-01-06 16:13:01,322 Training Data Eval:
2022-01-06 16:13:07,186   Average segmentation loss on training set: 0.0136
2022-01-06 16:13:07,186 Validation Data Eval:
2022-01-06 16:13:09,220   Average segmentation loss on validation set: 0.0743
2022-01-06 16:13:10,388 iteration 5865 : loss : 0.020300, loss_ce: 0.007906
 86%|█████████████████████████    | 345/400 [2:08:10<21:12, 23.14s/it]2022-01-06 16:13:11,673 iteration 5866 : loss : 0.018866, loss_ce: 0.006644
2022-01-06 16:13:12,885 iteration 5867 : loss : 0.019713, loss_ce: 0.008298
2022-01-06 16:13:13,997 iteration 5868 : loss : 0.016319, loss_ce: 0.006785
2022-01-06 16:13:15,159 iteration 5869 : loss : 0.016521, loss_ce: 0.005676
2022-01-06 16:13:16,267 iteration 5870 : loss : 0.019356, loss_ce: 0.007268
2022-01-06 16:13:17,447 iteration 5871 : loss : 0.019096, loss_ce: 0.008998
2022-01-06 16:13:18,613 iteration 5872 : loss : 0.021862, loss_ce: 0.009320
2022-01-06 16:13:19,727 iteration 5873 : loss : 0.018455, loss_ce: 0.008736
2022-01-06 16:13:20,844 iteration 5874 : loss : 0.024822, loss_ce: 0.006026
2022-01-06 16:13:22,108 iteration 5875 : loss : 0.020271, loss_ce: 0.008258
2022-01-06 16:13:23,377 iteration 5876 : loss : 0.018943, loss_ce: 0.008722
2022-01-06 16:13:24,573 iteration 5877 : loss : 0.023958, loss_ce: 0.008716
2022-01-06 16:13:25,808 iteration 5878 : loss : 0.029140, loss_ce: 0.008613
2022-01-06 16:13:26,952 iteration 5879 : loss : 0.021749, loss_ce: 0.006287
2022-01-06 16:13:28,123 iteration 5880 : loss : 0.022066, loss_ce: 0.007810
2022-01-06 16:13:29,362 iteration 5881 : loss : 0.023731, loss_ce: 0.008245
2022-01-06 16:13:30,520 iteration 5882 : loss : 0.018484, loss_ce: 0.007029
 86%|█████████████████████████    | 346/400 [2:08:30<20:00, 22.23s/it]2022-01-06 16:13:31,896 iteration 5883 : loss : 0.020777, loss_ce: 0.007026
2022-01-06 16:13:33,036 iteration 5884 : loss : 0.019159, loss_ce: 0.007732
2022-01-06 16:13:34,297 iteration 5885 : loss : 0.037645, loss_ce: 0.009730
2022-01-06 16:13:35,449 iteration 5886 : loss : 0.014145, loss_ce: 0.004394
2022-01-06 16:13:36,608 iteration 5887 : loss : 0.025365, loss_ce: 0.007460
2022-01-06 16:13:37,693 iteration 5888 : loss : 0.015737, loss_ce: 0.006777
2022-01-06 16:13:38,861 iteration 5889 : loss : 0.020321, loss_ce: 0.009235
2022-01-06 16:13:40,107 iteration 5890 : loss : 0.018849, loss_ce: 0.007914
2022-01-06 16:13:41,351 iteration 5891 : loss : 0.028603, loss_ce: 0.008320
2022-01-06 16:13:42,621 iteration 5892 : loss : 0.019647, loss_ce: 0.009769
2022-01-06 16:13:43,717 iteration 5893 : loss : 0.017265, loss_ce: 0.005394
2022-01-06 16:13:44,863 iteration 5894 : loss : 0.015339, loss_ce: 0.004756
2022-01-06 16:13:46,101 iteration 5895 : loss : 0.020715, loss_ce: 0.008634
2022-01-06 16:13:47,198 iteration 5896 : loss : 0.018852, loss_ce: 0.006240
2022-01-06 16:13:48,416 iteration 5897 : loss : 0.022406, loss_ce: 0.010232
2022-01-06 16:13:49,674 iteration 5898 : loss : 0.028721, loss_ce: 0.012016
2022-01-06 16:13:50,843 iteration 5899 : loss : 0.024836, loss_ce: 0.007654
 87%|█████████████████████████▏   | 347/400 [2:08:51<19:08, 21.66s/it]2022-01-06 16:13:51,976 iteration 5900 : loss : 0.015610, loss_ce: 0.006225
2022-01-06 16:13:53,119 iteration 5901 : loss : 0.015165, loss_ce: 0.006442
2022-01-06 16:13:54,209 iteration 5902 : loss : 0.017295, loss_ce: 0.006890
2022-01-06 16:13:55,364 iteration 5903 : loss : 0.022720, loss_ce: 0.009725
2022-01-06 16:13:56,563 iteration 5904 : loss : 0.023382, loss_ce: 0.006194
2022-01-06 16:13:57,793 iteration 5905 : loss : 0.028729, loss_ce: 0.012706
2022-01-06 16:13:59,035 iteration 5906 : loss : 0.022002, loss_ce: 0.006809
2022-01-06 16:14:00,135 iteration 5907 : loss : 0.016108, loss_ce: 0.007071
2022-01-06 16:14:01,322 iteration 5908 : loss : 0.017556, loss_ce: 0.005942
2022-01-06 16:14:02,557 iteration 5909 : loss : 0.021877, loss_ce: 0.013248
2022-01-06 16:14:03,729 iteration 5910 : loss : 0.016437, loss_ce: 0.005670
2022-01-06 16:14:04,943 iteration 5911 : loss : 0.028305, loss_ce: 0.007677
2022-01-06 16:14:06,136 iteration 5912 : loss : 0.030558, loss_ce: 0.007202
2022-01-06 16:14:07,309 iteration 5913 : loss : 0.016914, loss_ce: 0.007344
2022-01-06 16:14:08,471 iteration 5914 : loss : 0.028807, loss_ce: 0.016515
2022-01-06 16:14:09,693 iteration 5915 : loss : 0.019529, loss_ce: 0.009894
2022-01-06 16:14:10,833 iteration 5916 : loss : 0.016930, loss_ce: 0.005964
 87%|█████████████████████████▏   | 348/400 [2:09:11<18:20, 21.16s/it]2022-01-06 16:14:12,029 iteration 5917 : loss : 0.023134, loss_ce: 0.009571
2022-01-06 16:14:13,304 iteration 5918 : loss : 0.025422, loss_ce: 0.013025
2022-01-06 16:14:14,566 iteration 5919 : loss : 0.030505, loss_ce: 0.012392
2022-01-06 16:14:15,826 iteration 5920 : loss : 0.021972, loss_ce: 0.007719
2022-01-06 16:14:16,989 iteration 5921 : loss : 0.022354, loss_ce: 0.008856
2022-01-06 16:14:18,216 iteration 5922 : loss : 0.022594, loss_ce: 0.011277
2022-01-06 16:14:19,411 iteration 5923 : loss : 0.020384, loss_ce: 0.009108
2022-01-06 16:14:20,499 iteration 5924 : loss : 0.013709, loss_ce: 0.004969
2022-01-06 16:14:21,669 iteration 5925 : loss : 0.024591, loss_ce: 0.009262
2022-01-06 16:14:22,931 iteration 5926 : loss : 0.028852, loss_ce: 0.008789
2022-01-06 16:14:24,067 iteration 5927 : loss : 0.014842, loss_ce: 0.005994
2022-01-06 16:14:25,324 iteration 5928 : loss : 0.016689, loss_ce: 0.007002
2022-01-06 16:14:26,544 iteration 5929 : loss : 0.019973, loss_ce: 0.009171
2022-01-06 16:14:27,811 iteration 5930 : loss : 0.025564, loss_ce: 0.009378
2022-01-06 16:14:28,936 iteration 5931 : loss : 0.015504, loss_ce: 0.006594
2022-01-06 16:14:30,129 iteration 5932 : loss : 0.023597, loss_ce: 0.008977
2022-01-06 16:14:31,349 iteration 5933 : loss : 0.037693, loss_ce: 0.013667
 87%|█████████████████████████▎   | 349/400 [2:09:31<17:49, 20.97s/it]2022-01-06 16:14:32,601 iteration 5934 : loss : 0.014411, loss_ce: 0.004866
2022-01-06 16:14:33,785 iteration 5935 : loss : 0.024359, loss_ce: 0.007366
2022-01-06 16:14:35,093 iteration 5936 : loss : 0.028151, loss_ce: 0.010645
2022-01-06 16:14:36,365 iteration 5937 : loss : 0.024990, loss_ce: 0.011636
2022-01-06 16:14:37,573 iteration 5938 : loss : 0.038350, loss_ce: 0.014164
2022-01-06 16:14:38,738 iteration 5939 : loss : 0.022060, loss_ce: 0.010620
2022-01-06 16:14:39,960 iteration 5940 : loss : 0.021633, loss_ce: 0.009743
2022-01-06 16:14:41,049 iteration 5941 : loss : 0.015424, loss_ce: 0.007182
2022-01-06 16:14:42,282 iteration 5942 : loss : 0.022873, loss_ce: 0.007294
2022-01-06 16:14:43,392 iteration 5943 : loss : 0.013239, loss_ce: 0.004970
2022-01-06 16:14:44,589 iteration 5944 : loss : 0.018666, loss_ce: 0.006527
2022-01-06 16:14:45,769 iteration 5945 : loss : 0.018487, loss_ce: 0.005938
2022-01-06 16:14:46,907 iteration 5946 : loss : 0.024769, loss_ce: 0.009534
2022-01-06 16:14:48,040 iteration 5947 : loss : 0.019564, loss_ce: 0.008040
2022-01-06 16:14:49,309 iteration 5948 : loss : 0.023694, loss_ce: 0.010188
2022-01-06 16:14:50,446 iteration 5949 : loss : 0.016701, loss_ce: 0.006726
2022-01-06 16:14:50,446 Training Data Eval:
2022-01-06 16:14:56,289   Average segmentation loss on training set: 0.0164
2022-01-06 16:14:56,290 Validation Data Eval:
2022-01-06 16:14:58,296   Average segmentation loss on validation set: 0.0985
2022-01-06 16:14:59,579 iteration 5950 : loss : 0.029168, loss_ce: 0.007189
 88%|█████████████████████████▍   | 350/400 [2:09:59<19:17, 23.14s/it]2022-01-06 16:15:00,795 iteration 5951 : loss : 0.018424, loss_ce: 0.005378
2022-01-06 16:15:01,986 iteration 5952 : loss : 0.017285, loss_ce: 0.008375
2022-01-06 16:15:03,197 iteration 5953 : loss : 0.016403, loss_ce: 0.006913
2022-01-06 16:15:04,494 iteration 5954 : loss : 0.027743, loss_ce: 0.008843
2022-01-06 16:15:05,655 iteration 5955 : loss : 0.020493, loss_ce: 0.007484
2022-01-06 16:15:06,918 iteration 5956 : loss : 0.031206, loss_ce: 0.009324
2022-01-06 16:15:08,069 iteration 5957 : loss : 0.023377, loss_ce: 0.008620
2022-01-06 16:15:09,201 iteration 5958 : loss : 0.018092, loss_ce: 0.008123
2022-01-06 16:15:10,374 iteration 5959 : loss : 0.018099, loss_ce: 0.005687
2022-01-06 16:15:11,505 iteration 5960 : loss : 0.014082, loss_ce: 0.005027
2022-01-06 16:15:12,709 iteration 5961 : loss : 0.021864, loss_ce: 0.007603
2022-01-06 16:15:13,947 iteration 5962 : loss : 0.020655, loss_ce: 0.008496
2022-01-06 16:15:15,033 iteration 5963 : loss : 0.017300, loss_ce: 0.007978
2022-01-06 16:15:16,407 iteration 5964 : loss : 0.026706, loss_ce: 0.011824
2022-01-06 16:15:17,540 iteration 5965 : loss : 0.022126, loss_ce: 0.009718
2022-01-06 16:15:18,773 iteration 5966 : loss : 0.026171, loss_ce: 0.010077
2022-01-06 16:15:19,915 iteration 5967 : loss : 0.026413, loss_ce: 0.009429
 88%|█████████████████████████▍   | 351/400 [2:10:20<18:12, 22.30s/it]2022-01-06 16:15:21,133 iteration 5968 : loss : 0.016131, loss_ce: 0.006416
2022-01-06 16:15:22,353 iteration 5969 : loss : 0.025221, loss_ce: 0.006711
2022-01-06 16:15:23,534 iteration 5970 : loss : 0.020474, loss_ce: 0.006855
2022-01-06 16:15:24,610 iteration 5971 : loss : 0.016607, loss_ce: 0.006515
2022-01-06 16:15:25,743 iteration 5972 : loss : 0.017462, loss_ce: 0.006825
2022-01-06 16:15:26,941 iteration 5973 : loss : 0.016802, loss_ce: 0.008002
2022-01-06 16:15:28,072 iteration 5974 : loss : 0.019077, loss_ce: 0.007079
2022-01-06 16:15:29,302 iteration 5975 : loss : 0.025263, loss_ce: 0.008364
2022-01-06 16:15:30,493 iteration 5976 : loss : 0.021426, loss_ce: 0.005535
2022-01-06 16:15:31,700 iteration 5977 : loss : 0.021278, loss_ce: 0.008314
2022-01-06 16:15:32,899 iteration 5978 : loss : 0.018666, loss_ce: 0.006977
2022-01-06 16:15:34,040 iteration 5979 : loss : 0.016505, loss_ce: 0.006253
2022-01-06 16:15:35,265 iteration 5980 : loss : 0.014240, loss_ce: 0.004894
2022-01-06 16:15:36,470 iteration 5981 : loss : 0.020501, loss_ce: 0.007728
2022-01-06 16:15:37,707 iteration 5982 : loss : 0.036447, loss_ce: 0.013766
2022-01-06 16:15:38,791 iteration 5983 : loss : 0.023182, loss_ce: 0.010464
2022-01-06 16:15:39,912 iteration 5984 : loss : 0.018377, loss_ce: 0.006227
 88%|█████████████████████████▌   | 352/400 [2:10:40<17:17, 21.61s/it]2022-01-06 16:15:41,290 iteration 5985 : loss : 0.029133, loss_ce: 0.012040
2022-01-06 16:15:42,441 iteration 5986 : loss : 0.021521, loss_ce: 0.009123
2022-01-06 16:15:43,696 iteration 5987 : loss : 0.050727, loss_ce: 0.007631
2022-01-06 16:15:44,970 iteration 5988 : loss : 0.016478, loss_ce: 0.007436
2022-01-06 16:15:46,143 iteration 5989 : loss : 0.018192, loss_ce: 0.007410
2022-01-06 16:15:47,364 iteration 5990 : loss : 0.022175, loss_ce: 0.011703
2022-01-06 16:15:48,444 iteration 5991 : loss : 0.016249, loss_ce: 0.005680
2022-01-06 16:15:49,679 iteration 5992 : loss : 0.019565, loss_ce: 0.005909
2022-01-06 16:15:50,809 iteration 5993 : loss : 0.014318, loss_ce: 0.004308
2022-01-06 16:15:51,980 iteration 5994 : loss : 0.022357, loss_ce: 0.007363
2022-01-06 16:15:53,193 iteration 5995 : loss : 0.031418, loss_ce: 0.011712
2022-01-06 16:15:54,465 iteration 5996 : loss : 0.021679, loss_ce: 0.007637
2022-01-06 16:15:55,721 iteration 5997 : loss : 0.026648, loss_ce: 0.012605
2022-01-06 16:15:56,845 iteration 5998 : loss : 0.018187, loss_ce: 0.006826
2022-01-06 16:15:58,060 iteration 5999 : loss : 0.043271, loss_ce: 0.011391
2022-01-06 16:15:59,283 iteration 6000 : loss : 0.021749, loss_ce: 0.009957
2022-01-06 16:16:00,550 iteration 6001 : loss : 0.028551, loss_ce: 0.010036
 88%|█████████████████████████▌   | 353/400 [2:11:00<16:41, 21.32s/it]2022-01-06 16:16:01,813 iteration 6002 : loss : 0.019930, loss_ce: 0.009688
2022-01-06 16:16:03,052 iteration 6003 : loss : 0.023458, loss_ce: 0.008408
2022-01-06 16:16:04,230 iteration 6004 : loss : 0.018736, loss_ce: 0.008096
2022-01-06 16:16:05,457 iteration 6005 : loss : 0.017991, loss_ce: 0.008045
2022-01-06 16:16:06,579 iteration 6006 : loss : 0.019654, loss_ce: 0.010083
2022-01-06 16:16:07,770 iteration 6007 : loss : 0.023978, loss_ce: 0.007532
2022-01-06 16:16:08,956 iteration 6008 : loss : 0.020553, loss_ce: 0.007437
2022-01-06 16:16:10,246 iteration 6009 : loss : 0.033926, loss_ce: 0.012042
2022-01-06 16:16:11,323 iteration 6010 : loss : 0.016195, loss_ce: 0.007127
2022-01-06 16:16:12,534 iteration 6011 : loss : 0.026167, loss_ce: 0.010657
2022-01-06 16:16:13,712 iteration 6012 : loss : 0.025050, loss_ce: 0.008221
2022-01-06 16:16:15,013 iteration 6013 : loss : 0.023124, loss_ce: 0.008449
2022-01-06 16:16:16,177 iteration 6014 : loss : 0.023081, loss_ce: 0.008690
2022-01-06 16:16:17,312 iteration 6015 : loss : 0.031554, loss_ce: 0.011237
2022-01-06 16:16:18,506 iteration 6016 : loss : 0.016062, loss_ce: 0.004826
2022-01-06 16:16:19,622 iteration 6017 : loss : 0.032722, loss_ce: 0.008437
2022-01-06 16:16:20,919 iteration 6018 : loss : 0.021175, loss_ce: 0.008540
 88%|█████████████████████████▋   | 354/400 [2:11:21<16:07, 21.04s/it]2022-01-06 16:16:22,217 iteration 6019 : loss : 0.024970, loss_ce: 0.008843
2022-01-06 16:16:23,389 iteration 6020 : loss : 0.024559, loss_ce: 0.012476
2022-01-06 16:16:24,562 iteration 6021 : loss : 0.021245, loss_ce: 0.007709
2022-01-06 16:16:25,770 iteration 6022 : loss : 0.015692, loss_ce: 0.006095
2022-01-06 16:16:26,869 iteration 6023 : loss : 0.018118, loss_ce: 0.007255
2022-01-06 16:16:28,125 iteration 6024 : loss : 0.024084, loss_ce: 0.009962
2022-01-06 16:16:29,361 iteration 6025 : loss : 0.027464, loss_ce: 0.012554
2022-01-06 16:16:30,580 iteration 6026 : loss : 0.024297, loss_ce: 0.008178
2022-01-06 16:16:31,711 iteration 6027 : loss : 0.024653, loss_ce: 0.010915
2022-01-06 16:16:32,841 iteration 6028 : loss : 0.022821, loss_ce: 0.009343
2022-01-06 16:16:34,073 iteration 6029 : loss : 0.026614, loss_ce: 0.011875
2022-01-06 16:16:35,197 iteration 6030 : loss : 0.018945, loss_ce: 0.006783
2022-01-06 16:16:36,499 iteration 6031 : loss : 0.044458, loss_ce: 0.012771
2022-01-06 16:16:37,726 iteration 6032 : loss : 0.020619, loss_ce: 0.006780
2022-01-06 16:16:38,944 iteration 6033 : loss : 0.027861, loss_ce: 0.012489
2022-01-06 16:16:40,214 iteration 6034 : loss : 0.015513, loss_ce: 0.004810
2022-01-06 16:16:40,214 Training Data Eval:
2022-01-06 16:16:46,042   Average segmentation loss on training set: 0.0148
2022-01-06 16:16:46,042 Validation Data Eval:
2022-01-06 16:16:48,033   Average segmentation loss on validation set: 0.0621
2022-01-06 16:16:55,337 Found new lowest validation loss at iteration 6034! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_MLP_best_val_loss_seed100.pth
2022-01-06 16:16:56,450 iteration 6035 : loss : 0.032939, loss_ce: 0.011607
 89%|█████████████████████████▋   | 355/400 [2:11:56<19:02, 25.38s/it]2022-01-06 16:16:57,707 iteration 6036 : loss : 0.021697, loss_ce: 0.008345
2022-01-06 16:16:58,943 iteration 6037 : loss : 0.022540, loss_ce: 0.008044
2022-01-06 16:17:00,060 iteration 6038 : loss : 0.023318, loss_ce: 0.010600
2022-01-06 16:17:01,224 iteration 6039 : loss : 0.024886, loss_ce: 0.012033
2022-01-06 16:17:02,369 iteration 6040 : loss : 0.020127, loss_ce: 0.008875
2022-01-06 16:17:03,669 iteration 6041 : loss : 0.036422, loss_ce: 0.012093
2022-01-06 16:17:04,903 iteration 6042 : loss : 0.020482, loss_ce: 0.007736
2022-01-06 16:17:06,066 iteration 6043 : loss : 0.017919, loss_ce: 0.007898
2022-01-06 16:17:07,204 iteration 6044 : loss : 0.023147, loss_ce: 0.007752
2022-01-06 16:17:08,420 iteration 6045 : loss : 0.042621, loss_ce: 0.012800
2022-01-06 16:17:09,577 iteration 6046 : loss : 0.022562, loss_ce: 0.007803
2022-01-06 16:17:10,694 iteration 6047 : loss : 0.022906, loss_ce: 0.006119
2022-01-06 16:17:11,978 iteration 6048 : loss : 0.044319, loss_ce: 0.007834
2022-01-06 16:17:13,202 iteration 6049 : loss : 0.023678, loss_ce: 0.012131
2022-01-06 16:17:14,321 iteration 6050 : loss : 0.018329, loss_ce: 0.007728
2022-01-06 16:17:15,555 iteration 6051 : loss : 0.023694, loss_ce: 0.009785
2022-01-06 16:17:16,603 iteration 6052 : loss : 0.030923, loss_ce: 0.010222
 89%|█████████████████████████▊   | 356/400 [2:12:16<17:27, 23.81s/it]2022-01-06 16:17:17,786 iteration 6053 : loss : 0.016512, loss_ce: 0.007183
2022-01-06 16:17:19,058 iteration 6054 : loss : 0.035718, loss_ce: 0.014349
2022-01-06 16:17:20,196 iteration 6055 : loss : 0.024673, loss_ce: 0.008944
2022-01-06 16:17:21,333 iteration 6056 : loss : 0.015749, loss_ce: 0.005806
2022-01-06 16:17:22,479 iteration 6057 : loss : 0.018656, loss_ce: 0.006512
2022-01-06 16:17:23,737 iteration 6058 : loss : 0.018010, loss_ce: 0.007764
2022-01-06 16:17:24,847 iteration 6059 : loss : 0.021458, loss_ce: 0.010495
2022-01-06 16:17:26,096 iteration 6060 : loss : 0.021428, loss_ce: 0.009449
2022-01-06 16:17:27,214 iteration 6061 : loss : 0.032654, loss_ce: 0.012998
2022-01-06 16:17:28,510 iteration 6062 : loss : 0.025442, loss_ce: 0.012467
2022-01-06 16:17:29,788 iteration 6063 : loss : 0.027269, loss_ce: 0.010960
2022-01-06 16:17:30,972 iteration 6064 : loss : 0.026813, loss_ce: 0.007310
2022-01-06 16:17:32,250 iteration 6065 : loss : 0.029182, loss_ce: 0.014081
2022-01-06 16:17:33,413 iteration 6066 : loss : 0.016946, loss_ce: 0.006575
2022-01-06 16:17:34,551 iteration 6067 : loss : 0.028915, loss_ce: 0.006645
2022-01-06 16:17:35,860 iteration 6068 : loss : 0.032480, loss_ce: 0.011331
2022-01-06 16:17:37,013 iteration 6069 : loss : 0.032567, loss_ce: 0.005280
 89%|█████████████████████████▉   | 357/400 [2:12:37<16:20, 22.79s/it]2022-01-06 16:17:38,317 iteration 6070 : loss : 0.025985, loss_ce: 0.018018
2022-01-06 16:17:39,548 iteration 6071 : loss : 0.021568, loss_ce: 0.006399
2022-01-06 16:17:40,735 iteration 6072 : loss : 0.023588, loss_ce: 0.007354
2022-01-06 16:17:41,957 iteration 6073 : loss : 0.028179, loss_ce: 0.009813
2022-01-06 16:17:43,171 iteration 6074 : loss : 0.025611, loss_ce: 0.007802
2022-01-06 16:17:44,451 iteration 6075 : loss : 0.020709, loss_ce: 0.008953
2022-01-06 16:17:45,614 iteration 6076 : loss : 0.025290, loss_ce: 0.012825
2022-01-06 16:17:46,772 iteration 6077 : loss : 0.016502, loss_ce: 0.007707
2022-01-06 16:17:47,946 iteration 6078 : loss : 0.020770, loss_ce: 0.006371
2022-01-06 16:17:49,143 iteration 6079 : loss : 0.021276, loss_ce: 0.005215
2022-01-06 16:17:50,332 iteration 6080 : loss : 0.018114, loss_ce: 0.005261
2022-01-06 16:17:51,434 iteration 6081 : loss : 0.022728, loss_ce: 0.007590
2022-01-06 16:17:52,697 iteration 6082 : loss : 0.027070, loss_ce: 0.010592
2022-01-06 16:17:53,972 iteration 6083 : loss : 0.033640, loss_ce: 0.013588
2022-01-06 16:17:55,087 iteration 6084 : loss : 0.027285, loss_ce: 0.007858
2022-01-06 16:17:56,206 iteration 6085 : loss : 0.020396, loss_ce: 0.009702
2022-01-06 16:17:57,527 iteration 6086 : loss : 0.044487, loss_ce: 0.017995
 90%|█████████████████████████▉   | 358/400 [2:12:57<15:28, 22.11s/it]2022-01-06 16:17:58,764 iteration 6087 : loss : 0.028856, loss_ce: 0.009035
2022-01-06 16:17:59,940 iteration 6088 : loss : 0.020832, loss_ce: 0.006967
2022-01-06 16:18:01,185 iteration 6089 : loss : 0.021581, loss_ce: 0.010852
2022-01-06 16:18:02,353 iteration 6090 : loss : 0.019631, loss_ce: 0.006728
2022-01-06 16:18:03,489 iteration 6091 : loss : 0.023689, loss_ce: 0.007521
2022-01-06 16:18:04,765 iteration 6092 : loss : 0.024239, loss_ce: 0.008915
2022-01-06 16:18:06,067 iteration 6093 : loss : 0.025628, loss_ce: 0.011228
2022-01-06 16:18:07,339 iteration 6094 : loss : 0.022821, loss_ce: 0.010514
2022-01-06 16:18:08,452 iteration 6095 : loss : 0.029806, loss_ce: 0.007955
2022-01-06 16:18:09,734 iteration 6096 : loss : 0.026075, loss_ce: 0.006039
2022-01-06 16:18:10,858 iteration 6097 : loss : 0.018657, loss_ce: 0.009613
2022-01-06 16:18:12,069 iteration 6098 : loss : 0.020107, loss_ce: 0.010278
2022-01-06 16:18:13,253 iteration 6099 : loss : 0.026494, loss_ce: 0.012293
2022-01-06 16:18:14,476 iteration 6100 : loss : 0.026559, loss_ce: 0.008066
2022-01-06 16:18:15,765 iteration 6101 : loss : 0.026829, loss_ce: 0.011856
2022-01-06 16:18:16,895 iteration 6102 : loss : 0.016264, loss_ce: 0.007053
2022-01-06 16:18:18,109 iteration 6103 : loss : 0.024024, loss_ce: 0.007627
 90%|██████████████████████████   | 359/400 [2:13:18<14:47, 21.65s/it]2022-01-06 16:18:19,414 iteration 6104 : loss : 0.018720, loss_ce: 0.007546
2022-01-06 16:18:20,567 iteration 6105 : loss : 0.020691, loss_ce: 0.005974
2022-01-06 16:18:21,686 iteration 6106 : loss : 0.016983, loss_ce: 0.007792
2022-01-06 16:18:22,820 iteration 6107 : loss : 0.014173, loss_ce: 0.005911
2022-01-06 16:18:24,004 iteration 6108 : loss : 0.024516, loss_ce: 0.007803
2022-01-06 16:18:25,236 iteration 6109 : loss : 0.019658, loss_ce: 0.008052
2022-01-06 16:18:26,346 iteration 6110 : loss : 0.017615, loss_ce: 0.007865
2022-01-06 16:18:27,547 iteration 6111 : loss : 0.027102, loss_ce: 0.008821
2022-01-06 16:18:28,779 iteration 6112 : loss : 0.023706, loss_ce: 0.006910
2022-01-06 16:18:29,943 iteration 6113 : loss : 0.017940, loss_ce: 0.006500
2022-01-06 16:18:31,132 iteration 6114 : loss : 0.027493, loss_ce: 0.011611
2022-01-06 16:18:32,383 iteration 6115 : loss : 0.027535, loss_ce: 0.008261
2022-01-06 16:18:33,557 iteration 6116 : loss : 0.014749, loss_ce: 0.004982
2022-01-06 16:18:34,713 iteration 6117 : loss : 0.019256, loss_ce: 0.006665
2022-01-06 16:18:35,940 iteration 6118 : loss : 0.019468, loss_ce: 0.009929
2022-01-06 16:18:37,116 iteration 6119 : loss : 0.017342, loss_ce: 0.006935
2022-01-06 16:18:37,116 Training Data Eval:
2022-01-06 16:18:43,025   Average segmentation loss on training set: 0.0128
2022-01-06 16:18:43,026 Validation Data Eval:
2022-01-06 16:18:45,044   Average segmentation loss on validation set: 0.0730
2022-01-06 16:18:46,306 iteration 6120 : loss : 0.022293, loss_ce: 0.009196
 90%|██████████████████████████   | 360/400 [2:13:46<15:44, 23.62s/it]2022-01-06 16:18:47,611 iteration 6121 : loss : 0.018615, loss_ce: 0.006273
2022-01-06 16:18:48,795 iteration 6122 : loss : 0.022830, loss_ce: 0.011319
2022-01-06 16:18:49,961 iteration 6123 : loss : 0.021055, loss_ce: 0.007663
2022-01-06 16:18:51,249 iteration 6124 : loss : 0.026116, loss_ce: 0.008915
2022-01-06 16:18:52,448 iteration 6125 : loss : 0.018547, loss_ce: 0.005865
2022-01-06 16:18:53,771 iteration 6126 : loss : 0.024370, loss_ce: 0.009917
2022-01-06 16:18:55,088 iteration 6127 : loss : 0.027436, loss_ce: 0.014938
2022-01-06 16:18:56,277 iteration 6128 : loss : 0.016470, loss_ce: 0.005909
2022-01-06 16:18:57,431 iteration 6129 : loss : 0.017199, loss_ce: 0.006079
2022-01-06 16:18:58,659 iteration 6130 : loss : 0.025295, loss_ce: 0.009944
2022-01-06 16:18:59,823 iteration 6131 : loss : 0.031492, loss_ce: 0.008373
2022-01-06 16:19:01,109 iteration 6132 : loss : 0.030596, loss_ce: 0.008879
2022-01-06 16:19:02,366 iteration 6133 : loss : 0.020848, loss_ce: 0.009850
2022-01-06 16:19:03,550 iteration 6134 : loss : 0.018145, loss_ce: 0.006523
2022-01-06 16:19:04,695 iteration 6135 : loss : 0.020219, loss_ce: 0.010096
2022-01-06 16:19:05,871 iteration 6136 : loss : 0.014137, loss_ce: 0.003659
2022-01-06 16:19:07,061 iteration 6137 : loss : 0.022417, loss_ce: 0.007145
 90%|██████████████████████████▏  | 361/400 [2:14:07<14:47, 22.75s/it]2022-01-06 16:19:08,291 iteration 6138 : loss : 0.021351, loss_ce: 0.008153
2022-01-06 16:19:09,460 iteration 6139 : loss : 0.019001, loss_ce: 0.003838
2022-01-06 16:19:10,622 iteration 6140 : loss : 0.019595, loss_ce: 0.007043
2022-01-06 16:19:11,755 iteration 6141 : loss : 0.018428, loss_ce: 0.008233
2022-01-06 16:19:12,992 iteration 6142 : loss : 0.018383, loss_ce: 0.007676
2022-01-06 16:19:14,144 iteration 6143 : loss : 0.019392, loss_ce: 0.010314
2022-01-06 16:19:15,434 iteration 6144 : loss : 0.024076, loss_ce: 0.011904
2022-01-06 16:19:16,566 iteration 6145 : loss : 0.020177, loss_ce: 0.007895
2022-01-06 16:19:17,670 iteration 6146 : loss : 0.019741, loss_ce: 0.006066
2022-01-06 16:19:18,853 iteration 6147 : loss : 0.021405, loss_ce: 0.008085
2022-01-06 16:19:20,024 iteration 6148 : loss : 0.016554, loss_ce: 0.007075
2022-01-06 16:19:21,173 iteration 6149 : loss : 0.017761, loss_ce: 0.006505
2022-01-06 16:19:22,392 iteration 6150 : loss : 0.019722, loss_ce: 0.007441
2022-01-06 16:19:23,734 iteration 6151 : loss : 0.021252, loss_ce: 0.006012
2022-01-06 16:19:24,892 iteration 6152 : loss : 0.017221, loss_ce: 0.006718
2022-01-06 16:19:26,101 iteration 6153 : loss : 0.018964, loss_ce: 0.008263
2022-01-06 16:19:27,376 iteration 6154 : loss : 0.034077, loss_ce: 0.013191
 90%|██████████████████████████▏  | 362/400 [2:14:27<13:56, 22.02s/it]2022-01-06 16:19:28,717 iteration 6155 : loss : 0.025082, loss_ce: 0.010631
2022-01-06 16:19:29,841 iteration 6156 : loss : 0.013912, loss_ce: 0.005581
2022-01-06 16:19:31,021 iteration 6157 : loss : 0.015434, loss_ce: 0.005165
2022-01-06 16:19:32,238 iteration 6158 : loss : 0.024905, loss_ce: 0.009586
2022-01-06 16:19:33,367 iteration 6159 : loss : 0.027674, loss_ce: 0.008618
2022-01-06 16:19:34,552 iteration 6160 : loss : 0.019302, loss_ce: 0.007666
2022-01-06 16:19:35,763 iteration 6161 : loss : 0.021361, loss_ce: 0.005658
2022-01-06 16:19:37,012 iteration 6162 : loss : 0.027246, loss_ce: 0.005939
2022-01-06 16:19:38,224 iteration 6163 : loss : 0.018971, loss_ce: 0.008092
2022-01-06 16:19:39,476 iteration 6164 : loss : 0.027798, loss_ce: 0.013476
2022-01-06 16:19:40,664 iteration 6165 : loss : 0.018245, loss_ce: 0.006106
2022-01-06 16:19:41,834 iteration 6166 : loss : 0.017527, loss_ce: 0.006976
2022-01-06 16:19:42,978 iteration 6167 : loss : 0.017945, loss_ce: 0.007375
2022-01-06 16:19:44,222 iteration 6168 : loss : 0.031962, loss_ce: 0.011067
2022-01-06 16:19:45,443 iteration 6169 : loss : 0.027572, loss_ce: 0.010168
2022-01-06 16:19:46,610 iteration 6170 : loss : 0.021665, loss_ce: 0.008026
2022-01-06 16:19:47,743 iteration 6171 : loss : 0.016146, loss_ce: 0.006129
 91%|██████████████████████████▎  | 363/400 [2:14:48<13:16, 21.53s/it]2022-01-06 16:19:48,953 iteration 6172 : loss : 0.015614, loss_ce: 0.005263
2022-01-06 16:19:50,233 iteration 6173 : loss : 0.019151, loss_ce: 0.006737
2022-01-06 16:19:51,405 iteration 6174 : loss : 0.021325, loss_ce: 0.008883
2022-01-06 16:19:52,643 iteration 6175 : loss : 0.029929, loss_ce: 0.011323
2022-01-06 16:19:53,841 iteration 6176 : loss : 0.021225, loss_ce: 0.008304
2022-01-06 16:19:55,137 iteration 6177 : loss : 0.032819, loss_ce: 0.014071
2022-01-06 16:19:56,327 iteration 6178 : loss : 0.018755, loss_ce: 0.008273
2022-01-06 16:19:57,520 iteration 6179 : loss : 0.015036, loss_ce: 0.006985
2022-01-06 16:19:58,722 iteration 6180 : loss : 0.016781, loss_ce: 0.005665
2022-01-06 16:19:59,801 iteration 6181 : loss : 0.016868, loss_ce: 0.005549
2022-01-06 16:20:01,008 iteration 6182 : loss : 0.021829, loss_ce: 0.006701
2022-01-06 16:20:02,277 iteration 6183 : loss : 0.017286, loss_ce: 0.007148
2022-01-06 16:20:03,486 iteration 6184 : loss : 0.032350, loss_ce: 0.010089
2022-01-06 16:20:04,650 iteration 6185 : loss : 0.020856, loss_ce: 0.007724
2022-01-06 16:20:05,839 iteration 6186 : loss : 0.026906, loss_ce: 0.010974
2022-01-06 16:20:07,085 iteration 6187 : loss : 0.017899, loss_ce: 0.006268
2022-01-06 16:20:08,168 iteration 6188 : loss : 0.017230, loss_ce: 0.005561
 91%|██████████████████████████▍  | 364/400 [2:15:08<12:43, 21.20s/it]2022-01-06 16:20:09,347 iteration 6189 : loss : 0.015473, loss_ce: 0.004559
2022-01-06 16:20:10,443 iteration 6190 : loss : 0.021527, loss_ce: 0.007651
2022-01-06 16:20:11,800 iteration 6191 : loss : 0.023167, loss_ce: 0.008940
2022-01-06 16:20:13,034 iteration 6192 : loss : 0.021176, loss_ce: 0.007470
2022-01-06 16:20:14,080 iteration 6193 : loss : 0.014188, loss_ce: 0.005218
2022-01-06 16:20:15,264 iteration 6194 : loss : 0.018435, loss_ce: 0.006565
2022-01-06 16:20:16,497 iteration 6195 : loss : 0.018002, loss_ce: 0.008197
2022-01-06 16:20:17,727 iteration 6196 : loss : 0.016841, loss_ce: 0.007047
2022-01-06 16:20:18,920 iteration 6197 : loss : 0.019321, loss_ce: 0.009684
2022-01-06 16:20:20,090 iteration 6198 : loss : 0.018232, loss_ce: 0.006325
2022-01-06 16:20:21,276 iteration 6199 : loss : 0.021959, loss_ce: 0.006113
2022-01-06 16:20:22,373 iteration 6200 : loss : 0.013512, loss_ce: 0.006277
2022-01-06 16:20:23,575 iteration 6201 : loss : 0.029750, loss_ce: 0.010822
2022-01-06 16:20:24,706 iteration 6202 : loss : 0.019340, loss_ce: 0.007957
2022-01-06 16:20:25,841 iteration 6203 : loss : 0.019969, loss_ce: 0.004726
2022-01-06 16:20:27,051 iteration 6204 : loss : 0.014637, loss_ce: 0.005979
2022-01-06 16:20:27,051 Training Data Eval:
2022-01-06 16:20:32,861   Average segmentation loss on training set: 0.0125
2022-01-06 16:20:32,861 Validation Data Eval:
2022-01-06 16:20:34,856   Average segmentation loss on validation set: 0.0730
2022-01-06 16:20:36,135 iteration 6205 : loss : 0.035042, loss_ce: 0.010586
 91%|██████████████████████████▍  | 365/400 [2:15:36<13:33, 23.23s/it]2022-01-06 16:20:37,398 iteration 6206 : loss : 0.019292, loss_ce: 0.008848
2022-01-06 16:20:38,531 iteration 6207 : loss : 0.026083, loss_ce: 0.007974
2022-01-06 16:20:39,792 iteration 6208 : loss : 0.040155, loss_ce: 0.011055
2022-01-06 16:20:40,935 iteration 6209 : loss : 0.031452, loss_ce: 0.008655
2022-01-06 16:20:42,064 iteration 6210 : loss : 0.020889, loss_ce: 0.008306
2022-01-06 16:20:43,292 iteration 6211 : loss : 0.023742, loss_ce: 0.009579
2022-01-06 16:20:44,439 iteration 6212 : loss : 0.018190, loss_ce: 0.006124
2022-01-06 16:20:45,594 iteration 6213 : loss : 0.032078, loss_ce: 0.006605
2022-01-06 16:20:46,750 iteration 6214 : loss : 0.025347, loss_ce: 0.005554
2022-01-06 16:20:47,971 iteration 6215 : loss : 0.018779, loss_ce: 0.008753
2022-01-06 16:20:49,161 iteration 6216 : loss : 0.023019, loss_ce: 0.008256
2022-01-06 16:20:50,250 iteration 6217 : loss : 0.016522, loss_ce: 0.004969
2022-01-06 16:20:51,437 iteration 6218 : loss : 0.014317, loss_ce: 0.006550
2022-01-06 16:20:52,584 iteration 6219 : loss : 0.019574, loss_ce: 0.007520
2022-01-06 16:20:53,762 iteration 6220 : loss : 0.017976, loss_ce: 0.005448
2022-01-06 16:20:55,044 iteration 6221 : loss : 0.019769, loss_ce: 0.009805
2022-01-06 16:20:56,238 iteration 6222 : loss : 0.019037, loss_ce: 0.009434
 92%|██████████████████████████▌  | 366/400 [2:15:56<12:37, 22.29s/it]2022-01-06 16:20:57,513 iteration 6223 : loss : 0.040516, loss_ce: 0.009297
2022-01-06 16:20:58,673 iteration 6224 : loss : 0.024009, loss_ce: 0.007514
2022-01-06 16:20:59,839 iteration 6225 : loss : 0.021888, loss_ce: 0.008207
2022-01-06 16:21:01,104 iteration 6226 : loss : 0.030618, loss_ce: 0.014206
2022-01-06 16:21:02,284 iteration 6227 : loss : 0.019506, loss_ce: 0.004982
2022-01-06 16:21:03,490 iteration 6228 : loss : 0.020678, loss_ce: 0.009041
2022-01-06 16:21:04,604 iteration 6229 : loss : 0.016403, loss_ce: 0.006714
2022-01-06 16:21:05,741 iteration 6230 : loss : 0.025472, loss_ce: 0.010551
2022-01-06 16:21:06,941 iteration 6231 : loss : 0.017386, loss_ce: 0.007492
2022-01-06 16:21:08,148 iteration 6232 : loss : 0.025983, loss_ce: 0.010579
2022-01-06 16:21:09,312 iteration 6233 : loss : 0.016101, loss_ce: 0.007155
2022-01-06 16:21:10,513 iteration 6234 : loss : 0.018043, loss_ce: 0.007165
2022-01-06 16:21:11,783 iteration 6235 : loss : 0.039020, loss_ce: 0.011169
2022-01-06 16:21:12,845 iteration 6236 : loss : 0.014580, loss_ce: 0.005767
2022-01-06 16:21:14,009 iteration 6237 : loss : 0.018540, loss_ce: 0.008983
2022-01-06 16:21:15,122 iteration 6238 : loss : 0.011822, loss_ce: 0.004262
2022-01-06 16:21:16,274 iteration 6239 : loss : 0.015374, loss_ce: 0.006190
 92%|██████████████████████████▌  | 367/400 [2:16:16<11:53, 21.61s/it]2022-01-06 16:21:17,577 iteration 6240 : loss : 0.021272, loss_ce: 0.008139
2022-01-06 16:21:18,812 iteration 6241 : loss : 0.029177, loss_ce: 0.012430
2022-01-06 16:21:20,049 iteration 6242 : loss : 0.022713, loss_ce: 0.008388
2022-01-06 16:21:21,147 iteration 6243 : loss : 0.018913, loss_ce: 0.007575
2022-01-06 16:21:22,287 iteration 6244 : loss : 0.015959, loss_ce: 0.004569
2022-01-06 16:21:23,449 iteration 6245 : loss : 0.019357, loss_ce: 0.007071
2022-01-06 16:21:24,581 iteration 6246 : loss : 0.018118, loss_ce: 0.006618
2022-01-06 16:21:25,723 iteration 6247 : loss : 0.018291, loss_ce: 0.005124
2022-01-06 16:21:26,964 iteration 6248 : loss : 0.021735, loss_ce: 0.009108
2022-01-06 16:21:28,111 iteration 6249 : loss : 0.026658, loss_ce: 0.008919
2022-01-06 16:21:29,317 iteration 6250 : loss : 0.021700, loss_ce: 0.007970
2022-01-06 16:21:30,598 iteration 6251 : loss : 0.019677, loss_ce: 0.006756
2022-01-06 16:21:31,811 iteration 6252 : loss : 0.029104, loss_ce: 0.007107
2022-01-06 16:21:32,935 iteration 6253 : loss : 0.016652, loss_ce: 0.005337
2022-01-06 16:21:34,037 iteration 6254 : loss : 0.017662, loss_ce: 0.006994
2022-01-06 16:21:35,285 iteration 6255 : loss : 0.019300, loss_ce: 0.006489
2022-01-06 16:21:36,460 iteration 6256 : loss : 0.016274, loss_ce: 0.006417
 92%|██████████████████████████▋  | 368/400 [2:16:36<11:17, 21.18s/it]2022-01-06 16:21:37,733 iteration 6257 : loss : 0.012733, loss_ce: 0.004173
2022-01-06 16:21:38,838 iteration 6258 : loss : 0.024709, loss_ce: 0.008668
2022-01-06 16:21:40,079 iteration 6259 : loss : 0.022703, loss_ce: 0.007843
2022-01-06 16:21:41,413 iteration 6260 : loss : 0.035969, loss_ce: 0.009796
2022-01-06 16:21:42,624 iteration 6261 : loss : 0.025951, loss_ce: 0.012170
2022-01-06 16:21:43,868 iteration 6262 : loss : 0.014792, loss_ce: 0.005577
2022-01-06 16:21:45,008 iteration 6263 : loss : 0.017235, loss_ce: 0.005896
2022-01-06 16:21:46,131 iteration 6264 : loss : 0.021279, loss_ce: 0.008544
2022-01-06 16:21:47,331 iteration 6265 : loss : 0.016788, loss_ce: 0.006323
2022-01-06 16:21:48,489 iteration 6266 : loss : 0.014394, loss_ce: 0.005491
2022-01-06 16:21:49,874 iteration 6267 : loss : 0.028474, loss_ce: 0.012199
2022-01-06 16:21:51,039 iteration 6268 : loss : 0.022563, loss_ce: 0.007499
2022-01-06 16:21:52,231 iteration 6269 : loss : 0.019418, loss_ce: 0.007824
2022-01-06 16:21:53,389 iteration 6270 : loss : 0.023921, loss_ce: 0.010273
2022-01-06 16:21:54,673 iteration 6271 : loss : 0.023290, loss_ce: 0.009174
2022-01-06 16:21:55,814 iteration 6272 : loss : 0.022683, loss_ce: 0.006548
2022-01-06 16:21:57,103 iteration 6273 : loss : 0.028425, loss_ce: 0.011007
 92%|██████████████████████████▊  | 369/400 [2:16:57<10:51, 21.02s/it]2022-01-06 16:21:58,475 iteration 6274 : loss : 0.020851, loss_ce: 0.008116
2022-01-06 16:21:59,610 iteration 6275 : loss : 0.021331, loss_ce: 0.006272
2022-01-06 16:22:00,709 iteration 6276 : loss : 0.018322, loss_ce: 0.007553
2022-01-06 16:22:01,883 iteration 6277 : loss : 0.024403, loss_ce: 0.008566
2022-01-06 16:22:03,147 iteration 6278 : loss : 0.023194, loss_ce: 0.009387
2022-01-06 16:22:04,378 iteration 6279 : loss : 0.025892, loss_ce: 0.009293
2022-01-06 16:22:05,480 iteration 6280 : loss : 0.016478, loss_ce: 0.005068
2022-01-06 16:22:06,642 iteration 6281 : loss : 0.022329, loss_ce: 0.006646
2022-01-06 16:22:07,824 iteration 6282 : loss : 0.024434, loss_ce: 0.012117
2022-01-06 16:22:09,108 iteration 6283 : loss : 0.018186, loss_ce: 0.007019
2022-01-06 16:22:10,322 iteration 6284 : loss : 0.021320, loss_ce: 0.007452
2022-01-06 16:22:11,538 iteration 6285 : loss : 0.019139, loss_ce: 0.008706
2022-01-06 16:22:12,798 iteration 6286 : loss : 0.026286, loss_ce: 0.013093
2022-01-06 16:22:13,989 iteration 6287 : loss : 0.016728, loss_ce: 0.005959
2022-01-06 16:22:15,090 iteration 6288 : loss : 0.016757, loss_ce: 0.006059
2022-01-06 16:22:16,269 iteration 6289 : loss : 0.018317, loss_ce: 0.007763
2022-01-06 16:22:16,269 Training Data Eval:
2022-01-06 16:22:22,190   Average segmentation loss on training set: 0.0120
2022-01-06 16:22:22,190 Validation Data Eval:
2022-01-06 16:22:24,217   Average segmentation loss on validation set: 0.0671
2022-01-06 16:22:25,410 iteration 6290 : loss : 0.015732, loss_ce: 0.006336
 92%|██████████████████████████▊  | 370/400 [2:17:25<11:36, 23.21s/it]2022-01-06 16:22:26,642 iteration 6291 : loss : 0.018622, loss_ce: 0.005839
2022-01-06 16:22:27,731 iteration 6292 : loss : 0.017901, loss_ce: 0.005869
2022-01-06 16:22:28,878 iteration 6293 : loss : 0.022921, loss_ce: 0.006671
2022-01-06 16:22:30,166 iteration 6294 : loss : 0.021915, loss_ce: 0.006570
2022-01-06 16:22:31,347 iteration 6295 : loss : 0.019693, loss_ce: 0.007458
2022-01-06 16:22:32,449 iteration 6296 : loss : 0.015842, loss_ce: 0.006289
2022-01-06 16:22:33,710 iteration 6297 : loss : 0.024577, loss_ce: 0.011696
2022-01-06 16:22:34,883 iteration 6298 : loss : 0.020953, loss_ce: 0.010256
2022-01-06 16:22:36,059 iteration 6299 : loss : 0.020609, loss_ce: 0.008517
2022-01-06 16:22:37,300 iteration 6300 : loss : 0.018296, loss_ce: 0.005034
2022-01-06 16:22:38,498 iteration 6301 : loss : 0.018026, loss_ce: 0.008708
2022-01-06 16:22:39,758 iteration 6302 : loss : 0.019256, loss_ce: 0.007644
2022-01-06 16:22:40,897 iteration 6303 : loss : 0.015984, loss_ce: 0.006314
2022-01-06 16:22:42,096 iteration 6304 : loss : 0.021357, loss_ce: 0.008759
2022-01-06 16:22:43,228 iteration 6305 : loss : 0.021278, loss_ce: 0.007627
2022-01-06 16:22:44,413 iteration 6306 : loss : 0.018522, loss_ce: 0.008328
2022-01-06 16:22:45,574 iteration 6307 : loss : 0.017311, loss_ce: 0.005845
 93%|██████████████████████████▉  | 371/400 [2:17:45<10:46, 22.30s/it]2022-01-06 16:22:46,861 iteration 6308 : loss : 0.016029, loss_ce: 0.005825
2022-01-06 16:22:48,013 iteration 6309 : loss : 0.020032, loss_ce: 0.005646
2022-01-06 16:22:49,188 iteration 6310 : loss : 0.015650, loss_ce: 0.006870
2022-01-06 16:22:50,431 iteration 6311 : loss : 0.026043, loss_ce: 0.007989
2022-01-06 16:22:51,578 iteration 6312 : loss : 0.015919, loss_ce: 0.006687
2022-01-06 16:22:52,776 iteration 6313 : loss : 0.036010, loss_ce: 0.011822
2022-01-06 16:22:54,000 iteration 6314 : loss : 0.015624, loss_ce: 0.006200
2022-01-06 16:22:55,151 iteration 6315 : loss : 0.020342, loss_ce: 0.008570
2022-01-06 16:22:56,246 iteration 6316 : loss : 0.019341, loss_ce: 0.006222
2022-01-06 16:22:57,336 iteration 6317 : loss : 0.020340, loss_ce: 0.009497
2022-01-06 16:22:58,578 iteration 6318 : loss : 0.025271, loss_ce: 0.010728
2022-01-06 16:22:59,735 iteration 6319 : loss : 0.026291, loss_ce: 0.009434
2022-01-06 16:23:00,905 iteration 6320 : loss : 0.017558, loss_ce: 0.008109
2022-01-06 16:23:02,069 iteration 6321 : loss : 0.018955, loss_ce: 0.007122
2022-01-06 16:23:03,342 iteration 6322 : loss : 0.025529, loss_ce: 0.009609
2022-01-06 16:23:04,637 iteration 6323 : loss : 0.031774, loss_ce: 0.009612
2022-01-06 16:23:05,780 iteration 6324 : loss : 0.018680, loss_ce: 0.007641
 93%|██████████████████████████▉  | 372/400 [2:18:06<10:06, 21.67s/it]2022-01-06 16:23:06,957 iteration 6325 : loss : 0.017545, loss_ce: 0.008586
2022-01-06 16:23:08,063 iteration 6326 : loss : 0.015671, loss_ce: 0.006165
2022-01-06 16:23:09,308 iteration 6327 : loss : 0.016877, loss_ce: 0.006696
2022-01-06 16:23:10,480 iteration 6328 : loss : 0.014282, loss_ce: 0.004239
2022-01-06 16:23:11,605 iteration 6329 : loss : 0.019256, loss_ce: 0.005250
2022-01-06 16:23:12,783 iteration 6330 : loss : 0.013940, loss_ce: 0.004515
2022-01-06 16:23:14,183 iteration 6331 : loss : 0.038630, loss_ce: 0.013969
2022-01-06 16:23:15,304 iteration 6332 : loss : 0.018921, loss_ce: 0.008168
2022-01-06 16:23:16,502 iteration 6333 : loss : 0.015932, loss_ce: 0.004417
2022-01-06 16:23:17,673 iteration 6334 : loss : 0.019009, loss_ce: 0.008791
2022-01-06 16:23:18,960 iteration 6335 : loss : 0.020708, loss_ce: 0.008238
2022-01-06 16:23:20,055 iteration 6336 : loss : 0.011351, loss_ce: 0.004105
2022-01-06 16:23:21,269 iteration 6337 : loss : 0.022522, loss_ce: 0.008824
2022-01-06 16:23:22,571 iteration 6338 : loss : 0.032836, loss_ce: 0.005411
2022-01-06 16:23:23,714 iteration 6339 : loss : 0.020458, loss_ce: 0.009271
2022-01-06 16:23:24,852 iteration 6340 : loss : 0.017715, loss_ce: 0.009430
2022-01-06 16:23:26,045 iteration 6341 : loss : 0.020567, loss_ce: 0.007280
 93%|███████████████████████████  | 373/400 [2:18:26<09:33, 21.25s/it]2022-01-06 16:23:27,288 iteration 6342 : loss : 0.026642, loss_ce: 0.009366
2022-01-06 16:23:28,434 iteration 6343 : loss : 0.053465, loss_ce: 0.016282
2022-01-06 16:23:29,708 iteration 6344 : loss : 0.028923, loss_ce: 0.013000
2022-01-06 16:23:30,888 iteration 6345 : loss : 0.019310, loss_ce: 0.006055
2022-01-06 16:23:32,031 iteration 6346 : loss : 0.039744, loss_ce: 0.019659
2022-01-06 16:23:33,259 iteration 6347 : loss : 0.024974, loss_ce: 0.013600
2022-01-06 16:23:34,394 iteration 6348 : loss : 0.018676, loss_ce: 0.007890
2022-01-06 16:23:35,566 iteration 6349 : loss : 0.017942, loss_ce: 0.005111
2022-01-06 16:23:36,737 iteration 6350 : loss : 0.019996, loss_ce: 0.006932
2022-01-06 16:23:37,941 iteration 6351 : loss : 0.026755, loss_ce: 0.016965
2022-01-06 16:23:39,178 iteration 6352 : loss : 0.022507, loss_ce: 0.009106
2022-01-06 16:23:40,490 iteration 6353 : loss : 0.035988, loss_ce: 0.011993
2022-01-06 16:23:41,627 iteration 6354 : loss : 0.026713, loss_ce: 0.006433
2022-01-06 16:23:42,894 iteration 6355 : loss : 0.021361, loss_ce: 0.007513
2022-01-06 16:23:44,176 iteration 6356 : loss : 0.020431, loss_ce: 0.008273
2022-01-06 16:23:45,384 iteration 6357 : loss : 0.021639, loss_ce: 0.011541
2022-01-06 16:23:46,632 iteration 6358 : loss : 0.023849, loss_ce: 0.006975
 94%|███████████████████████████  | 374/400 [2:18:46<09:07, 21.05s/it]2022-01-06 16:23:47,869 iteration 6359 : loss : 0.017169, loss_ce: 0.006053
2022-01-06 16:23:48,927 iteration 6360 : loss : 0.012698, loss_ce: 0.004799
2022-01-06 16:23:50,215 iteration 6361 : loss : 0.019032, loss_ce: 0.006533
2022-01-06 16:23:51,422 iteration 6362 : loss : 0.019860, loss_ce: 0.007459
2022-01-06 16:23:52,720 iteration 6363 : loss : 0.026795, loss_ce: 0.009152
2022-01-06 16:23:53,943 iteration 6364 : loss : 0.017123, loss_ce: 0.005902
2022-01-06 16:23:55,117 iteration 6365 : loss : 0.026947, loss_ce: 0.011042
2022-01-06 16:23:56,292 iteration 6366 : loss : 0.018866, loss_ce: 0.006768
2022-01-06 16:23:57,554 iteration 6367 : loss : 0.054362, loss_ce: 0.020962
2022-01-06 16:23:58,784 iteration 6368 : loss : 0.028586, loss_ce: 0.007607
2022-01-06 16:23:59,964 iteration 6369 : loss : 0.025044, loss_ce: 0.005965
2022-01-06 16:24:01,192 iteration 6370 : loss : 0.021874, loss_ce: 0.011166
2022-01-06 16:24:02,328 iteration 6371 : loss : 0.014987, loss_ce: 0.006209
2022-01-06 16:24:03,542 iteration 6372 : loss : 0.019107, loss_ce: 0.007554
2022-01-06 16:24:04,671 iteration 6373 : loss : 0.016439, loss_ce: 0.006870
2022-01-06 16:24:05,752 iteration 6374 : loss : 0.014982, loss_ce: 0.005736
2022-01-06 16:24:05,752 Training Data Eval:
2022-01-06 16:24:11,625   Average segmentation loss on training set: 0.0120
2022-01-06 16:24:11,626 Validation Data Eval:
2022-01-06 16:24:13,616   Average segmentation loss on validation set: 0.0654
2022-01-06 16:24:14,850 iteration 6375 : loss : 0.016369, loss_ce: 0.008165
 94%|███████████████████████████▏ | 375/400 [2:19:15<09:39, 23.20s/it]2022-01-06 16:24:16,182 iteration 6376 : loss : 0.030819, loss_ce: 0.011642
2022-01-06 16:24:17,386 iteration 6377 : loss : 0.029247, loss_ce: 0.006763
2022-01-06 16:24:18,539 iteration 6378 : loss : 0.019525, loss_ce: 0.009396
2022-01-06 16:24:19,729 iteration 6379 : loss : 0.020743, loss_ce: 0.008718
2022-01-06 16:24:20,978 iteration 6380 : loss : 0.023687, loss_ce: 0.009430
2022-01-06 16:24:22,232 iteration 6381 : loss : 0.032296, loss_ce: 0.015480
2022-01-06 16:24:23,410 iteration 6382 : loss : 0.015372, loss_ce: 0.007318
2022-01-06 16:24:24,639 iteration 6383 : loss : 0.024026, loss_ce: 0.008211
2022-01-06 16:24:25,887 iteration 6384 : loss : 0.023867, loss_ce: 0.009973
2022-01-06 16:24:26,914 iteration 6385 : loss : 0.015837, loss_ce: 0.005714
2022-01-06 16:24:28,154 iteration 6386 : loss : 0.023793, loss_ce: 0.007703
2022-01-06 16:24:29,381 iteration 6387 : loss : 0.021716, loss_ce: 0.009659
2022-01-06 16:24:30,483 iteration 6388 : loss : 0.016221, loss_ce: 0.007900
2022-01-06 16:24:31,722 iteration 6389 : loss : 0.024372, loss_ce: 0.011996
2022-01-06 16:24:32,942 iteration 6390 : loss : 0.027082, loss_ce: 0.007648
2022-01-06 16:24:34,185 iteration 6391 : loss : 0.019217, loss_ce: 0.008115
2022-01-06 16:24:35,443 iteration 6392 : loss : 0.035208, loss_ce: 0.008291
 94%|███████████████████████████▎ | 376/400 [2:19:35<08:57, 22.42s/it]2022-01-06 16:24:36,651 iteration 6393 : loss : 0.016060, loss_ce: 0.008122
2022-01-06 16:24:37,817 iteration 6394 : loss : 0.014867, loss_ce: 0.006008
2022-01-06 16:24:38,944 iteration 6395 : loss : 0.012238, loss_ce: 0.003655
2022-01-06 16:24:40,158 iteration 6396 : loss : 0.022703, loss_ce: 0.008289
2022-01-06 16:24:41,400 iteration 6397 : loss : 0.021815, loss_ce: 0.006603
2022-01-06 16:24:42,649 iteration 6398 : loss : 0.022765, loss_ce: 0.007873
2022-01-06 16:24:43,753 iteration 6399 : loss : 0.017994, loss_ce: 0.005303
2022-01-06 16:24:44,904 iteration 6400 : loss : 0.016329, loss_ce: 0.005980
2022-01-06 16:24:46,003 iteration 6401 : loss : 0.011803, loss_ce: 0.004307
2022-01-06 16:24:47,234 iteration 6402 : loss : 0.022107, loss_ce: 0.008643
2022-01-06 16:24:48,369 iteration 6403 : loss : 0.023036, loss_ce: 0.008541
2022-01-06 16:24:49,477 iteration 6404 : loss : 0.016161, loss_ce: 0.007700
2022-01-06 16:24:50,817 iteration 6405 : loss : 0.023325, loss_ce: 0.009679
2022-01-06 16:24:51,928 iteration 6406 : loss : 0.013684, loss_ce: 0.005416
2022-01-06 16:24:53,134 iteration 6407 : loss : 0.022222, loss_ce: 0.009124
2022-01-06 16:24:54,353 iteration 6408 : loss : 0.018604, loss_ce: 0.006587
2022-01-06 16:24:55,464 iteration 6409 : loss : 0.015741, loss_ce: 0.005424
 94%|███████████████████████████▎ | 377/400 [2:19:55<08:19, 21.70s/it]2022-01-06 16:24:56,640 iteration 6410 : loss : 0.018340, loss_ce: 0.006871
2022-01-06 16:24:57,796 iteration 6411 : loss : 0.015647, loss_ce: 0.005894
2022-01-06 16:24:59,059 iteration 6412 : loss : 0.026696, loss_ce: 0.009169
2022-01-06 16:25:00,248 iteration 6413 : loss : 0.022963, loss_ce: 0.009940
2022-01-06 16:25:01,482 iteration 6414 : loss : 0.018984, loss_ce: 0.008214
2022-01-06 16:25:02,709 iteration 6415 : loss : 0.023068, loss_ce: 0.006750
2022-01-06 16:25:03,825 iteration 6416 : loss : 0.017373, loss_ce: 0.006280
2022-01-06 16:25:05,046 iteration 6417 : loss : 0.026630, loss_ce: 0.009581
2022-01-06 16:25:06,221 iteration 6418 : loss : 0.027436, loss_ce: 0.011972
2022-01-06 16:25:07,410 iteration 6419 : loss : 0.019834, loss_ce: 0.007957
2022-01-06 16:25:08,634 iteration 6420 : loss : 0.027262, loss_ce: 0.009732
2022-01-06 16:25:09,879 iteration 6421 : loss : 0.017454, loss_ce: 0.006909
2022-01-06 16:25:11,044 iteration 6422 : loss : 0.018192, loss_ce: 0.007128
2022-01-06 16:25:12,238 iteration 6423 : loss : 0.032155, loss_ce: 0.012547
2022-01-06 16:25:13,359 iteration 6424 : loss : 0.017878, loss_ce: 0.005698
2022-01-06 16:25:14,702 iteration 6425 : loss : 0.017459, loss_ce: 0.006195
2022-01-06 16:25:15,804 iteration 6426 : loss : 0.018302, loss_ce: 0.006313
 94%|███████████████████████████▍ | 378/400 [2:20:16<07:48, 21.29s/it]2022-01-06 16:25:17,078 iteration 6427 : loss : 0.019506, loss_ce: 0.007522
2022-01-06 16:25:18,306 iteration 6428 : loss : 0.022392, loss_ce: 0.007159
2022-01-06 16:25:19,460 iteration 6429 : loss : 0.016864, loss_ce: 0.007064
2022-01-06 16:25:20,711 iteration 6430 : loss : 0.020670, loss_ce: 0.009630
2022-01-06 16:25:21,881 iteration 6431 : loss : 0.028466, loss_ce: 0.007835
2022-01-06 16:25:23,056 iteration 6432 : loss : 0.023864, loss_ce: 0.007772
2022-01-06 16:25:24,229 iteration 6433 : loss : 0.018230, loss_ce: 0.004788
2022-01-06 16:25:25,412 iteration 6434 : loss : 0.034310, loss_ce: 0.012291
2022-01-06 16:25:26,483 iteration 6435 : loss : 0.016118, loss_ce: 0.005205
2022-01-06 16:25:27,605 iteration 6436 : loss : 0.013545, loss_ce: 0.006970
2022-01-06 16:25:28,709 iteration 6437 : loss : 0.014923, loss_ce: 0.005660
2022-01-06 16:25:29,803 iteration 6438 : loss : 0.013832, loss_ce: 0.006423
2022-01-06 16:25:31,054 iteration 6439 : loss : 0.027404, loss_ce: 0.008622
2022-01-06 16:25:32,242 iteration 6440 : loss : 0.021351, loss_ce: 0.008832
2022-01-06 16:25:33,438 iteration 6441 : loss : 0.014324, loss_ce: 0.005938
2022-01-06 16:25:34,617 iteration 6442 : loss : 0.014685, loss_ce: 0.006691
2022-01-06 16:25:35,772 iteration 6443 : loss : 0.015463, loss_ce: 0.005304
 95%|███████████████████████████▍ | 379/400 [2:20:36<07:18, 20.89s/it]2022-01-06 16:25:36,984 iteration 6444 : loss : 0.020333, loss_ce: 0.009381
2022-01-06 16:25:38,102 iteration 6445 : loss : 0.012304, loss_ce: 0.004654
2022-01-06 16:25:39,278 iteration 6446 : loss : 0.028167, loss_ce: 0.008609
2022-01-06 16:25:40,448 iteration 6447 : loss : 0.014693, loss_ce: 0.005967
2022-01-06 16:25:41,611 iteration 6448 : loss : 0.023954, loss_ce: 0.006689
2022-01-06 16:25:42,817 iteration 6449 : loss : 0.020515, loss_ce: 0.006833
2022-01-06 16:25:44,074 iteration 6450 : loss : 0.036709, loss_ce: 0.009061
2022-01-06 16:25:45,238 iteration 6451 : loss : 0.019243, loss_ce: 0.006907
2022-01-06 16:25:46,470 iteration 6452 : loss : 0.022589, loss_ce: 0.006428
2022-01-06 16:25:47,618 iteration 6453 : loss : 0.015139, loss_ce: 0.006386
2022-01-06 16:25:48,787 iteration 6454 : loss : 0.020543, loss_ce: 0.007020
2022-01-06 16:25:49,902 iteration 6455 : loss : 0.016466, loss_ce: 0.006326
2022-01-06 16:25:51,092 iteration 6456 : loss : 0.019954, loss_ce: 0.009057
2022-01-06 16:25:52,237 iteration 6457 : loss : 0.013889, loss_ce: 0.005564
2022-01-06 16:25:53,363 iteration 6458 : loss : 0.015616, loss_ce: 0.008363
2022-01-06 16:25:54,614 iteration 6459 : loss : 0.018491, loss_ce: 0.007195
2022-01-06 16:25:54,614 Training Data Eval:
2022-01-06 16:26:00,426   Average segmentation loss on training set: 0.0119
2022-01-06 16:26:00,427 Validation Data Eval:
2022-01-06 16:26:02,438   Average segmentation loss on validation set: 0.0706
2022-01-06 16:26:03,733 iteration 6460 : loss : 0.016996, loss_ce: 0.005529
 95%|███████████████████████████▌ | 380/400 [2:21:04<07:40, 23.01s/it]2022-01-06 16:26:04,953 iteration 6461 : loss : 0.020541, loss_ce: 0.007473
2022-01-06 16:26:06,155 iteration 6462 : loss : 0.016941, loss_ce: 0.006893
2022-01-06 16:26:07,286 iteration 6463 : loss : 0.016450, loss_ce: 0.007101
2022-01-06 16:26:08,448 iteration 6464 : loss : 0.020895, loss_ce: 0.007232
2022-01-06 16:26:09,628 iteration 6465 : loss : 0.018868, loss_ce: 0.006893
2022-01-06 16:26:10,805 iteration 6466 : loss : 0.027768, loss_ce: 0.010487
2022-01-06 16:26:11,986 iteration 6467 : loss : 0.022583, loss_ce: 0.009098
2022-01-06 16:26:13,095 iteration 6468 : loss : 0.016104, loss_ce: 0.005103
2022-01-06 16:26:14,335 iteration 6469 : loss : 0.025345, loss_ce: 0.007062
2022-01-06 16:26:15,553 iteration 6470 : loss : 0.015095, loss_ce: 0.006443
2022-01-06 16:26:16,755 iteration 6471 : loss : 0.019651, loss_ce: 0.007968
2022-01-06 16:26:17,873 iteration 6472 : loss : 0.019124, loss_ce: 0.006972
2022-01-06 16:26:19,101 iteration 6473 : loss : 0.022409, loss_ce: 0.010324
2022-01-06 16:26:20,312 iteration 6474 : loss : 0.018503, loss_ce: 0.008115
2022-01-06 16:26:21,547 iteration 6475 : loss : 0.024328, loss_ce: 0.007757
2022-01-06 16:26:22,758 iteration 6476 : loss : 0.018850, loss_ce: 0.006644
2022-01-06 16:26:23,956 iteration 6477 : loss : 0.018916, loss_ce: 0.007739
 95%|███████████████████████████▌ | 381/400 [2:21:24<07:01, 22.18s/it]2022-01-06 16:26:25,205 iteration 6478 : loss : 0.018548, loss_ce: 0.007722
2022-01-06 16:26:26,386 iteration 6479 : loss : 0.025269, loss_ce: 0.008189
2022-01-06 16:26:27,523 iteration 6480 : loss : 0.016232, loss_ce: 0.006079
2022-01-06 16:26:28,858 iteration 6481 : loss : 0.030408, loss_ce: 0.011829
2022-01-06 16:26:30,006 iteration 6482 : loss : 0.020584, loss_ce: 0.008207
2022-01-06 16:26:31,247 iteration 6483 : loss : 0.019758, loss_ce: 0.008034
2022-01-06 16:26:32,314 iteration 6484 : loss : 0.018311, loss_ce: 0.006748
2022-01-06 16:26:33,492 iteration 6485 : loss : 0.017171, loss_ce: 0.005098
2022-01-06 16:26:34,690 iteration 6486 : loss : 0.022308, loss_ce: 0.008052
2022-01-06 16:26:35,910 iteration 6487 : loss : 0.019567, loss_ce: 0.008149
2022-01-06 16:26:37,147 iteration 6488 : loss : 0.015814, loss_ce: 0.005662
2022-01-06 16:26:38,341 iteration 6489 : loss : 0.030228, loss_ce: 0.006934
2022-01-06 16:26:39,566 iteration 6490 : loss : 0.017235, loss_ce: 0.006123
2022-01-06 16:26:40,702 iteration 6491 : loss : 0.017215, loss_ce: 0.008568
2022-01-06 16:26:41,831 iteration 6492 : loss : 0.014714, loss_ce: 0.007497
2022-01-06 16:26:43,000 iteration 6493 : loss : 0.021976, loss_ce: 0.007962
2022-01-06 16:26:44,218 iteration 6494 : loss : 0.021545, loss_ce: 0.007848
 96%|███████████████████████████▋ | 382/400 [2:21:44<06:28, 21.60s/it]2022-01-06 16:26:45,389 iteration 6495 : loss : 0.016745, loss_ce: 0.007401
2022-01-06 16:26:46,559 iteration 6496 : loss : 0.021055, loss_ce: 0.007100
2022-01-06 16:26:47,779 iteration 6497 : loss : 0.013500, loss_ce: 0.006263
2022-01-06 16:26:48,908 iteration 6498 : loss : 0.021611, loss_ce: 0.005161
2022-01-06 16:26:50,077 iteration 6499 : loss : 0.024280, loss_ce: 0.006131
2022-01-06 16:26:51,383 iteration 6500 : loss : 0.034333, loss_ce: 0.010459
2022-01-06 16:26:52,655 iteration 6501 : loss : 0.020782, loss_ce: 0.009785
2022-01-06 16:26:53,866 iteration 6502 : loss : 0.016512, loss_ce: 0.005547
2022-01-06 16:26:55,077 iteration 6503 : loss : 0.019843, loss_ce: 0.008213
2022-01-06 16:26:56,288 iteration 6504 : loss : 0.020960, loss_ce: 0.008351
2022-01-06 16:26:57,391 iteration 6505 : loss : 0.014607, loss_ce: 0.005472
2022-01-06 16:26:58,607 iteration 6506 : loss : 0.020928, loss_ce: 0.007304
2022-01-06 16:26:59,764 iteration 6507 : loss : 0.017335, loss_ce: 0.007672
2022-01-06 16:27:00,846 iteration 6508 : loss : 0.022978, loss_ce: 0.008054
2022-01-06 16:27:02,010 iteration 6509 : loss : 0.015728, loss_ce: 0.005345
2022-01-06 16:27:03,058 iteration 6510 : loss : 0.011965, loss_ce: 0.004657
2022-01-06 16:27:04,271 iteration 6511 : loss : 0.019476, loss_ce: 0.007781
 96%|███████████████████████████▊ | 383/400 [2:22:04<05:59, 21.14s/it]2022-01-06 16:27:05,558 iteration 6512 : loss : 0.021803, loss_ce: 0.007125
2022-01-06 16:27:06,818 iteration 6513 : loss : 0.021108, loss_ce: 0.007641
2022-01-06 16:27:08,023 iteration 6514 : loss : 0.034062, loss_ce: 0.009753
2022-01-06 16:27:09,290 iteration 6515 : loss : 0.022329, loss_ce: 0.008367
2022-01-06 16:27:10,449 iteration 6516 : loss : 0.015565, loss_ce: 0.006074
2022-01-06 16:27:11,591 iteration 6517 : loss : 0.018409, loss_ce: 0.009181
2022-01-06 16:27:12,832 iteration 6518 : loss : 0.024556, loss_ce: 0.009549
2022-01-06 16:27:14,090 iteration 6519 : loss : 0.026205, loss_ce: 0.011584
2022-01-06 16:27:15,197 iteration 6520 : loss : 0.019760, loss_ce: 0.008345
2022-01-06 16:27:16,431 iteration 6521 : loss : 0.020322, loss_ce: 0.009045
2022-01-06 16:27:17,634 iteration 6522 : loss : 0.018288, loss_ce: 0.006088
2022-01-06 16:27:18,866 iteration 6523 : loss : 0.019538, loss_ce: 0.008395
2022-01-06 16:27:20,087 iteration 6524 : loss : 0.018741, loss_ce: 0.006449
2022-01-06 16:27:21,327 iteration 6525 : loss : 0.019538, loss_ce: 0.006820
2022-01-06 16:27:22,511 iteration 6526 : loss : 0.023491, loss_ce: 0.008446
2022-01-06 16:27:23,641 iteration 6527 : loss : 0.019903, loss_ce: 0.006721
2022-01-06 16:27:24,775 iteration 6528 : loss : 0.015302, loss_ce: 0.005963
 96%|███████████████████████████▊ | 384/400 [2:22:25<05:35, 20.95s/it]2022-01-06 16:27:26,226 iteration 6529 : loss : 0.018523, loss_ce: 0.007798
2022-01-06 16:27:27,399 iteration 6530 : loss : 0.022057, loss_ce: 0.009686
2022-01-06 16:27:28,691 iteration 6531 : loss : 0.021352, loss_ce: 0.007904
2022-01-06 16:27:29,891 iteration 6532 : loss : 0.029054, loss_ce: 0.010936
2022-01-06 16:27:31,138 iteration 6533 : loss : 0.054013, loss_ce: 0.012056
2022-01-06 16:27:32,502 iteration 6534 : loss : 0.037397, loss_ce: 0.010174
2022-01-06 16:27:33,606 iteration 6535 : loss : 0.029947, loss_ce: 0.005172
2022-01-06 16:27:34,797 iteration 6536 : loss : 0.027366, loss_ce: 0.010692
2022-01-06 16:27:35,984 iteration 6537 : loss : 0.018468, loss_ce: 0.006875
2022-01-06 16:27:37,192 iteration 6538 : loss : 0.021632, loss_ce: 0.010292
2022-01-06 16:27:38,484 iteration 6539 : loss : 0.023369, loss_ce: 0.008651
2022-01-06 16:27:39,663 iteration 6540 : loss : 0.018842, loss_ce: 0.008696
2022-01-06 16:27:40,820 iteration 6541 : loss : 0.015099, loss_ce: 0.006373
2022-01-06 16:27:42,074 iteration 6542 : loss : 0.019432, loss_ce: 0.008371
2022-01-06 16:27:43,288 iteration 6543 : loss : 0.037142, loss_ce: 0.015673
2022-01-06 16:27:44,383 iteration 6544 : loss : 0.014189, loss_ce: 0.004764
2022-01-06 16:27:44,383 Training Data Eval:
2022-01-06 16:27:50,183   Average segmentation loss on training set: 0.0122
2022-01-06 16:27:50,183 Validation Data Eval:
2022-01-06 16:27:52,166   Average segmentation loss on validation set: 0.0687
2022-01-06 16:27:53,229 iteration 6545 : loss : 0.012525, loss_ce: 0.003075
 96%|███████████████████████████▉ | 385/400 [2:22:53<05:47, 23.20s/it]2022-01-06 16:27:54,526 iteration 6546 : loss : 0.024722, loss_ce: 0.008419
2022-01-06 16:27:55,928 iteration 6547 : loss : 0.029368, loss_ce: 0.010485
2022-01-06 16:27:57,002 iteration 6548 : loss : 0.016866, loss_ce: 0.009296
2022-01-06 16:27:58,074 iteration 6549 : loss : 0.011569, loss_ce: 0.003879
2022-01-06 16:27:59,228 iteration 6550 : loss : 0.016923, loss_ce: 0.006010
2022-01-06 16:28:00,476 iteration 6551 : loss : 0.016670, loss_ce: 0.007938
2022-01-06 16:28:01,558 iteration 6552 : loss : 0.017189, loss_ce: 0.008076
2022-01-06 16:28:02,834 iteration 6553 : loss : 0.030243, loss_ce: 0.011552
2022-01-06 16:28:03,999 iteration 6554 : loss : 0.019488, loss_ce: 0.005871
2022-01-06 16:28:05,247 iteration 6555 : loss : 0.018048, loss_ce: 0.006637
2022-01-06 16:28:06,466 iteration 6556 : loss : 0.019213, loss_ce: 0.005003
2022-01-06 16:28:07,583 iteration 6557 : loss : 0.022730, loss_ce: 0.007628
2022-01-06 16:28:08,721 iteration 6558 : loss : 0.027773, loss_ce: 0.007823
2022-01-06 16:28:09,952 iteration 6559 : loss : 0.020202, loss_ce: 0.008755
2022-01-06 16:28:11,238 iteration 6560 : loss : 0.050755, loss_ce: 0.013139
2022-01-06 16:28:12,366 iteration 6561 : loss : 0.016231, loss_ce: 0.006665
2022-01-06 16:28:13,442 iteration 6562 : loss : 0.011622, loss_ce: 0.003730
 96%|███████████████████████████▉ | 386/400 [2:23:13<05:12, 22.31s/it]2022-01-06 16:28:14,678 iteration 6563 : loss : 0.014168, loss_ce: 0.006281
2022-01-06 16:28:15,802 iteration 6564 : loss : 0.022230, loss_ce: 0.006698
2022-01-06 16:28:16,904 iteration 6565 : loss : 0.015377, loss_ce: 0.004478
2022-01-06 16:28:18,056 iteration 6566 : loss : 0.013512, loss_ce: 0.005406
2022-01-06 16:28:19,283 iteration 6567 : loss : 0.014855, loss_ce: 0.005922
2022-01-06 16:28:20,492 iteration 6568 : loss : 0.022487, loss_ce: 0.009057
2022-01-06 16:28:21,743 iteration 6569 : loss : 0.028264, loss_ce: 0.010970
2022-01-06 16:28:22,918 iteration 6570 : loss : 0.019159, loss_ce: 0.005038
2022-01-06 16:28:24,249 iteration 6571 : loss : 0.019241, loss_ce: 0.008821
2022-01-06 16:28:25,355 iteration 6572 : loss : 0.013150, loss_ce: 0.005554
2022-01-06 16:28:26,610 iteration 6573 : loss : 0.030135, loss_ce: 0.009396
2022-01-06 16:28:27,767 iteration 6574 : loss : 0.012679, loss_ce: 0.003764
2022-01-06 16:28:29,035 iteration 6575 : loss : 0.028291, loss_ce: 0.009891
2022-01-06 16:28:30,218 iteration 6576 : loss : 0.021457, loss_ce: 0.008948
2022-01-06 16:28:31,356 iteration 6577 : loss : 0.015897, loss_ce: 0.004902
2022-01-06 16:28:32,580 iteration 6578 : loss : 0.012432, loss_ce: 0.004552
2022-01-06 16:28:33,738 iteration 6579 : loss : 0.017236, loss_ce: 0.007706
 97%|████████████████████████████ | 387/400 [2:23:34<04:42, 21.70s/it]2022-01-06 16:28:35,050 iteration 6580 : loss : 0.030083, loss_ce: 0.013163
2022-01-06 16:28:36,173 iteration 6581 : loss : 0.018141, loss_ce: 0.005561
2022-01-06 16:28:37,456 iteration 6582 : loss : 0.030523, loss_ce: 0.010673
2022-01-06 16:28:38,639 iteration 6583 : loss : 0.027724, loss_ce: 0.009506
2022-01-06 16:28:39,756 iteration 6584 : loss : 0.019988, loss_ce: 0.008368
2022-01-06 16:28:40,850 iteration 6585 : loss : 0.015270, loss_ce: 0.004921
2022-01-06 16:28:42,010 iteration 6586 : loss : 0.014400, loss_ce: 0.003094
2022-01-06 16:28:43,119 iteration 6587 : loss : 0.023510, loss_ce: 0.010338
2022-01-06 16:28:44,323 iteration 6588 : loss : 0.013490, loss_ce: 0.005886
2022-01-06 16:28:45,530 iteration 6589 : loss : 0.017521, loss_ce: 0.007190
2022-01-06 16:28:46,674 iteration 6590 : loss : 0.015420, loss_ce: 0.007054
2022-01-06 16:28:47,832 iteration 6591 : loss : 0.012365, loss_ce: 0.004607
2022-01-06 16:28:49,101 iteration 6592 : loss : 0.026968, loss_ce: 0.014373
2022-01-06 16:28:50,174 iteration 6593 : loss : 0.013804, loss_ce: 0.007138
2022-01-06 16:28:51,326 iteration 6594 : loss : 0.013881, loss_ce: 0.004782
2022-01-06 16:28:52,557 iteration 6595 : loss : 0.025085, loss_ce: 0.007706
2022-01-06 16:28:53,672 iteration 6596 : loss : 0.018266, loss_ce: 0.005313
 97%|████████████████████████████▏| 388/400 [2:23:53<04:14, 21.17s/it]2022-01-06 16:28:54,869 iteration 6597 : loss : 0.013921, loss_ce: 0.005966
2022-01-06 16:28:56,107 iteration 6598 : loss : 0.023932, loss_ce: 0.004176
2022-01-06 16:28:57,348 iteration 6599 : loss : 0.022186, loss_ce: 0.008425
2022-01-06 16:28:58,455 iteration 6600 : loss : 0.014530, loss_ce: 0.006198
2022-01-06 16:28:59,749 iteration 6601 : loss : 0.020609, loss_ce: 0.008790
2022-01-06 16:29:00,974 iteration 6602 : loss : 0.021534, loss_ce: 0.008754
2022-01-06 16:29:02,108 iteration 6603 : loss : 0.016913, loss_ce: 0.007293
2022-01-06 16:29:03,344 iteration 6604 : loss : 0.015972, loss_ce: 0.007586
2022-01-06 16:29:04,508 iteration 6605 : loss : 0.035705, loss_ce: 0.016409
2022-01-06 16:29:05,770 iteration 6606 : loss : 0.024204, loss_ce: 0.009282
2022-01-06 16:29:07,032 iteration 6607 : loss : 0.022906, loss_ce: 0.010181
2022-01-06 16:29:08,134 iteration 6608 : loss : 0.022789, loss_ce: 0.009911
2022-01-06 16:29:09,372 iteration 6609 : loss : 0.020919, loss_ce: 0.009412
2022-01-06 16:29:10,531 iteration 6610 : loss : 0.015774, loss_ce: 0.005934
2022-01-06 16:29:11,686 iteration 6611 : loss : 0.021767, loss_ce: 0.006565
2022-01-06 16:29:12,875 iteration 6612 : loss : 0.019177, loss_ce: 0.005277
2022-01-06 16:29:14,113 iteration 6613 : loss : 0.017515, loss_ce: 0.005656
 97%|████████████████████████████▏| 389/400 [2:24:14<03:50, 20.95s/it]2022-01-06 16:29:15,310 iteration 6614 : loss : 0.014759, loss_ce: 0.005693
2022-01-06 16:29:16,399 iteration 6615 : loss : 0.013488, loss_ce: 0.005072
2022-01-06 16:29:17,630 iteration 6616 : loss : 0.019518, loss_ce: 0.006372
2022-01-06 16:29:18,797 iteration 6617 : loss : 0.021529, loss_ce: 0.008467
2022-01-06 16:29:20,070 iteration 6618 : loss : 0.019546, loss_ce: 0.007447
2022-01-06 16:29:21,250 iteration 6619 : loss : 0.020978, loss_ce: 0.007708
2022-01-06 16:29:22,435 iteration 6620 : loss : 0.019682, loss_ce: 0.006383
2022-01-06 16:29:23,679 iteration 6621 : loss : 0.019890, loss_ce: 0.007093
2022-01-06 16:29:24,822 iteration 6622 : loss : 0.016299, loss_ce: 0.007365
2022-01-06 16:29:25,934 iteration 6623 : loss : 0.017106, loss_ce: 0.005544
2022-01-06 16:29:27,109 iteration 6624 : loss : 0.016952, loss_ce: 0.006936
2022-01-06 16:29:28,329 iteration 6625 : loss : 0.034569, loss_ce: 0.012565
2022-01-06 16:29:29,514 iteration 6626 : loss : 0.018309, loss_ce: 0.007065
2022-01-06 16:29:30,750 iteration 6627 : loss : 0.040772, loss_ce: 0.018442
2022-01-06 16:29:31,978 iteration 6628 : loss : 0.019998, loss_ce: 0.008573
2022-01-06 16:29:33,309 iteration 6629 : loss : 0.031163, loss_ce: 0.008497
2022-01-06 16:29:33,309 Training Data Eval:
2022-01-06 16:29:39,105   Average segmentation loss on training set: 0.0110
2022-01-06 16:29:39,105 Validation Data Eval:
2022-01-06 16:29:41,097   Average segmentation loss on validation set: 0.0749
2022-01-06 16:29:42,265 iteration 6630 : loss : 0.016090, loss_ce: 0.006828
 98%|████████████████████████████▎| 390/400 [2:24:42<03:51, 23.11s/it]2022-01-06 16:29:43,507 iteration 6631 : loss : 0.022851, loss_ce: 0.007761
2022-01-06 16:29:44,647 iteration 6632 : loss : 0.016249, loss_ce: 0.007969
2022-01-06 16:29:45,877 iteration 6633 : loss : 0.020665, loss_ce: 0.005970
2022-01-06 16:29:47,033 iteration 6634 : loss : 0.016728, loss_ce: 0.006693
2022-01-06 16:29:48,219 iteration 6635 : loss : 0.016923, loss_ce: 0.007607
2022-01-06 16:29:49,418 iteration 6636 : loss : 0.015285, loss_ce: 0.007764
2022-01-06 16:29:50,556 iteration 6637 : loss : 0.018667, loss_ce: 0.006375
2022-01-06 16:29:51,796 iteration 6638 : loss : 0.037485, loss_ce: 0.008689
2022-01-06 16:29:53,033 iteration 6639 : loss : 0.015944, loss_ce: 0.006394
2022-01-06 16:29:54,196 iteration 6640 : loss : 0.015543, loss_ce: 0.005061
2022-01-06 16:29:55,396 iteration 6641 : loss : 0.015786, loss_ce: 0.004919
2022-01-06 16:29:56,589 iteration 6642 : loss : 0.037868, loss_ce: 0.017600
2022-01-06 16:29:57,793 iteration 6643 : loss : 0.020640, loss_ce: 0.009014
2022-01-06 16:29:59,127 iteration 6644 : loss : 0.031649, loss_ce: 0.012429
2022-01-06 16:30:00,271 iteration 6645 : loss : 0.016371, loss_ce: 0.006539
2022-01-06 16:30:01,581 iteration 6646 : loss : 0.031685, loss_ce: 0.009951
2022-01-06 16:30:02,739 iteration 6647 : loss : 0.019452, loss_ce: 0.007115
 98%|████████████████████████████▎| 391/400 [2:25:03<03:20, 22.32s/it]2022-01-06 16:30:03,957 iteration 6648 : loss : 0.028968, loss_ce: 0.007509
2022-01-06 16:30:05,194 iteration 6649 : loss : 0.025298, loss_ce: 0.007428
2022-01-06 16:30:06,481 iteration 6650 : loss : 0.029316, loss_ce: 0.011766
2022-01-06 16:30:07,792 iteration 6651 : loss : 0.026427, loss_ce: 0.010763
2022-01-06 16:30:08,960 iteration 6652 : loss : 0.015130, loss_ce: 0.006092
2022-01-06 16:30:10,066 iteration 6653 : loss : 0.013231, loss_ce: 0.005433
2022-01-06 16:30:11,249 iteration 6654 : loss : 0.018094, loss_ce: 0.005836
2022-01-06 16:30:12,489 iteration 6655 : loss : 0.023498, loss_ce: 0.008458
2022-01-06 16:30:13,717 iteration 6656 : loss : 0.025589, loss_ce: 0.010334
2022-01-06 16:30:14,928 iteration 6657 : loss : 0.021165, loss_ce: 0.005523
2022-01-06 16:30:16,156 iteration 6658 : loss : 0.018620, loss_ce: 0.006571
2022-01-06 16:30:17,346 iteration 6659 : loss : 0.022907, loss_ce: 0.011141
2022-01-06 16:30:18,598 iteration 6660 : loss : 0.031142, loss_ce: 0.014318
2022-01-06 16:30:19,804 iteration 6661 : loss : 0.020929, loss_ce: 0.008557
2022-01-06 16:30:21,029 iteration 6662 : loss : 0.023139, loss_ce: 0.005720
2022-01-06 16:30:22,227 iteration 6663 : loss : 0.019405, loss_ce: 0.006786
2022-01-06 16:30:23,468 iteration 6664 : loss : 0.024382, loss_ce: 0.012256
 98%|████████████████████████████▍| 392/400 [2:25:23<02:54, 21.85s/it]2022-01-06 16:30:24,756 iteration 6665 : loss : 0.020166, loss_ce: 0.005850
2022-01-06 16:30:26,080 iteration 6666 : loss : 0.036585, loss_ce: 0.008244
2022-01-06 16:30:27,310 iteration 6667 : loss : 0.024624, loss_ce: 0.004618
2022-01-06 16:30:28,415 iteration 6668 : loss : 0.018405, loss_ce: 0.009293
2022-01-06 16:30:29,599 iteration 6669 : loss : 0.021792, loss_ce: 0.008240
2022-01-06 16:30:30,782 iteration 6670 : loss : 0.015908, loss_ce: 0.006895
2022-01-06 16:30:32,009 iteration 6671 : loss : 0.022541, loss_ce: 0.008162
2022-01-06 16:30:33,247 iteration 6672 : loss : 0.018100, loss_ce: 0.005394
2022-01-06 16:30:34,427 iteration 6673 : loss : 0.023731, loss_ce: 0.006372
2022-01-06 16:30:35,606 iteration 6674 : loss : 0.022477, loss_ce: 0.009618
2022-01-06 16:30:36,882 iteration 6675 : loss : 0.027480, loss_ce: 0.014067
2022-01-06 16:30:38,026 iteration 6676 : loss : 0.016966, loss_ce: 0.007169
2022-01-06 16:30:39,159 iteration 6677 : loss : 0.016711, loss_ce: 0.005604
2022-01-06 16:30:40,348 iteration 6678 : loss : 0.018824, loss_ce: 0.009837
2022-01-06 16:30:41,604 iteration 6679 : loss : 0.019030, loss_ce: 0.008187
2022-01-06 16:30:42,728 iteration 6680 : loss : 0.012882, loss_ce: 0.005008
2022-01-06 16:30:43,824 iteration 6681 : loss : 0.014391, loss_ce: 0.004769
 98%|████████████████████████████▍| 393/400 [2:25:44<02:29, 21.40s/it]2022-01-06 16:30:45,180 iteration 6682 : loss : 0.018954, loss_ce: 0.009400
2022-01-06 16:30:46,297 iteration 6683 : loss : 0.017285, loss_ce: 0.006960
2022-01-06 16:30:47,483 iteration 6684 : loss : 0.014673, loss_ce: 0.006698
2022-01-06 16:30:48,608 iteration 6685 : loss : 0.020126, loss_ce: 0.007050
2022-01-06 16:30:49,887 iteration 6686 : loss : 0.022483, loss_ce: 0.009565
2022-01-06 16:30:51,064 iteration 6687 : loss : 0.018383, loss_ce: 0.006601
2022-01-06 16:30:52,271 iteration 6688 : loss : 0.019304, loss_ce: 0.006742
2022-01-06 16:30:53,389 iteration 6689 : loss : 0.012693, loss_ce: 0.003215
2022-01-06 16:30:54,589 iteration 6690 : loss : 0.012230, loss_ce: 0.003663
2022-01-06 16:30:55,835 iteration 6691 : loss : 0.019432, loss_ce: 0.006253
2022-01-06 16:30:56,978 iteration 6692 : loss : 0.029683, loss_ce: 0.010285
2022-01-06 16:30:58,158 iteration 6693 : loss : 0.020371, loss_ce: 0.006696
2022-01-06 16:30:59,372 iteration 6694 : loss : 0.020723, loss_ce: 0.007256
2022-01-06 16:31:00,520 iteration 6695 : loss : 0.016703, loss_ce: 0.005689
2022-01-06 16:31:01,718 iteration 6696 : loss : 0.014575, loss_ce: 0.005703
2022-01-06 16:31:02,915 iteration 6697 : loss : 0.016619, loss_ce: 0.005920
2022-01-06 16:31:04,232 iteration 6698 : loss : 0.014980, loss_ce: 0.004711
 98%|████████████████████████████▌| 394/400 [2:26:04<02:06, 21.10s/it]2022-01-06 16:31:05,443 iteration 6699 : loss : 0.016789, loss_ce: 0.006096
2022-01-06 16:31:06,698 iteration 6700 : loss : 0.022391, loss_ce: 0.006308
2022-01-06 16:31:07,873 iteration 6701 : loss : 0.016461, loss_ce: 0.005395
2022-01-06 16:31:08,988 iteration 6702 : loss : 0.013611, loss_ce: 0.006708
2022-01-06 16:31:10,096 iteration 6703 : loss : 0.015597, loss_ce: 0.005052
2022-01-06 16:31:11,343 iteration 6704 : loss : 0.027454, loss_ce: 0.014326
2022-01-06 16:31:12,468 iteration 6705 : loss : 0.020363, loss_ce: 0.009809
2022-01-06 16:31:13,732 iteration 6706 : loss : 0.021131, loss_ce: 0.007222
2022-01-06 16:31:14,852 iteration 6707 : loss : 0.015730, loss_ce: 0.006292
2022-01-06 16:31:16,054 iteration 6708 : loss : 0.018093, loss_ce: 0.006985
2022-01-06 16:31:17,258 iteration 6709 : loss : 0.019475, loss_ce: 0.007813
2022-01-06 16:31:18,495 iteration 6710 : loss : 0.026969, loss_ce: 0.005803
2022-01-06 16:31:19,728 iteration 6711 : loss : 0.015205, loss_ce: 0.004052
2022-01-06 16:31:21,025 iteration 6712 : loss : 0.016580, loss_ce: 0.006620
2022-01-06 16:31:22,133 iteration 6713 : loss : 0.021410, loss_ce: 0.008565
2022-01-06 16:31:23,321 iteration 6714 : loss : 0.017178, loss_ce: 0.005569
2022-01-06 16:31:23,321 Training Data Eval:
2022-01-06 16:31:29,155   Average segmentation loss on training set: 0.0113
2022-01-06 16:31:29,156 Validation Data Eval:
2022-01-06 16:31:31,156   Average segmentation loss on validation set: 0.0652
2022-01-06 16:31:32,292 iteration 6715 : loss : 0.019264, loss_ce: 0.006909
 99%|████████████████████████████▋| 395/400 [2:26:32<01:55, 23.19s/it]2022-01-06 16:31:33,529 iteration 6716 : loss : 0.018414, loss_ce: 0.007896
2022-01-06 16:31:34,682 iteration 6717 : loss : 0.020793, loss_ce: 0.008606
2022-01-06 16:31:35,865 iteration 6718 : loss : 0.019552, loss_ce: 0.007610
2022-01-06 16:31:37,000 iteration 6719 : loss : 0.018930, loss_ce: 0.006942
2022-01-06 16:31:38,144 iteration 6720 : loss : 0.015660, loss_ce: 0.004370
2022-01-06 16:31:39,282 iteration 6721 : loss : 0.015046, loss_ce: 0.004881
2022-01-06 16:31:40,559 iteration 6722 : loss : 0.015572, loss_ce: 0.005962
2022-01-06 16:31:41,631 iteration 6723 : loss : 0.017488, loss_ce: 0.006580
2022-01-06 16:31:42,815 iteration 6724 : loss : 0.022721, loss_ce: 0.007230
2022-01-06 16:31:44,027 iteration 6725 : loss : 0.014486, loss_ce: 0.004633
2022-01-06 16:31:45,258 iteration 6726 : loss : 0.024330, loss_ce: 0.011520
2022-01-06 16:31:46,564 iteration 6727 : loss : 0.022680, loss_ce: 0.007355
2022-01-06 16:31:47,729 iteration 6728 : loss : 0.022584, loss_ce: 0.007821
2022-01-06 16:31:48,894 iteration 6729 : loss : 0.018812, loss_ce: 0.005323
2022-01-06 16:31:50,013 iteration 6730 : loss : 0.012610, loss_ce: 0.004567
2022-01-06 16:31:51,187 iteration 6731 : loss : 0.017201, loss_ce: 0.006846
2022-01-06 16:31:52,411 iteration 6732 : loss : 0.019384, loss_ce: 0.007338
 99%|████████████████████████████▋| 396/400 [2:26:52<01:29, 22.27s/it]2022-01-06 16:31:53,632 iteration 6733 : loss : 0.013457, loss_ce: 0.004923
2022-01-06 16:31:54,921 iteration 6734 : loss : 0.018828, loss_ce: 0.007053
2022-01-06 16:31:56,046 iteration 6735 : loss : 0.014435, loss_ce: 0.005633
2022-01-06 16:31:57,273 iteration 6736 : loss : 0.021476, loss_ce: 0.008862
2022-01-06 16:31:58,497 iteration 6737 : loss : 0.025912, loss_ce: 0.011023
2022-01-06 16:31:59,569 iteration 6738 : loss : 0.014900, loss_ce: 0.004735
2022-01-06 16:32:00,713 iteration 6739 : loss : 0.017102, loss_ce: 0.006873
2022-01-06 16:32:01,947 iteration 6740 : loss : 0.013593, loss_ce: 0.005748
2022-01-06 16:32:03,116 iteration 6741 : loss : 0.013269, loss_ce: 0.005194
2022-01-06 16:32:04,269 iteration 6742 : loss : 0.016716, loss_ce: 0.005236
2022-01-06 16:32:05,501 iteration 6743 : loss : 0.020938, loss_ce: 0.010072
2022-01-06 16:32:06,722 iteration 6744 : loss : 0.030963, loss_ce: 0.012362
2022-01-06 16:32:07,854 iteration 6745 : loss : 0.017400, loss_ce: 0.006425
2022-01-06 16:32:09,062 iteration 6746 : loss : 0.018297, loss_ce: 0.006996
2022-01-06 16:32:10,333 iteration 6747 : loss : 0.026805, loss_ce: 0.007788
2022-01-06 16:32:11,584 iteration 6748 : loss : 0.021275, loss_ce: 0.006241
2022-01-06 16:32:12,798 iteration 6749 : loss : 0.016339, loss_ce: 0.006925
 99%|████████████████████████████▊| 397/400 [2:27:13<01:05, 21.70s/it]2022-01-06 16:32:14,094 iteration 6750 : loss : 0.013819, loss_ce: 0.004623
2022-01-06 16:32:15,448 iteration 6751 : loss : 0.023952, loss_ce: 0.007760
2022-01-06 16:32:16,702 iteration 6752 : loss : 0.023677, loss_ce: 0.009920
2022-01-06 16:32:17,870 iteration 6753 : loss : 0.049412, loss_ce: 0.021654
2022-01-06 16:32:19,151 iteration 6754 : loss : 0.035042, loss_ce: 0.014831
2022-01-06 16:32:20,293 iteration 6755 : loss : 0.020349, loss_ce: 0.005273
2022-01-06 16:32:21,355 iteration 6756 : loss : 0.010860, loss_ce: 0.004962
2022-01-06 16:32:22,477 iteration 6757 : loss : 0.016189, loss_ce: 0.006428
2022-01-06 16:32:23,649 iteration 6758 : loss : 0.018324, loss_ce: 0.010292
2022-01-06 16:32:24,881 iteration 6759 : loss : 0.031286, loss_ce: 0.009766
2022-01-06 16:32:26,029 iteration 6760 : loss : 0.015251, loss_ce: 0.006438
2022-01-06 16:32:27,186 iteration 6761 : loss : 0.017661, loss_ce: 0.006910
2022-01-06 16:32:28,382 iteration 6762 : loss : 0.028792, loss_ce: 0.008366
2022-01-06 16:32:29,562 iteration 6763 : loss : 0.017529, loss_ce: 0.008180
2022-01-06 16:32:30,832 iteration 6764 : loss : 0.022047, loss_ce: 0.007533
2022-01-06 16:32:32,051 iteration 6765 : loss : 0.020497, loss_ce: 0.005849
2022-01-06 16:32:33,228 iteration 6766 : loss : 0.013866, loss_ce: 0.006266
100%|████████████████████████████▊| 398/400 [2:27:33<00:42, 21.32s/it]2022-01-06 16:32:34,559 iteration 6767 : loss : 0.024957, loss_ce: 0.011196
2022-01-06 16:32:35,714 iteration 6768 : loss : 0.021736, loss_ce: 0.006921
2022-01-06 16:32:36,880 iteration 6769 : loss : 0.014568, loss_ce: 0.004562
2022-01-06 16:32:38,088 iteration 6770 : loss : 0.019792, loss_ce: 0.006249
2022-01-06 16:32:39,252 iteration 6771 : loss : 0.018665, loss_ce: 0.007752
2022-01-06 16:32:40,451 iteration 6772 : loss : 0.018522, loss_ce: 0.007353
2022-01-06 16:32:41,560 iteration 6773 : loss : 0.015134, loss_ce: 0.005758
2022-01-06 16:32:42,731 iteration 6774 : loss : 0.018411, loss_ce: 0.005534
2022-01-06 16:32:43,864 iteration 6775 : loss : 0.020327, loss_ce: 0.004687
2022-01-06 16:32:45,041 iteration 6776 : loss : 0.016302, loss_ce: 0.006449
2022-01-06 16:32:46,230 iteration 6777 : loss : 0.017570, loss_ce: 0.006992
2022-01-06 16:32:47,352 iteration 6778 : loss : 0.013724, loss_ce: 0.005356
2022-01-06 16:32:48,520 iteration 6779 : loss : 0.019309, loss_ce: 0.009102
2022-01-06 16:32:49,752 iteration 6780 : loss : 0.017455, loss_ce: 0.006186
2022-01-06 16:32:50,938 iteration 6781 : loss : 0.018026, loss_ce: 0.006821
2022-01-06 16:32:52,081 iteration 6782 : loss : 0.020715, loss_ce: 0.008273
2022-01-06 16:32:53,332 iteration 6783 : loss : 0.026127, loss_ce: 0.008866
100%|████████████████████████████▉| 399/400 [2:27:53<00:20, 20.96s/it]2022-01-06 16:32:54,493 iteration 6784 : loss : 0.016760, loss_ce: 0.008931
2022-01-06 16:32:55,659 iteration 6785 : loss : 0.018180, loss_ce: 0.005772
2022-01-06 16:32:56,886 iteration 6786 : loss : 0.017197, loss_ce: 0.006679
2022-01-06 16:32:58,052 iteration 6787 : loss : 0.020836, loss_ce: 0.005829
2022-01-06 16:32:59,234 iteration 6788 : loss : 0.016627, loss_ce: 0.008044
2022-01-06 16:33:00,499 iteration 6789 : loss : 0.027736, loss_ce: 0.007518
2022-01-06 16:33:01,687 iteration 6790 : loss : 0.019937, loss_ce: 0.008620
2022-01-06 16:33:02,944 iteration 6791 : loss : 0.039030, loss_ce: 0.012350
2022-01-06 16:33:04,099 iteration 6792 : loss : 0.020426, loss_ce: 0.009129
2022-01-06 16:33:05,180 iteration 6793 : loss : 0.013034, loss_ce: 0.004854
2022-01-06 16:33:06,407 iteration 6794 : loss : 0.019214, loss_ce: 0.007021
2022-01-06 16:33:07,540 iteration 6795 : loss : 0.014126, loss_ce: 0.005563
2022-01-06 16:33:08,716 iteration 6796 : loss : 0.014384, loss_ce: 0.003602
2022-01-06 16:33:09,866 iteration 6797 : loss : 0.022763, loss_ce: 0.008250
2022-01-06 16:33:11,097 iteration 6798 : loss : 0.018381, loss_ce: 0.006513
2022-01-06 16:33:12,269 iteration 6799 : loss : 0.017142, loss_ce: 0.005946
2022-01-06 16:33:12,269 Training Data Eval:
2022-01-06 16:33:18,098   Average segmentation loss on training set: 0.0108
2022-01-06 16:33:18,098 Validation Data Eval:
2022-01-06 16:33:20,098   Average segmentation loss on validation set: 0.0690
2022-01-06 16:33:21,291 iteration 6800 : loss : 0.016005, loss_ce: 0.006398
100%|█████████████████████████████| 400/400 [2:28:21<00:00, 23.06s/it]100%|█████████████████████████████| 400/400 [2:28:21<00:00, 22.25s/it]
