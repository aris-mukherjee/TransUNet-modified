2021-12-12 22:49:58,363 load_pretrained: resized variant: torch.Size([1, 197, 768]) to torch.Size([1, 256, 768])
load_pretrained: grid-size from 14 to 16
2021-12-12 22:50:02,581 Logging directory: /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/TransUNet/log_dir/trRUNMC_cv1_r3/i2i2l/
2021-12-12 22:50:02,581 Tensorboard directory: /scratch_net/biwidl217_second/arismu/Tensorboard/trRUNMC_cv1_r3/i2i2l/
2021-12-12 22:50:02,581 ============================================================
2021-12-12 22:50:02,582 EXPERIMENT NAME: trRUNMC_cv1_r3/i2i2l/
2021-12-12 22:50:02,582 ============================================================
2021-12-12 22:50:02,582 Loading data...
2021-12-12 22:50:02,582 Reading NCI - RUNMC images...
2021-12-12 22:50:02,582 Data root directory: /itet-stor/arismu/bmicdatasets-originals/Originals/Challenge_Datasets/NCI_Prostate/
2021-12-12 22:50:02,710 Already preprocessed this configuration. Loading now!
2021-12-12 22:50:02,728 Training Images: (256, 256, 286)
2021-12-12 22:50:02,729 Training Labels: (256, 256, 286)
2021-12-12 22:50:02,729 Validation Images: (256, 256, 98)
2021-12-12 22:50:02,729 Validation Labels: (256, 256, 98)
2021-12-12 22:50:02,729 ============================================================
2021-12-12 22:50:02,778 17 iterations per epoch. 6800 max iterations 
  0%|                                         | 0/400 [00:00<?, ?it/s]2021-12-12 22:50:05,536 iteration 1 : loss : 0.963463, loss_ce: 1.184421
2021-12-12 22:50:06,873 iteration 2 : loss : 0.971168, loss_ce: 1.181570
2021-12-12 22:50:08,189 iteration 3 : loss : 0.970397, loss_ce: 1.164581
2021-12-12 22:50:09,561 iteration 4 : loss : 0.938903, loss_ce: 1.139310
2021-12-12 22:50:10,853 iteration 5 : loss : 0.938355, loss_ce: 1.114563
2021-12-12 22:50:12,267 iteration 6 : loss : 0.914334, loss_ce: 1.079298
2021-12-12 22:50:13,652 iteration 7 : loss : 0.893548, loss_ce: 1.040007
2021-12-12 22:50:15,041 iteration 8 : loss : 0.866629, loss_ce: 0.999683
2021-12-12 22:50:16,307 iteration 9 : loss : 0.828476, loss_ce: 0.959203
2021-12-12 22:50:17,704 iteration 10 : loss : 0.805360, loss_ce: 0.913095
2021-12-12 22:50:19,106 iteration 11 : loss : 0.785467, loss_ce: 0.875793
2021-12-12 22:50:20,546 iteration 12 : loss : 0.760888, loss_ce: 0.835931
2021-12-12 22:50:21,889 iteration 13 : loss : 0.740688, loss_ce: 0.790732
2021-12-12 22:50:23,333 iteration 14 : loss : 0.715153, loss_ce: 0.753841
2021-12-12 22:50:24,700 iteration 15 : loss : 0.691160, loss_ce: 0.712506
2021-12-12 22:50:26,045 iteration 16 : loss : 0.668139, loss_ce: 0.681460
2021-12-12 22:50:27,366 iteration 17 : loss : 0.660679, loss_ce: 0.639668
  0%|                               | 1/400 [00:24<2:43:56, 24.65s/it]2021-12-12 22:50:28,816 iteration 18 : loss : 0.628584, loss_ce: 0.618628
2021-12-12 22:50:30,208 iteration 19 : loss : 0.621444, loss_ce: 0.578517
2021-12-12 22:50:31,696 iteration 20 : loss : 0.594720, loss_ce: 0.563487
2021-12-12 22:50:33,039 iteration 21 : loss : 0.582614, loss_ce: 0.528650
2021-12-12 22:50:34,344 iteration 22 : loss : 0.572483, loss_ce: 0.504112
2021-12-12 22:50:35,846 iteration 23 : loss : 0.553573, loss_ce: 0.481513
2021-12-12 22:50:37,380 iteration 24 : loss : 0.536378, loss_ce: 0.472271
2021-12-12 22:50:38,943 iteration 25 : loss : 0.519821, loss_ce: 0.467486
2021-12-12 22:50:40,418 iteration 26 : loss : 0.512759, loss_ce: 0.438819
2021-12-12 22:50:41,843 iteration 27 : loss : 0.509551, loss_ce: 0.415960
2021-12-12 22:50:43,253 iteration 28 : loss : 0.493626, loss_ce: 0.391134
2021-12-12 22:50:44,757 iteration 29 : loss : 0.485728, loss_ce: 0.391551
2021-12-12 22:50:46,199 iteration 30 : loss : 0.474682, loss_ce: 0.368221
2021-12-12 22:50:47,623 iteration 31 : loss : 0.476295, loss_ce: 0.343118
2021-12-12 22:50:49,086 iteration 32 : loss : 0.464321, loss_ce: 0.371203
2021-12-12 22:50:50,602 iteration 33 : loss : 0.456089, loss_ce: 0.345313
2021-12-12 22:50:52,189 iteration 34 : loss : 0.449763, loss_ce: 0.325292
  0%|▏                              | 2/400 [00:49<2:44:06, 24.74s/it]2021-12-12 22:50:53,742 iteration 35 : loss : 0.444966, loss_ce: 0.331321
2021-12-12 22:50:55,243 iteration 36 : loss : 0.445488, loss_ce: 0.286402
2021-12-12 22:50:56,767 iteration 37 : loss : 0.437983, loss_ce: 0.292712
2021-12-12 22:50:58,259 iteration 38 : loss : 0.430188, loss_ce: 0.288759
2021-12-12 22:50:59,648 iteration 39 : loss : 0.421058, loss_ce: 0.277326
2021-12-12 22:51:01,098 iteration 40 : loss : 0.422649, loss_ce: 0.285201
2021-12-12 22:51:02,470 iteration 41 : loss : 0.406518, loss_ce: 0.271059
2021-12-12 22:51:03,977 iteration 42 : loss : 0.406635, loss_ce: 0.271723
2021-12-12 22:51:05,461 iteration 43 : loss : 0.403926, loss_ce: 0.260776
2021-12-12 22:51:06,849 iteration 44 : loss : 0.397641, loss_ce: 0.270403
2021-12-12 22:51:08,400 iteration 45 : loss : 0.399437, loss_ce: 0.237533
2021-12-12 22:51:09,888 iteration 46 : loss : 0.389329, loss_ce: 0.244032
2021-12-12 22:51:11,373 iteration 47 : loss : 0.386511, loss_ce: 0.231015
2021-12-12 22:51:12,855 iteration 48 : loss : 0.373367, loss_ce: 0.238294
2021-12-12 22:51:14,392 iteration 49 : loss : 0.379646, loss_ce: 0.227784
2021-12-12 22:51:15,932 iteration 50 : loss : 0.367400, loss_ce: 0.220619
2021-12-12 22:51:17,486 iteration 51 : loss : 0.365829, loss_ce: 0.220528
  1%|▏                              | 3/400 [01:14<2:45:22, 24.99s/it]2021-12-12 22:51:18,950 iteration 52 : loss : 0.371215, loss_ce: 0.212410
2021-12-12 22:51:20,348 iteration 53 : loss : 0.354817, loss_ce: 0.212027
2021-12-12 22:51:21,887 iteration 54 : loss : 0.354148, loss_ce: 0.204579
2021-12-12 22:51:23,398 iteration 55 : loss : 0.359481, loss_ce: 0.194939
2021-12-12 22:51:24,918 iteration 56 : loss : 0.356316, loss_ce: 0.221034
2021-12-12 22:51:26,366 iteration 57 : loss : 0.339946, loss_ce: 0.205121
2021-12-12 22:51:27,788 iteration 58 : loss : 0.347220, loss_ce: 0.208224
2021-12-12 22:51:29,207 iteration 59 : loss : 0.336306, loss_ce: 0.191861
2021-12-12 22:51:30,655 iteration 60 : loss : 0.330828, loss_ce: 0.192683
2021-12-12 22:51:32,117 iteration 61 : loss : 0.329394, loss_ce: 0.192929
2021-12-12 22:51:33,667 iteration 62 : loss : 0.338416, loss_ce: 0.173538
2021-12-12 22:51:35,101 iteration 63 : loss : 0.317795, loss_ce: 0.182739
2021-12-12 22:51:36,604 iteration 64 : loss : 0.317662, loss_ce: 0.178256
2021-12-12 22:51:38,093 iteration 65 : loss : 0.329803, loss_ce: 0.171192
2021-12-12 22:51:39,636 iteration 66 : loss : 0.321904, loss_ce: 0.166702
2021-12-12 22:51:41,104 iteration 67 : loss : 0.317178, loss_ce: 0.174036
2021-12-12 22:51:42,508 iteration 68 : loss : 0.313809, loss_ce: 0.176931
  1%|▎                              | 4/400 [01:39<2:45:01, 25.00s/it]2021-12-12 22:51:44,062 iteration 69 : loss : 0.313336, loss_ce: 0.166538
2021-12-12 22:51:45,523 iteration 70 : loss : 0.307410, loss_ce: 0.150862
2021-12-12 22:51:46,994 iteration 71 : loss : 0.298979, loss_ce: 0.172700
2021-12-12 22:51:48,490 iteration 72 : loss : 0.300741, loss_ce: 0.158901
2021-12-12 22:51:49,999 iteration 73 : loss : 0.301694, loss_ce: 0.149121
2021-12-12 22:51:51,498 iteration 74 : loss : 0.299521, loss_ce: 0.152379
2021-12-12 22:51:52,944 iteration 75 : loss : 0.306736, loss_ce: 0.174909
2021-12-12 22:51:54,335 iteration 76 : loss : 0.285123, loss_ce: 0.142683
2021-12-12 22:51:55,828 iteration 77 : loss : 0.285876, loss_ce: 0.141992
2021-12-12 22:51:57,309 iteration 78 : loss : 0.299487, loss_ce: 0.145567
2021-12-12 22:51:58,806 iteration 79 : loss : 0.299623, loss_ce: 0.141762
2021-12-12 22:52:00,184 iteration 80 : loss : 0.289476, loss_ce: 0.134894
2021-12-12 22:52:01,731 iteration 81 : loss : 0.292706, loss_ce: 0.155628
2021-12-12 22:52:03,184 iteration 82 : loss : 0.287594, loss_ce: 0.127018
2021-12-12 22:52:04,605 iteration 83 : loss : 0.307640, loss_ce: 0.130666
2021-12-12 22:52:06,174 iteration 84 : loss : 0.290008, loss_ce: 0.148576
2021-12-12 22:52:06,174 Training Data Eval:
2021-12-12 22:52:13,705   Average segmentation loss on training set: 0.3072
2021-12-12 22:52:13,705 Validation Data Eval:
2021-12-12 22:52:16,457   Average segmentation loss on validation set: 0.3254
2021-12-12 22:52:22,667 Found new lowest validation loss at iteration 84! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/TU_3seeds/TU_SGD_best_val_loss_seed100.pth
2021-12-12 22:52:23,988 iteration 85 : loss : 0.281317, loss_ce: 0.145429
  1%|▍                              | 5/400 [02:21<3:23:44, 30.95s/it]2021-12-12 22:52:25,429 iteration 86 : loss : 0.287160, loss_ce: 0.151024
2021-12-12 22:52:26,774 iteration 87 : loss : 0.275306, loss_ce: 0.127346
2021-12-12 22:52:28,175 iteration 88 : loss : 0.274350, loss_ce: 0.146790
2021-12-12 22:52:29,669 iteration 89 : loss : 0.287259, loss_ce: 0.124965
2021-12-12 22:52:31,134 iteration 90 : loss : 0.271281, loss_ce: 0.128269
2021-12-12 22:52:32,663 iteration 91 : loss : 0.271071, loss_ce: 0.136875
2021-12-12 22:52:34,203 iteration 92 : loss : 0.282941, loss_ce: 0.123007
2021-12-12 22:52:35,617 iteration 93 : loss : 0.289892, loss_ce: 0.121113
2021-12-12 22:52:37,131 iteration 94 : loss : 0.268087, loss_ce: 0.127455
2021-12-12 22:52:38,601 iteration 95 : loss : 0.265299, loss_ce: 0.130462
2021-12-12 22:52:40,151 iteration 96 : loss : 0.270733, loss_ce: 0.122370
2021-12-12 22:52:41,633 iteration 97 : loss : 0.278634, loss_ce: 0.126353
2021-12-12 22:52:43,152 iteration 98 : loss : 0.270039, loss_ce: 0.111618
2021-12-12 22:52:44,626 iteration 99 : loss : 0.262634, loss_ce: 0.106934
2021-12-12 22:52:46,145 iteration 100 : loss : 0.265165, loss_ce: 0.117412
2021-12-12 22:52:47,605 iteration 101 : loss : 0.270445, loss_ce: 0.118397
2021-12-12 22:52:49,053 iteration 102 : loss : 0.260790, loss_ce: 0.125075
  2%|▍                              | 6/400 [02:46<3:10:04, 28.95s/it]2021-12-12 22:52:50,599 iteration 103 : loss : 0.251912, loss_ce: 0.101087
2021-12-12 22:52:52,134 iteration 104 : loss : 0.256734, loss_ce: 0.114273
2021-12-12 22:52:53,668 iteration 105 : loss : 0.259236, loss_ce: 0.112198
2021-12-12 22:52:55,280 iteration 106 : loss : 0.259826, loss_ce: 0.107141
2021-12-12 22:52:56,806 iteration 107 : loss : 0.260074, loss_ce: 0.101808
2021-12-12 22:52:58,285 iteration 108 : loss : 0.272464, loss_ce: 0.101597
2021-12-12 22:52:59,692 iteration 109 : loss : 0.275507, loss_ce: 0.115821
2021-12-12 22:53:01,134 iteration 110 : loss : 0.251671, loss_ce: 0.114889
2021-12-12 22:53:02,675 iteration 111 : loss : 0.251857, loss_ce: 0.118032
2021-12-12 22:53:04,154 iteration 112 : loss : 0.257241, loss_ce: 0.112077
2021-12-12 22:53:05,574 iteration 113 : loss : 0.253123, loss_ce: 0.100632
2021-12-12 22:53:07,089 iteration 114 : loss : 0.263384, loss_ce: 0.102579
2021-12-12 22:53:08,546 iteration 115 : loss : 0.250424, loss_ce: 0.119148
2021-12-12 22:53:10,060 iteration 116 : loss : 0.252327, loss_ce: 0.113746
2021-12-12 22:53:11,507 iteration 117 : loss : 0.239052, loss_ce: 0.105264
2021-12-12 22:53:12,971 iteration 118 : loss : 0.262420, loss_ce: 0.121387
2021-12-12 22:53:14,456 iteration 119 : loss : 0.249041, loss_ce: 0.101706
  2%|▌                              | 7/400 [03:11<3:02:00, 27.79s/it]2021-12-12 22:53:16,052 iteration 120 : loss : 0.258706, loss_ce: 0.109663
2021-12-12 22:53:17,575 iteration 121 : loss : 0.249486, loss_ce: 0.096676
2021-12-12 22:53:18,980 iteration 122 : loss : 0.250678, loss_ce: 0.111817
2021-12-12 22:53:20,503 iteration 123 : loss : 0.233576, loss_ce: 0.098790
2021-12-12 22:53:22,028 iteration 124 : loss : 0.238002, loss_ce: 0.091724
2021-12-12 22:53:23,500 iteration 125 : loss : 0.244989, loss_ce: 0.114726
2021-12-12 22:53:25,024 iteration 126 : loss : 0.239520, loss_ce: 0.091635
2021-12-12 22:53:26,557 iteration 127 : loss : 0.245363, loss_ce: 0.104190
2021-12-12 22:53:28,105 iteration 128 : loss : 0.245784, loss_ce: 0.107935
2021-12-12 22:53:29,558 iteration 129 : loss : 0.237264, loss_ce: 0.096075
2021-12-12 22:53:31,104 iteration 130 : loss : 0.242443, loss_ce: 0.109527
2021-12-12 22:53:32,588 iteration 131 : loss : 0.236626, loss_ce: 0.100769
2021-12-12 22:53:34,019 iteration 132 : loss : 0.239462, loss_ce: 0.087787
2021-12-12 22:53:35,560 iteration 133 : loss : 0.246712, loss_ce: 0.091740
2021-12-12 22:53:37,058 iteration 134 : loss : 0.234244, loss_ce: 0.091914
2021-12-12 22:53:38,565 iteration 135 : loss : 0.239545, loss_ce: 0.099778
2021-12-12 22:53:40,035 iteration 136 : loss : 0.245039, loss_ce: 0.107931
  2%|▌                              | 8/400 [03:37<2:56:56, 27.08s/it]2021-12-12 22:53:41,486 iteration 137 : loss : 0.230712, loss_ce: 0.089222
2021-12-12 22:53:42,918 iteration 138 : loss : 0.224006, loss_ce: 0.105466
2021-12-12 22:53:44,470 iteration 139 : loss : 0.256254, loss_ce: 0.095855
2021-12-12 22:53:46,064 iteration 140 : loss : 0.235996, loss_ce: 0.085792
2021-12-12 22:53:47,568 iteration 141 : loss : 0.236889, loss_ce: 0.087432
2021-12-12 22:53:49,008 iteration 142 : loss : 0.209820, loss_ce: 0.082636
2021-12-12 22:53:50,440 iteration 143 : loss : 0.210607, loss_ce: 0.082485
2021-12-12 22:53:51,970 iteration 144 : loss : 0.229401, loss_ce: 0.085565
2021-12-12 22:53:53,646 iteration 145 : loss : 0.235373, loss_ce: 0.104705
2021-12-12 22:53:55,107 iteration 146 : loss : 0.222991, loss_ce: 0.085180
2021-12-12 22:53:56,579 iteration 147 : loss : 0.221100, loss_ce: 0.091278
2021-12-12 22:53:58,070 iteration 148 : loss : 0.217135, loss_ce: 0.091475
2021-12-12 22:53:59,573 iteration 149 : loss : 0.215390, loss_ce: 0.086524
2021-12-12 22:54:01,052 iteration 150 : loss : 0.222712, loss_ce: 0.094117
2021-12-12 22:54:02,540 iteration 151 : loss : 0.218851, loss_ce: 0.088182
2021-12-12 22:54:04,093 iteration 152 : loss : 0.221168, loss_ce: 0.089934
2021-12-12 22:54:05,646 iteration 153 : loss : 0.205933, loss_ce: 0.092375
  2%|▋                              | 9/400 [04:02<2:53:29, 26.62s/it]2021-12-12 22:54:07,172 iteration 154 : loss : 0.201502, loss_ce: 0.080345
2021-12-12 22:54:08,688 iteration 155 : loss : 0.204054, loss_ce: 0.081632
2021-12-12 22:54:10,194 iteration 156 : loss : 0.211046, loss_ce: 0.093776
2021-12-12 22:54:11,712 iteration 157 : loss : 0.199944, loss_ce: 0.078174
2021-12-12 22:54:13,219 iteration 158 : loss : 0.202974, loss_ce: 0.085118
2021-12-12 22:54:14,711 iteration 159 : loss : 0.195022, loss_ce: 0.078145
2021-12-12 22:54:16,185 iteration 160 : loss : 0.195139, loss_ce: 0.070693
2021-12-12 22:54:17,703 iteration 161 : loss : 0.204552, loss_ce: 0.083284
2021-12-12 22:54:19,284 iteration 162 : loss : 0.206811, loss_ce: 0.086433
2021-12-12 22:54:20,675 iteration 163 : loss : 0.170808, loss_ce: 0.071356
2021-12-12 22:54:22,217 iteration 164 : loss : 0.194923, loss_ce: 0.078518
2021-12-12 22:54:23,686 iteration 165 : loss : 0.182692, loss_ce: 0.076011
2021-12-12 22:54:25,240 iteration 166 : loss : 0.184253, loss_ce: 0.075377
2021-12-12 22:54:26,784 iteration 167 : loss : 0.184435, loss_ce: 0.079264
2021-12-12 22:54:28,317 iteration 168 : loss : 0.194439, loss_ce: 0.084392
2021-12-12 22:54:29,800 iteration 169 : loss : 0.183721, loss_ce: 0.077664
2021-12-12 22:54:29,800 Training Data Eval:
2021-12-12 22:54:37,337   Average segmentation loss on training set: 0.1775
2021-12-12 22:54:37,338 Validation Data Eval:
2021-12-12 22:54:39,944   Average segmentation loss on validation set: 0.1948
2021-12-12 22:54:46,263 Found new lowest validation loss at iteration 169! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/TU_3seeds/TU_SGD_best_val_loss_seed100.pth
2021-12-12 22:54:47,607 iteration 170 : loss : 0.180557, loss_ce: 0.076674
  2%|▊                             | 10/400 [04:44<3:23:49, 31.36s/it]2021-12-12 22:54:49,037 iteration 171 : loss : 0.188977, loss_ce: 0.083107
2021-12-12 22:54:50,296 iteration 172 : loss : 0.195695, loss_ce: 0.067885
2021-12-12 22:54:51,660 iteration 173 : loss : 0.168734, loss_ce: 0.071606
2021-12-12 22:54:53,209 iteration 174 : loss : 0.183886, loss_ce: 0.067426
2021-12-12 22:54:54,672 iteration 175 : loss : 0.179847, loss_ce: 0.078434
2021-12-12 22:54:56,264 iteration 176 : loss : 0.176258, loss_ce: 0.071758
2021-12-12 22:54:57,741 iteration 177 : loss : 0.187977, loss_ce: 0.069390
2021-12-12 22:54:59,283 iteration 178 : loss : 0.182076, loss_ce: 0.065906
2021-12-12 22:55:00,803 iteration 179 : loss : 0.186372, loss_ce: 0.079456
2021-12-12 22:55:02,328 iteration 180 : loss : 0.177078, loss_ce: 0.065252
2021-12-12 22:55:03,861 iteration 181 : loss : 0.180034, loss_ce: 0.070440
2021-12-12 22:55:05,385 iteration 182 : loss : 0.181924, loss_ce: 0.073499
2021-12-12 22:55:06,919 iteration 183 : loss : 0.173352, loss_ce: 0.067193
2021-12-12 22:55:08,416 iteration 184 : loss : 0.170614, loss_ce: 0.075244
2021-12-12 22:55:09,867 iteration 185 : loss : 0.165052, loss_ce: 0.075886
2021-12-12 22:55:11,311 iteration 186 : loss : 0.167978, loss_ce: 0.073911
2021-12-12 22:55:12,732 iteration 187 : loss : 0.169439, loss_ce: 0.069045
  3%|▊                             | 11/400 [05:09<3:10:55, 29.45s/it]2021-12-12 22:55:14,274 iteration 188 : loss : 0.181979, loss_ce: 0.071072
2021-12-12 22:55:15,808 iteration 189 : loss : 0.167778, loss_ce: 0.071094
2021-12-12 22:55:17,365 iteration 190 : loss : 0.174774, loss_ce: 0.066656
2021-12-12 22:55:18,864 iteration 191 : loss : 0.170866, loss_ce: 0.066889
2021-12-12 22:55:20,441 iteration 192 : loss : 0.163437, loss_ce: 0.069863
2021-12-12 22:55:21,882 iteration 193 : loss : 0.169977, loss_ce: 0.065981
2021-12-12 22:55:23,309 iteration 194 : loss : 0.156067, loss_ce: 0.059031
2021-12-12 22:55:24,886 iteration 195 : loss : 0.170920, loss_ce: 0.069411
2021-12-12 22:55:26,420 iteration 196 : loss : 0.167036, loss_ce: 0.061064
2021-12-12 22:55:27,927 iteration 197 : loss : 0.166882, loss_ce: 0.068572
2021-12-12 22:55:29,462 iteration 198 : loss : 0.176039, loss_ce: 0.075699
2021-12-12 22:55:30,992 iteration 199 : loss : 0.175201, loss_ce: 0.077360
2021-12-12 22:55:32,469 iteration 200 : loss : 0.159286, loss_ce: 0.059969
2021-12-12 22:55:33,951 iteration 201 : loss : 0.163685, loss_ce: 0.062696
2021-12-12 22:55:35,327 iteration 202 : loss : 0.178596, loss_ce: 0.063539
2021-12-12 22:55:36,875 iteration 203 : loss : 0.175764, loss_ce: 0.076300
2021-12-12 22:55:38,352 iteration 204 : loss : 0.170029, loss_ce: 0.077702
  3%|▉                             | 12/400 [05:35<3:02:56, 28.29s/it]2021-12-12 22:55:39,975 iteration 205 : loss : 0.154052, loss_ce: 0.058991
2021-12-12 22:55:41,487 iteration 206 : loss : 0.166585, loss_ce: 0.062059
2021-12-12 22:55:43,065 iteration 207 : loss : 0.148760, loss_ce: 0.058025
2021-12-12 22:55:44,616 iteration 208 : loss : 0.170261, loss_ce: 0.075412
2021-12-12 22:55:46,140 iteration 209 : loss : 0.172346, loss_ce: 0.066589
2021-12-12 22:55:47,614 iteration 210 : loss : 0.162378, loss_ce: 0.068927
2021-12-12 22:55:49,131 iteration 211 : loss : 0.167492, loss_ce: 0.072369
2021-12-12 22:55:50,700 iteration 212 : loss : 0.158322, loss_ce: 0.064030
2021-12-12 22:55:52,258 iteration 213 : loss : 0.168162, loss_ce: 0.072831
2021-12-12 22:55:53,750 iteration 214 : loss : 0.155704, loss_ce: 0.061960
2021-12-12 22:55:55,208 iteration 215 : loss : 0.155943, loss_ce: 0.058533
2021-12-12 22:55:56,763 iteration 216 : loss : 0.156371, loss_ce: 0.061751
2021-12-12 22:55:58,335 iteration 217 : loss : 0.168530, loss_ce: 0.060030
2021-12-12 22:55:59,823 iteration 218 : loss : 0.161661, loss_ce: 0.065292
2021-12-12 22:56:01,301 iteration 219 : loss : 0.152865, loss_ce: 0.061301
2021-12-12 22:56:02,823 iteration 220 : loss : 0.146564, loss_ce: 0.054299
2021-12-12 22:56:04,370 iteration 221 : loss : 0.159614, loss_ce: 0.054639
  3%|▉                             | 13/400 [06:01<2:58:00, 27.60s/it]2021-12-12 22:56:05,926 iteration 222 : loss : 0.139818, loss_ce: 0.053428
2021-12-12 22:56:07,410 iteration 223 : loss : 0.178308, loss_ce: 0.071839
2021-12-12 22:56:08,893 iteration 224 : loss : 0.150509, loss_ce: 0.052771
2021-12-12 22:56:10,530 iteration 225 : loss : 0.191441, loss_ce: 0.080977
2021-12-12 22:56:12,075 iteration 226 : loss : 0.158218, loss_ce: 0.059267
2021-12-12 22:56:13,570 iteration 227 : loss : 0.152282, loss_ce: 0.052513
2021-12-12 22:56:15,060 iteration 228 : loss : 0.153452, loss_ce: 0.056738
2021-12-12 22:56:16,593 iteration 229 : loss : 0.150736, loss_ce: 0.056122
2021-12-12 22:56:18,044 iteration 230 : loss : 0.145301, loss_ce: 0.057676
2021-12-12 22:56:19,543 iteration 231 : loss : 0.150577, loss_ce: 0.061124
2021-12-12 22:56:21,032 iteration 232 : loss : 0.165613, loss_ce: 0.065380
2021-12-12 22:56:22,459 iteration 233 : loss : 0.144403, loss_ce: 0.061677
2021-12-12 22:56:23,944 iteration 234 : loss : 0.168609, loss_ce: 0.064453
2021-12-12 22:56:25,397 iteration 235 : loss : 0.162803, loss_ce: 0.065734
2021-12-12 22:56:26,886 iteration 236 : loss : 0.153634, loss_ce: 0.055644
2021-12-12 22:56:28,327 iteration 237 : loss : 0.156031, loss_ce: 0.064574
2021-12-12 22:56:29,869 iteration 238 : loss : 0.176238, loss_ce: 0.059998
  4%|█                             | 14/400 [06:27<2:53:29, 26.97s/it]2021-12-12 22:56:31,412 iteration 239 : loss : 0.158023, loss_ce: 0.059089
2021-12-12 22:56:33,009 iteration 240 : loss : 0.157242, loss_ce: 0.055391
2021-12-12 22:56:34,562 iteration 241 : loss : 0.145705, loss_ce: 0.053643
2021-12-12 22:56:36,049 iteration 242 : loss : 0.145608, loss_ce: 0.051433
2021-12-12 22:56:37,528 iteration 243 : loss : 0.140745, loss_ce: 0.050533
2021-12-12 22:56:39,016 iteration 244 : loss : 0.162468, loss_ce: 0.062235
2021-12-12 22:56:40,462 iteration 245 : loss : 0.159351, loss_ce: 0.054737
2021-12-12 22:56:41,941 iteration 246 : loss : 0.145314, loss_ce: 0.050844
2021-12-12 22:56:43,464 iteration 247 : loss : 0.156752, loss_ce: 0.060392
2021-12-12 22:56:45,001 iteration 248 : loss : 0.166340, loss_ce: 0.072747
2021-12-12 22:56:46,485 iteration 249 : loss : 0.140436, loss_ce: 0.047961
2021-12-12 22:56:47,980 iteration 250 : loss : 0.148605, loss_ce: 0.050663
2021-12-12 22:56:49,417 iteration 251 : loss : 0.151105, loss_ce: 0.058775
2021-12-12 22:56:50,994 iteration 252 : loss : 0.158625, loss_ce: 0.067002
2021-12-12 22:56:52,498 iteration 253 : loss : 0.161580, loss_ce: 0.057213
2021-12-12 22:56:53,970 iteration 254 : loss : 0.139051, loss_ce: 0.054793
2021-12-12 22:56:53,970 Training Data Eval:
2021-12-12 22:57:01,631   Average segmentation loss on training set: 0.1545
2021-12-12 22:57:01,632 Validation Data Eval:
2021-12-12 22:57:04,380   Average segmentation loss on validation set: 0.1619
2021-12-12 22:57:10,762 Found new lowest validation loss at iteration 254! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/TU_3seeds/TU_SGD_best_val_loss_seed100.pth
2021-12-12 22:57:12,093 iteration 255 : loss : 0.143363, loss_ce: 0.057061
  4%|█▏                            | 15/400 [07:09<3:22:31, 31.56s/it]2021-12-12 22:57:13,447 iteration 256 : loss : 0.143284, loss_ce: 0.051328
2021-12-12 22:57:14,771 iteration 257 : loss : 0.150998, loss_ce: 0.059149
2021-12-12 22:57:16,206 iteration 258 : loss : 0.149739, loss_ce: 0.058587
2021-12-12 22:57:17,706 iteration 259 : loss : 0.149984, loss_ce: 0.055902
2021-12-12 22:57:19,148 iteration 260 : loss : 0.152233, loss_ce: 0.070916
2021-12-12 22:57:20,669 iteration 261 : loss : 0.150757, loss_ce: 0.056091
2021-12-12 22:57:22,215 iteration 262 : loss : 0.157474, loss_ce: 0.056494
2021-12-12 22:57:23,659 iteration 263 : loss : 0.143504, loss_ce: 0.052813
2021-12-12 22:57:25,202 iteration 264 : loss : 0.171466, loss_ce: 0.048676
2021-12-12 22:57:26,808 iteration 265 : loss : 0.153612, loss_ce: 0.060539
2021-12-12 22:57:28,331 iteration 266 : loss : 0.152200, loss_ce: 0.049812
2021-12-12 22:57:29,931 iteration 267 : loss : 0.150228, loss_ce: 0.048105
2021-12-12 22:57:31,455 iteration 268 : loss : 0.143988, loss_ce: 0.052743
2021-12-12 22:57:32,974 iteration 269 : loss : 0.161334, loss_ce: 0.057330
2021-12-12 22:57:34,459 iteration 270 : loss : 0.133356, loss_ce: 0.043177
2021-12-12 22:57:35,930 iteration 271 : loss : 0.140032, loss_ce: 0.050088
2021-12-12 22:57:37,378 iteration 272 : loss : 0.141350, loss_ce: 0.047083
  4%|█▏                            | 16/400 [07:34<3:09:55, 29.68s/it]2021-12-12 22:57:38,978 iteration 273 : loss : 0.141389, loss_ce: 0.045988
2021-12-12 22:57:40,481 iteration 274 : loss : 0.135213, loss_ce: 0.046609
2021-12-12 22:57:41,962 iteration 275 : loss : 0.182233, loss_ce: 0.049571
2021-12-12 22:57:43,436 iteration 276 : loss : 0.144021, loss_ce: 0.048539
2021-12-12 22:57:44,903 iteration 277 : loss : 0.147997, loss_ce: 0.049949
2021-12-12 22:57:46,398 iteration 278 : loss : 0.155359, loss_ce: 0.052407
2021-12-12 22:57:47,884 iteration 279 : loss : 0.139837, loss_ce: 0.048162
2021-12-12 22:57:49,376 iteration 280 : loss : 0.150750, loss_ce: 0.055547
2021-12-12 22:57:50,925 iteration 281 : loss : 0.159115, loss_ce: 0.066360
2021-12-12 22:57:52,440 iteration 282 : loss : 0.148860, loss_ce: 0.049070
2021-12-12 22:57:53,909 iteration 283 : loss : 0.138155, loss_ce: 0.049259
2021-12-12 22:57:55,425 iteration 284 : loss : 0.157055, loss_ce: 0.070331
2021-12-12 22:57:56,963 iteration 285 : loss : 0.152385, loss_ce: 0.050024
2021-12-12 22:57:58,357 iteration 286 : loss : 0.157097, loss_ce: 0.056960
2021-12-12 22:57:59,846 iteration 287 : loss : 0.145899, loss_ce: 0.052851
2021-12-12 22:58:01,356 iteration 288 : loss : 0.139993, loss_ce: 0.054448
2021-12-12 22:58:02,827 iteration 289 : loss : 0.156683, loss_ce: 0.054746
  4%|█▎                            | 17/400 [08:00<3:01:18, 28.40s/it]2021-12-12 22:58:04,402 iteration 290 : loss : 0.139889, loss_ce: 0.050227
2021-12-12 22:58:05,848 iteration 291 : loss : 0.139076, loss_ce: 0.044788
2021-12-12 22:58:07,288 iteration 292 : loss : 0.136301, loss_ce: 0.050283
2021-12-12 22:58:08,835 iteration 293 : loss : 0.138048, loss_ce: 0.048529
2021-12-12 22:58:10,313 iteration 294 : loss : 0.140113, loss_ce: 0.050733
2021-12-12 22:58:11,860 iteration 295 : loss : 0.141980, loss_ce: 0.056080
2021-12-12 22:58:13,423 iteration 296 : loss : 0.141363, loss_ce: 0.052623
2021-12-12 22:58:14,943 iteration 297 : loss : 0.143354, loss_ce: 0.052730
2021-12-12 22:58:16,418 iteration 298 : loss : 0.152159, loss_ce: 0.058701
2021-12-12 22:58:17,842 iteration 299 : loss : 0.159315, loss_ce: 0.051960
2021-12-12 22:58:19,307 iteration 300 : loss : 0.147102, loss_ce: 0.049869
2021-12-12 22:58:20,709 iteration 301 : loss : 0.142454, loss_ce: 0.046193
2021-12-12 22:58:22,263 iteration 302 : loss : 0.152581, loss_ce: 0.049144
2021-12-12 22:58:23,802 iteration 303 : loss : 0.148981, loss_ce: 0.043309
2021-12-12 22:58:25,346 iteration 304 : loss : 0.142094, loss_ce: 0.050745
2021-12-12 22:58:26,814 iteration 305 : loss : 0.142939, loss_ce: 0.050908
2021-12-12 22:58:28,295 iteration 306 : loss : 0.154087, loss_ce: 0.048183
  4%|█▎                            | 18/400 [08:25<2:55:13, 27.52s/it]2021-12-12 22:58:29,763 iteration 307 : loss : 0.130195, loss_ce: 0.040816
2021-12-12 22:58:31,176 iteration 308 : loss : 0.154534, loss_ce: 0.057048
2021-12-12 22:58:32,606 iteration 309 : loss : 0.144264, loss_ce: 0.056916
2021-12-12 22:58:34,108 iteration 310 : loss : 0.135963, loss_ce: 0.048128
2021-12-12 22:58:35,637 iteration 311 : loss : 0.143359, loss_ce: 0.048663
2021-12-12 22:58:37,179 iteration 312 : loss : 0.137553, loss_ce: 0.045777
2021-12-12 22:58:38,642 iteration 313 : loss : 0.131047, loss_ce: 0.043538
2021-12-12 22:58:40,146 iteration 314 : loss : 0.143307, loss_ce: 0.055111
2021-12-12 22:58:41,700 iteration 315 : loss : 0.145135, loss_ce: 0.049453
2021-12-12 22:58:43,202 iteration 316 : loss : 0.132065, loss_ce: 0.042858
2021-12-12 22:58:44,640 iteration 317 : loss : 0.139766, loss_ce: 0.042756
2021-12-12 22:58:46,098 iteration 318 : loss : 0.140404, loss_ce: 0.048956
2021-12-12 22:58:47,653 iteration 319 : loss : 0.136669, loss_ce: 0.045166
2021-12-12 22:58:49,113 iteration 320 : loss : 0.142774, loss_ce: 0.046166
2021-12-12 22:58:50,542 iteration 321 : loss : 0.156474, loss_ce: 0.071904
2021-12-12 22:58:52,037 iteration 322 : loss : 0.148372, loss_ce: 0.051807
2021-12-12 22:58:53,582 iteration 323 : loss : 0.139754, loss_ce: 0.047929
  5%|█▍                            | 19/400 [08:50<2:50:30, 26.85s/it]2021-12-12 22:58:55,036 iteration 324 : loss : 0.128622, loss_ce: 0.035271
2021-12-12 22:58:56,517 iteration 325 : loss : 0.132472, loss_ce: 0.040182
2021-12-12 22:58:57,996 iteration 326 : loss : 0.131804, loss_ce: 0.046340
2021-12-12 22:58:59,572 iteration 327 : loss : 0.138500, loss_ce: 0.044785
2021-12-12 22:59:01,107 iteration 328 : loss : 0.150066, loss_ce: 0.057395
2021-12-12 22:59:02,606 iteration 329 : loss : 0.141151, loss_ce: 0.044781
2021-12-12 22:59:04,001 iteration 330 : loss : 0.135161, loss_ce: 0.047220
2021-12-12 22:59:05,550 iteration 331 : loss : 0.136415, loss_ce: 0.042379
2021-12-12 22:59:07,099 iteration 332 : loss : 0.135411, loss_ce: 0.048720
2021-12-12 22:59:08,538 iteration 333 : loss : 0.122459, loss_ce: 0.039673
2021-12-12 22:59:10,074 iteration 334 : loss : 0.157318, loss_ce: 0.060936
2021-12-12 22:59:11,538 iteration 335 : loss : 0.136725, loss_ce: 0.040494
2021-12-12 22:59:13,077 iteration 336 : loss : 0.147070, loss_ce: 0.048017
2021-12-12 22:59:14,538 iteration 337 : loss : 0.133188, loss_ce: 0.045860
2021-12-12 22:59:15,951 iteration 338 : loss : 0.128470, loss_ce: 0.044048
2021-12-12 22:59:17,440 iteration 339 : loss : 0.134447, loss_ce: 0.043268
2021-12-12 22:59:17,440 Training Data Eval:
2021-12-12 22:59:25,106   Average segmentation loss on training set: 0.1284
2021-12-12 22:59:25,106 Validation Data Eval:
2021-12-12 22:59:27,726   Average segmentation loss on validation set: 0.1394
2021-12-12 22:59:35,047 Found new lowest validation loss at iteration 339! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/TU_3seeds/TU_SGD_best_val_loss_seed100.pth
2021-12-12 22:59:36,402 iteration 340 : loss : 0.141345, loss_ce: 0.047498
  5%|█▌                            | 20/400 [09:33<3:20:24, 31.64s/it]2021-12-12 22:59:37,867 iteration 341 : loss : 0.131680, loss_ce: 0.042828
2021-12-12 22:59:39,305 iteration 342 : loss : 0.142745, loss_ce: 0.042216
2021-12-12 22:59:40,815 iteration 343 : loss : 0.146271, loss_ce: 0.056650
2021-12-12 22:59:42,275 iteration 344 : loss : 0.134573, loss_ce: 0.039165
2021-12-12 22:59:43,786 iteration 345 : loss : 0.138181, loss_ce: 0.048021
2021-12-12 22:59:45,355 iteration 346 : loss : 0.137243, loss_ce: 0.043695
2021-12-12 22:59:46,819 iteration 347 : loss : 0.134878, loss_ce: 0.047417
2021-12-12 22:59:48,419 iteration 348 : loss : 0.143955, loss_ce: 0.046034
2021-12-12 22:59:49,907 iteration 349 : loss : 0.130753, loss_ce: 0.043179
2021-12-12 22:59:51,306 iteration 350 : loss : 0.125552, loss_ce: 0.036577
2021-12-12 22:59:52,864 iteration 351 : loss : 0.135312, loss_ce: 0.043925
2021-12-12 22:59:54,353 iteration 352 : loss : 0.128359, loss_ce: 0.038151
2021-12-12 22:59:55,949 iteration 353 : loss : 0.127550, loss_ce: 0.038274
2021-12-12 22:59:57,472 iteration 354 : loss : 0.150626, loss_ce: 0.056879
2021-12-12 22:59:59,044 iteration 355 : loss : 0.147641, loss_ce: 0.048093
2021-12-12 23:00:00,492 iteration 356 : loss : 0.136290, loss_ce: 0.047782
2021-12-12 23:00:01,967 iteration 357 : loss : 0.150222, loss_ce: 0.051394
  5%|█▌                            | 21/400 [09:59<3:08:25, 29.83s/it]2021-12-12 23:00:03,612 iteration 358 : loss : 0.145067, loss_ce: 0.046115
2021-12-12 23:00:05,171 iteration 359 : loss : 0.137711, loss_ce: 0.051707
2021-12-12 23:00:06,675 iteration 360 : loss : 0.187696, loss_ce: 0.052151
2021-12-12 23:00:08,207 iteration 361 : loss : 0.136692, loss_ce: 0.040043
2021-12-12 23:00:09,616 iteration 362 : loss : 0.138677, loss_ce: 0.043023
2021-12-12 23:00:11,189 iteration 363 : loss : 0.152437, loss_ce: 0.059352
2021-12-12 23:00:12,702 iteration 364 : loss : 0.138256, loss_ce: 0.050822
2021-12-12 23:00:14,185 iteration 365 : loss : 0.131006, loss_ce: 0.042755
2021-12-12 23:00:15,676 iteration 366 : loss : 0.128519, loss_ce: 0.046744
2021-12-12 23:00:17,218 iteration 367 : loss : 0.142280, loss_ce: 0.050731
2021-12-12 23:00:18,658 iteration 368 : loss : 0.124629, loss_ce: 0.034144
2021-12-12 23:00:20,183 iteration 369 : loss : 0.136014, loss_ce: 0.044371
2021-12-12 23:00:21,595 iteration 370 : loss : 0.141229, loss_ce: 0.037546
2021-12-12 23:00:23,154 iteration 371 : loss : 0.137030, loss_ce: 0.049725
2021-12-12 23:00:24,715 iteration 372 : loss : 0.131697, loss_ce: 0.040433
2021-12-12 23:00:26,237 iteration 373 : loss : 0.129535, loss_ce: 0.040879
2021-12-12 23:00:27,702 iteration 374 : loss : 0.133199, loss_ce: 0.047496
  6%|█▋                            | 22/400 [10:24<3:00:07, 28.59s/it]2021-12-12 23:00:29,276 iteration 375 : loss : 0.150683, loss_ce: 0.052934
2021-12-12 23:00:30,805 iteration 376 : loss : 0.137591, loss_ce: 0.043015
2021-12-12 23:00:32,337 iteration 377 : loss : 0.140389, loss_ce: 0.039013
2021-12-12 23:00:33,857 iteration 378 : loss : 0.141618, loss_ce: 0.044355
2021-12-12 23:00:35,294 iteration 379 : loss : 0.147775, loss_ce: 0.041308
2021-12-12 23:00:36,828 iteration 380 : loss : 0.139700, loss_ce: 0.053095
2021-12-12 23:00:38,223 iteration 381 : loss : 0.131150, loss_ce: 0.036858
2021-12-12 23:00:39,834 iteration 382 : loss : 0.138752, loss_ce: 0.045264
2021-12-12 23:00:41,380 iteration 383 : loss : 0.134058, loss_ce: 0.042093
2021-12-12 23:00:42,877 iteration 384 : loss : 0.135860, loss_ce: 0.046929
2021-12-12 23:00:44,331 iteration 385 : loss : 0.133439, loss_ce: 0.047404
2021-12-12 23:00:45,868 iteration 386 : loss : 0.138239, loss_ce: 0.043294
2021-12-12 23:00:47,391 iteration 387 : loss : 0.144173, loss_ce: 0.049284
2021-12-12 23:00:48,948 iteration 388 : loss : 0.140005, loss_ce: 0.051418
2021-12-12 23:00:50,419 iteration 389 : loss : 0.129736, loss_ce: 0.037787
2021-12-12 23:00:51,948 iteration 390 : loss : 0.139495, loss_ce: 0.049370
2021-12-12 23:00:53,412 iteration 391 : loss : 0.132052, loss_ce: 0.039277
  6%|█▋                            | 23/400 [10:50<2:54:12, 27.73s/it]2021-12-12 23:00:55,080 iteration 392 : loss : 0.124247, loss_ce: 0.041978
2021-12-12 23:00:56,544 iteration 393 : loss : 0.133002, loss_ce: 0.049071
2021-12-12 23:00:58,063 iteration 394 : loss : 0.139592, loss_ce: 0.043208
2021-12-12 23:00:59,546 iteration 395 : loss : 0.129049, loss_ce: 0.046348
2021-12-12 23:01:01,092 iteration 396 : loss : 0.127879, loss_ce: 0.043148
2021-12-12 23:01:02,624 iteration 397 : loss : 0.131446, loss_ce: 0.041374
2021-12-12 23:01:04,148 iteration 398 : loss : 0.136854, loss_ce: 0.050230
2021-12-12 23:01:05,639 iteration 399 : loss : 0.121607, loss_ce: 0.037230
2021-12-12 23:01:07,199 iteration 400 : loss : 0.139175, loss_ce: 0.048591
2021-12-12 23:01:08,620 iteration 401 : loss : 0.127032, loss_ce: 0.036288
2021-12-12 23:01:10,170 iteration 402 : loss : 0.144951, loss_ce: 0.050546
2021-12-12 23:01:11,761 iteration 403 : loss : 0.129682, loss_ce: 0.042602
2021-12-12 23:01:13,264 iteration 404 : loss : 0.143854, loss_ce: 0.039859
2021-12-12 23:01:14,818 iteration 405 : loss : 0.155012, loss_ce: 0.046540
2021-12-12 23:01:16,280 iteration 406 : loss : 0.129138, loss_ce: 0.044211
2021-12-12 23:01:17,737 iteration 407 : loss : 0.138415, loss_ce: 0.040348
2021-12-12 23:01:19,190 iteration 408 : loss : 0.131360, loss_ce: 0.041606
  6%|█▊                            | 24/400 [11:16<2:50:04, 27.14s/it]2021-12-12 23:01:20,862 iteration 409 : loss : 0.136638, loss_ce: 0.042500
2021-12-12 23:01:22,360 iteration 410 : loss : 0.130413, loss_ce: 0.038020
2021-12-12 23:01:23,889 iteration 411 : loss : 0.141956, loss_ce: 0.041377
2021-12-12 23:01:25,403 iteration 412 : loss : 0.124593, loss_ce: 0.039319
2021-12-12 23:01:26,878 iteration 413 : loss : 0.138897, loss_ce: 0.047532
2021-12-12 23:01:28,396 iteration 414 : loss : 0.134677, loss_ce: 0.046115
2021-12-12 23:01:29,949 iteration 415 : loss : 0.146901, loss_ce: 0.058143
2021-12-12 23:01:31,325 iteration 416 : loss : 0.129690, loss_ce: 0.036699
2021-12-12 23:01:32,811 iteration 417 : loss : 0.128651, loss_ce: 0.038969
2021-12-12 23:01:34,320 iteration 418 : loss : 0.123543, loss_ce: 0.035733
2021-12-12 23:01:35,796 iteration 419 : loss : 0.149666, loss_ce: 0.040701
2021-12-12 23:01:37,301 iteration 420 : loss : 0.137269, loss_ce: 0.046109
2021-12-12 23:01:38,700 iteration 421 : loss : 0.133812, loss_ce: 0.035401
2021-12-12 23:01:40,231 iteration 422 : loss : 0.144793, loss_ce: 0.047201
2021-12-12 23:01:41,703 iteration 423 : loss : 0.130236, loss_ce: 0.043006
2021-12-12 23:01:43,166 iteration 424 : loss : 0.133661, loss_ce: 0.049796
2021-12-12 23:01:43,166 Training Data Eval:
2021-12-12 23:01:50,813   Average segmentation loss on training set: 0.1242
2021-12-12 23:01:50,813 Validation Data Eval:
2021-12-12 23:01:53,411   Average segmentation loss on validation set: 0.1448
2021-12-12 23:01:54,959 iteration 425 : loss : 0.130688, loss_ce: 0.041610
  6%|█▉                            | 25/400 [11:52<3:05:48, 29.73s/it]2021-12-12 23:01:56,543 iteration 426 : loss : 0.142098, loss_ce: 0.049245
2021-12-12 23:01:57,993 iteration 427 : loss : 0.130004, loss_ce: 0.043306
2021-12-12 23:01:59,595 iteration 428 : loss : 0.129580, loss_ce: 0.039207
2021-12-12 23:02:01,063 iteration 429 : loss : 0.123734, loss_ce: 0.035159
2021-12-12 23:02:02,479 iteration 430 : loss : 0.133571, loss_ce: 0.048049
2021-12-12 23:02:04,006 iteration 431 : loss : 0.136106, loss_ce: 0.051698
2021-12-12 23:02:05,520 iteration 432 : loss : 0.143945, loss_ce: 0.039004
2021-12-12 23:02:06,939 iteration 433 : loss : 0.122016, loss_ce: 0.041475
2021-12-12 23:02:08,380 iteration 434 : loss : 0.118668, loss_ce: 0.033283
2021-12-12 23:02:09,891 iteration 435 : loss : 0.126809, loss_ce: 0.039638
2021-12-12 23:02:11,417 iteration 436 : loss : 0.119010, loss_ce: 0.030905
2021-12-12 23:02:12,874 iteration 437 : loss : 0.127729, loss_ce: 0.043172
2021-12-12 23:02:14,324 iteration 438 : loss : 0.121206, loss_ce: 0.035075
2021-12-12 23:02:15,792 iteration 439 : loss : 0.129977, loss_ce: 0.039277
2021-12-12 23:02:17,433 iteration 440 : loss : 0.144556, loss_ce: 0.048215
2021-12-12 23:02:18,919 iteration 441 : loss : 0.131184, loss_ce: 0.043581
2021-12-12 23:02:20,383 iteration 442 : loss : 0.124879, loss_ce: 0.035434
  6%|█▉                            | 26/400 [12:17<2:57:16, 28.44s/it]2021-12-12 23:02:21,960 iteration 443 : loss : 0.123360, loss_ce: 0.037512
2021-12-12 23:02:23,408 iteration 444 : loss : 0.120874, loss_ce: 0.034548
2021-12-12 23:02:24,844 iteration 445 : loss : 0.119452, loss_ce: 0.038689
2021-12-12 23:02:26,313 iteration 446 : loss : 0.137657, loss_ce: 0.041089
2021-12-12 23:02:27,851 iteration 447 : loss : 0.129959, loss_ce: 0.040150
2021-12-12 23:02:29,259 iteration 448 : loss : 0.123076, loss_ce: 0.033853
2021-12-12 23:02:30,712 iteration 449 : loss : 0.138097, loss_ce: 0.034208
2021-12-12 23:02:32,115 iteration 450 : loss : 0.136182, loss_ce: 0.050650
2021-12-12 23:02:33,592 iteration 451 : loss : 0.130399, loss_ce: 0.042802
2021-12-12 23:02:35,090 iteration 452 : loss : 0.118811, loss_ce: 0.033558
2021-12-12 23:02:36,548 iteration 453 : loss : 0.112895, loss_ce: 0.035178
2021-12-12 23:02:38,144 iteration 454 : loss : 0.151122, loss_ce: 0.056080
2021-12-12 23:02:39,676 iteration 455 : loss : 0.134471, loss_ce: 0.047277
2021-12-12 23:02:41,163 iteration 456 : loss : 0.122741, loss_ce: 0.034594
2021-12-12 23:02:42,634 iteration 457 : loss : 0.129919, loss_ce: 0.046411
2021-12-12 23:02:44,099 iteration 458 : loss : 0.126511, loss_ce: 0.044029
2021-12-12 23:02:45,595 iteration 459 : loss : 0.122002, loss_ce: 0.035487
  7%|██                            | 27/400 [12:42<2:50:45, 27.47s/it]2021-12-12 23:02:47,061 iteration 460 : loss : 0.120186, loss_ce: 0.041720
2021-12-12 23:02:48,520 iteration 461 : loss : 0.120154, loss_ce: 0.037714
2021-12-12 23:02:50,083 iteration 462 : loss : 0.133275, loss_ce: 0.040756
2021-12-12 23:02:51,586 iteration 463 : loss : 0.130042, loss_ce: 0.037255
2021-12-12 23:02:53,226 iteration 464 : loss : 0.141686, loss_ce: 0.041478
2021-12-12 23:02:54,769 iteration 465 : loss : 0.127274, loss_ce: 0.040690
2021-12-12 23:02:56,234 iteration 466 : loss : 0.119766, loss_ce: 0.041570
2021-12-12 23:02:57,710 iteration 467 : loss : 0.126004, loss_ce: 0.039949
2021-12-12 23:02:59,170 iteration 468 : loss : 0.119517, loss_ce: 0.036593
2021-12-12 23:03:00,704 iteration 469 : loss : 0.120709, loss_ce: 0.040290
2021-12-12 23:03:02,197 iteration 470 : loss : 0.117745, loss_ce: 0.034414
2021-12-12 23:03:03,690 iteration 471 : loss : 0.139937, loss_ce: 0.048298
2021-12-12 23:03:05,215 iteration 472 : loss : 0.119824, loss_ce: 0.038387
2021-12-12 23:03:06,718 iteration 473 : loss : 0.128521, loss_ce: 0.040471
2021-12-12 23:03:08,170 iteration 474 : loss : 0.127104, loss_ce: 0.034814
2021-12-12 23:03:09,652 iteration 475 : loss : 0.126963, loss_ce: 0.034205
2021-12-12 23:03:11,218 iteration 476 : loss : 0.133136, loss_ce: 0.040064
  7%|██                            | 28/400 [13:08<2:46:52, 26.92s/it]2021-12-12 23:03:12,750 iteration 477 : loss : 0.124993, loss_ce: 0.038952
2021-12-12 23:03:14,176 iteration 478 : loss : 0.128345, loss_ce: 0.042073
2021-12-12 23:03:15,704 iteration 479 : loss : 0.129985, loss_ce: 0.038434
2021-12-12 23:03:17,194 iteration 480 : loss : 0.133284, loss_ce: 0.033896
2021-12-12 23:03:18,639 iteration 481 : loss : 0.131332, loss_ce: 0.032277
2021-12-12 23:03:20,191 iteration 482 : loss : 0.141175, loss_ce: 0.044207
2021-12-12 23:03:21,627 iteration 483 : loss : 0.129052, loss_ce: 0.036701
2021-12-12 23:03:23,136 iteration 484 : loss : 0.127610, loss_ce: 0.036817
2021-12-12 23:03:24,648 iteration 485 : loss : 0.129729, loss_ce: 0.043435
2021-12-12 23:03:26,255 iteration 486 : loss : 0.121460, loss_ce: 0.035899
2021-12-12 23:03:27,757 iteration 487 : loss : 0.129950, loss_ce: 0.036593
2021-12-12 23:03:29,173 iteration 488 : loss : 0.132602, loss_ce: 0.046013
2021-12-12 23:03:30,703 iteration 489 : loss : 0.124409, loss_ce: 0.044492
2021-12-12 23:03:32,197 iteration 490 : loss : 0.120039, loss_ce: 0.039319
2021-12-12 23:03:33,756 iteration 491 : loss : 0.134051, loss_ce: 0.033278
2021-12-12 23:03:35,223 iteration 492 : loss : 0.120104, loss_ce: 0.041963
2021-12-12 23:03:36,673 iteration 493 : loss : 0.120744, loss_ce: 0.040848
  7%|██▏                           | 29/400 [13:33<2:43:44, 26.48s/it]2021-12-12 23:03:38,203 iteration 494 : loss : 0.124550, loss_ce: 0.034453
2021-12-12 23:03:39,631 iteration 495 : loss : 0.124619, loss_ce: 0.033391
2021-12-12 23:03:41,139 iteration 496 : loss : 0.132023, loss_ce: 0.036403
2021-12-12 23:03:42,722 iteration 497 : loss : 0.121762, loss_ce: 0.037581
2021-12-12 23:03:44,332 iteration 498 : loss : 0.122737, loss_ce: 0.043449
2021-12-12 23:03:45,864 iteration 499 : loss : 0.134473, loss_ce: 0.034990
2021-12-12 23:03:47,506 iteration 500 : loss : 0.142070, loss_ce: 0.045581
2021-12-12 23:03:49,010 iteration 501 : loss : 0.124758, loss_ce: 0.032743
2021-12-12 23:03:50,453 iteration 502 : loss : 0.126656, loss_ce: 0.044261
2021-12-12 23:03:52,062 iteration 503 : loss : 0.120098, loss_ce: 0.029145
2021-12-12 23:03:53,691 iteration 504 : loss : 0.130201, loss_ce: 0.039202
2021-12-12 23:03:55,164 iteration 505 : loss : 0.125446, loss_ce: 0.039107
2021-12-12 23:03:56,697 iteration 506 : loss : 0.139201, loss_ce: 0.044409
2021-12-12 23:03:58,263 iteration 507 : loss : 0.118508, loss_ce: 0.035185
2021-12-12 23:03:59,766 iteration 508 : loss : 0.122689, loss_ce: 0.038222
2021-12-12 23:04:01,288 iteration 509 : loss : 0.129715, loss_ce: 0.043954
2021-12-12 23:04:01,288 Training Data Eval:
2021-12-12 23:04:08,921   Average segmentation loss on training set: 0.1107
2021-12-12 23:04:08,921 Validation Data Eval:
2021-12-12 23:04:11,519   Average segmentation loss on validation set: 0.1337
2021-12-12 23:04:17,957 Found new lowest validation loss at iteration 509! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/TU_3seeds/TU_SGD_best_val_loss_seed100.pth
2021-12-12 23:04:19,248 iteration 510 : loss : 0.115667, loss_ce: 0.035166
  8%|██▎                           | 30/400 [14:16<3:13:02, 31.31s/it]2021-12-12 23:04:20,522 iteration 511 : loss : 0.109452, loss_ce: 0.032381
2021-12-12 23:04:22,062 iteration 512 : loss : 0.140081, loss_ce: 0.043979
2021-12-12 23:04:23,384 iteration 513 : loss : 0.130657, loss_ce: 0.043204
2021-12-12 23:04:24,894 iteration 514 : loss : 0.126479, loss_ce: 0.034114
2021-12-12 23:04:26,353 iteration 515 : loss : 0.119068, loss_ce: 0.038320
2021-12-12 23:04:27,750 iteration 516 : loss : 0.114821, loss_ce: 0.039966
2021-12-12 23:04:29,397 iteration 517 : loss : 0.120041, loss_ce: 0.034410
2021-12-12 23:04:30,922 iteration 518 : loss : 0.132543, loss_ce: 0.051026
2021-12-12 23:04:32,376 iteration 519 : loss : 0.123931, loss_ce: 0.035295
2021-12-12 23:04:33,822 iteration 520 : loss : 0.118946, loss_ce: 0.039506
2021-12-12 23:04:35,412 iteration 521 : loss : 0.148816, loss_ce: 0.034718
2021-12-12 23:04:36,867 iteration 522 : loss : 0.110402, loss_ce: 0.033939
2021-12-12 23:04:38,395 iteration 523 : loss : 0.117467, loss_ce: 0.036379
2021-12-12 23:04:39,766 iteration 524 : loss : 0.108798, loss_ce: 0.030016
2021-12-12 23:04:41,236 iteration 525 : loss : 0.119691, loss_ce: 0.034903
2021-12-12 23:04:42,752 iteration 526 : loss : 0.121638, loss_ce: 0.036556
2021-12-12 23:04:44,232 iteration 527 : loss : 0.126973, loss_ce: 0.036626
  8%|██▎                           | 31/400 [14:41<3:00:53, 29.41s/it]2021-12-12 23:04:45,811 iteration 528 : loss : 0.113194, loss_ce: 0.030423
2021-12-12 23:04:47,304 iteration 529 : loss : 0.118141, loss_ce: 0.035876
2021-12-12 23:04:48,767 iteration 530 : loss : 0.112839, loss_ce: 0.031467
2021-12-12 23:04:50,314 iteration 531 : loss : 0.134185, loss_ce: 0.035538
2021-12-12 23:04:51,729 iteration 532 : loss : 0.116684, loss_ce: 0.028537
2021-12-12 23:04:53,237 iteration 533 : loss : 0.119547, loss_ce: 0.041296
2021-12-12 23:04:54,738 iteration 534 : loss : 0.135313, loss_ce: 0.048702
2021-12-12 23:04:56,237 iteration 535 : loss : 0.121002, loss_ce: 0.036763
2021-12-12 23:04:57,655 iteration 536 : loss : 0.120660, loss_ce: 0.038729
2021-12-12 23:04:59,135 iteration 537 : loss : 0.122588, loss_ce: 0.038370
2021-12-12 23:05:00,603 iteration 538 : loss : 0.116842, loss_ce: 0.034562
2021-12-12 23:05:02,049 iteration 539 : loss : 0.125081, loss_ce: 0.034924
2021-12-12 23:05:03,479 iteration 540 : loss : 0.120605, loss_ce: 0.038675
2021-12-12 23:05:04,959 iteration 541 : loss : 0.120189, loss_ce: 0.044833
2021-12-12 23:05:06,374 iteration 542 : loss : 0.111538, loss_ce: 0.031571
2021-12-12 23:05:07,810 iteration 543 : loss : 0.128591, loss_ce: 0.035246
2021-12-12 23:05:09,279 iteration 544 : loss : 0.119976, loss_ce: 0.033700
  8%|██▍                           | 32/400 [15:06<2:52:21, 28.10s/it]2021-12-12 23:05:10,788 iteration 545 : loss : 0.124408, loss_ce: 0.036146
2021-12-12 23:05:12,286 iteration 546 : loss : 0.108742, loss_ce: 0.031182
2021-12-12 23:05:13,774 iteration 547 : loss : 0.127303, loss_ce: 0.037577
2021-12-12 23:05:15,186 iteration 548 : loss : 0.127543, loss_ce: 0.034862
2021-12-12 23:05:16,683 iteration 549 : loss : 0.130672, loss_ce: 0.043591
2021-12-12 23:05:18,277 iteration 550 : loss : 0.116050, loss_ce: 0.031669
2021-12-12 23:05:19,801 iteration 551 : loss : 0.123118, loss_ce: 0.032822
2021-12-12 23:05:21,343 iteration 552 : loss : 0.128084, loss_ce: 0.042889
2021-12-12 23:05:22,898 iteration 553 : loss : 0.130014, loss_ce: 0.045499
2021-12-12 23:05:24,325 iteration 554 : loss : 0.115826, loss_ce: 0.033052
2021-12-12 23:05:25,839 iteration 555 : loss : 0.119587, loss_ce: 0.039265
2021-12-12 23:05:27,421 iteration 556 : loss : 0.119257, loss_ce: 0.032850
2021-12-12 23:05:28,905 iteration 557 : loss : 0.116068, loss_ce: 0.037730
2021-12-12 23:05:30,353 iteration 558 : loss : 0.129329, loss_ce: 0.038368
2021-12-12 23:05:31,863 iteration 559 : loss : 0.125287, loss_ce: 0.040128
2021-12-12 23:05:33,392 iteration 560 : loss : 0.124020, loss_ce: 0.034351
2021-12-12 23:05:34,834 iteration 561 : loss : 0.147661, loss_ce: 0.037694
  8%|██▍                           | 33/400 [15:32<2:47:12, 27.34s/it]2021-12-12 23:05:36,349 iteration 562 : loss : 0.113802, loss_ce: 0.031479
2021-12-12 23:05:37,860 iteration 563 : loss : 0.124043, loss_ce: 0.043781
2021-12-12 23:05:39,415 iteration 564 : loss : 0.123827, loss_ce: 0.040830
2021-12-12 23:05:40,930 iteration 565 : loss : 0.126077, loss_ce: 0.044516
2021-12-12 23:05:42,343 iteration 566 : loss : 0.115972, loss_ce: 0.032122
2021-12-12 23:05:43,889 iteration 567 : loss : 0.129648, loss_ce: 0.033167
2021-12-12 23:05:45,349 iteration 568 : loss : 0.119435, loss_ce: 0.034156
2021-12-12 23:05:46,835 iteration 569 : loss : 0.108504, loss_ce: 0.028355
2021-12-12 23:05:48,418 iteration 570 : loss : 0.119041, loss_ce: 0.035047
2021-12-12 23:05:49,964 iteration 571 : loss : 0.117031, loss_ce: 0.034969
2021-12-12 23:05:51,494 iteration 572 : loss : 0.122990, loss_ce: 0.036313
2021-12-12 23:05:52,964 iteration 573 : loss : 0.110808, loss_ce: 0.032857
2021-12-12 23:05:54,440 iteration 574 : loss : 0.119956, loss_ce: 0.035493
2021-12-12 23:05:55,981 iteration 575 : loss : 0.119788, loss_ce: 0.035874
2021-12-12 23:05:57,478 iteration 576 : loss : 0.122798, loss_ce: 0.042078
2021-12-12 23:05:58,995 iteration 577 : loss : 0.135109, loss_ce: 0.032479
2021-12-12 23:06:00,511 iteration 578 : loss : 0.129624, loss_ce: 0.038165
  8%|██▌                           | 34/400 [15:57<2:43:42, 26.84s/it]2021-12-12 23:06:02,037 iteration 579 : loss : 0.108012, loss_ce: 0.026570
2021-12-12 23:06:03,518 iteration 580 : loss : 0.123840, loss_ce: 0.039561
2021-12-12 23:06:05,021 iteration 581 : loss : 0.124377, loss_ce: 0.039598
2021-12-12 23:06:06,534 iteration 582 : loss : 0.115873, loss_ce: 0.035342
2021-12-12 23:06:07,999 iteration 583 : loss : 0.115016, loss_ce: 0.035670
2021-12-12 23:06:09,376 iteration 584 : loss : 0.110691, loss_ce: 0.030845
2021-12-12 23:06:10,807 iteration 585 : loss : 0.116648, loss_ce: 0.031429
2021-12-12 23:06:12,329 iteration 586 : loss : 0.122546, loss_ce: 0.039770
2021-12-12 23:06:13,806 iteration 587 : loss : 0.120519, loss_ce: 0.035415
2021-12-12 23:06:15,332 iteration 588 : loss : 0.133161, loss_ce: 0.036415
2021-12-12 23:06:16,824 iteration 589 : loss : 0.106491, loss_ce: 0.031526
2021-12-12 23:06:18,276 iteration 590 : loss : 0.120750, loss_ce: 0.044214
2021-12-12 23:06:19,752 iteration 591 : loss : 0.120906, loss_ce: 0.038294
2021-12-12 23:06:21,219 iteration 592 : loss : 0.116029, loss_ce: 0.034241
2021-12-12 23:06:22,751 iteration 593 : loss : 0.122394, loss_ce: 0.037320
2021-12-12 23:06:24,300 iteration 594 : loss : 0.114715, loss_ce: 0.037220
2021-12-12 23:06:24,301 Training Data Eval:
2021-12-12 23:06:31,945   Average segmentation loss on training set: 0.1127
2021-12-12 23:06:31,945 Validation Data Eval:
2021-12-12 23:06:34,549   Average segmentation loss on validation set: 0.1342
2021-12-12 23:06:36,065 iteration 595 : loss : 0.114152, loss_ce: 0.030154
  9%|██▋                           | 35/400 [16:33<2:59:11, 29.46s/it]2021-12-12 23:06:37,545 iteration 596 : loss : 0.109968, loss_ce: 0.034012
2021-12-12 23:06:39,040 iteration 597 : loss : 0.125034, loss_ce: 0.042007
2021-12-12 23:06:40,557 iteration 598 : loss : 0.116738, loss_ce: 0.037391
2021-12-12 23:06:42,104 iteration 599 : loss : 0.123010, loss_ce: 0.036399
2021-12-12 23:06:43,505 iteration 600 : loss : 0.109798, loss_ce: 0.035131
2021-12-12 23:06:44,990 iteration 601 : loss : 0.111438, loss_ce: 0.029642
2021-12-12 23:06:46,527 iteration 602 : loss : 0.107426, loss_ce: 0.031573
2021-12-12 23:06:47,921 iteration 603 : loss : 0.119428, loss_ce: 0.036134
2021-12-12 23:06:49,394 iteration 604 : loss : 0.115502, loss_ce: 0.033286
2021-12-12 23:06:50,877 iteration 605 : loss : 0.118432, loss_ce: 0.036625
2021-12-12 23:06:52,322 iteration 606 : loss : 0.112186, loss_ce: 0.032123
2021-12-12 23:06:53,796 iteration 607 : loss : 0.122964, loss_ce: 0.033599
2021-12-12 23:06:55,393 iteration 608 : loss : 0.120623, loss_ce: 0.034338
2021-12-12 23:06:56,861 iteration 609 : loss : 0.114157, loss_ce: 0.032301
2021-12-12 23:06:58,405 iteration 610 : loss : 0.127934, loss_ce: 0.038751
2021-12-12 23:06:59,887 iteration 611 : loss : 0.128915, loss_ce: 0.035915
2021-12-12 23:07:01,371 iteration 612 : loss : 0.129747, loss_ce: 0.043269
  9%|██▋                           | 36/400 [16:58<2:51:07, 28.21s/it]2021-12-12 23:07:02,915 iteration 613 : loss : 0.120464, loss_ce: 0.038792
2021-12-12 23:07:04,520 iteration 614 : loss : 0.117467, loss_ce: 0.037708
2021-12-12 23:07:06,007 iteration 615 : loss : 0.115594, loss_ce: 0.032322
2021-12-12 23:07:07,440 iteration 616 : loss : 0.129460, loss_ce: 0.056111
2021-12-12 23:07:08,923 iteration 617 : loss : 0.126803, loss_ce: 0.041126
2021-12-12 23:07:10,356 iteration 618 : loss : 0.119058, loss_ce: 0.032738
2021-12-12 23:07:11,825 iteration 619 : loss : 0.117467, loss_ce: 0.031114
2021-12-12 23:07:13,397 iteration 620 : loss : 0.122695, loss_ce: 0.032946
2021-12-12 23:07:14,846 iteration 621 : loss : 0.108430, loss_ce: 0.030040
2021-12-12 23:07:16,351 iteration 622 : loss : 0.111546, loss_ce: 0.033486
2021-12-12 23:07:17,903 iteration 623 : loss : 0.117787, loss_ce: 0.029769
2021-12-12 23:07:19,369 iteration 624 : loss : 0.127289, loss_ce: 0.031009
2021-12-12 23:07:20,792 iteration 625 : loss : 0.110561, loss_ce: 0.031535
2021-12-12 23:07:22,213 iteration 626 : loss : 0.113474, loss_ce: 0.032777
2021-12-12 23:07:23,663 iteration 627 : loss : 0.127820, loss_ce: 0.036582
2021-12-12 23:07:25,127 iteration 628 : loss : 0.127648, loss_ce: 0.052556
2021-12-12 23:07:26,639 iteration 629 : loss : 0.116888, loss_ce: 0.033792
  9%|██▊                           | 37/400 [17:23<2:45:19, 27.33s/it]2021-12-12 23:07:28,147 iteration 630 : loss : 0.116418, loss_ce: 0.035860
2021-12-12 23:07:29,674 iteration 631 : loss : 0.124202, loss_ce: 0.045254
2021-12-12 23:07:31,177 iteration 632 : loss : 0.130602, loss_ce: 0.041450
2021-12-12 23:07:32,704 iteration 633 : loss : 0.118182, loss_ce: 0.036506
2021-12-12 23:07:34,254 iteration 634 : loss : 0.121210, loss_ce: 0.039833
2021-12-12 23:07:35,730 iteration 635 : loss : 0.114682, loss_ce: 0.031845
2021-12-12 23:07:37,300 iteration 636 : loss : 0.112073, loss_ce: 0.036582
2021-12-12 23:07:38,750 iteration 637 : loss : 0.108763, loss_ce: 0.027477
2021-12-12 23:07:40,204 iteration 638 : loss : 0.118273, loss_ce: 0.032154
2021-12-12 23:07:41,736 iteration 639 : loss : 0.119280, loss_ce: 0.035165
2021-12-12 23:07:43,204 iteration 640 : loss : 0.117569, loss_ce: 0.033948
2021-12-12 23:07:44,724 iteration 641 : loss : 0.108092, loss_ce: 0.029619
2021-12-12 23:07:46,236 iteration 642 : loss : 0.122347, loss_ce: 0.035593
2021-12-12 23:07:47,726 iteration 643 : loss : 0.118994, loss_ce: 0.037516
2021-12-12 23:07:49,212 iteration 644 : loss : 0.110646, loss_ce: 0.029314
2021-12-12 23:07:50,724 iteration 645 : loss : 0.106909, loss_ce: 0.028431
2021-12-12 23:07:52,208 iteration 646 : loss : 0.114590, loss_ce: 0.037775
 10%|██▊                           | 38/400 [17:49<2:41:41, 26.80s/it]2021-12-12 23:07:53,727 iteration 647 : loss : 0.109474, loss_ce: 0.025793
2021-12-12 23:07:55,176 iteration 648 : loss : 0.114499, loss_ce: 0.037941
2021-12-12 23:07:56,624 iteration 649 : loss : 0.110285, loss_ce: 0.035965
2021-12-12 23:07:58,156 iteration 650 : loss : 0.136444, loss_ce: 0.032322
2021-12-12 23:07:59,684 iteration 651 : loss : 0.111906, loss_ce: 0.030123
2021-12-12 23:08:01,118 iteration 652 : loss : 0.108620, loss_ce: 0.034132
2021-12-12 23:08:02,582 iteration 653 : loss : 0.103050, loss_ce: 0.029153
2021-12-12 23:08:04,078 iteration 654 : loss : 0.120752, loss_ce: 0.031436
2021-12-12 23:08:05,493 iteration 655 : loss : 0.109048, loss_ce: 0.030709
2021-12-12 23:08:07,029 iteration 656 : loss : 0.128891, loss_ce: 0.042473
2021-12-12 23:08:08,508 iteration 657 : loss : 0.103521, loss_ce: 0.025382
2021-12-12 23:08:09,999 iteration 658 : loss : 0.147724, loss_ce: 0.050678
2021-12-12 23:08:11,531 iteration 659 : loss : 0.113181, loss_ce: 0.028673
2021-12-12 23:08:13,040 iteration 660 : loss : 0.112970, loss_ce: 0.035499
2021-12-12 23:08:14,544 iteration 661 : loss : 0.109729, loss_ce: 0.036141
2021-12-12 23:08:16,063 iteration 662 : loss : 0.123468, loss_ce: 0.038917
2021-12-12 23:08:17,548 iteration 663 : loss : 0.111377, loss_ce: 0.032057
 10%|██▉                           | 39/400 [18:14<2:38:36, 26.36s/it]2021-12-12 23:08:19,034 iteration 664 : loss : 0.119161, loss_ce: 0.038166
2021-12-12 23:08:20,559 iteration 665 : loss : 0.130695, loss_ce: 0.033812
2021-12-12 23:08:22,066 iteration 666 : loss : 0.106320, loss_ce: 0.025763
2021-12-12 23:08:23,617 iteration 667 : loss : 0.125920, loss_ce: 0.039294
2021-12-12 23:08:25,097 iteration 668 : loss : 0.102930, loss_ce: 0.027068
2021-12-12 23:08:26,651 iteration 669 : loss : 0.110698, loss_ce: 0.034324
2021-12-12 23:08:28,194 iteration 670 : loss : 0.120815, loss_ce: 0.040233
2021-12-12 23:08:29,707 iteration 671 : loss : 0.138227, loss_ce: 0.045154
2021-12-12 23:08:31,116 iteration 672 : loss : 0.101891, loss_ce: 0.029125
2021-12-12 23:08:32,588 iteration 673 : loss : 0.119708, loss_ce: 0.030959
2021-12-12 23:08:34,230 iteration 674 : loss : 0.111388, loss_ce: 0.035702
2021-12-12 23:08:35,739 iteration 675 : loss : 0.114040, loss_ce: 0.030232
2021-12-12 23:08:37,224 iteration 676 : loss : 0.119177, loss_ce: 0.035694
2021-12-12 23:08:38,836 iteration 677 : loss : 0.113767, loss_ce: 0.035751
2021-12-12 23:08:40,364 iteration 678 : loss : 0.126968, loss_ce: 0.032289
2021-12-12 23:08:41,814 iteration 679 : loss : 0.114291, loss_ce: 0.036596
2021-12-12 23:08:41,815 Training Data Eval:
2021-12-12 23:08:49,466   Average segmentation loss on training set: 0.1040
2021-12-12 23:08:49,466 Validation Data Eval:
2021-12-12 23:08:52,079   Average segmentation loss on validation set: 0.1251
2021-12-12 23:08:58,360 Found new lowest validation loss at iteration 679! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/TU_3seeds/TU_SGD_best_val_loss_seed100.pth
2021-12-12 23:08:59,709 iteration 680 : loss : 0.106526, loss_ce: 0.028716
 10%|███                           | 40/400 [18:56<3:06:37, 31.10s/it]2021-12-12 23:09:01,258 iteration 681 : loss : 0.117484, loss_ce: 0.034235
2021-12-12 23:09:02,640 iteration 682 : loss : 0.122631, loss_ce: 0.036793
2021-12-12 23:09:03,981 iteration 683 : loss : 0.109225, loss_ce: 0.025501
2021-12-12 23:09:05,530 iteration 684 : loss : 0.117430, loss_ce: 0.036776
2021-12-12 23:09:07,103 iteration 685 : loss : 0.113180, loss_ce: 0.031519
2021-12-12 23:09:08,640 iteration 686 : loss : 0.118011, loss_ce: 0.035684
2021-12-12 23:09:10,184 iteration 687 : loss : 0.107741, loss_ce: 0.033841
2021-12-12 23:09:11,759 iteration 688 : loss : 0.137797, loss_ce: 0.043922
2021-12-12 23:09:13,299 iteration 689 : loss : 0.113186, loss_ce: 0.036553
2021-12-12 23:09:14,794 iteration 690 : loss : 0.111108, loss_ce: 0.033208
2021-12-12 23:09:16,344 iteration 691 : loss : 0.117478, loss_ce: 0.032298
2021-12-12 23:09:17,778 iteration 692 : loss : 0.110762, loss_ce: 0.034611
2021-12-12 23:09:19,249 iteration 693 : loss : 0.102855, loss_ce: 0.029287
2021-12-12 23:09:20,746 iteration 694 : loss : 0.116093, loss_ce: 0.031255
2021-12-12 23:09:22,238 iteration 695 : loss : 0.122956, loss_ce: 0.030339
2021-12-12 23:09:23,693 iteration 696 : loss : 0.129119, loss_ce: 0.028879
2021-12-12 23:09:25,090 iteration 697 : loss : 0.105880, loss_ce: 0.033425
 10%|███                           | 41/400 [19:22<2:55:49, 29.39s/it]2021-12-12 23:09:26,631 iteration 698 : loss : 0.117919, loss_ce: 0.040554
2021-12-12 23:09:28,150 iteration 699 : loss : 0.114170, loss_ce: 0.034208
2021-12-12 23:09:29,608 iteration 700 : loss : 0.111222, loss_ce: 0.031275
2021-12-12 23:09:31,069 iteration 701 : loss : 0.103251, loss_ce: 0.027711
2021-12-12 23:09:32,604 iteration 702 : loss : 0.107154, loss_ce: 0.033597
2021-12-12 23:09:34,109 iteration 703 : loss : 0.116537, loss_ce: 0.026536
2021-12-12 23:09:35,690 iteration 704 : loss : 0.121817, loss_ce: 0.030143
2021-12-12 23:09:37,240 iteration 705 : loss : 0.122856, loss_ce: 0.044401
2021-12-12 23:09:38,798 iteration 706 : loss : 0.126496, loss_ce: 0.028313
2021-12-12 23:09:40,279 iteration 707 : loss : 0.113888, loss_ce: 0.035922
2021-12-12 23:09:41,795 iteration 708 : loss : 0.111374, loss_ce: 0.031769
2021-12-12 23:09:43,317 iteration 709 : loss : 0.125168, loss_ce: 0.031859
2021-12-12 23:09:44,758 iteration 710 : loss : 0.111195, loss_ce: 0.035402
2021-12-12 23:09:46,305 iteration 711 : loss : 0.116217, loss_ce: 0.036865
2021-12-12 23:09:47,788 iteration 712 : loss : 0.118364, loss_ce: 0.025803
2021-12-12 23:09:49,213 iteration 713 : loss : 0.099118, loss_ce: 0.028721
2021-12-12 23:09:50,649 iteration 714 : loss : 0.109073, loss_ce: 0.030287
 10%|███▏                          | 42/400 [19:47<2:48:28, 28.24s/it]2021-12-12 23:09:52,251 iteration 715 : loss : 0.117990, loss_ce: 0.040048
2021-12-12 23:09:53,727 iteration 716 : loss : 0.111114, loss_ce: 0.032381
2021-12-12 23:09:55,202 iteration 717 : loss : 0.113328, loss_ce: 0.029674
2021-12-12 23:09:56,689 iteration 718 : loss : 0.112610, loss_ce: 0.033306
2021-12-12 23:09:58,266 iteration 719 : loss : 0.127078, loss_ce: 0.037613
2021-12-12 23:09:59,721 iteration 720 : loss : 0.120520, loss_ce: 0.029032
2021-12-12 23:10:01,261 iteration 721 : loss : 0.104731, loss_ce: 0.026177
2021-12-12 23:10:02,769 iteration 722 : loss : 0.106625, loss_ce: 0.029610
2021-12-12 23:10:04,223 iteration 723 : loss : 0.115307, loss_ce: 0.030224
2021-12-12 23:10:05,829 iteration 724 : loss : 0.114827, loss_ce: 0.036335
2021-12-12 23:10:07,262 iteration 725 : loss : 0.100804, loss_ce: 0.028429
2021-12-12 23:10:08,768 iteration 726 : loss : 0.125637, loss_ce: 0.030255
2021-12-12 23:10:10,315 iteration 727 : loss : 0.109596, loss_ce: 0.028081
2021-12-12 23:10:11,993 iteration 728 : loss : 0.129423, loss_ce: 0.044903
2021-12-12 23:10:13,381 iteration 729 : loss : 0.104431, loss_ce: 0.032005
2021-12-12 23:10:14,886 iteration 730 : loss : 0.111258, loss_ce: 0.036119
2021-12-12 23:10:16,390 iteration 731 : loss : 0.121217, loss_ce: 0.034689
 11%|███▏                          | 43/400 [20:13<2:43:32, 27.49s/it]2021-12-12 23:10:18,041 iteration 732 : loss : 0.118501, loss_ce: 0.037633
2021-12-12 23:10:19,563 iteration 733 : loss : 0.114838, loss_ce: 0.034659
2021-12-12 23:10:21,003 iteration 734 : loss : 0.110093, loss_ce: 0.031487
2021-12-12 23:10:22,499 iteration 735 : loss : 0.114235, loss_ce: 0.033209
2021-12-12 23:10:24,007 iteration 736 : loss : 0.118879, loss_ce: 0.033759
2021-12-12 23:10:25,562 iteration 737 : loss : 0.108423, loss_ce: 0.028474
2021-12-12 23:10:27,067 iteration 738 : loss : 0.111598, loss_ce: 0.035738
2021-12-12 23:10:28,546 iteration 739 : loss : 0.125836, loss_ce: 0.039009
2021-12-12 23:10:30,027 iteration 740 : loss : 0.114290, loss_ce: 0.039661
2021-12-12 23:10:31,529 iteration 741 : loss : 0.104254, loss_ce: 0.028967
2021-12-12 23:10:33,098 iteration 742 : loss : 0.115006, loss_ce: 0.030230
2021-12-12 23:10:34,564 iteration 743 : loss : 0.128150, loss_ce: 0.036797
2021-12-12 23:10:36,035 iteration 744 : loss : 0.106822, loss_ce: 0.026774
2021-12-12 23:10:37,488 iteration 745 : loss : 0.097841, loss_ce: 0.028921
2021-12-12 23:10:38,968 iteration 746 : loss : 0.113439, loss_ce: 0.038414
2021-12-12 23:10:40,486 iteration 747 : loss : 0.110857, loss_ce: 0.028508
2021-12-12 23:10:41,886 iteration 748 : loss : 0.102645, loss_ce: 0.030313
 11%|███▎                          | 44/400 [20:39<2:39:33, 26.89s/it]2021-12-12 23:10:43,421 iteration 749 : loss : 0.110413, loss_ce: 0.032703
2021-12-12 23:10:44,956 iteration 750 : loss : 0.104959, loss_ce: 0.031202
2021-12-12 23:10:46,380 iteration 751 : loss : 0.114894, loss_ce: 0.032519
2021-12-12 23:10:47,877 iteration 752 : loss : 0.107901, loss_ce: 0.029657
2021-12-12 23:10:49,462 iteration 753 : loss : 0.121653, loss_ce: 0.035595
2021-12-12 23:10:50,930 iteration 754 : loss : 0.124952, loss_ce: 0.031848
2021-12-12 23:10:52,429 iteration 755 : loss : 0.102863, loss_ce: 0.028347
2021-12-12 23:10:53,867 iteration 756 : loss : 0.107246, loss_ce: 0.031922
2021-12-12 23:10:55,325 iteration 757 : loss : 0.113665, loss_ce: 0.031492
2021-12-12 23:10:56,827 iteration 758 : loss : 0.101039, loss_ce: 0.028734
2021-12-12 23:10:58,238 iteration 759 : loss : 0.102635, loss_ce: 0.027081
2021-12-12 23:10:59,757 iteration 760 : loss : 0.130338, loss_ce: 0.032694
2021-12-12 23:11:01,301 iteration 761 : loss : 0.116696, loss_ce: 0.028066
2021-12-12 23:11:02,841 iteration 762 : loss : 0.118032, loss_ce: 0.039257
2021-12-12 23:11:04,330 iteration 763 : loss : 0.105470, loss_ce: 0.031704
2021-12-12 23:11:05,774 iteration 764 : loss : 0.106253, loss_ce: 0.028452
2021-12-12 23:11:05,775 Training Data Eval:
2021-12-12 23:11:13,440   Average segmentation loss on training set: 0.0993
2021-12-12 23:11:13,440 Validation Data Eval:
2021-12-12 23:11:16,047   Average segmentation loss on validation set: 0.1247
2021-12-12 23:11:22,371 Found new lowest validation loss at iteration 764! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/TU_3seeds/TU_SGD_best_val_loss_seed100.pth
2021-12-12 23:11:23,783 iteration 765 : loss : 0.106289, loss_ce: 0.029967
 11%|███▍                          | 45/400 [21:21<3:05:44, 31.39s/it]2021-12-12 23:11:25,180 iteration 766 : loss : 0.110664, loss_ce: 0.026864
2021-12-12 23:11:26,609 iteration 767 : loss : 0.107622, loss_ce: 0.026220
2021-12-12 23:11:27,933 iteration 768 : loss : 0.112899, loss_ce: 0.033053
2021-12-12 23:11:29,425 iteration 769 : loss : 0.132524, loss_ce: 0.027080
2021-12-12 23:11:30,860 iteration 770 : loss : 0.099807, loss_ce: 0.027496
2021-12-12 23:11:32,421 iteration 771 : loss : 0.111017, loss_ce: 0.025674
2021-12-12 23:11:33,984 iteration 772 : loss : 0.125125, loss_ce: 0.040864
2021-12-12 23:11:35,497 iteration 773 : loss : 0.115962, loss_ce: 0.033748
2021-12-12 23:11:36,994 iteration 774 : loss : 0.112526, loss_ce: 0.035776
2021-12-12 23:11:38,476 iteration 775 : loss : 0.112525, loss_ce: 0.031429
2021-12-12 23:11:39,876 iteration 776 : loss : 0.110718, loss_ce: 0.035467
2021-12-12 23:11:41,428 iteration 777 : loss : 0.108362, loss_ce: 0.029941
2021-12-12 23:11:42,930 iteration 778 : loss : 0.103745, loss_ce: 0.030867
2021-12-12 23:11:44,432 iteration 779 : loss : 0.113896, loss_ce: 0.039919
2021-12-12 23:11:45,867 iteration 780 : loss : 0.100371, loss_ce: 0.025099
2021-12-12 23:11:47,415 iteration 781 : loss : 0.111662, loss_ce: 0.035709
2021-12-12 23:11:48,951 iteration 782 : loss : 0.114365, loss_ce: 0.033347
 12%|███▍                          | 46/400 [21:46<2:54:11, 29.52s/it]2021-12-12 23:11:50,536 iteration 783 : loss : 0.105804, loss_ce: 0.032688
2021-12-12 23:11:52,035 iteration 784 : loss : 0.118907, loss_ce: 0.031642
2021-12-12 23:11:53,579 iteration 785 : loss : 0.111362, loss_ce: 0.035858
2021-12-12 23:11:55,068 iteration 786 : loss : 0.119908, loss_ce: 0.033004
2021-12-12 23:11:56,571 iteration 787 : loss : 0.106386, loss_ce: 0.033744
2021-12-12 23:11:58,096 iteration 788 : loss : 0.108107, loss_ce: 0.026882
2021-12-12 23:11:59,721 iteration 789 : loss : 0.128253, loss_ce: 0.054421
2021-12-12 23:12:01,277 iteration 790 : loss : 0.107219, loss_ce: 0.027747
2021-12-12 23:12:02,749 iteration 791 : loss : 0.113078, loss_ce: 0.032176
2021-12-12 23:12:04,277 iteration 792 : loss : 0.125236, loss_ce: 0.038211
2021-12-12 23:12:05,678 iteration 793 : loss : 0.111744, loss_ce: 0.029772
2021-12-12 23:12:07,158 iteration 794 : loss : 0.106451, loss_ce: 0.031359
2021-12-12 23:12:08,649 iteration 795 : loss : 0.109837, loss_ce: 0.035777
2021-12-12 23:12:10,141 iteration 796 : loss : 0.104782, loss_ce: 0.026647
2021-12-12 23:12:11,587 iteration 797 : loss : 0.108509, loss_ce: 0.034616
2021-12-12 23:12:13,041 iteration 798 : loss : 0.118367, loss_ce: 0.028106
2021-12-12 23:12:14,567 iteration 799 : loss : 0.102674, loss_ce: 0.025699
 12%|███▌                          | 47/400 [22:11<2:46:48, 28.35s/it]2021-12-12 23:12:16,152 iteration 800 : loss : 0.119844, loss_ce: 0.024534
2021-12-12 23:12:17,610 iteration 801 : loss : 0.111020, loss_ce: 0.027328
2021-12-12 23:12:19,094 iteration 802 : loss : 0.101380, loss_ce: 0.029435
2021-12-12 23:12:20,669 iteration 803 : loss : 0.134707, loss_ce: 0.057472
2021-12-12 23:12:22,097 iteration 804 : loss : 0.104680, loss_ce: 0.032452
2021-12-12 23:12:23,518 iteration 805 : loss : 0.117937, loss_ce: 0.026588
2021-12-12 23:12:25,009 iteration 806 : loss : 0.116846, loss_ce: 0.033600
2021-12-12 23:12:26,592 iteration 807 : loss : 0.113821, loss_ce: 0.034692
2021-12-12 23:12:28,129 iteration 808 : loss : 0.111345, loss_ce: 0.030692
2021-12-12 23:12:29,611 iteration 809 : loss : 0.122352, loss_ce: 0.028593
2021-12-12 23:12:31,156 iteration 810 : loss : 0.118554, loss_ce: 0.032264
2021-12-12 23:12:32,615 iteration 811 : loss : 0.096514, loss_ce: 0.027991
2021-12-12 23:12:34,180 iteration 812 : loss : 0.114530, loss_ce: 0.032185
2021-12-12 23:12:35,663 iteration 813 : loss : 0.108783, loss_ce: 0.033774
2021-12-12 23:12:37,171 iteration 814 : loss : 0.120356, loss_ce: 0.041485
2021-12-12 23:12:38,713 iteration 815 : loss : 0.116179, loss_ce: 0.030788
2021-12-12 23:12:40,080 iteration 816 : loss : 0.098798, loss_ce: 0.027408
 12%|███▌                          | 48/400 [22:37<2:41:20, 27.50s/it]2021-12-12 23:12:41,573 iteration 817 : loss : 0.102861, loss_ce: 0.027020
2021-12-12 23:12:43,091 iteration 818 : loss : 0.107675, loss_ce: 0.032228
2021-12-12 23:12:44,574 iteration 819 : loss : 0.106417, loss_ce: 0.029664
2021-12-12 23:12:46,172 iteration 820 : loss : 0.112033, loss_ce: 0.031387
2021-12-12 23:12:47,573 iteration 821 : loss : 0.105003, loss_ce: 0.028693
2021-12-12 23:12:49,095 iteration 822 : loss : 0.104472, loss_ce: 0.029757
2021-12-12 23:12:50,580 iteration 823 : loss : 0.101159, loss_ce: 0.026236
2021-12-12 23:12:52,044 iteration 824 : loss : 0.110301, loss_ce: 0.031413
2021-12-12 23:12:53,530 iteration 825 : loss : 0.108903, loss_ce: 0.027850
2021-12-12 23:12:55,048 iteration 826 : loss : 0.110435, loss_ce: 0.037475
2021-12-12 23:12:56,526 iteration 827 : loss : 0.116760, loss_ce: 0.022908
2021-12-12 23:12:58,065 iteration 828 : loss : 0.129153, loss_ce: 0.041358
2021-12-12 23:12:59,550 iteration 829 : loss : 0.105069, loss_ce: 0.029272
2021-12-12 23:13:01,146 iteration 830 : loss : 0.109848, loss_ce: 0.027394
2021-12-12 23:13:02,656 iteration 831 : loss : 0.103106, loss_ce: 0.029624
2021-12-12 23:13:04,126 iteration 832 : loss : 0.098529, loss_ce: 0.026723
2021-12-12 23:13:05,571 iteration 833 : loss : 0.109211, loss_ce: 0.036414
 12%|███▋                          | 49/400 [23:02<2:37:20, 26.90s/it]2021-12-12 23:13:07,052 iteration 834 : loss : 0.128966, loss_ce: 0.032368
2021-12-12 23:13:08,573 iteration 835 : loss : 0.107906, loss_ce: 0.033047
2021-12-12 23:13:10,085 iteration 836 : loss : 0.107415, loss_ce: 0.031802
2021-12-12 23:13:11,561 iteration 837 : loss : 0.114283, loss_ce: 0.037174
2021-12-12 23:13:13,024 iteration 838 : loss : 0.112795, loss_ce: 0.033434
2021-12-12 23:13:14,588 iteration 839 : loss : 0.110240, loss_ce: 0.028298
2021-12-12 23:13:16,104 iteration 840 : loss : 0.128492, loss_ce: 0.039109
2021-12-12 23:13:17,615 iteration 841 : loss : 0.109666, loss_ce: 0.039934
2021-12-12 23:13:19,241 iteration 842 : loss : 0.121064, loss_ce: 0.035339
2021-12-12 23:13:20,708 iteration 843 : loss : 0.100124, loss_ce: 0.025358
2021-12-12 23:13:22,248 iteration 844 : loss : 0.111831, loss_ce: 0.031542
2021-12-12 23:13:23,703 iteration 845 : loss : 0.105475, loss_ce: 0.031011
2021-12-12 23:13:25,198 iteration 846 : loss : 0.101532, loss_ce: 0.028129
2021-12-12 23:13:26,734 iteration 847 : loss : 0.113467, loss_ce: 0.036785
2021-12-12 23:13:28,137 iteration 848 : loss : 0.099057, loss_ce: 0.027794
2021-12-12 23:13:29,655 iteration 849 : loss : 0.104573, loss_ce: 0.029906
2021-12-12 23:13:29,655 Training Data Eval:
2021-12-12 23:13:37,306   Average segmentation loss on training set: 0.1037
2021-12-12 23:13:37,307 Validation Data Eval:
2021-12-12 23:13:39,911   Average segmentation loss on validation set: 0.1282
2021-12-12 23:13:41,467 iteration 850 : loss : 0.110068, loss_ce: 0.027959
 12%|███▊                          | 50/400 [23:38<2:52:38, 29.60s/it]2021-12-12 23:13:43,029 iteration 851 : loss : 0.102618, loss_ce: 0.033546
2021-12-12 23:13:44,562 iteration 852 : loss : 0.110033, loss_ce: 0.029074
2021-12-12 23:13:46,102 iteration 853 : loss : 0.123215, loss_ce: 0.036296
2021-12-12 23:13:47,539 iteration 854 : loss : 0.098042, loss_ce: 0.025685
2021-12-12 23:13:48,934 iteration 855 : loss : 0.101140, loss_ce: 0.026497
2021-12-12 23:13:50,377 iteration 856 : loss : 0.101966, loss_ce: 0.027182
2021-12-12 23:13:51,953 iteration 857 : loss : 0.104963, loss_ce: 0.030185
2021-12-12 23:13:53,440 iteration 858 : loss : 0.106977, loss_ce: 0.030803
2021-12-12 23:13:55,024 iteration 859 : loss : 0.106337, loss_ce: 0.029101
2021-12-12 23:13:56,566 iteration 860 : loss : 0.113065, loss_ce: 0.029909
2021-12-12 23:13:58,068 iteration 861 : loss : 0.115843, loss_ce: 0.034138
2021-12-12 23:13:59,561 iteration 862 : loss : 0.100066, loss_ce: 0.020240
2021-12-12 23:14:00,972 iteration 863 : loss : 0.104287, loss_ce: 0.034512
2021-12-12 23:14:02,451 iteration 864 : loss : 0.096986, loss_ce: 0.027500
2021-12-12 23:14:03,867 iteration 865 : loss : 0.106682, loss_ce: 0.034483
2021-12-12 23:14:05,299 iteration 866 : loss : 0.096109, loss_ce: 0.022096
2021-12-12 23:14:06,747 iteration 867 : loss : 0.100783, loss_ce: 0.030082
 13%|███▊                          | 51/400 [24:04<2:44:37, 28.30s/it]2021-12-12 23:14:08,303 iteration 868 : loss : 0.099196, loss_ce: 0.025005
2021-12-12 23:14:09,760 iteration 869 : loss : 0.116383, loss_ce: 0.040746
2021-12-12 23:14:11,275 iteration 870 : loss : 0.105049, loss_ce: 0.021195
2021-12-12 23:14:12,786 iteration 871 : loss : 0.104206, loss_ce: 0.023361
2021-12-12 23:14:14,207 iteration 872 : loss : 0.096166, loss_ce: 0.026426
2021-12-12 23:14:15,638 iteration 873 : loss : 0.092806, loss_ce: 0.021827
2021-12-12 23:14:17,148 iteration 874 : loss : 0.111796, loss_ce: 0.036710
2021-12-12 23:14:18,651 iteration 875 : loss : 0.106485, loss_ce: 0.035573
2021-12-12 23:14:20,131 iteration 876 : loss : 0.111136, loss_ce: 0.036065
2021-12-12 23:14:21,557 iteration 877 : loss : 0.095604, loss_ce: 0.024705
2021-12-12 23:14:22,994 iteration 878 : loss : 0.100366, loss_ce: 0.026598
2021-12-12 23:14:24,488 iteration 879 : loss : 0.107401, loss_ce: 0.028536
2021-12-12 23:14:25,932 iteration 880 : loss : 0.099348, loss_ce: 0.027808
2021-12-12 23:14:27,460 iteration 881 : loss : 0.112399, loss_ce: 0.032671
2021-12-12 23:14:28,920 iteration 882 : loss : 0.106026, loss_ce: 0.032276
2021-12-12 23:14:30,384 iteration 883 : loss : 0.114675, loss_ce: 0.027640
2021-12-12 23:14:32,000 iteration 884 : loss : 0.112431, loss_ce: 0.031133
 13%|███▉                          | 52/400 [24:29<2:38:50, 27.39s/it]2021-12-12 23:14:33,603 iteration 885 : loss : 0.114023, loss_ce: 0.038657
2021-12-12 23:14:35,076 iteration 886 : loss : 0.110851, loss_ce: 0.032198
2021-12-12 23:14:36,547 iteration 887 : loss : 0.112569, loss_ce: 0.035556
2021-12-12 23:14:38,043 iteration 888 : loss : 0.107748, loss_ce: 0.030032
2021-12-12 23:14:39,496 iteration 889 : loss : 0.110229, loss_ce: 0.028638
2021-12-12 23:14:41,041 iteration 890 : loss : 0.107920, loss_ce: 0.032727
2021-12-12 23:14:42,554 iteration 891 : loss : 0.111430, loss_ce: 0.040677
2021-12-12 23:14:44,052 iteration 892 : loss : 0.119734, loss_ce: 0.025595
2021-12-12 23:14:45,626 iteration 893 : loss : 0.105787, loss_ce: 0.030374
2021-12-12 23:14:47,125 iteration 894 : loss : 0.105982, loss_ce: 0.030825
2021-12-12 23:14:48,641 iteration 895 : loss : 0.104549, loss_ce: 0.026418
2021-12-12 23:14:50,193 iteration 896 : loss : 0.127134, loss_ce: 0.028307
2021-12-12 23:14:51,615 iteration 897 : loss : 0.099228, loss_ce: 0.025651
2021-12-12 23:14:53,222 iteration 898 : loss : 0.108244, loss_ce: 0.030053
2021-12-12 23:14:54,712 iteration 899 : loss : 0.098996, loss_ce: 0.026929
2021-12-12 23:14:56,236 iteration 900 : loss : 0.106194, loss_ce: 0.030928
2021-12-12 23:14:57,715 iteration 901 : loss : 0.111246, loss_ce: 0.042091
 13%|███▉                          | 53/400 [24:54<2:35:29, 26.89s/it]2021-12-12 23:14:59,254 iteration 902 : loss : 0.107096, loss_ce: 0.031891
2021-12-12 23:15:00,779 iteration 903 : loss : 0.098521, loss_ce: 0.026531
2021-12-12 23:15:02,246 iteration 904 : loss : 0.108655, loss_ce: 0.037675
2021-12-12 23:15:03,684 iteration 905 : loss : 0.116676, loss_ce: 0.024699
2021-12-12 23:15:05,124 iteration 906 : loss : 0.099916, loss_ce: 0.028483
2021-12-12 23:15:06,658 iteration 907 : loss : 0.098535, loss_ce: 0.028395
2021-12-12 23:15:08,093 iteration 908 : loss : 0.097155, loss_ce: 0.031127
2021-12-12 23:15:09,613 iteration 909 : loss : 0.113547, loss_ce: 0.037670
2021-12-12 23:15:11,106 iteration 910 : loss : 0.117709, loss_ce: 0.033340
2021-12-12 23:15:12,615 iteration 911 : loss : 0.102958, loss_ce: 0.028480
2021-12-12 23:15:14,017 iteration 912 : loss : 0.112754, loss_ce: 0.030787
2021-12-12 23:15:15,548 iteration 913 : loss : 0.118713, loss_ce: 0.034336
2021-12-12 23:15:17,001 iteration 914 : loss : 0.094906, loss_ce: 0.028019
2021-12-12 23:15:18,519 iteration 915 : loss : 0.101897, loss_ce: 0.030317
2021-12-12 23:15:19,983 iteration 916 : loss : 0.103232, loss_ce: 0.028061
2021-12-12 23:15:21,464 iteration 917 : loss : 0.113139, loss_ce: 0.030125
2021-12-12 23:15:22,946 iteration 918 : loss : 0.116611, loss_ce: 0.029924
 14%|████                          | 54/400 [25:20<2:32:10, 26.39s/it]2021-12-12 23:15:24,545 iteration 919 : loss : 0.098156, loss_ce: 0.026633
2021-12-12 23:15:25,978 iteration 920 : loss : 0.117353, loss_ce: 0.047507
2021-12-12 23:15:27,555 iteration 921 : loss : 0.098768, loss_ce: 0.028035
2021-12-12 23:15:28,941 iteration 922 : loss : 0.101302, loss_ce: 0.030963
2021-12-12 23:15:30,373 iteration 923 : loss : 0.101190, loss_ce: 0.026792
2021-12-12 23:15:31,846 iteration 924 : loss : 0.094292, loss_ce: 0.023471
2021-12-12 23:15:33,273 iteration 925 : loss : 0.098148, loss_ce: 0.024323
2021-12-12 23:15:34,694 iteration 926 : loss : 0.105524, loss_ce: 0.025300
2021-12-12 23:15:36,189 iteration 927 : loss : 0.107014, loss_ce: 0.033503
2021-12-12 23:15:37,623 iteration 928 : loss : 0.102109, loss_ce: 0.027265
2021-12-12 23:15:39,261 iteration 929 : loss : 0.114476, loss_ce: 0.036007
2021-12-12 23:15:40,762 iteration 930 : loss : 0.119068, loss_ce: 0.048874
2021-12-12 23:15:42,275 iteration 931 : loss : 0.109524, loss_ce: 0.028369
2021-12-12 23:15:43,780 iteration 932 : loss : 0.118121, loss_ce: 0.031027
2021-12-12 23:15:45,417 iteration 933 : loss : 0.112064, loss_ce: 0.028810
2021-12-12 23:15:46,899 iteration 934 : loss : 0.109307, loss_ce: 0.033604
2021-12-12 23:15:46,900 Training Data Eval:
2021-12-12 23:15:54,561   Average segmentation loss on training set: 0.1006
2021-12-12 23:15:54,562 Validation Data Eval:
2021-12-12 23:15:57,169   Average segmentation loss on validation set: 0.1199
2021-12-12 23:16:03,416 Found new lowest validation loss at iteration 934! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/TU_3seeds/TU_SGD_best_val_loss_seed100.pth
2021-12-12 23:16:04,771 iteration 935 : loss : 0.105440, loss_ce: 0.025035
 14%|████▏                         | 55/400 [26:02<2:58:21, 31.02s/it]2021-12-12 23:16:06,170 iteration 936 : loss : 0.112292, loss_ce: 0.022973
2021-12-12 23:16:07,517 iteration 937 : loss : 0.101978, loss_ce: 0.029095
2021-12-12 23:16:08,936 iteration 938 : loss : 0.102521, loss_ce: 0.035018
2021-12-12 23:16:10,419 iteration 939 : loss : 0.104303, loss_ce: 0.030589
2021-12-12 23:16:11,872 iteration 940 : loss : 0.105182, loss_ce: 0.028178
2021-12-12 23:16:13,382 iteration 941 : loss : 0.111700, loss_ce: 0.031800
2021-12-12 23:16:14,854 iteration 942 : loss : 0.093519, loss_ce: 0.022658
2021-12-12 23:16:16,257 iteration 943 : loss : 0.113522, loss_ce: 0.026426
2021-12-12 23:16:17,706 iteration 944 : loss : 0.093672, loss_ce: 0.023873
2021-12-12 23:16:19,158 iteration 945 : loss : 0.121029, loss_ce: 0.052388
2021-12-12 23:16:20,652 iteration 946 : loss : 0.097620, loss_ce: 0.023887
2021-12-12 23:16:22,256 iteration 947 : loss : 0.110656, loss_ce: 0.035892
2021-12-12 23:16:23,710 iteration 948 : loss : 0.105992, loss_ce: 0.028649
2021-12-12 23:16:25,228 iteration 949 : loss : 0.099177, loss_ce: 0.027368
2021-12-12 23:16:26,755 iteration 950 : loss : 0.105414, loss_ce: 0.022021
2021-12-12 23:16:28,326 iteration 951 : loss : 0.107644, loss_ce: 0.035869
2021-12-12 23:16:29,761 iteration 952 : loss : 0.104663, loss_ce: 0.023162
 14%|████▏                         | 56/400 [26:27<2:47:28, 29.21s/it]2021-12-12 23:16:31,298 iteration 953 : loss : 0.112629, loss_ce: 0.031135
2021-12-12 23:16:32,894 iteration 954 : loss : 0.108001, loss_ce: 0.029964
2021-12-12 23:16:34,373 iteration 955 : loss : 0.097840, loss_ce: 0.029131
2021-12-12 23:16:35,851 iteration 956 : loss : 0.102692, loss_ce: 0.029595
2021-12-12 23:16:37,212 iteration 957 : loss : 0.094793, loss_ce: 0.023504
2021-12-12 23:16:38,665 iteration 958 : loss : 0.101511, loss_ce: 0.023697
2021-12-12 23:16:40,091 iteration 959 : loss : 0.097811, loss_ce: 0.029686
2021-12-12 23:16:41,516 iteration 960 : loss : 0.096745, loss_ce: 0.024870
2021-12-12 23:16:43,007 iteration 961 : loss : 0.103114, loss_ce: 0.026518
2021-12-12 23:16:44,464 iteration 962 : loss : 0.100396, loss_ce: 0.026060
2021-12-12 23:16:45,991 iteration 963 : loss : 0.115346, loss_ce: 0.032912
2021-12-12 23:16:47,503 iteration 964 : loss : 0.104884, loss_ce: 0.031529
2021-12-12 23:16:48,980 iteration 965 : loss : 0.100203, loss_ce: 0.026801
2021-12-12 23:16:50,553 iteration 966 : loss : 0.125664, loss_ce: 0.042916
2021-12-12 23:16:52,044 iteration 967 : loss : 0.103267, loss_ce: 0.029262
2021-12-12 23:16:53,551 iteration 968 : loss : 0.109650, loss_ce: 0.032978
2021-12-12 23:16:55,097 iteration 969 : loss : 0.098286, loss_ce: 0.024961
 14%|████▎                         | 57/400 [26:52<2:40:20, 28.05s/it]2021-12-12 23:16:56,620 iteration 970 : loss : 0.094590, loss_ce: 0.025598
2021-12-12 23:16:58,199 iteration 971 : loss : 0.105625, loss_ce: 0.027501
2021-12-12 23:16:59,635 iteration 972 : loss : 0.099623, loss_ce: 0.026566
2021-12-12 23:17:01,067 iteration 973 : loss : 0.099405, loss_ce: 0.028972
2021-12-12 23:17:02,547 iteration 974 : loss : 0.100113, loss_ce: 0.024784
2021-12-12 23:17:03,988 iteration 975 : loss : 0.097072, loss_ce: 0.023936
2021-12-12 23:17:05,441 iteration 976 : loss : 0.109828, loss_ce: 0.027099
2021-12-12 23:17:06,991 iteration 977 : loss : 0.104553, loss_ce: 0.029507
2021-12-12 23:17:08,527 iteration 978 : loss : 0.119546, loss_ce: 0.038225
2021-12-12 23:17:10,061 iteration 979 : loss : 0.103412, loss_ce: 0.034642
2021-12-12 23:17:11,486 iteration 980 : loss : 0.103817, loss_ce: 0.029585
2021-12-12 23:17:13,032 iteration 981 : loss : 0.112731, loss_ce: 0.033454
2021-12-12 23:17:14,451 iteration 982 : loss : 0.112087, loss_ce: 0.042513
2021-12-12 23:17:15,963 iteration 983 : loss : 0.103193, loss_ce: 0.030401
2021-12-12 23:17:17,454 iteration 984 : loss : 0.100969, loss_ce: 0.030479
2021-12-12 23:17:18,935 iteration 985 : loss : 0.100947, loss_ce: 0.023870
2021-12-12 23:17:20,409 iteration 986 : loss : 0.106893, loss_ce: 0.025382
 14%|████▎                         | 58/400 [27:17<2:35:11, 27.23s/it]2021-12-12 23:17:21,947 iteration 987 : loss : 0.097054, loss_ce: 0.028791
2021-12-12 23:17:23,514 iteration 988 : loss : 0.106928, loss_ce: 0.030786
2021-12-12 23:17:24,966 iteration 989 : loss : 0.096950, loss_ce: 0.025241
2021-12-12 23:17:26,521 iteration 990 : loss : 0.115271, loss_ce: 0.045220
2021-12-12 23:17:28,063 iteration 991 : loss : 0.109400, loss_ce: 0.032529
2021-12-12 23:17:29,502 iteration 992 : loss : 0.107991, loss_ce: 0.025599
2021-12-12 23:17:31,120 iteration 993 : loss : 0.096635, loss_ce: 0.026495
2021-12-12 23:17:32,667 iteration 994 : loss : 0.120210, loss_ce: 0.032121
2021-12-12 23:17:34,182 iteration 995 : loss : 0.102167, loss_ce: 0.028829
2021-12-12 23:17:35,650 iteration 996 : loss : 0.102257, loss_ce: 0.028036
2021-12-12 23:17:37,177 iteration 997 : loss : 0.108249, loss_ce: 0.036876
2021-12-12 23:17:38,675 iteration 998 : loss : 0.108592, loss_ce: 0.031146
2021-12-12 23:17:40,261 iteration 999 : loss : 0.107721, loss_ce: 0.031795
2021-12-12 23:17:41,785 iteration 1000 : loss : 0.106704, loss_ce: 0.035065
2021-12-12 23:17:43,307 iteration 1001 : loss : 0.103938, loss_ce: 0.029468
2021-12-12 23:17:44,740 iteration 1002 : loss : 0.091876, loss_ce: 0.019623
2021-12-12 23:17:46,198 iteration 1003 : loss : 0.101398, loss_ce: 0.028972
 15%|████▍                         | 59/400 [27:43<2:32:17, 26.80s/it]2021-12-12 23:17:47,712 iteration 1004 : loss : 0.100168, loss_ce: 0.028755
2021-12-12 23:17:49,303 iteration 1005 : loss : 0.114652, loss_ce: 0.025526
2021-12-12 23:17:50,807 iteration 1006 : loss : 0.103426, loss_ce: 0.031056
2021-12-12 23:17:52,331 iteration 1007 : loss : 0.105833, loss_ce: 0.029353
2021-12-12 23:17:53,843 iteration 1008 : loss : 0.108864, loss_ce: 0.028189
2021-12-12 23:17:55,366 iteration 1009 : loss : 0.101635, loss_ce: 0.027617
2021-12-12 23:17:56,827 iteration 1010 : loss : 0.101486, loss_ce: 0.031342
2021-12-12 23:17:58,303 iteration 1011 : loss : 0.095018, loss_ce: 0.025530
2021-12-12 23:17:59,876 iteration 1012 : loss : 0.131407, loss_ce: 0.028884
2021-12-12 23:18:01,342 iteration 1013 : loss : 0.105112, loss_ce: 0.027897
2021-12-12 23:18:02,879 iteration 1014 : loss : 0.093893, loss_ce: 0.022533
2021-12-12 23:18:04,359 iteration 1015 : loss : 0.100958, loss_ce: 0.034578
2021-12-12 23:18:05,768 iteration 1016 : loss : 0.094978, loss_ce: 0.031809
2021-12-12 23:18:07,366 iteration 1017 : loss : 0.104712, loss_ce: 0.026336
2021-12-12 23:18:08,923 iteration 1018 : loss : 0.101012, loss_ce: 0.030644
2021-12-12 23:18:10,494 iteration 1019 : loss : 0.110459, loss_ce: 0.035494
2021-12-12 23:18:10,494 Training Data Eval:
2021-12-12 23:18:18,149   Average segmentation loss on training set: 0.0903
2021-12-12 23:18:18,149 Validation Data Eval:
2021-12-12 23:18:20,761   Average segmentation loss on validation set: 0.1225
2021-12-12 23:18:22,304 iteration 1020 : loss : 0.097580, loss_ce: 0.024042
 15%|████▌                         | 60/400 [28:19<2:47:41, 29.59s/it]2021-12-12 23:18:23,931 iteration 1021 : loss : 0.108575, loss_ce: 0.031738
2021-12-12 23:18:25,417 iteration 1022 : loss : 0.101412, loss_ce: 0.028460
2021-12-12 23:18:26,880 iteration 1023 : loss : 0.108748, loss_ce: 0.025940
2021-12-12 23:18:28,406 iteration 1024 : loss : 0.094972, loss_ce: 0.028189
2021-12-12 23:18:29,874 iteration 1025 : loss : 0.095775, loss_ce: 0.027139
2021-12-12 23:18:31,408 iteration 1026 : loss : 0.106215, loss_ce: 0.028704
2021-12-12 23:18:32,892 iteration 1027 : loss : 0.112760, loss_ce: 0.041668
2021-12-12 23:18:34,337 iteration 1028 : loss : 0.094118, loss_ce: 0.021830
2021-12-12 23:18:35,857 iteration 1029 : loss : 0.098024, loss_ce: 0.021962
2021-12-12 23:18:37,379 iteration 1030 : loss : 0.099478, loss_ce: 0.028161
2021-12-12 23:18:38,954 iteration 1031 : loss : 0.107890, loss_ce: 0.030200
2021-12-12 23:18:40,394 iteration 1032 : loss : 0.097944, loss_ce: 0.028660
2021-12-12 23:18:41,954 iteration 1033 : loss : 0.104756, loss_ce: 0.030756
2021-12-12 23:18:43,530 iteration 1034 : loss : 0.094648, loss_ce: 0.029526
2021-12-12 23:18:45,073 iteration 1035 : loss : 0.099383, loss_ce: 0.028514
2021-12-12 23:18:46,673 iteration 1036 : loss : 0.107217, loss_ce: 0.032528
2021-12-12 23:18:48,052 iteration 1037 : loss : 0.095817, loss_ce: 0.026672
 15%|████▌                         | 61/400 [28:45<2:40:40, 28.44s/it]2021-12-12 23:18:49,531 iteration 1038 : loss : 0.101263, loss_ce: 0.031259
2021-12-12 23:18:51,035 iteration 1039 : loss : 0.095961, loss_ce: 0.027145
2021-12-12 23:18:52,462 iteration 1040 : loss : 0.095332, loss_ce: 0.024832
2021-12-12 23:18:54,040 iteration 1041 : loss : 0.104194, loss_ce: 0.027919
2021-12-12 23:18:55,615 iteration 1042 : loss : 0.094436, loss_ce: 0.023346
2021-12-12 23:18:57,074 iteration 1043 : loss : 0.095478, loss_ce: 0.025629
2021-12-12 23:18:58,564 iteration 1044 : loss : 0.100398, loss_ce: 0.027320
2021-12-12 23:19:00,033 iteration 1045 : loss : 0.106844, loss_ce: 0.040638
2021-12-12 23:19:01,536 iteration 1046 : loss : 0.104068, loss_ce: 0.026650
2021-12-12 23:19:03,125 iteration 1047 : loss : 0.113434, loss_ce: 0.036029
2021-12-12 23:19:04,634 iteration 1048 : loss : 0.109598, loss_ce: 0.032881
2021-12-12 23:19:06,062 iteration 1049 : loss : 0.099138, loss_ce: 0.030158
2021-12-12 23:19:07,514 iteration 1050 : loss : 0.094554, loss_ce: 0.025745
2021-12-12 23:19:09,020 iteration 1051 : loss : 0.098772, loss_ce: 0.027327
2021-12-12 23:19:10,516 iteration 1052 : loss : 0.099101, loss_ce: 0.028205
2021-12-12 23:19:12,131 iteration 1053 : loss : 0.100516, loss_ce: 0.030636
2021-12-12 23:19:13,614 iteration 1054 : loss : 0.099526, loss_ce: 0.023666
 16%|████▋                         | 62/400 [29:10<2:35:19, 27.57s/it]2021-12-12 23:19:15,150 iteration 1055 : loss : 0.101817, loss_ce: 0.028945
2021-12-12 23:19:16,644 iteration 1056 : loss : 0.105772, loss_ce: 0.032702
2021-12-12 23:19:18,178 iteration 1057 : loss : 0.122393, loss_ce: 0.025996
2021-12-12 23:19:19,609 iteration 1058 : loss : 0.092313, loss_ce: 0.028576
2021-12-12 23:19:21,189 iteration 1059 : loss : 0.095963, loss_ce: 0.027127
2021-12-12 23:19:22,666 iteration 1060 : loss : 0.107238, loss_ce: 0.035067
2021-12-12 23:19:24,202 iteration 1061 : loss : 0.093518, loss_ce: 0.024955
2021-12-12 23:19:25,695 iteration 1062 : loss : 0.099209, loss_ce: 0.029072
2021-12-12 23:19:27,276 iteration 1063 : loss : 0.098319, loss_ce: 0.025306
2021-12-12 23:19:28,742 iteration 1064 : loss : 0.095714, loss_ce: 0.029813
2021-12-12 23:19:30,146 iteration 1065 : loss : 0.101253, loss_ce: 0.027016
2021-12-12 23:19:31,659 iteration 1066 : loss : 0.092887, loss_ce: 0.028343
2021-12-12 23:19:33,296 iteration 1067 : loss : 0.106264, loss_ce: 0.027292
2021-12-12 23:19:34,849 iteration 1068 : loss : 0.119782, loss_ce: 0.035102
2021-12-12 23:19:36,289 iteration 1069 : loss : 0.094451, loss_ce: 0.025188
2021-12-12 23:19:37,885 iteration 1070 : loss : 0.095055, loss_ce: 0.022060
2021-12-12 23:19:39,375 iteration 1071 : loss : 0.097436, loss_ce: 0.028857
 16%|████▋                         | 63/400 [29:36<2:31:48, 27.03s/it]2021-12-12 23:19:40,884 iteration 1072 : loss : 0.093261, loss_ce: 0.026590
2021-12-12 23:19:42,481 iteration 1073 : loss : 0.112002, loss_ce: 0.030624
2021-12-12 23:19:43,950 iteration 1074 : loss : 0.090926, loss_ce: 0.027468
2021-12-12 23:19:45,459 iteration 1075 : loss : 0.097506, loss_ce: 0.028191
2021-12-12 23:19:46,915 iteration 1076 : loss : 0.102070, loss_ce: 0.029938
2021-12-12 23:19:48,455 iteration 1077 : loss : 0.115506, loss_ce: 0.021997
2021-12-12 23:19:49,879 iteration 1078 : loss : 0.110653, loss_ce: 0.032477
2021-12-12 23:19:51,373 iteration 1079 : loss : 0.103255, loss_ce: 0.036530
2021-12-12 23:19:52,896 iteration 1080 : loss : 0.098091, loss_ce: 0.027634
2021-12-12 23:19:54,423 iteration 1081 : loss : 0.096526, loss_ce: 0.026725
2021-12-12 23:19:55,927 iteration 1082 : loss : 0.110717, loss_ce: 0.034262
2021-12-12 23:19:57,433 iteration 1083 : loss : 0.100401, loss_ce: 0.026129
2021-12-12 23:19:58,997 iteration 1084 : loss : 0.117198, loss_ce: 0.027432
2021-12-12 23:20:00,479 iteration 1085 : loss : 0.099939, loss_ce: 0.025697
2021-12-12 23:20:02,011 iteration 1086 : loss : 0.099087, loss_ce: 0.026516
2021-12-12 23:20:03,437 iteration 1087 : loss : 0.094234, loss_ce: 0.026347
2021-12-12 23:20:04,907 iteration 1088 : loss : 0.104305, loss_ce: 0.036282
 16%|████▊                         | 64/400 [30:02<2:28:51, 26.58s/it]2021-12-12 23:20:06,334 iteration 1089 : loss : 0.097441, loss_ce: 0.030443
2021-12-12 23:20:07,828 iteration 1090 : loss : 0.102307, loss_ce: 0.026892
2021-12-12 23:20:09,367 iteration 1091 : loss : 0.106412, loss_ce: 0.039694
2021-12-12 23:20:10,877 iteration 1092 : loss : 0.101493, loss_ce: 0.023014
2021-12-12 23:20:12,396 iteration 1093 : loss : 0.105092, loss_ce: 0.022210
2021-12-12 23:20:14,006 iteration 1094 : loss : 0.130093, loss_ce: 0.026597
2021-12-12 23:20:15,432 iteration 1095 : loss : 0.095230, loss_ce: 0.022389
2021-12-12 23:20:16,880 iteration 1096 : loss : 0.097028, loss_ce: 0.030497
2021-12-12 23:20:18,301 iteration 1097 : loss : 0.096221, loss_ce: 0.028066
2021-12-12 23:20:19,859 iteration 1098 : loss : 0.101166, loss_ce: 0.027812
2021-12-12 23:20:21,353 iteration 1099 : loss : 0.091955, loss_ce: 0.021456
2021-12-12 23:20:22,909 iteration 1100 : loss : 0.107329, loss_ce: 0.036685
2021-12-12 23:20:24,477 iteration 1101 : loss : 0.101603, loss_ce: 0.029862
2021-12-12 23:20:25,972 iteration 1102 : loss : 0.109757, loss_ce: 0.034859
2021-12-12 23:20:27,490 iteration 1103 : loss : 0.090433, loss_ce: 0.024335
2021-12-12 23:20:28,968 iteration 1104 : loss : 0.094435, loss_ce: 0.029268
2021-12-12 23:20:28,968 Training Data Eval:
2021-12-12 23:20:36,614   Average segmentation loss on training set: 0.0877
2021-12-12 23:20:36,615 Validation Data Eval:
2021-12-12 23:20:39,221   Average segmentation loss on validation set: 0.1159
2021-12-12 23:20:45,583 Found new lowest validation loss at iteration 1104! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/TU_3seeds/TU_SGD_best_val_loss_seed100.pth
2021-12-12 23:20:46,976 iteration 1105 : loss : 0.093983, loss_ce: 0.027080
 16%|████▉                         | 65/400 [30:44<2:54:21, 31.23s/it]2021-12-12 23:20:48,443 iteration 1106 : loss : 0.095520, loss_ce: 0.024576
2021-12-12 23:20:49,823 iteration 1107 : loss : 0.098324, loss_ce: 0.026044
2021-12-12 23:20:51,173 iteration 1108 : loss : 0.092128, loss_ce: 0.023114
2021-12-12 23:20:52,707 iteration 1109 : loss : 0.113135, loss_ce: 0.031955
2021-12-12 23:20:54,196 iteration 1110 : loss : 0.094433, loss_ce: 0.025375
2021-12-12 23:20:55,776 iteration 1111 : loss : 0.110225, loss_ce: 0.022330
2021-12-12 23:20:57,220 iteration 1112 : loss : 0.091112, loss_ce: 0.027274
2021-12-12 23:20:58,715 iteration 1113 : loss : 0.088001, loss_ce: 0.020801
2021-12-12 23:21:00,222 iteration 1114 : loss : 0.095858, loss_ce: 0.026821
2021-12-12 23:21:01,734 iteration 1115 : loss : 0.091342, loss_ce: 0.025524
2021-12-12 23:21:03,150 iteration 1116 : loss : 0.094054, loss_ce: 0.030855
2021-12-12 23:21:04,589 iteration 1117 : loss : 0.092892, loss_ce: 0.027992
2021-12-12 23:21:06,105 iteration 1118 : loss : 0.100405, loss_ce: 0.032030
2021-12-12 23:21:07,636 iteration 1119 : loss : 0.097118, loss_ce: 0.028635
2021-12-12 23:21:09,139 iteration 1120 : loss : 0.098744, loss_ce: 0.022263
2021-12-12 23:21:10,609 iteration 1121 : loss : 0.097753, loss_ce: 0.029821
2021-12-12 23:21:12,029 iteration 1122 : loss : 0.095231, loss_ce: 0.022372
 16%|████▉                         | 66/400 [31:09<2:43:31, 29.37s/it]2021-12-12 23:21:13,660 iteration 1123 : loss : 0.096095, loss_ce: 0.024928
2021-12-12 23:21:15,152 iteration 1124 : loss : 0.088912, loss_ce: 0.026111
2021-12-12 23:21:16,606 iteration 1125 : loss : 0.084621, loss_ce: 0.017777
2021-12-12 23:21:18,071 iteration 1126 : loss : 0.092538, loss_ce: 0.023599
2021-12-12 23:21:19,536 iteration 1127 : loss : 0.095599, loss_ce: 0.027808
2021-12-12 23:21:21,111 iteration 1128 : loss : 0.089411, loss_ce: 0.025006
2021-12-12 23:21:22,704 iteration 1129 : loss : 0.104460, loss_ce: 0.033372
2021-12-12 23:21:24,140 iteration 1130 : loss : 0.092989, loss_ce: 0.024979
2021-12-12 23:21:25,684 iteration 1131 : loss : 0.105617, loss_ce: 0.033494
2021-12-12 23:21:27,269 iteration 1132 : loss : 0.106240, loss_ce: 0.029223
2021-12-12 23:21:28,837 iteration 1133 : loss : 0.100746, loss_ce: 0.027610
2021-12-12 23:21:30,400 iteration 1134 : loss : 0.110850, loss_ce: 0.025865
2021-12-12 23:21:31,854 iteration 1135 : loss : 0.119262, loss_ce: 0.031194
2021-12-12 23:21:33,326 iteration 1136 : loss : 0.094921, loss_ce: 0.029284
2021-12-12 23:21:34,798 iteration 1137 : loss : 0.101094, loss_ce: 0.034530
2021-12-12 23:21:36,322 iteration 1138 : loss : 0.104647, loss_ce: 0.027696
2021-12-12 23:21:37,757 iteration 1139 : loss : 0.101877, loss_ce: 0.030087
 17%|█████                         | 67/400 [31:35<2:36:57, 28.28s/it]2021-12-12 23:21:39,289 iteration 1140 : loss : 0.100118, loss_ce: 0.029704
2021-12-12 23:21:40,809 iteration 1141 : loss : 0.113424, loss_ce: 0.037987
2021-12-12 23:21:42,436 iteration 1142 : loss : 0.104425, loss_ce: 0.032543
2021-12-12 23:21:43,954 iteration 1143 : loss : 0.094673, loss_ce: 0.028890
2021-12-12 23:21:45,480 iteration 1144 : loss : 0.094487, loss_ce: 0.020954
2021-12-12 23:21:46,986 iteration 1145 : loss : 0.110308, loss_ce: 0.030236
2021-12-12 23:21:48,460 iteration 1146 : loss : 0.089640, loss_ce: 0.024019
2021-12-12 23:21:49,935 iteration 1147 : loss : 0.093583, loss_ce: 0.026954
2021-12-12 23:21:51,394 iteration 1148 : loss : 0.091139, loss_ce: 0.027478
2021-12-12 23:21:52,884 iteration 1149 : loss : 0.093155, loss_ce: 0.031034
2021-12-12 23:21:54,343 iteration 1150 : loss : 0.101122, loss_ce: 0.029572
2021-12-12 23:21:55,813 iteration 1151 : loss : 0.087007, loss_ce: 0.023715
2021-12-12 23:21:57,258 iteration 1152 : loss : 0.099214, loss_ce: 0.030173
2021-12-12 23:21:58,845 iteration 1153 : loss : 0.105856, loss_ce: 0.023940
2021-12-12 23:22:00,379 iteration 1154 : loss : 0.088201, loss_ce: 0.021167
2021-12-12 23:22:01,927 iteration 1155 : loss : 0.100067, loss_ce: 0.025645
2021-12-12 23:22:03,540 iteration 1156 : loss : 0.106115, loss_ce: 0.028260
 17%|█████                         | 68/400 [32:00<2:32:19, 27.53s/it]2021-12-12 23:22:05,107 iteration 1157 : loss : 0.108476, loss_ce: 0.032631
2021-12-12 23:22:06,592 iteration 1158 : loss : 0.092708, loss_ce: 0.024101
2021-12-12 23:22:07,987 iteration 1159 : loss : 0.088448, loss_ce: 0.023774
2021-12-12 23:22:09,446 iteration 1160 : loss : 0.097571, loss_ce: 0.034051
2021-12-12 23:22:10,961 iteration 1161 : loss : 0.098095, loss_ce: 0.026138
2021-12-12 23:22:12,479 iteration 1162 : loss : 0.096824, loss_ce: 0.027342
2021-12-12 23:22:13,975 iteration 1163 : loss : 0.098448, loss_ce: 0.033821
2021-12-12 23:22:15,535 iteration 1164 : loss : 0.103837, loss_ce: 0.031881
2021-12-12 23:22:16,952 iteration 1165 : loss : 0.099476, loss_ce: 0.032316
2021-12-12 23:22:18,398 iteration 1166 : loss : 0.091052, loss_ce: 0.023721
2021-12-12 23:22:19,904 iteration 1167 : loss : 0.112459, loss_ce: 0.037689
2021-12-12 23:22:21,476 iteration 1168 : loss : 0.109474, loss_ce: 0.020677
2021-12-12 23:22:22,914 iteration 1169 : loss : 0.111161, loss_ce: 0.025854
2021-12-12 23:22:24,428 iteration 1170 : loss : 0.096593, loss_ce: 0.022404
2021-12-12 23:22:26,058 iteration 1171 : loss : 0.139536, loss_ce: 0.031436
2021-12-12 23:22:27,487 iteration 1172 : loss : 0.087015, loss_ce: 0.023568
2021-12-12 23:22:28,914 iteration 1173 : loss : 0.084843, loss_ce: 0.022778
 17%|█████▏                        | 69/400 [32:26<2:28:18, 26.89s/it]2021-12-12 23:22:30,484 iteration 1174 : loss : 0.092824, loss_ce: 0.024544
2021-12-12 23:22:32,071 iteration 1175 : loss : 0.103415, loss_ce: 0.030014
2021-12-12 23:22:33,548 iteration 1176 : loss : 0.108529, loss_ce: 0.028464
2021-12-12 23:22:35,018 iteration 1177 : loss : 0.100433, loss_ce: 0.031560
2021-12-12 23:22:36,450 iteration 1178 : loss : 0.096216, loss_ce: 0.027636
2021-12-12 23:22:38,008 iteration 1179 : loss : 0.103189, loss_ce: 0.029907
2021-12-12 23:22:39,535 iteration 1180 : loss : 0.103944, loss_ce: 0.031736
2021-12-12 23:22:40,972 iteration 1181 : loss : 0.096507, loss_ce: 0.022807
2021-12-12 23:22:42,450 iteration 1182 : loss : 0.086195, loss_ce: 0.024359
2021-12-12 23:22:43,938 iteration 1183 : loss : 0.091964, loss_ce: 0.027828
2021-12-12 23:22:45,448 iteration 1184 : loss : 0.099711, loss_ce: 0.028901
2021-12-12 23:22:46,931 iteration 1185 : loss : 0.096332, loss_ce: 0.037899
2021-12-12 23:22:48,444 iteration 1186 : loss : 0.089734, loss_ce: 0.020284
2021-12-12 23:22:49,884 iteration 1187 : loss : 0.091614, loss_ce: 0.030303
2021-12-12 23:22:51,422 iteration 1188 : loss : 0.105362, loss_ce: 0.031841
2021-12-12 23:22:52,963 iteration 1189 : loss : 0.104231, loss_ce: 0.021477
2021-12-12 23:22:52,964 Training Data Eval:
2021-12-12 23:23:00,604   Average segmentation loss on training set: 0.0900
2021-12-12 23:23:00,604 Validation Data Eval:
2021-12-12 23:23:03,205   Average segmentation loss on validation set: 0.1176
2021-12-12 23:23:04,649 iteration 1190 : loss : 0.093018, loss_ce: 0.021632
 18%|█████▎                        | 70/400 [33:01<2:42:27, 29.54s/it]2021-12-12 23:23:06,176 iteration 1191 : loss : 0.093570, loss_ce: 0.026292
2021-12-12 23:23:07,638 iteration 1192 : loss : 0.106612, loss_ce: 0.041937
2021-12-12 23:23:09,037 iteration 1193 : loss : 0.098735, loss_ce: 0.026089
2021-12-12 23:23:10,532 iteration 1194 : loss : 0.093033, loss_ce: 0.024838
2021-12-12 23:23:12,017 iteration 1195 : loss : 0.094030, loss_ce: 0.028501
2021-12-12 23:23:13,526 iteration 1196 : loss : 0.090255, loss_ce: 0.021975
2021-12-12 23:23:15,062 iteration 1197 : loss : 0.102952, loss_ce: 0.028764
2021-12-12 23:23:16,479 iteration 1198 : loss : 0.082241, loss_ce: 0.018294
2021-12-12 23:23:17,955 iteration 1199 : loss : 0.100623, loss_ce: 0.035823
2021-12-12 23:23:19,366 iteration 1200 : loss : 0.086900, loss_ce: 0.021719
2021-12-12 23:23:20,815 iteration 1201 : loss : 0.100977, loss_ce: 0.021297
2021-12-12 23:23:22,362 iteration 1202 : loss : 0.088857, loss_ce: 0.024651
2021-12-12 23:23:23,913 iteration 1203 : loss : 0.100296, loss_ce: 0.032181
2021-12-12 23:23:25,328 iteration 1204 : loss : 0.086863, loss_ce: 0.017395
2021-12-12 23:23:26,851 iteration 1205 : loss : 0.092252, loss_ce: 0.026748
2021-12-12 23:23:28,302 iteration 1206 : loss : 0.097846, loss_ce: 0.025393
2021-12-12 23:23:29,857 iteration 1207 : loss : 0.101322, loss_ce: 0.031363
 18%|█████▎                        | 71/400 [33:27<2:34:50, 28.24s/it]2021-12-12 23:23:31,364 iteration 1208 : loss : 0.091901, loss_ce: 0.026352
2021-12-12 23:23:32,864 iteration 1209 : loss : 0.094649, loss_ce: 0.023634
2021-12-12 23:23:34,286 iteration 1210 : loss : 0.089990, loss_ce: 0.023978
2021-12-12 23:23:35,840 iteration 1211 : loss : 0.095664, loss_ce: 0.031908
2021-12-12 23:23:37,316 iteration 1212 : loss : 0.090350, loss_ce: 0.026289
2021-12-12 23:23:38,703 iteration 1213 : loss : 0.085597, loss_ce: 0.023076
2021-12-12 23:23:40,277 iteration 1214 : loss : 0.099561, loss_ce: 0.030884
2021-12-12 23:23:41,756 iteration 1215 : loss : 0.092839, loss_ce: 0.028346
2021-12-12 23:23:43,235 iteration 1216 : loss : 0.093608, loss_ce: 0.021368
2021-12-12 23:23:44,850 iteration 1217 : loss : 0.100627, loss_ce: 0.030568
2021-12-12 23:23:46,278 iteration 1218 : loss : 0.086042, loss_ce: 0.024741
2021-12-12 23:23:47,922 iteration 1219 : loss : 0.107632, loss_ce: 0.041106
2021-12-12 23:23:49,408 iteration 1220 : loss : 0.094146, loss_ce: 0.028369
2021-12-12 23:23:50,969 iteration 1221 : loss : 0.098470, loss_ce: 0.023075
2021-12-12 23:23:52,429 iteration 1222 : loss : 0.093291, loss_ce: 0.023780
2021-12-12 23:23:53,891 iteration 1223 : loss : 0.097492, loss_ce: 0.025228
2021-12-12 23:23:55,367 iteration 1224 : loss : 0.096924, loss_ce: 0.020604
 18%|█████▍                        | 72/400 [33:52<2:29:53, 27.42s/it]2021-12-12 23:23:56,888 iteration 1225 : loss : 0.087178, loss_ce: 0.020317
2021-12-12 23:23:58,398 iteration 1226 : loss : 0.090660, loss_ce: 0.021007
2021-12-12 23:23:59,836 iteration 1227 : loss : 0.089835, loss_ce: 0.024642
2021-12-12 23:24:01,284 iteration 1228 : loss : 0.099214, loss_ce: 0.026738
2021-12-12 23:24:02,895 iteration 1229 : loss : 0.107237, loss_ce: 0.036396
2021-12-12 23:24:04,433 iteration 1230 : loss : 0.099205, loss_ce: 0.028372
2021-12-12 23:24:05,944 iteration 1231 : loss : 0.089972, loss_ce: 0.026577
2021-12-12 23:24:07,397 iteration 1232 : loss : 0.098174, loss_ce: 0.029534
2021-12-12 23:24:08,945 iteration 1233 : loss : 0.086200, loss_ce: 0.021232
2021-12-12 23:24:10,553 iteration 1234 : loss : 0.105334, loss_ce: 0.031429
2021-12-12 23:24:11,966 iteration 1235 : loss : 0.084301, loss_ce: 0.021304
2021-12-12 23:24:13,502 iteration 1236 : loss : 0.099181, loss_ce: 0.033463
2021-12-12 23:24:14,996 iteration 1237 : loss : 0.096660, loss_ce: 0.022852
2021-12-12 23:24:16,426 iteration 1238 : loss : 0.096974, loss_ce: 0.024099
2021-12-12 23:24:17,897 iteration 1239 : loss : 0.091635, loss_ce: 0.028070
2021-12-12 23:24:19,469 iteration 1240 : loss : 0.094628, loss_ce: 0.029203
2021-12-12 23:24:20,961 iteration 1241 : loss : 0.089023, loss_ce: 0.025384
 18%|█████▍                        | 73/400 [34:18<2:26:28, 26.87s/it]2021-12-12 23:24:22,544 iteration 1242 : loss : 0.096604, loss_ce: 0.033759
2021-12-12 23:24:24,043 iteration 1243 : loss : 0.097545, loss_ce: 0.028008
2021-12-12 23:24:25,642 iteration 1244 : loss : 0.099386, loss_ce: 0.029263
2021-12-12 23:24:27,218 iteration 1245 : loss : 0.094241, loss_ce: 0.024442
2021-12-12 23:24:28,708 iteration 1246 : loss : 0.094445, loss_ce: 0.028928
2021-12-12 23:24:30,384 iteration 1247 : loss : 0.118208, loss_ce: 0.031700
2021-12-12 23:24:31,844 iteration 1248 : loss : 0.092247, loss_ce: 0.023755
2021-12-12 23:24:33,336 iteration 1249 : loss : 0.094643, loss_ce: 0.031088
2021-12-12 23:24:34,839 iteration 1250 : loss : 0.104352, loss_ce: 0.021444
2021-12-12 23:24:36,404 iteration 1251 : loss : 0.099409, loss_ce: 0.030657
2021-12-12 23:24:37,840 iteration 1252 : loss : 0.096142, loss_ce: 0.021407
2021-12-12 23:24:39,322 iteration 1253 : loss : 0.092204, loss_ce: 0.025202
2021-12-12 23:24:40,814 iteration 1254 : loss : 0.096631, loss_ce: 0.026334
2021-12-12 23:24:42,432 iteration 1255 : loss : 0.098050, loss_ce: 0.031035
2021-12-12 23:24:43,919 iteration 1256 : loss : 0.092225, loss_ce: 0.022894
2021-12-12 23:24:45,317 iteration 1257 : loss : 0.084886, loss_ce: 0.022720
2021-12-12 23:24:46,716 iteration 1258 : loss : 0.087322, loss_ce: 0.029562
 18%|█████▌                        | 74/400 [34:43<2:24:11, 26.54s/it]2021-12-12 23:24:48,203 iteration 1259 : loss : 0.096538, loss_ce: 0.024547
2021-12-12 23:24:49,801 iteration 1260 : loss : 0.091865, loss_ce: 0.029188
2021-12-12 23:24:51,278 iteration 1261 : loss : 0.093406, loss_ce: 0.021317
2021-12-12 23:24:52,700 iteration 1262 : loss : 0.086234, loss_ce: 0.023246
2021-12-12 23:24:54,186 iteration 1263 : loss : 0.092877, loss_ce: 0.025089
2021-12-12 23:24:55,694 iteration 1264 : loss : 0.087819, loss_ce: 0.024547
2021-12-12 23:24:57,187 iteration 1265 : loss : 0.087313, loss_ce: 0.019482
2021-12-12 23:24:58,635 iteration 1266 : loss : 0.086929, loss_ce: 0.024496
2021-12-12 23:25:00,074 iteration 1267 : loss : 0.098594, loss_ce: 0.030093
2021-12-12 23:25:01,522 iteration 1268 : loss : 0.084437, loss_ce: 0.022748
2021-12-12 23:25:02,988 iteration 1269 : loss : 0.103195, loss_ce: 0.035876
2021-12-12 23:25:04,475 iteration 1270 : loss : 0.088304, loss_ce: 0.026346
2021-12-12 23:25:06,034 iteration 1271 : loss : 0.107672, loss_ce: 0.034647
2021-12-12 23:25:07,416 iteration 1272 : loss : 0.081758, loss_ce: 0.021761
2021-12-12 23:25:08,858 iteration 1273 : loss : 0.085372, loss_ce: 0.020634
2021-12-12 23:25:10,371 iteration 1274 : loss : 0.093552, loss_ce: 0.028562
2021-12-12 23:25:10,371 Training Data Eval:
2021-12-12 23:25:18,018   Average segmentation loss on training set: 0.0880
2021-12-12 23:25:18,019 Validation Data Eval:
2021-12-12 23:25:20,621   Average segmentation loss on validation set: 0.1169
2021-12-12 23:25:22,057 iteration 1275 : loss : 0.086569, loss_ce: 0.025220
 19%|█████▋                        | 75/400 [35:19<2:38:03, 29.18s/it]2021-12-12 23:25:23,562 iteration 1276 : loss : 0.089792, loss_ce: 0.025023
2021-12-12 23:25:25,172 iteration 1277 : loss : 0.093757, loss_ce: 0.024757
2021-12-12 23:25:26,663 iteration 1278 : loss : 0.098171, loss_ce: 0.025280
2021-12-12 23:25:28,141 iteration 1279 : loss : 0.102216, loss_ce: 0.028116
2021-12-12 23:25:29,659 iteration 1280 : loss : 0.092804, loss_ce: 0.029444
2021-12-12 23:25:31,103 iteration 1281 : loss : 0.097857, loss_ce: 0.030935
2021-12-12 23:25:32,583 iteration 1282 : loss : 0.085799, loss_ce: 0.024329
2021-12-12 23:25:33,987 iteration 1283 : loss : 0.090431, loss_ce: 0.019823
2021-12-12 23:25:35,458 iteration 1284 : loss : 0.084139, loss_ce: 0.022003
2021-12-12 23:25:36,877 iteration 1285 : loss : 0.099940, loss_ce: 0.026614
2021-12-12 23:25:38,415 iteration 1286 : loss : 0.103088, loss_ce: 0.027769
2021-12-12 23:25:39,948 iteration 1287 : loss : 0.090221, loss_ce: 0.027399
2021-12-12 23:25:41,391 iteration 1288 : loss : 0.095959, loss_ce: 0.027819
2021-12-12 23:25:42,874 iteration 1289 : loss : 0.096176, loss_ce: 0.027023
2021-12-12 23:25:44,319 iteration 1290 : loss : 0.106503, loss_ce: 0.030163
2021-12-12 23:25:45,759 iteration 1291 : loss : 0.087185, loss_ce: 0.023516
2021-12-12 23:25:47,191 iteration 1292 : loss : 0.106765, loss_ce: 0.043022
 19%|█████▋                        | 76/400 [35:44<2:31:00, 27.96s/it]2021-12-12 23:25:48,819 iteration 1293 : loss : 0.092623, loss_ce: 0.029183
2021-12-12 23:25:50,228 iteration 1294 : loss : 0.080947, loss_ce: 0.021930
2021-12-12 23:25:51,765 iteration 1295 : loss : 0.093628, loss_ce: 0.023498
2021-12-12 23:25:53,270 iteration 1296 : loss : 0.104996, loss_ce: 0.023508
2021-12-12 23:25:54,637 iteration 1297 : loss : 0.083880, loss_ce: 0.023377
2021-12-12 23:25:56,196 iteration 1298 : loss : 0.104686, loss_ce: 0.030369
2021-12-12 23:25:57,653 iteration 1299 : loss : 0.087785, loss_ce: 0.025992
2021-12-12 23:25:59,226 iteration 1300 : loss : 0.093769, loss_ce: 0.024081
2021-12-12 23:26:00,803 iteration 1301 : loss : 0.095443, loss_ce: 0.023122
2021-12-12 23:26:02,218 iteration 1302 : loss : 0.088095, loss_ce: 0.021703
2021-12-12 23:26:03,756 iteration 1303 : loss : 0.088462, loss_ce: 0.021401
2021-12-12 23:26:05,257 iteration 1304 : loss : 0.092058, loss_ce: 0.030189
2021-12-12 23:26:06,845 iteration 1305 : loss : 0.096165, loss_ce: 0.026564
2021-12-12 23:26:08,359 iteration 1306 : loss : 0.086282, loss_ce: 0.024983
2021-12-12 23:26:09,820 iteration 1307 : loss : 0.088810, loss_ce: 0.024831
2021-12-12 23:26:11,315 iteration 1308 : loss : 0.097456, loss_ce: 0.035388
2021-12-12 23:26:12,775 iteration 1309 : loss : 0.085275, loss_ce: 0.023720
 19%|█████▊                        | 77/400 [36:10<2:26:42, 27.25s/it]2021-12-12 23:26:14,283 iteration 1310 : loss : 0.088725, loss_ce: 0.025010
2021-12-12 23:26:15,887 iteration 1311 : loss : 0.104574, loss_ce: 0.028214
2021-12-12 23:26:17,387 iteration 1312 : loss : 0.097317, loss_ce: 0.023186
2021-12-12 23:26:18,869 iteration 1313 : loss : 0.096447, loss_ce: 0.029066
2021-12-12 23:26:20,423 iteration 1314 : loss : 0.097908, loss_ce: 0.036086
2021-12-12 23:26:22,003 iteration 1315 : loss : 0.106102, loss_ce: 0.040000
2021-12-12 23:26:23,467 iteration 1316 : loss : 0.086022, loss_ce: 0.024529
2021-12-12 23:26:24,894 iteration 1317 : loss : 0.082220, loss_ce: 0.018978
2021-12-12 23:26:26,320 iteration 1318 : loss : 0.088635, loss_ce: 0.024359
2021-12-12 23:26:27,836 iteration 1319 : loss : 0.091597, loss_ce: 0.025139
2021-12-12 23:26:29,258 iteration 1320 : loss : 0.090928, loss_ce: 0.024294
2021-12-12 23:26:30,751 iteration 1321 : loss : 0.089867, loss_ce: 0.029007
2021-12-12 23:26:32,216 iteration 1322 : loss : 0.087983, loss_ce: 0.021189
2021-12-12 23:26:33,766 iteration 1323 : loss : 0.109183, loss_ce: 0.026128
2021-12-12 23:26:35,346 iteration 1324 : loss : 0.086562, loss_ce: 0.026693
2021-12-12 23:26:36,838 iteration 1325 : loss : 0.092800, loss_ce: 0.028027
2021-12-12 23:26:38,336 iteration 1326 : loss : 0.086289, loss_ce: 0.023021
 20%|█████▊                        | 78/400 [36:35<2:23:31, 26.74s/it]2021-12-12 23:26:39,855 iteration 1327 : loss : 0.081674, loss_ce: 0.019340
2021-12-12 23:26:41,363 iteration 1328 : loss : 0.092176, loss_ce: 0.026316
2021-12-12 23:26:42,859 iteration 1329 : loss : 0.105490, loss_ce: 0.032225
2021-12-12 23:26:44,437 iteration 1330 : loss : 0.089121, loss_ce: 0.019555
2021-12-12 23:26:45,934 iteration 1331 : loss : 0.090086, loss_ce: 0.026793
2021-12-12 23:26:47,397 iteration 1332 : loss : 0.093026, loss_ce: 0.026847
2021-12-12 23:26:48,867 iteration 1333 : loss : 0.087359, loss_ce: 0.026164
2021-12-12 23:26:50,406 iteration 1334 : loss : 0.089094, loss_ce: 0.026050
2021-12-12 23:26:51,931 iteration 1335 : loss : 0.084730, loss_ce: 0.024229
2021-12-12 23:26:53,381 iteration 1336 : loss : 0.083802, loss_ce: 0.027115
2021-12-12 23:26:54,934 iteration 1337 : loss : 0.099033, loss_ce: 0.029260
2021-12-12 23:26:56,403 iteration 1338 : loss : 0.081958, loss_ce: 0.023629
2021-12-12 23:26:57,858 iteration 1339 : loss : 0.089067, loss_ce: 0.023132
2021-12-12 23:26:59,305 iteration 1340 : loss : 0.085165, loss_ce: 0.021687
2021-12-12 23:27:00,821 iteration 1341 : loss : 0.090401, loss_ce: 0.026192
2021-12-12 23:27:02,375 iteration 1342 : loss : 0.095897, loss_ce: 0.025018
2021-12-12 23:27:03,777 iteration 1343 : loss : 0.081813, loss_ce: 0.018999
 20%|█████▉                        | 79/400 [37:01<2:20:58, 26.35s/it]2021-12-12 23:27:05,349 iteration 1344 : loss : 0.083572, loss_ce: 0.019231
2021-12-12 23:27:06,805 iteration 1345 : loss : 0.079158, loss_ce: 0.017219
2021-12-12 23:27:08,348 iteration 1346 : loss : 0.095817, loss_ce: 0.026691
2021-12-12 23:27:09,845 iteration 1347 : loss : 0.090478, loss_ce: 0.028462
2021-12-12 23:27:11,299 iteration 1348 : loss : 0.090447, loss_ce: 0.026307
2021-12-12 23:27:12,789 iteration 1349 : loss : 0.088348, loss_ce: 0.024252
2021-12-12 23:27:14,274 iteration 1350 : loss : 0.090828, loss_ce: 0.018659
2021-12-12 23:27:15,855 iteration 1351 : loss : 0.102540, loss_ce: 0.029441
2021-12-12 23:27:17,290 iteration 1352 : loss : 0.088898, loss_ce: 0.027797
2021-12-12 23:27:18,779 iteration 1353 : loss : 0.086227, loss_ce: 0.024782
2021-12-12 23:27:20,279 iteration 1354 : loss : 0.090842, loss_ce: 0.025060
2021-12-12 23:27:21,697 iteration 1355 : loss : 0.084465, loss_ce: 0.020243
2021-12-12 23:27:23,261 iteration 1356 : loss : 0.093136, loss_ce: 0.024260
2021-12-12 23:27:24,683 iteration 1357 : loss : 0.088263, loss_ce: 0.028754
2021-12-12 23:27:26,137 iteration 1358 : loss : 0.094921, loss_ce: 0.030075
2021-12-12 23:27:27,709 iteration 1359 : loss : 0.090012, loss_ce: 0.032482
2021-12-12 23:27:27,709 Training Data Eval:
2021-12-12 23:27:35,340   Average segmentation loss on training set: 0.0817
2021-12-12 23:27:35,340 Validation Data Eval:
2021-12-12 23:27:37,942   Average segmentation loss on validation set: 0.1144
2021-12-12 23:27:44,355 Found new lowest validation loss at iteration 1359! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/TU_3seeds/TU_SGD_best_val_loss_seed100.pth
2021-12-12 23:27:45,769 iteration 1360 : loss : 0.093077, loss_ce: 0.027767
 20%|██████                        | 80/400 [37:43<2:45:33, 31.04s/it]2021-12-12 23:27:47,250 iteration 1361 : loss : 0.093411, loss_ce: 0.022981
2021-12-12 23:27:48,611 iteration 1362 : loss : 0.089814, loss_ce: 0.020094
2021-12-12 23:27:50,005 iteration 1363 : loss : 0.082906, loss_ce: 0.022455
2021-12-12 23:27:51,389 iteration 1364 : loss : 0.088747, loss_ce: 0.030502
2021-12-12 23:27:52,837 iteration 1365 : loss : 0.093013, loss_ce: 0.020195
2021-12-12 23:27:54,405 iteration 1366 : loss : 0.092505, loss_ce: 0.025906
2021-12-12 23:27:55,911 iteration 1367 : loss : 0.090233, loss_ce: 0.032064
2021-12-12 23:27:57,591 iteration 1368 : loss : 0.098767, loss_ce: 0.027341
2021-12-12 23:27:59,075 iteration 1369 : loss : 0.079750, loss_ce: 0.021467
2021-12-12 23:28:00,525 iteration 1370 : loss : 0.090110, loss_ce: 0.025697
2021-12-12 23:28:02,004 iteration 1371 : loss : 0.087372, loss_ce: 0.024124
2021-12-12 23:28:03,646 iteration 1372 : loss : 0.116850, loss_ce: 0.036060
2021-12-12 23:28:05,122 iteration 1373 : loss : 0.092066, loss_ce: 0.029350
2021-12-12 23:28:06,575 iteration 1374 : loss : 0.097927, loss_ce: 0.026730
2021-12-12 23:28:08,040 iteration 1375 : loss : 0.095122, loss_ce: 0.032039
2021-12-12 23:28:09,529 iteration 1376 : loss : 0.079395, loss_ce: 0.018165
2021-12-12 23:28:11,088 iteration 1377 : loss : 0.089812, loss_ce: 0.028188
 20%|██████                        | 81/400 [38:08<2:35:55, 29.33s/it]2021-12-12 23:28:12,730 iteration 1378 : loss : 0.094268, loss_ce: 0.031771
2021-12-12 23:28:14,285 iteration 1379 : loss : 0.097602, loss_ce: 0.030579
2021-12-12 23:28:15,788 iteration 1380 : loss : 0.105622, loss_ce: 0.026404
2021-12-12 23:28:17,344 iteration 1381 : loss : 0.084938, loss_ce: 0.023661
2021-12-12 23:28:18,868 iteration 1382 : loss : 0.094126, loss_ce: 0.027593
2021-12-12 23:28:20,305 iteration 1383 : loss : 0.097425, loss_ce: 0.023909
2021-12-12 23:28:21,842 iteration 1384 : loss : 0.092645, loss_ce: 0.030408
2021-12-12 23:28:23,362 iteration 1385 : loss : 0.086544, loss_ce: 0.026677
2021-12-12 23:28:24,846 iteration 1386 : loss : 0.093991, loss_ce: 0.027310
2021-12-12 23:28:26,455 iteration 1387 : loss : 0.099759, loss_ce: 0.026921
2021-12-12 23:28:27,938 iteration 1388 : loss : 0.085078, loss_ce: 0.023494
2021-12-12 23:28:29,417 iteration 1389 : loss : 0.090006, loss_ce: 0.025325
2021-12-12 23:28:30,941 iteration 1390 : loss : 0.088557, loss_ce: 0.025426
2021-12-12 23:28:32,450 iteration 1391 : loss : 0.088841, loss_ce: 0.018565
2021-12-12 23:28:33,964 iteration 1392 : loss : 0.087460, loss_ce: 0.021164
2021-12-12 23:28:35,436 iteration 1393 : loss : 0.090292, loss_ce: 0.022063
2021-12-12 23:28:36,918 iteration 1394 : loss : 0.085135, loss_ce: 0.026697
 20%|██████▏                       | 82/400 [38:34<2:29:52, 28.28s/it]2021-12-12 23:28:38,456 iteration 1395 : loss : 0.080920, loss_ce: 0.021815
2021-12-12 23:28:39,912 iteration 1396 : loss : 0.080568, loss_ce: 0.022575
2021-12-12 23:28:41,422 iteration 1397 : loss : 0.093006, loss_ce: 0.023923
2021-12-12 23:28:43,011 iteration 1398 : loss : 0.085947, loss_ce: 0.026193
2021-12-12 23:28:44,538 iteration 1399 : loss : 0.100686, loss_ce: 0.034882
2021-12-12 23:28:46,004 iteration 1400 : loss : 0.087090, loss_ce: 0.025618
2021-12-12 23:28:47,469 iteration 1401 : loss : 0.091620, loss_ce: 0.023836
2021-12-12 23:28:48,886 iteration 1402 : loss : 0.083466, loss_ce: 0.022504
2021-12-12 23:28:50,489 iteration 1403 : loss : 0.097548, loss_ce: 0.027304
2021-12-12 23:28:52,036 iteration 1404 : loss : 0.096021, loss_ce: 0.026040
2021-12-12 23:28:53,567 iteration 1405 : loss : 0.091815, loss_ce: 0.028372
2021-12-12 23:28:55,082 iteration 1406 : loss : 0.094794, loss_ce: 0.026448
2021-12-12 23:28:56,521 iteration 1407 : loss : 0.084202, loss_ce: 0.028083
2021-12-12 23:28:57,987 iteration 1408 : loss : 0.106993, loss_ce: 0.028953
2021-12-12 23:28:59,565 iteration 1409 : loss : 0.097763, loss_ce: 0.022270
2021-12-12 23:29:01,095 iteration 1410 : loss : 0.089379, loss_ce: 0.022062
2021-12-12 23:29:02,569 iteration 1411 : loss : 0.084253, loss_ce: 0.025032
 21%|██████▏                       | 83/400 [38:59<2:25:13, 27.49s/it]2021-12-12 23:29:04,009 iteration 1412 : loss : 0.086075, loss_ce: 0.017412
2021-12-12 23:29:05,551 iteration 1413 : loss : 0.081593, loss_ce: 0.022490
2021-12-12 23:29:07,049 iteration 1414 : loss : 0.083159, loss_ce: 0.026146
2021-12-12 23:29:08,472 iteration 1415 : loss : 0.090978, loss_ce: 0.023497
2021-12-12 23:29:09,968 iteration 1416 : loss : 0.079541, loss_ce: 0.017103
2021-12-12 23:29:11,510 iteration 1417 : loss : 0.100263, loss_ce: 0.026763
2021-12-12 23:29:12,931 iteration 1418 : loss : 0.082180, loss_ce: 0.018433
2021-12-12 23:29:14,550 iteration 1419 : loss : 0.089928, loss_ce: 0.020475
2021-12-12 23:29:16,111 iteration 1420 : loss : 0.086636, loss_ce: 0.017706
2021-12-12 23:29:17,568 iteration 1421 : loss : 0.087549, loss_ce: 0.026914
2021-12-12 23:29:19,061 iteration 1422 : loss : 0.092515, loss_ce: 0.026552
2021-12-12 23:29:20,518 iteration 1423 : loss : 0.095925, loss_ce: 0.028403
2021-12-12 23:29:22,051 iteration 1424 : loss : 0.095747, loss_ce: 0.028236
2021-12-12 23:29:23,622 iteration 1425 : loss : 0.093544, loss_ce: 0.030591
2021-12-12 23:29:24,990 iteration 1426 : loss : 0.084143, loss_ce: 0.024575
2021-12-12 23:29:26,453 iteration 1427 : loss : 0.096863, loss_ce: 0.034202
2021-12-12 23:29:27,945 iteration 1428 : loss : 0.090844, loss_ce: 0.022647
 21%|██████▎                       | 84/400 [39:25<2:21:26, 26.85s/it]2021-12-12 23:29:29,581 iteration 1429 : loss : 0.096356, loss_ce: 0.024733
2021-12-12 23:29:31,032 iteration 1430 : loss : 0.088027, loss_ce: 0.021835
2021-12-12 23:29:32,516 iteration 1431 : loss : 0.087962, loss_ce: 0.027415
2021-12-12 23:29:34,023 iteration 1432 : loss : 0.092191, loss_ce: 0.029117
2021-12-12 23:29:35,616 iteration 1433 : loss : 0.097132, loss_ce: 0.030428
2021-12-12 23:29:37,171 iteration 1434 : loss : 0.089993, loss_ce: 0.026688
2021-12-12 23:29:38,560 iteration 1435 : loss : 0.086722, loss_ce: 0.020795
2021-12-12 23:29:40,130 iteration 1436 : loss : 0.099085, loss_ce: 0.033670
2021-12-12 23:29:41,546 iteration 1437 : loss : 0.106044, loss_ce: 0.023585
2021-12-12 23:29:42,939 iteration 1438 : loss : 0.083459, loss_ce: 0.025686
2021-12-12 23:29:44,422 iteration 1439 : loss : 0.075708, loss_ce: 0.017347
2021-12-12 23:29:45,987 iteration 1440 : loss : 0.091125, loss_ce: 0.027751
2021-12-12 23:29:47,446 iteration 1441 : loss : 0.079321, loss_ce: 0.019803
2021-12-12 23:29:48,882 iteration 1442 : loss : 0.086971, loss_ce: 0.023426
2021-12-12 23:29:50,397 iteration 1443 : loss : 0.096217, loss_ce: 0.026879
2021-12-12 23:29:51,951 iteration 1444 : loss : 0.080474, loss_ce: 0.023211
2021-12-12 23:29:51,952 Training Data Eval:
2021-12-12 23:29:59,568   Average segmentation loss on training set: 0.0786
2021-12-12 23:29:59,568 Validation Data Eval:
2021-12-12 23:30:02,164   Average segmentation loss on validation set: 0.1145
2021-12-12 23:30:03,684 iteration 1445 : loss : 0.091534, loss_ce: 0.023323
 21%|██████▍                       | 85/400 [40:00<2:34:59, 29.52s/it]2021-12-12 23:30:05,277 iteration 1446 : loss : 0.080245, loss_ce: 0.020232
2021-12-12 23:30:06,815 iteration 1447 : loss : 0.093268, loss_ce: 0.024128
2021-12-12 23:30:08,289 iteration 1448 : loss : 0.087131, loss_ce: 0.031596
2021-12-12 23:30:09,791 iteration 1449 : loss : 0.090452, loss_ce: 0.025455
2021-12-12 23:30:11,187 iteration 1450 : loss : 0.089082, loss_ce: 0.028074
2021-12-12 23:30:12,617 iteration 1451 : loss : 0.078374, loss_ce: 0.017123
2021-12-12 23:30:14,109 iteration 1452 : loss : 0.097524, loss_ce: 0.018627
2021-12-12 23:30:15,660 iteration 1453 : loss : 0.090085, loss_ce: 0.023491
2021-12-12 23:30:17,112 iteration 1454 : loss : 0.082430, loss_ce: 0.025849
2021-12-12 23:30:18,562 iteration 1455 : loss : 0.079745, loss_ce: 0.021081
2021-12-12 23:30:20,055 iteration 1456 : loss : 0.092482, loss_ce: 0.026189
2021-12-12 23:30:21,551 iteration 1457 : loss : 0.089328, loss_ce: 0.027178
2021-12-12 23:30:23,033 iteration 1458 : loss : 0.085239, loss_ce: 0.021983
2021-12-12 23:30:24,417 iteration 1459 : loss : 0.079104, loss_ce: 0.022240
2021-12-12 23:30:25,958 iteration 1460 : loss : 0.092698, loss_ce: 0.028717
2021-12-12 23:30:27,439 iteration 1461 : loss : 0.087922, loss_ce: 0.021173
2021-12-12 23:30:28,925 iteration 1462 : loss : 0.084552, loss_ce: 0.021554
 22%|██████▍                       | 86/400 [40:26<2:27:46, 28.24s/it]2021-12-12 23:30:30,437 iteration 1463 : loss : 0.087359, loss_ce: 0.026202
2021-12-12 23:30:31,967 iteration 1464 : loss : 0.083099, loss_ce: 0.019537
2021-12-12 23:30:33,529 iteration 1465 : loss : 0.103239, loss_ce: 0.038270
2021-12-12 23:30:35,027 iteration 1466 : loss : 0.099690, loss_ce: 0.028045
2021-12-12 23:30:36,507 iteration 1467 : loss : 0.108280, loss_ce: 0.032085
2021-12-12 23:30:38,022 iteration 1468 : loss : 0.091208, loss_ce: 0.022873
2021-12-12 23:30:39,549 iteration 1469 : loss : 0.077891, loss_ce: 0.020961
2021-12-12 23:30:41,056 iteration 1470 : loss : 0.090760, loss_ce: 0.026487
2021-12-12 23:30:42,476 iteration 1471 : loss : 0.078603, loss_ce: 0.017243
2021-12-12 23:30:43,960 iteration 1472 : loss : 0.080867, loss_ce: 0.019175
2021-12-12 23:30:45,521 iteration 1473 : loss : 0.087165, loss_ce: 0.026588
2021-12-12 23:30:46,957 iteration 1474 : loss : 0.084674, loss_ce: 0.026448
2021-12-12 23:30:48,435 iteration 1475 : loss : 0.087769, loss_ce: 0.023244
2021-12-12 23:30:49,978 iteration 1476 : loss : 0.090392, loss_ce: 0.031586
2021-12-12 23:30:51,463 iteration 1477 : loss : 0.090048, loss_ce: 0.020632
2021-12-12 23:30:53,017 iteration 1478 : loss : 0.085912, loss_ce: 0.023950
2021-12-12 23:30:54,535 iteration 1479 : loss : 0.083333, loss_ce: 0.022634
 22%|██████▌                       | 87/400 [40:51<2:23:11, 27.45s/it]2021-12-12 23:30:56,068 iteration 1480 : loss : 0.087982, loss_ce: 0.034491
2021-12-12 23:30:57,637 iteration 1481 : loss : 0.090744, loss_ce: 0.027347
2021-12-12 23:30:59,104 iteration 1482 : loss : 0.083038, loss_ce: 0.026199
2021-12-12 23:31:00,529 iteration 1483 : loss : 0.082014, loss_ce: 0.023711
2021-12-12 23:31:01,989 iteration 1484 : loss : 0.087278, loss_ce: 0.028163
2021-12-12 23:31:03,434 iteration 1485 : loss : 0.078846, loss_ce: 0.018920
2021-12-12 23:31:04,899 iteration 1486 : loss : 0.086416, loss_ce: 0.021455
2021-12-12 23:31:06,457 iteration 1487 : loss : 0.088778, loss_ce: 0.023999
2021-12-12 23:31:07,924 iteration 1488 : loss : 0.076998, loss_ce: 0.019125
2021-12-12 23:31:09,435 iteration 1489 : loss : 0.080523, loss_ce: 0.019485
2021-12-12 23:31:10,923 iteration 1490 : loss : 0.083904, loss_ce: 0.025230
2021-12-12 23:31:12,346 iteration 1491 : loss : 0.083740, loss_ce: 0.021237
2021-12-12 23:31:13,862 iteration 1492 : loss : 0.081927, loss_ce: 0.020668
2021-12-12 23:31:15,465 iteration 1493 : loss : 0.097665, loss_ce: 0.033417
2021-12-12 23:31:16,993 iteration 1494 : loss : 0.086392, loss_ce: 0.018979
2021-12-12 23:31:18,486 iteration 1495 : loss : 0.085693, loss_ce: 0.022351
2021-12-12 23:31:19,981 iteration 1496 : loss : 0.088193, loss_ce: 0.024162
 22%|██████▌                       | 88/400 [41:17<2:19:36, 26.85s/it]2021-12-12 23:31:21,476 iteration 1497 : loss : 0.081678, loss_ce: 0.023152
2021-12-12 23:31:22,848 iteration 1498 : loss : 0.078707, loss_ce: 0.022893
2021-12-12 23:31:24,268 iteration 1499 : loss : 0.078583, loss_ce: 0.023511
2021-12-12 23:31:25,894 iteration 1500 : loss : 0.091216, loss_ce: 0.025032
2021-12-12 23:31:27,303 iteration 1501 : loss : 0.085720, loss_ce: 0.021255
2021-12-12 23:31:28,801 iteration 1502 : loss : 0.081498, loss_ce: 0.021487
2021-12-12 23:31:30,363 iteration 1503 : loss : 0.087937, loss_ce: 0.020884
2021-12-12 23:31:31,823 iteration 1504 : loss : 0.091660, loss_ce: 0.029672
2021-12-12 23:31:33,261 iteration 1505 : loss : 0.080459, loss_ce: 0.023372
2021-12-12 23:31:34,799 iteration 1506 : loss : 0.091668, loss_ce: 0.024612
2021-12-12 23:31:36,261 iteration 1507 : loss : 0.078408, loss_ce: 0.021403
2021-12-12 23:31:37,760 iteration 1508 : loss : 0.091111, loss_ce: 0.014810
2021-12-12 23:31:39,234 iteration 1509 : loss : 0.087216, loss_ce: 0.026674
2021-12-12 23:31:40,694 iteration 1510 : loss : 0.090245, loss_ce: 0.020935
2021-12-12 23:31:42,284 iteration 1511 : loss : 0.095149, loss_ce: 0.036280
2021-12-12 23:31:43,788 iteration 1512 : loss : 0.095072, loss_ce: 0.024520
2021-12-12 23:31:45,290 iteration 1513 : loss : 0.091762, loss_ce: 0.026715
 22%|██████▋                       | 89/400 [41:42<2:16:45, 26.39s/it]2021-12-12 23:31:46,845 iteration 1514 : loss : 0.097444, loss_ce: 0.028288
2021-12-12 23:31:48,340 iteration 1515 : loss : 0.081701, loss_ce: 0.024034
2021-12-12 23:31:49,870 iteration 1516 : loss : 0.088607, loss_ce: 0.029956
2021-12-12 23:31:51,330 iteration 1517 : loss : 0.086268, loss_ce: 0.030871
2021-12-12 23:31:52,750 iteration 1518 : loss : 0.082823, loss_ce: 0.026331
2021-12-12 23:31:54,223 iteration 1519 : loss : 0.081391, loss_ce: 0.020544
2021-12-12 23:31:55,867 iteration 1520 : loss : 0.101245, loss_ce: 0.031844
2021-12-12 23:31:57,314 iteration 1521 : loss : 0.084457, loss_ce: 0.020770
2021-12-12 23:31:58,874 iteration 1522 : loss : 0.087755, loss_ce: 0.020869
2021-12-12 23:32:00,370 iteration 1523 : loss : 0.092370, loss_ce: 0.025707
2021-12-12 23:32:01,802 iteration 1524 : loss : 0.080796, loss_ce: 0.021638
2021-12-12 23:32:03,273 iteration 1525 : loss : 0.084417, loss_ce: 0.023799
2021-12-12 23:32:04,737 iteration 1526 : loss : 0.081751, loss_ce: 0.018655
2021-12-12 23:32:06,306 iteration 1527 : loss : 0.085719, loss_ce: 0.021624
2021-12-12 23:32:07,836 iteration 1528 : loss : 0.087642, loss_ce: 0.023243
2021-12-12 23:32:09,318 iteration 1529 : loss : 0.083800, loss_ce: 0.016363
2021-12-12 23:32:09,318 Training Data Eval:
2021-12-12 23:32:16,965   Average segmentation loss on training set: 0.0713
2021-12-12 23:32:16,965 Validation Data Eval:
2021-12-12 23:32:19,569   Average segmentation loss on validation set: 0.1104
2021-12-12 23:32:25,948 Found new lowest validation loss at iteration 1529! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/TU_3seeds/TU_SGD_best_val_loss_seed100.pth
2021-12-12 23:32:27,310 iteration 1530 : loss : 0.081376, loss_ce: 0.026319
 22%|██████▊                       | 90/400 [42:24<2:40:34, 31.08s/it]2021-12-12 23:32:28,758 iteration 1531 : loss : 0.090361, loss_ce: 0.029721
2021-12-12 23:32:30,178 iteration 1532 : loss : 0.112075, loss_ce: 0.028995
2021-12-12 23:32:31,504 iteration 1533 : loss : 0.075332, loss_ce: 0.018284
2021-12-12 23:32:32,919 iteration 1534 : loss : 0.084983, loss_ce: 0.027750
2021-12-12 23:32:34,356 iteration 1535 : loss : 0.082741, loss_ce: 0.025875
2021-12-12 23:32:35,860 iteration 1536 : loss : 0.085751, loss_ce: 0.020903
2021-12-12 23:32:37,315 iteration 1537 : loss : 0.085709, loss_ce: 0.029658
2021-12-12 23:32:38,683 iteration 1538 : loss : 0.090733, loss_ce: 0.017129
2021-12-12 23:32:40,103 iteration 1539 : loss : 0.082955, loss_ce: 0.024943
2021-12-12 23:32:41,592 iteration 1540 : loss : 0.087293, loss_ce: 0.029491
2021-12-12 23:32:43,133 iteration 1541 : loss : 0.087096, loss_ce: 0.026107
2021-12-12 23:32:44,727 iteration 1542 : loss : 0.088129, loss_ce: 0.024921
2021-12-12 23:32:46,227 iteration 1543 : loss : 0.084293, loss_ce: 0.017928
2021-12-12 23:32:47,736 iteration 1544 : loss : 0.080402, loss_ce: 0.024835
2021-12-12 23:32:49,157 iteration 1545 : loss : 0.080078, loss_ce: 0.021673
2021-12-12 23:32:50,618 iteration 1546 : loss : 0.084664, loss_ce: 0.019046
2021-12-12 23:32:52,130 iteration 1547 : loss : 0.080554, loss_ce: 0.025488
 23%|██████▊                       | 91/400 [42:49<2:30:22, 29.20s/it]2021-12-12 23:32:53,643 iteration 1548 : loss : 0.090076, loss_ce: 0.021830
2021-12-12 23:32:55,060 iteration 1549 : loss : 0.088533, loss_ce: 0.017626
2021-12-12 23:32:56,582 iteration 1550 : loss : 0.082309, loss_ce: 0.018933
2021-12-12 23:32:58,087 iteration 1551 : loss : 0.086279, loss_ce: 0.023026
2021-12-12 23:32:59,552 iteration 1552 : loss : 0.078197, loss_ce: 0.014784
2021-12-12 23:33:01,079 iteration 1553 : loss : 0.088213, loss_ce: 0.027494
2021-12-12 23:33:02,470 iteration 1554 : loss : 0.086533, loss_ce: 0.031488
2021-12-12 23:33:03,917 iteration 1555 : loss : 0.090735, loss_ce: 0.024632
2021-12-12 23:33:05,505 iteration 1556 : loss : 0.084798, loss_ce: 0.024076
2021-12-12 23:33:06,978 iteration 1557 : loss : 0.091830, loss_ce: 0.023452
2021-12-12 23:33:08,393 iteration 1558 : loss : 0.086387, loss_ce: 0.021585
2021-12-12 23:33:09,960 iteration 1559 : loss : 0.095040, loss_ce: 0.027473
2021-12-12 23:33:11,414 iteration 1560 : loss : 0.092675, loss_ce: 0.034649
2021-12-12 23:33:12,900 iteration 1561 : loss : 0.087513, loss_ce: 0.026920
2021-12-12 23:33:14,386 iteration 1562 : loss : 0.092862, loss_ce: 0.036023
2021-12-12 23:33:15,901 iteration 1563 : loss : 0.082244, loss_ce: 0.022515
2021-12-12 23:33:17,367 iteration 1564 : loss : 0.081731, loss_ce: 0.022365
 23%|██████▉                       | 92/400 [43:14<2:23:47, 28.01s/it]2021-12-12 23:33:18,845 iteration 1565 : loss : 0.077676, loss_ce: 0.025301
2021-12-12 23:33:20,373 iteration 1566 : loss : 0.089028, loss_ce: 0.021599
2021-12-12 23:33:21,869 iteration 1567 : loss : 0.086040, loss_ce: 0.025782
2021-12-12 23:33:23,469 iteration 1568 : loss : 0.087882, loss_ce: 0.024426
2021-12-12 23:33:24,958 iteration 1569 : loss : 0.086112, loss_ce: 0.024644
2021-12-12 23:33:26,461 iteration 1570 : loss : 0.074170, loss_ce: 0.018192
2021-12-12 23:33:27,954 iteration 1571 : loss : 0.096068, loss_ce: 0.023473
2021-12-12 23:33:29,539 iteration 1572 : loss : 0.102319, loss_ce: 0.031838
2021-12-12 23:33:30,942 iteration 1573 : loss : 0.080769, loss_ce: 0.026872
2021-12-12 23:33:32,464 iteration 1574 : loss : 0.082668, loss_ce: 0.024175
2021-12-12 23:33:34,033 iteration 1575 : loss : 0.089713, loss_ce: 0.028259
2021-12-12 23:33:35,575 iteration 1576 : loss : 0.081096, loss_ce: 0.023597
2021-12-12 23:33:36,989 iteration 1577 : loss : 0.078177, loss_ce: 0.020947
2021-12-12 23:33:38,456 iteration 1578 : loss : 0.079110, loss_ce: 0.019835
2021-12-12 23:33:40,009 iteration 1579 : loss : 0.086549, loss_ce: 0.022340
2021-12-12 23:33:41,541 iteration 1580 : loss : 0.089389, loss_ce: 0.023409
2021-12-12 23:33:43,175 iteration 1581 : loss : 0.086095, loss_ce: 0.026188
 23%|██████▉                       | 93/400 [43:40<2:19:56, 27.35s/it]2021-12-12 23:33:44,744 iteration 1582 : loss : 0.077055, loss_ce: 0.020644
2021-12-12 23:33:46,272 iteration 1583 : loss : 0.081445, loss_ce: 0.021990
2021-12-12 23:33:47,689 iteration 1584 : loss : 0.088314, loss_ce: 0.021463
2021-12-12 23:33:49,194 iteration 1585 : loss : 0.096724, loss_ce: 0.017135
2021-12-12 23:33:50,757 iteration 1586 : loss : 0.088025, loss_ce: 0.020021
2021-12-12 23:33:52,266 iteration 1587 : loss : 0.081387, loss_ce: 0.018437
2021-12-12 23:33:53,825 iteration 1588 : loss : 0.083291, loss_ce: 0.026817
2021-12-12 23:33:55,320 iteration 1589 : loss : 0.083401, loss_ce: 0.017142
2021-12-12 23:33:56,845 iteration 1590 : loss : 0.083611, loss_ce: 0.028150
2021-12-12 23:33:58,300 iteration 1591 : loss : 0.090524, loss_ce: 0.030850
2021-12-12 23:33:59,753 iteration 1592 : loss : 0.094163, loss_ce: 0.029844
2021-12-12 23:34:01,320 iteration 1593 : loss : 0.080558, loss_ce: 0.016684
2021-12-12 23:34:02,843 iteration 1594 : loss : 0.082607, loss_ce: 0.028567
2021-12-12 23:34:04,362 iteration 1595 : loss : 0.095823, loss_ce: 0.031552
2021-12-12 23:34:05,864 iteration 1596 : loss : 0.081601, loss_ce: 0.023924
2021-12-12 23:34:07,318 iteration 1597 : loss : 0.077274, loss_ce: 0.020510
2021-12-12 23:34:08,857 iteration 1598 : loss : 0.081573, loss_ce: 0.024332
 24%|███████                       | 94/400 [44:06<2:16:55, 26.85s/it]2021-12-12 23:34:10,426 iteration 1599 : loss : 0.075535, loss_ce: 0.020224
2021-12-12 23:34:11,894 iteration 1600 : loss : 0.077912, loss_ce: 0.020988
2021-12-12 23:34:13,387 iteration 1601 : loss : 0.097954, loss_ce: 0.023356
2021-12-12 23:34:14,850 iteration 1602 : loss : 0.093400, loss_ce: 0.030162
2021-12-12 23:34:16,432 iteration 1603 : loss : 0.104282, loss_ce: 0.030018
2021-12-12 23:34:17,908 iteration 1604 : loss : 0.079979, loss_ce: 0.025728
2021-12-12 23:34:19,489 iteration 1605 : loss : 0.091359, loss_ce: 0.022929
2021-12-12 23:34:21,044 iteration 1606 : loss : 0.079239, loss_ce: 0.017964
2021-12-12 23:34:22,545 iteration 1607 : loss : 0.080001, loss_ce: 0.022930
2021-12-12 23:34:24,068 iteration 1608 : loss : 0.072328, loss_ce: 0.018391
2021-12-12 23:34:25,592 iteration 1609 : loss : 0.083330, loss_ce: 0.024257
2021-12-12 23:34:27,081 iteration 1610 : loss : 0.085693, loss_ce: 0.026701
2021-12-12 23:34:28,602 iteration 1611 : loss : 0.087542, loss_ce: 0.023068
2021-12-12 23:34:30,104 iteration 1612 : loss : 0.083531, loss_ce: 0.026950
2021-12-12 23:34:31,615 iteration 1613 : loss : 0.077486, loss_ce: 0.020615
2021-12-12 23:34:33,135 iteration 1614 : loss : 0.077200, loss_ce: 0.018384
2021-12-12 23:34:33,135 Training Data Eval:
2021-12-12 23:34:40,775   Average segmentation loss on training set: 0.0757
2021-12-12 23:34:40,775 Validation Data Eval:
2021-12-12 23:34:43,379   Average segmentation loss on validation set: 0.1080
2021-12-12 23:34:50,020 Found new lowest validation loss at iteration 1614! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/TU_3seeds/TU_SGD_best_val_loss_seed100.pth
2021-12-12 23:34:51,469 iteration 1615 : loss : 0.089670, loss_ce: 0.027312
 24%|███████▏                      | 95/400 [44:48<2:40:31, 31.58s/it]2021-12-12 23:34:52,873 iteration 1616 : loss : 0.082188, loss_ce: 0.019731
2021-12-12 23:34:54,193 iteration 1617 : loss : 0.079082, loss_ce: 0.022477
2021-12-12 23:34:55,567 iteration 1618 : loss : 0.072061, loss_ce: 0.016982
2021-12-12 23:34:57,079 iteration 1619 : loss : 0.081614, loss_ce: 0.028874
2021-12-12 23:34:58,505 iteration 1620 : loss : 0.075037, loss_ce: 0.020068
2021-12-12 23:35:00,030 iteration 1621 : loss : 0.086168, loss_ce: 0.026446
2021-12-12 23:35:01,497 iteration 1622 : loss : 0.082856, loss_ce: 0.023785
2021-12-12 23:35:02,919 iteration 1623 : loss : 0.074569, loss_ce: 0.021378
2021-12-12 23:35:04,419 iteration 1624 : loss : 0.084404, loss_ce: 0.024599
2021-12-12 23:35:05,819 iteration 1625 : loss : 0.076845, loss_ce: 0.026123
2021-12-12 23:35:07,266 iteration 1626 : loss : 0.077711, loss_ce: 0.021371
2021-12-12 23:35:08,703 iteration 1627 : loss : 0.072728, loss_ce: 0.013930
2021-12-12 23:35:10,212 iteration 1628 : loss : 0.079371, loss_ce: 0.017075
2021-12-12 23:35:11,721 iteration 1629 : loss : 0.087332, loss_ce: 0.027831
2021-12-12 23:35:13,139 iteration 1630 : loss : 0.082814, loss_ce: 0.023776
2021-12-12 23:35:14,620 iteration 1631 : loss : 0.082312, loss_ce: 0.027606
2021-12-12 23:35:15,995 iteration 1632 : loss : 0.071209, loss_ce: 0.017198
 24%|███████▏                      | 96/400 [45:13<2:29:16, 29.46s/it]2021-12-12 23:35:17,586 iteration 1633 : loss : 0.080942, loss_ce: 0.019577
2021-12-12 23:35:19,108 iteration 1634 : loss : 0.082656, loss_ce: 0.026531
2021-12-12 23:35:20,574 iteration 1635 : loss : 0.081196, loss_ce: 0.026341
2021-12-12 23:35:21,953 iteration 1636 : loss : 0.075163, loss_ce: 0.019953
2021-12-12 23:35:23,494 iteration 1637 : loss : 0.079618, loss_ce: 0.021016
2021-12-12 23:35:25,005 iteration 1638 : loss : 0.078276, loss_ce: 0.020707
2021-12-12 23:35:26,563 iteration 1639 : loss : 0.080145, loss_ce: 0.023693
2021-12-12 23:35:28,050 iteration 1640 : loss : 0.085979, loss_ce: 0.023191
2021-12-12 23:35:29,569 iteration 1641 : loss : 0.088399, loss_ce: 0.025826
2021-12-12 23:35:31,177 iteration 1642 : loss : 0.094108, loss_ce: 0.023931
2021-12-12 23:35:32,702 iteration 1643 : loss : 0.080807, loss_ce: 0.025211
2021-12-12 23:35:34,215 iteration 1644 : loss : 0.096960, loss_ce: 0.029646
2021-12-12 23:35:35,609 iteration 1645 : loss : 0.078222, loss_ce: 0.018246
2021-12-12 23:35:37,084 iteration 1646 : loss : 0.090671, loss_ce: 0.028703
2021-12-12 23:35:38,656 iteration 1647 : loss : 0.075473, loss_ce: 0.024535
2021-12-12 23:35:40,138 iteration 1648 : loss : 0.086169, loss_ce: 0.027133
2021-12-12 23:35:41,720 iteration 1649 : loss : 0.082392, loss_ce: 0.016912
 24%|███████▎                      | 97/400 [45:38<2:23:07, 28.34s/it]2021-12-12 23:35:43,263 iteration 1650 : loss : 0.075546, loss_ce: 0.017250
2021-12-12 23:35:44,829 iteration 1651 : loss : 0.108828, loss_ce: 0.035309
2021-12-12 23:35:46,394 iteration 1652 : loss : 0.093014, loss_ce: 0.023746
2021-12-12 23:35:47,932 iteration 1653 : loss : 0.094629, loss_ce: 0.034239
2021-12-12 23:35:49,418 iteration 1654 : loss : 0.076280, loss_ce: 0.019684
2021-12-12 23:35:50,891 iteration 1655 : loss : 0.085884, loss_ce: 0.018718
2021-12-12 23:35:52,357 iteration 1656 : loss : 0.077792, loss_ce: 0.025536
2021-12-12 23:35:53,927 iteration 1657 : loss : 0.090871, loss_ce: 0.028732
2021-12-12 23:35:55,365 iteration 1658 : loss : 0.076301, loss_ce: 0.023467
2021-12-12 23:35:56,860 iteration 1659 : loss : 0.082904, loss_ce: 0.026777
2021-12-12 23:35:58,484 iteration 1660 : loss : 0.100045, loss_ce: 0.035680
2021-12-12 23:35:59,988 iteration 1661 : loss : 0.081207, loss_ce: 0.020064
2021-12-12 23:36:01,462 iteration 1662 : loss : 0.081314, loss_ce: 0.022995
2021-12-12 23:36:03,018 iteration 1663 : loss : 0.091887, loss_ce: 0.025448
2021-12-12 23:36:04,573 iteration 1664 : loss : 0.091451, loss_ce: 0.024302
2021-12-12 23:36:06,035 iteration 1665 : loss : 0.072001, loss_ce: 0.020700
2021-12-12 23:36:07,491 iteration 1666 : loss : 0.082846, loss_ce: 0.022442
 24%|███████▎                      | 98/400 [46:04<2:18:46, 27.57s/it]2021-12-12 23:36:09,019 iteration 1667 : loss : 0.074779, loss_ce: 0.020508
2021-12-12 23:36:10,582 iteration 1668 : loss : 0.086666, loss_ce: 0.026519
2021-12-12 23:36:12,062 iteration 1669 : loss : 0.077876, loss_ce: 0.023575
2021-12-12 23:36:13,548 iteration 1670 : loss : 0.080797, loss_ce: 0.018877
2021-12-12 23:36:15,076 iteration 1671 : loss : 0.078999, loss_ce: 0.022631
2021-12-12 23:36:16,546 iteration 1672 : loss : 0.076870, loss_ce: 0.017857
2021-12-12 23:36:18,000 iteration 1673 : loss : 0.074263, loss_ce: 0.019464
2021-12-12 23:36:19,555 iteration 1674 : loss : 0.081730, loss_ce: 0.023365
2021-12-12 23:36:21,022 iteration 1675 : loss : 0.074016, loss_ce: 0.020107
2021-12-12 23:36:22,568 iteration 1676 : loss : 0.086069, loss_ce: 0.026914
2021-12-12 23:36:24,129 iteration 1677 : loss : 0.084039, loss_ce: 0.025515
2021-12-12 23:36:25,661 iteration 1678 : loss : 0.085524, loss_ce: 0.018680
2021-12-12 23:36:27,207 iteration 1679 : loss : 0.075978, loss_ce: 0.015390
2021-12-12 23:36:28,727 iteration 1680 : loss : 0.083868, loss_ce: 0.028374
2021-12-12 23:36:30,220 iteration 1681 : loss : 0.090306, loss_ce: 0.018965
2021-12-12 23:36:31,684 iteration 1682 : loss : 0.088086, loss_ce: 0.027912
2021-12-12 23:36:33,211 iteration 1683 : loss : 0.097718, loss_ce: 0.034194
 25%|███████▍                      | 99/400 [46:30<2:15:31, 27.02s/it]2021-12-12 23:36:34,755 iteration 1684 : loss : 0.071982, loss_ce: 0.017330
2021-12-12 23:36:36,207 iteration 1685 : loss : 0.074984, loss_ce: 0.019897
2021-12-12 23:36:37,759 iteration 1686 : loss : 0.079648, loss_ce: 0.021924
2021-12-12 23:36:39,206 iteration 1687 : loss : 0.080033, loss_ce: 0.018257
2021-12-12 23:36:40,722 iteration 1688 : loss : 0.084428, loss_ce: 0.023599
2021-12-12 23:36:42,252 iteration 1689 : loss : 0.080085, loss_ce: 0.024891
2021-12-12 23:36:43,682 iteration 1690 : loss : 0.082717, loss_ce: 0.020241
2021-12-12 23:36:45,110 iteration 1691 : loss : 0.085176, loss_ce: 0.021434
2021-12-12 23:36:46,605 iteration 1692 : loss : 0.075599, loss_ce: 0.022176
2021-12-12 23:36:48,164 iteration 1693 : loss : 0.087496, loss_ce: 0.025577
2021-12-12 23:36:49,706 iteration 1694 : loss : 0.077655, loss_ce: 0.017106
2021-12-12 23:36:51,172 iteration 1695 : loss : 0.085291, loss_ce: 0.019505
2021-12-12 23:36:52,676 iteration 1696 : loss : 0.084952, loss_ce: 0.030047
2021-12-12 23:36:54,196 iteration 1697 : loss : 0.081385, loss_ce: 0.025271
2021-12-12 23:36:55,715 iteration 1698 : loss : 0.078702, loss_ce: 0.022110
2021-12-12 23:36:57,183 iteration 1699 : loss : 0.075456, loss_ce: 0.024992
2021-12-12 23:36:57,183 Training Data Eval:
2021-12-12 23:37:04,835   Average segmentation loss on training set: 0.0681
2021-12-12 23:37:04,835 Validation Data Eval:
2021-12-12 23:37:07,435   Average segmentation loss on validation set: 0.1038
2021-12-12 23:37:13,843 Found new lowest validation loss at iteration 1699! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/TU_3seeds/TU_SGD_best_val_loss_seed100.pth
2021-12-12 23:37:15,266 iteration 1700 : loss : 0.079829, loss_ce: 0.022925
 25%|███████▎                     | 100/400 [47:12<2:37:38, 31.53s/it]2021-12-12 23:37:16,695 iteration 1701 : loss : 0.079026, loss_ce: 0.024470
2021-12-12 23:37:18,075 iteration 1702 : loss : 0.083413, loss_ce: 0.023711
2021-12-12 23:37:19,553 iteration 1703 : loss : 0.095294, loss_ce: 0.034183
2021-12-12 23:37:21,038 iteration 1704 : loss : 0.080459, loss_ce: 0.029160
2021-12-12 23:37:22,565 iteration 1705 : loss : 0.087439, loss_ce: 0.025402
2021-12-12 23:37:24,135 iteration 1706 : loss : 0.087127, loss_ce: 0.026547
2021-12-12 23:37:25,608 iteration 1707 : loss : 0.080255, loss_ce: 0.026864
2021-12-12 23:37:27,098 iteration 1708 : loss : 0.092337, loss_ce: 0.020800
2021-12-12 23:37:28,578 iteration 1709 : loss : 0.085455, loss_ce: 0.028733
2021-12-12 23:37:30,116 iteration 1710 : loss : 0.079664, loss_ce: 0.019152
2021-12-12 23:37:31,609 iteration 1711 : loss : 0.071765, loss_ce: 0.020813
2021-12-12 23:37:33,032 iteration 1712 : loss : 0.075032, loss_ce: 0.023003
2021-12-12 23:37:34,549 iteration 1713 : loss : 0.099964, loss_ce: 0.024804
2021-12-12 23:37:36,124 iteration 1714 : loss : 0.082314, loss_ce: 0.017946
2021-12-12 23:37:37,677 iteration 1715 : loss : 0.081712, loss_ce: 0.024981
2021-12-12 23:37:39,099 iteration 1716 : loss : 0.075962, loss_ce: 0.016622
2021-12-12 23:37:40,646 iteration 1717 : loss : 0.098850, loss_ce: 0.026706
 25%|███████▎                     | 101/400 [47:37<2:27:54, 29.68s/it]2021-12-12 23:37:42,196 iteration 1718 : loss : 0.078338, loss_ce: 0.024452
2021-12-12 23:37:43,678 iteration 1719 : loss : 0.080553, loss_ce: 0.016076
2021-12-12 23:37:45,220 iteration 1720 : loss : 0.079718, loss_ce: 0.020410
2021-12-12 23:37:46,683 iteration 1721 : loss : 0.088470, loss_ce: 0.024285
2021-12-12 23:37:48,215 iteration 1722 : loss : 0.075488, loss_ce: 0.018059
2021-12-12 23:37:49,660 iteration 1723 : loss : 0.088788, loss_ce: 0.022140
2021-12-12 23:37:51,177 iteration 1724 : loss : 0.078158, loss_ce: 0.020440
2021-12-12 23:37:52,678 iteration 1725 : loss : 0.087925, loss_ce: 0.021367
2021-12-12 23:37:54,180 iteration 1726 : loss : 0.078119, loss_ce: 0.023138
2021-12-12 23:37:55,754 iteration 1727 : loss : 0.090651, loss_ce: 0.030355
2021-12-12 23:37:57,162 iteration 1728 : loss : 0.084470, loss_ce: 0.022125
2021-12-12 23:37:58,676 iteration 1729 : loss : 0.085964, loss_ce: 0.024898
2021-12-12 23:38:00,236 iteration 1730 : loss : 0.078123, loss_ce: 0.025949
2021-12-12 23:38:01,720 iteration 1731 : loss : 0.089991, loss_ce: 0.035006
2021-12-12 23:38:03,190 iteration 1732 : loss : 0.083321, loss_ce: 0.023812
2021-12-12 23:38:04,631 iteration 1733 : loss : 0.076390, loss_ce: 0.020858
2021-12-12 23:38:06,096 iteration 1734 : loss : 0.077018, loss_ce: 0.020976
 26%|███████▍                     | 102/400 [48:03<2:21:06, 28.41s/it]2021-12-12 23:38:07,758 iteration 1735 : loss : 0.083386, loss_ce: 0.024117
2021-12-12 23:38:09,228 iteration 1736 : loss : 0.086552, loss_ce: 0.023866
2021-12-12 23:38:10,853 iteration 1737 : loss : 0.100185, loss_ce: 0.032398
2021-12-12 23:38:12,346 iteration 1738 : loss : 0.075912, loss_ce: 0.023712
2021-12-12 23:38:13,777 iteration 1739 : loss : 0.078235, loss_ce: 0.021948
2021-12-12 23:38:15,320 iteration 1740 : loss : 0.077946, loss_ce: 0.024636
2021-12-12 23:38:16,847 iteration 1741 : loss : 0.078430, loss_ce: 0.021008
2021-12-12 23:38:18,385 iteration 1742 : loss : 0.078583, loss_ce: 0.021939
2021-12-12 23:38:19,802 iteration 1743 : loss : 0.077442, loss_ce: 0.018082
2021-12-12 23:38:21,370 iteration 1744 : loss : 0.084146, loss_ce: 0.024013
2021-12-12 23:38:22,927 iteration 1745 : loss : 0.081957, loss_ce: 0.024197
2021-12-12 23:38:24,431 iteration 1746 : loss : 0.080818, loss_ce: 0.026756
2021-12-12 23:38:25,894 iteration 1747 : loss : 0.076055, loss_ce: 0.021922
2021-12-12 23:38:27,415 iteration 1748 : loss : 0.069950, loss_ce: 0.017830
2021-12-12 23:38:28,804 iteration 1749 : loss : 0.072036, loss_ce: 0.016446
2021-12-12 23:38:30,309 iteration 1750 : loss : 0.081675, loss_ce: 0.014922
2021-12-12 23:38:31,907 iteration 1751 : loss : 0.078060, loss_ce: 0.021304
 26%|███████▍                     | 103/400 [48:29<2:16:47, 27.63s/it]2021-12-12 23:38:33,524 iteration 1752 : loss : 0.106161, loss_ce: 0.018633
2021-12-12 23:38:34,916 iteration 1753 : loss : 0.079111, loss_ce: 0.020605
2021-12-12 23:38:36,467 iteration 1754 : loss : 0.080644, loss_ce: 0.020028
2021-12-12 23:38:37,918 iteration 1755 : loss : 0.077655, loss_ce: 0.024945
2021-12-12 23:38:39,426 iteration 1756 : loss : 0.080354, loss_ce: 0.024321
2021-12-12 23:38:40,868 iteration 1757 : loss : 0.070631, loss_ce: 0.020808
2021-12-12 23:38:42,344 iteration 1758 : loss : 0.074925, loss_ce: 0.025802
2021-12-12 23:38:43,789 iteration 1759 : loss : 0.081128, loss_ce: 0.021784
2021-12-12 23:38:45,236 iteration 1760 : loss : 0.075504, loss_ce: 0.019154
2021-12-12 23:38:46,777 iteration 1761 : loss : 0.079157, loss_ce: 0.017720
2021-12-12 23:38:48,180 iteration 1762 : loss : 0.078142, loss_ce: 0.022981
2021-12-12 23:38:49,612 iteration 1763 : loss : 0.074488, loss_ce: 0.019281
2021-12-12 23:38:50,994 iteration 1764 : loss : 0.078959, loss_ce: 0.018743
2021-12-12 23:38:52,499 iteration 1765 : loss : 0.075221, loss_ce: 0.025034
2021-12-12 23:38:54,050 iteration 1766 : loss : 0.085176, loss_ce: 0.030740
2021-12-12 23:38:55,521 iteration 1767 : loss : 0.078408, loss_ce: 0.023231
2021-12-12 23:38:57,031 iteration 1768 : loss : 0.083834, loss_ce: 0.024660
 26%|███████▌                     | 104/400 [48:54<2:12:36, 26.88s/it]2021-12-12 23:38:58,570 iteration 1769 : loss : 0.078758, loss_ce: 0.021665
2021-12-12 23:39:00,029 iteration 1770 : loss : 0.076278, loss_ce: 0.022368
2021-12-12 23:39:01,433 iteration 1771 : loss : 0.083947, loss_ce: 0.024850
2021-12-12 23:39:02,868 iteration 1772 : loss : 0.073716, loss_ce: 0.021901
2021-12-12 23:39:04,405 iteration 1773 : loss : 0.085870, loss_ce: 0.018285
2021-12-12 23:39:05,914 iteration 1774 : loss : 0.080980, loss_ce: 0.021147
2021-12-12 23:39:07,446 iteration 1775 : loss : 0.079063, loss_ce: 0.023650
2021-12-12 23:39:08,899 iteration 1776 : loss : 0.075707, loss_ce: 0.025712
2021-12-12 23:39:10,340 iteration 1777 : loss : 0.081700, loss_ce: 0.020888
2021-12-12 23:39:11,755 iteration 1778 : loss : 0.072278, loss_ce: 0.015837
2021-12-12 23:39:13,283 iteration 1779 : loss : 0.081002, loss_ce: 0.018451
2021-12-12 23:39:14,776 iteration 1780 : loss : 0.080528, loss_ce: 0.026835
2021-12-12 23:39:16,204 iteration 1781 : loss : 0.072007, loss_ce: 0.021401
2021-12-12 23:39:17,726 iteration 1782 : loss : 0.076106, loss_ce: 0.021508
2021-12-12 23:39:19,237 iteration 1783 : loss : 0.083356, loss_ce: 0.022708
2021-12-12 23:39:20,694 iteration 1784 : loss : 0.075918, loss_ce: 0.020766
2021-12-12 23:39:20,694 Training Data Eval:
2021-12-12 23:39:28,351   Average segmentation loss on training set: 0.0663
2021-12-12 23:39:28,352 Validation Data Eval:
2021-12-12 23:39:30,963   Average segmentation loss on validation set: 0.1042
2021-12-12 23:39:32,496 iteration 1785 : loss : 0.078222, loss_ce: 0.021547
 26%|███████▌                     | 105/400 [49:29<2:24:48, 29.45s/it]2021-12-12 23:39:34,049 iteration 1786 : loss : 0.076524, loss_ce: 0.023056
2021-12-12 23:39:35,462 iteration 1787 : loss : 0.088052, loss_ce: 0.026176
2021-12-12 23:39:37,002 iteration 1788 : loss : 0.076924, loss_ce: 0.025220
2021-12-12 23:39:38,524 iteration 1789 : loss : 0.072720, loss_ce: 0.018536
2021-12-12 23:39:40,032 iteration 1790 : loss : 0.086183, loss_ce: 0.031476
2021-12-12 23:39:41,590 iteration 1791 : loss : 0.080444, loss_ce: 0.019256
2021-12-12 23:39:43,032 iteration 1792 : loss : 0.081349, loss_ce: 0.020181
2021-12-12 23:39:44,491 iteration 1793 : loss : 0.084415, loss_ce: 0.020524
2021-12-12 23:39:45,967 iteration 1794 : loss : 0.073672, loss_ce: 0.022356
2021-12-12 23:39:47,495 iteration 1795 : loss : 0.090799, loss_ce: 0.025353
2021-12-12 23:39:48,978 iteration 1796 : loss : 0.079445, loss_ce: 0.022478
2021-12-12 23:39:50,554 iteration 1797 : loss : 0.082001, loss_ce: 0.018046
2021-12-12 23:39:52,121 iteration 1798 : loss : 0.084174, loss_ce: 0.024557
2021-12-12 23:39:53,585 iteration 1799 : loss : 0.085478, loss_ce: 0.028803
2021-12-12 23:39:55,100 iteration 1800 : loss : 0.076278, loss_ce: 0.018803
2021-12-12 23:39:56,539 iteration 1801 : loss : 0.076348, loss_ce: 0.018672
2021-12-12 23:39:58,006 iteration 1802 : loss : 0.077432, loss_ce: 0.020060
 26%|███████▋                     | 106/400 [49:55<2:18:31, 28.27s/it]2021-12-12 23:39:59,620 iteration 1803 : loss : 0.087529, loss_ce: 0.022818
2021-12-12 23:40:01,189 iteration 1804 : loss : 0.074689, loss_ce: 0.016433
2021-12-12 23:40:02,652 iteration 1805 : loss : 0.087557, loss_ce: 0.017598
2021-12-12 23:40:04,108 iteration 1806 : loss : 0.082734, loss_ce: 0.028417
2021-12-12 23:40:05,564 iteration 1807 : loss : 0.086608, loss_ce: 0.025003
2021-12-12 23:40:07,043 iteration 1808 : loss : 0.081694, loss_ce: 0.022915
2021-12-12 23:40:08,496 iteration 1809 : loss : 0.071736, loss_ce: 0.019961
2021-12-12 23:40:09,878 iteration 1810 : loss : 0.066516, loss_ce: 0.016519
2021-12-12 23:40:11,410 iteration 1811 : loss : 0.076565, loss_ce: 0.024076
2021-12-12 23:40:12,966 iteration 1812 : loss : 0.084367, loss_ce: 0.021917
2021-12-12 23:40:14,589 iteration 1813 : loss : 0.083737, loss_ce: 0.022210
2021-12-12 23:40:16,114 iteration 1814 : loss : 0.080553, loss_ce: 0.028467
2021-12-12 23:40:17,551 iteration 1815 : loss : 0.079384, loss_ce: 0.016586
2021-12-12 23:40:18,949 iteration 1816 : loss : 0.073684, loss_ce: 0.022643
2021-12-12 23:40:20,500 iteration 1817 : loss : 0.082758, loss_ce: 0.022432
2021-12-12 23:40:21,987 iteration 1818 : loss : 0.090185, loss_ce: 0.029755
2021-12-12 23:40:23,520 iteration 1819 : loss : 0.079056, loss_ce: 0.025441
 27%|███████▊                     | 107/400 [50:20<2:14:01, 27.44s/it]2021-12-12 23:40:25,129 iteration 1820 : loss : 0.077322, loss_ce: 0.021305
2021-12-12 23:40:26,593 iteration 1821 : loss : 0.072856, loss_ce: 0.021403
2021-12-12 23:40:28,089 iteration 1822 : loss : 0.078795, loss_ce: 0.020552
2021-12-12 23:40:29,495 iteration 1823 : loss : 0.074345, loss_ce: 0.017838
2021-12-12 23:40:30,860 iteration 1824 : loss : 0.065546, loss_ce: 0.017831
2021-12-12 23:40:32,383 iteration 1825 : loss : 0.081461, loss_ce: 0.022430
2021-12-12 23:40:33,775 iteration 1826 : loss : 0.075075, loss_ce: 0.025084
2021-12-12 23:40:35,365 iteration 1827 : loss : 0.115533, loss_ce: 0.023600
2021-12-12 23:40:36,838 iteration 1828 : loss : 0.077366, loss_ce: 0.021066
2021-12-12 23:40:38,308 iteration 1829 : loss : 0.083890, loss_ce: 0.019800
2021-12-12 23:40:39,774 iteration 1830 : loss : 0.075347, loss_ce: 0.023504
2021-12-12 23:40:41,337 iteration 1831 : loss : 0.090901, loss_ce: 0.033362
2021-12-12 23:40:42,807 iteration 1832 : loss : 0.073675, loss_ce: 0.016965
2021-12-12 23:40:44,291 iteration 1833 : loss : 0.081042, loss_ce: 0.027825
2021-12-12 23:40:45,877 iteration 1834 : loss : 0.082481, loss_ce: 0.023314
2021-12-12 23:40:47,344 iteration 1835 : loss : 0.083028, loss_ce: 0.026591
2021-12-12 23:40:48,830 iteration 1836 : loss : 0.084730, loss_ce: 0.019025
 27%|███████▊                     | 108/400 [50:46<2:10:27, 26.81s/it]2021-12-12 23:40:50,417 iteration 1837 : loss : 0.075263, loss_ce: 0.021187
2021-12-12 23:40:51,971 iteration 1838 : loss : 0.082384, loss_ce: 0.028347
2021-12-12 23:40:53,443 iteration 1839 : loss : 0.076064, loss_ce: 0.021942
2021-12-12 23:40:54,853 iteration 1840 : loss : 0.072053, loss_ce: 0.023384
2021-12-12 23:40:56,379 iteration 1841 : loss : 0.078093, loss_ce: 0.024123
2021-12-12 23:40:57,932 iteration 1842 : loss : 0.102264, loss_ce: 0.028971
2021-12-12 23:40:59,357 iteration 1843 : loss : 0.071292, loss_ce: 0.023078
2021-12-12 23:41:00,847 iteration 1844 : loss : 0.071848, loss_ce: 0.018644
2021-12-12 23:41:02,265 iteration 1845 : loss : 0.073812, loss_ce: 0.022075
2021-12-12 23:41:03,852 iteration 1846 : loss : 0.078849, loss_ce: 0.025353
2021-12-12 23:41:05,394 iteration 1847 : loss : 0.089211, loss_ce: 0.026445
2021-12-12 23:41:06,889 iteration 1848 : loss : 0.074057, loss_ce: 0.021404
2021-12-12 23:41:08,280 iteration 1849 : loss : 0.069410, loss_ce: 0.020309
2021-12-12 23:41:09,779 iteration 1850 : loss : 0.086367, loss_ce: 0.020339
2021-12-12 23:41:11,315 iteration 1851 : loss : 0.073219, loss_ce: 0.019233
2021-12-12 23:41:12,818 iteration 1852 : loss : 0.076698, loss_ce: 0.020740
2021-12-12 23:41:14,327 iteration 1853 : loss : 0.086371, loss_ce: 0.015528
 27%|███████▉                     | 109/400 [51:11<2:08:06, 26.41s/it]2021-12-12 23:41:15,847 iteration 1854 : loss : 0.073641, loss_ce: 0.018286
2021-12-12 23:41:17,331 iteration 1855 : loss : 0.079503, loss_ce: 0.020690
2021-12-12 23:41:18,831 iteration 1856 : loss : 0.069794, loss_ce: 0.011592
2021-12-12 23:41:20,328 iteration 1857 : loss : 0.086001, loss_ce: 0.023090
2021-12-12 23:41:21,808 iteration 1858 : loss : 0.078598, loss_ce: 0.026534
2021-12-12 23:41:23,242 iteration 1859 : loss : 0.072565, loss_ce: 0.022975
2021-12-12 23:41:24,855 iteration 1860 : loss : 0.090326, loss_ce: 0.030868
2021-12-12 23:41:26,381 iteration 1861 : loss : 0.080780, loss_ce: 0.019664
2021-12-12 23:41:27,868 iteration 1862 : loss : 0.073364, loss_ce: 0.023838
2021-12-12 23:41:29,410 iteration 1863 : loss : 0.080533, loss_ce: 0.022045
2021-12-12 23:41:30,918 iteration 1864 : loss : 0.081826, loss_ce: 0.022687
2021-12-12 23:41:32,513 iteration 1865 : loss : 0.084248, loss_ce: 0.030468
2021-12-12 23:41:34,072 iteration 1866 : loss : 0.078893, loss_ce: 0.028422
2021-12-12 23:41:35,594 iteration 1867 : loss : 0.093330, loss_ce: 0.020309
2021-12-12 23:41:37,061 iteration 1868 : loss : 0.077333, loss_ce: 0.027386
2021-12-12 23:41:38,556 iteration 1869 : loss : 0.082821, loss_ce: 0.020337
2021-12-12 23:41:38,556 Training Data Eval:
2021-12-12 23:41:46,208   Average segmentation loss on training set: 0.0699
2021-12-12 23:41:46,209 Validation Data Eval:
2021-12-12 23:41:48,818   Average segmentation loss on validation set: 0.1068
2021-12-12 23:41:50,242 iteration 1870 : loss : 0.080028, loss_ce: 0.018247
 28%|███████▉                     | 110/400 [51:47<2:21:25, 29.26s/it]2021-12-12 23:41:51,753 iteration 1871 : loss : 0.076235, loss_ce: 0.026415
2021-12-12 23:41:53,208 iteration 1872 : loss : 0.076902, loss_ce: 0.019644
2021-12-12 23:41:54,708 iteration 1873 : loss : 0.077236, loss_ce: 0.022433
2021-12-12 23:41:56,255 iteration 1874 : loss : 0.084261, loss_ce: 0.030152
2021-12-12 23:41:57,832 iteration 1875 : loss : 0.079488, loss_ce: 0.030313
2021-12-12 23:41:59,412 iteration 1876 : loss : 0.107332, loss_ce: 0.025163
2021-12-12 23:42:00,966 iteration 1877 : loss : 0.084651, loss_ce: 0.021012
2021-12-12 23:42:02,410 iteration 1878 : loss : 0.071473, loss_ce: 0.015971
2021-12-12 23:42:03,865 iteration 1879 : loss : 0.074881, loss_ce: 0.023321
2021-12-12 23:42:05,313 iteration 1880 : loss : 0.072092, loss_ce: 0.018247
2021-12-12 23:42:06,759 iteration 1881 : loss : 0.071519, loss_ce: 0.018240
2021-12-12 23:42:08,244 iteration 1882 : loss : 0.075995, loss_ce: 0.013920
2021-12-12 23:42:09,668 iteration 1883 : loss : 0.075389, loss_ce: 0.020792
2021-12-12 23:42:11,225 iteration 1884 : loss : 0.075456, loss_ce: 0.022760
2021-12-12 23:42:12,663 iteration 1885 : loss : 0.067895, loss_ce: 0.019219
2021-12-12 23:42:14,176 iteration 1886 : loss : 0.081385, loss_ce: 0.028224
2021-12-12 23:42:15,622 iteration 1887 : loss : 0.072898, loss_ce: 0.017717
 28%|████████                     | 111/400 [52:12<2:15:20, 28.10s/it]2021-12-12 23:42:17,183 iteration 1888 : loss : 0.078191, loss_ce: 0.021380
2021-12-12 23:42:18,752 iteration 1889 : loss : 0.082714, loss_ce: 0.024060
2021-12-12 23:42:20,166 iteration 1890 : loss : 0.068726, loss_ce: 0.018212
2021-12-12 23:42:21,663 iteration 1891 : loss : 0.080683, loss_ce: 0.029728
2021-12-12 23:42:23,151 iteration 1892 : loss : 0.072387, loss_ce: 0.017261
2021-12-12 23:42:24,684 iteration 1893 : loss : 0.086796, loss_ce: 0.024698
2021-12-12 23:42:26,168 iteration 1894 : loss : 0.083474, loss_ce: 0.023065
2021-12-12 23:42:27,684 iteration 1895 : loss : 0.076665, loss_ce: 0.020055
2021-12-12 23:42:29,180 iteration 1896 : loss : 0.077339, loss_ce: 0.024819
2021-12-12 23:42:30,669 iteration 1897 : loss : 0.078066, loss_ce: 0.021582
2021-12-12 23:42:32,245 iteration 1898 : loss : 0.083107, loss_ce: 0.023797
2021-12-12 23:42:33,657 iteration 1899 : loss : 0.069029, loss_ce: 0.018737
2021-12-12 23:42:35,224 iteration 1900 : loss : 0.088149, loss_ce: 0.029554
2021-12-12 23:42:36,818 iteration 1901 : loss : 0.084739, loss_ce: 0.021120
2021-12-12 23:42:38,299 iteration 1902 : loss : 0.074335, loss_ce: 0.016557
2021-12-12 23:42:39,835 iteration 1903 : loss : 0.081143, loss_ce: 0.026807
2021-12-12 23:42:41,381 iteration 1904 : loss : 0.073987, loss_ce: 0.016562
 28%|████████                     | 112/400 [52:38<2:11:30, 27.40s/it]2021-12-12 23:42:42,962 iteration 1905 : loss : 0.087306, loss_ce: 0.027738
2021-12-12 23:42:44,465 iteration 1906 : loss : 0.074828, loss_ce: 0.017958
2021-12-12 23:42:45,948 iteration 1907 : loss : 0.071565, loss_ce: 0.018246
2021-12-12 23:42:47,574 iteration 1908 : loss : 0.090730, loss_ce: 0.020692
2021-12-12 23:42:49,096 iteration 1909 : loss : 0.072000, loss_ce: 0.020175
2021-12-12 23:42:50,629 iteration 1910 : loss : 0.085161, loss_ce: 0.014221
2021-12-12 23:42:52,176 iteration 1911 : loss : 0.075879, loss_ce: 0.019280
2021-12-12 23:42:53,587 iteration 1912 : loss : 0.070650, loss_ce: 0.019531
2021-12-12 23:42:55,079 iteration 1913 : loss : 0.085488, loss_ce: 0.028793
2021-12-12 23:42:56,583 iteration 1914 : loss : 0.073074, loss_ce: 0.023278
2021-12-12 23:42:58,104 iteration 1915 : loss : 0.071749, loss_ce: 0.020594
2021-12-12 23:42:59,597 iteration 1916 : loss : 0.087413, loss_ce: 0.026865
2021-12-12 23:43:01,086 iteration 1917 : loss : 0.079278, loss_ce: 0.027052
2021-12-12 23:43:02,607 iteration 1918 : loss : 0.073373, loss_ce: 0.018421
2021-12-12 23:43:04,070 iteration 1919 : loss : 0.073469, loss_ce: 0.019468
2021-12-12 23:43:05,536 iteration 1920 : loss : 0.079855, loss_ce: 0.024239
2021-12-12 23:43:07,106 iteration 1921 : loss : 0.088623, loss_ce: 0.030223
 28%|████████▏                    | 113/400 [53:04<2:08:39, 26.90s/it]2021-12-12 23:43:08,608 iteration 1922 : loss : 0.073905, loss_ce: 0.025470
2021-12-12 23:43:10,101 iteration 1923 : loss : 0.069686, loss_ce: 0.022014
2021-12-12 23:43:11,651 iteration 1924 : loss : 0.073652, loss_ce: 0.023685
2021-12-12 23:43:13,161 iteration 1925 : loss : 0.090500, loss_ce: 0.026540
2021-12-12 23:43:14,628 iteration 1926 : loss : 0.071958, loss_ce: 0.022520
2021-12-12 23:43:16,088 iteration 1927 : loss : 0.072892, loss_ce: 0.020797
2021-12-12 23:43:17,673 iteration 1928 : loss : 0.076305, loss_ce: 0.023991
2021-12-12 23:43:19,128 iteration 1929 : loss : 0.073652, loss_ce: 0.019128
2021-12-12 23:43:20,574 iteration 1930 : loss : 0.068710, loss_ce: 0.021850
2021-12-12 23:43:22,090 iteration 1931 : loss : 0.092691, loss_ce: 0.019256
2021-12-12 23:43:23,646 iteration 1932 : loss : 0.088888, loss_ce: 0.025236
2021-12-12 23:43:25,209 iteration 1933 : loss : 0.084138, loss_ce: 0.027171
2021-12-12 23:43:26,604 iteration 1934 : loss : 0.068318, loss_ce: 0.019156
2021-12-12 23:43:28,111 iteration 1935 : loss : 0.077566, loss_ce: 0.018509
2021-12-12 23:43:29,684 iteration 1936 : loss : 0.077688, loss_ce: 0.022402
2021-12-12 23:43:31,111 iteration 1937 : loss : 0.083763, loss_ce: 0.019299
2021-12-12 23:43:32,524 iteration 1938 : loss : 0.071894, loss_ce: 0.018308
 28%|████████▎                    | 114/400 [53:29<2:06:04, 26.45s/it]2021-12-12 23:43:34,018 iteration 1939 : loss : 0.070073, loss_ce: 0.016834
2021-12-12 23:43:35,611 iteration 1940 : loss : 0.082937, loss_ce: 0.025950
2021-12-12 23:43:37,211 iteration 1941 : loss : 0.075901, loss_ce: 0.021778
2021-12-12 23:43:38,742 iteration 1942 : loss : 0.078133, loss_ce: 0.021037
2021-12-12 23:43:40,216 iteration 1943 : loss : 0.070616, loss_ce: 0.020561
2021-12-12 23:43:41,716 iteration 1944 : loss : 0.072710, loss_ce: 0.020475
2021-12-12 23:43:43,165 iteration 1945 : loss : 0.072834, loss_ce: 0.020309
2021-12-12 23:43:44,661 iteration 1946 : loss : 0.074323, loss_ce: 0.020501
2021-12-12 23:43:46,191 iteration 1947 : loss : 0.081662, loss_ce: 0.027157
2021-12-12 23:43:47,688 iteration 1948 : loss : 0.083086, loss_ce: 0.018018
2021-12-12 23:43:49,153 iteration 1949 : loss : 0.066848, loss_ce: 0.018859
2021-12-12 23:43:50,581 iteration 1950 : loss : 0.077105, loss_ce: 0.026207
2021-12-12 23:43:52,136 iteration 1951 : loss : 0.075372, loss_ce: 0.019847
2021-12-12 23:43:53,671 iteration 1952 : loss : 0.078723, loss_ce: 0.018793
2021-12-12 23:43:55,674 iteration 1953 : loss : 0.085036, loss_ce: 0.029756
2021-12-12 23:43:57,187 iteration 1954 : loss : 0.075144, loss_ce: 0.023591
2021-12-12 23:43:57,187 Training Data Eval:
2021-12-12 23:44:04,853   Average segmentation loss on training set: 0.0669
2021-12-12 23:44:04,854 Validation Data Eval:
2021-12-12 23:44:07,456   Average segmentation loss on validation set: 0.1020
2021-12-12 23:44:13,782 Found new lowest validation loss at iteration 1954! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/TU_3seeds/TU_SGD_best_val_loss_seed100.pth
2021-12-12 23:44:15,202 iteration 1955 : loss : 0.084704, loss_ce: 0.019599
 29%|████████▎                    | 115/400 [54:12<2:28:46, 31.32s/it]2021-12-12 23:44:16,583 iteration 1956 : loss : 0.083991, loss_ce: 0.016661
2021-12-12 23:44:18,056 iteration 1957 : loss : 0.084453, loss_ce: 0.028696
2021-12-12 23:44:19,535 iteration 1958 : loss : 0.094938, loss_ce: 0.031632
2021-12-12 23:44:21,006 iteration 1959 : loss : 0.079592, loss_ce: 0.023229
2021-12-12 23:44:22,428 iteration 1960 : loss : 0.074375, loss_ce: 0.018249
2021-12-12 23:44:23,944 iteration 1961 : loss : 0.080986, loss_ce: 0.026309
2021-12-12 23:44:25,478 iteration 1962 : loss : 0.074587, loss_ce: 0.021895
2021-12-12 23:44:26,910 iteration 1963 : loss : 0.067733, loss_ce: 0.019811
2021-12-12 23:44:28,353 iteration 1964 : loss : 0.072815, loss_ce: 0.020993
2021-12-12 23:44:29,797 iteration 1965 : loss : 0.074977, loss_ce: 0.024602
2021-12-12 23:44:31,362 iteration 1966 : loss : 0.077401, loss_ce: 0.024753
2021-12-12 23:44:32,929 iteration 1967 : loss : 0.079918, loss_ce: 0.022640
2021-12-12 23:44:34,481 iteration 1968 : loss : 0.073908, loss_ce: 0.022355
2021-12-12 23:44:36,016 iteration 1969 : loss : 0.082769, loss_ce: 0.023039
2021-12-12 23:44:37,502 iteration 1970 : loss : 0.080066, loss_ce: 0.017563
2021-12-12 23:44:39,050 iteration 1971 : loss : 0.084268, loss_ce: 0.028899
2021-12-12 23:44:40,517 iteration 1972 : loss : 0.083741, loss_ce: 0.020796
 29%|████████▍                    | 116/400 [54:37<2:19:42, 29.52s/it]2021-12-12 23:44:41,923 iteration 1973 : loss : 0.068394, loss_ce: 0.020776
2021-12-12 23:44:43,432 iteration 1974 : loss : 0.075048, loss_ce: 0.023062
2021-12-12 23:44:44,913 iteration 1975 : loss : 0.069223, loss_ce: 0.016872
2021-12-12 23:44:46,479 iteration 1976 : loss : 0.074717, loss_ce: 0.022597
2021-12-12 23:44:47,985 iteration 1977 : loss : 0.073985, loss_ce: 0.016954
2021-12-12 23:44:49,401 iteration 1978 : loss : 0.066067, loss_ce: 0.018689
2021-12-12 23:44:50,812 iteration 1979 : loss : 0.067194, loss_ce: 0.018039
2021-12-12 23:44:52,320 iteration 1980 : loss : 0.075150, loss_ce: 0.024611
2021-12-12 23:44:53,766 iteration 1981 : loss : 0.067739, loss_ce: 0.016133
2021-12-12 23:44:55,162 iteration 1982 : loss : 0.072708, loss_ce: 0.018548
2021-12-12 23:44:56,721 iteration 1983 : loss : 0.084232, loss_ce: 0.029345
2021-12-12 23:44:58,142 iteration 1984 : loss : 0.081484, loss_ce: 0.018402
2021-12-12 23:44:59,639 iteration 1985 : loss : 0.072216, loss_ce: 0.019226
2021-12-12 23:45:01,145 iteration 1986 : loss : 0.091951, loss_ce: 0.034640
2021-12-12 23:45:02,694 iteration 1987 : loss : 0.069929, loss_ce: 0.020406
2021-12-12 23:45:04,137 iteration 1988 : loss : 0.083724, loss_ce: 0.016950
2021-12-12 23:45:05,611 iteration 1989 : loss : 0.081746, loss_ce: 0.025398
 29%|████████▍                    | 117/400 [55:02<2:12:58, 28.19s/it]2021-12-12 23:45:07,041 iteration 1990 : loss : 0.066859, loss_ce: 0.019762
2021-12-12 23:45:08,479 iteration 1991 : loss : 0.065872, loss_ce: 0.018504
2021-12-12 23:45:09,995 iteration 1992 : loss : 0.077130, loss_ce: 0.020773
2021-12-12 23:45:11,623 iteration 1993 : loss : 0.083132, loss_ce: 0.031307
2021-12-12 23:45:13,062 iteration 1994 : loss : 0.071108, loss_ce: 0.019955
2021-12-12 23:45:14,552 iteration 1995 : loss : 0.086636, loss_ce: 0.017995
2021-12-12 23:45:16,054 iteration 1996 : loss : 0.086312, loss_ce: 0.020395
2021-12-12 23:45:17,540 iteration 1997 : loss : 0.071010, loss_ce: 0.017149
2021-12-12 23:45:19,031 iteration 1998 : loss : 0.076094, loss_ce: 0.019749
2021-12-12 23:45:20,501 iteration 1999 : loss : 0.073582, loss_ce: 0.022042
2021-12-12 23:45:22,071 iteration 2000 : loss : 0.080193, loss_ce: 0.015505
2021-12-12 23:45:23,546 iteration 2001 : loss : 0.071319, loss_ce: 0.021760
2021-12-12 23:45:25,110 iteration 2002 : loss : 0.075123, loss_ce: 0.025947
2021-12-12 23:45:26,556 iteration 2003 : loss : 0.075989, loss_ce: 0.026370
2021-12-12 23:45:28,071 iteration 2004 : loss : 0.071054, loss_ce: 0.019374
2021-12-12 23:45:29,613 iteration 2005 : loss : 0.079926, loss_ce: 0.020423
2021-12-12 23:45:31,144 iteration 2006 : loss : 0.076061, loss_ce: 0.020652
 30%|████████▌                    | 118/400 [55:28<2:08:45, 27.39s/it]2021-12-12 23:45:32,708 iteration 2007 : loss : 0.080828, loss_ce: 0.025577
2021-12-12 23:45:34,158 iteration 2008 : loss : 0.070998, loss_ce: 0.019091
2021-12-12 23:45:35,631 iteration 2009 : loss : 0.080441, loss_ce: 0.017275
2021-12-12 23:45:37,057 iteration 2010 : loss : 0.073989, loss_ce: 0.021014
2021-12-12 23:45:38,531 iteration 2011 : loss : 0.077245, loss_ce: 0.026599
2021-12-12 23:45:40,035 iteration 2012 : loss : 0.077400, loss_ce: 0.021725
2021-12-12 23:45:41,583 iteration 2013 : loss : 0.073669, loss_ce: 0.018019
2021-12-12 23:45:43,117 iteration 2014 : loss : 0.068520, loss_ce: 0.016801
2021-12-12 23:45:44,539 iteration 2015 : loss : 0.072583, loss_ce: 0.019727
2021-12-12 23:45:46,019 iteration 2016 : loss : 0.074399, loss_ce: 0.017560
2021-12-12 23:45:47,407 iteration 2017 : loss : 0.066781, loss_ce: 0.019102
2021-12-12 23:45:48,900 iteration 2018 : loss : 0.072381, loss_ce: 0.021911
2021-12-12 23:45:50,329 iteration 2019 : loss : 0.067761, loss_ce: 0.021031
2021-12-12 23:45:51,827 iteration 2020 : loss : 0.072983, loss_ce: 0.021461
2021-12-12 23:45:53,351 iteration 2021 : loss : 0.068294, loss_ce: 0.018112
2021-12-12 23:45:54,907 iteration 2022 : loss : 0.086856, loss_ce: 0.027339
2021-12-12 23:45:56,394 iteration 2023 : loss : 0.082782, loss_ce: 0.018579
 30%|████████▋                    | 119/400 [55:53<2:05:16, 26.75s/it]2021-12-12 23:45:57,962 iteration 2024 : loss : 0.077782, loss_ce: 0.023829
2021-12-12 23:45:59,397 iteration 2025 : loss : 0.069028, loss_ce: 0.018234
2021-12-12 23:46:00,846 iteration 2026 : loss : 0.072561, loss_ce: 0.020371
2021-12-12 23:46:02,375 iteration 2027 : loss : 0.072256, loss_ce: 0.019338
2021-12-12 23:46:03,883 iteration 2028 : loss : 0.079204, loss_ce: 0.022711
2021-12-12 23:46:05,302 iteration 2029 : loss : 0.074432, loss_ce: 0.025407
2021-12-12 23:46:06,800 iteration 2030 : loss : 0.069757, loss_ce: 0.018514
2021-12-12 23:46:08,322 iteration 2031 : loss : 0.064625, loss_ce: 0.016357
2021-12-12 23:46:09,848 iteration 2032 : loss : 0.090432, loss_ce: 0.026225
2021-12-12 23:46:11,329 iteration 2033 : loss : 0.075975, loss_ce: 0.017212
2021-12-12 23:46:12,740 iteration 2034 : loss : 0.071437, loss_ce: 0.016450
2021-12-12 23:46:14,154 iteration 2035 : loss : 0.067009, loss_ce: 0.020633
2021-12-12 23:46:15,680 iteration 2036 : loss : 0.070726, loss_ce: 0.022444
2021-12-12 23:46:17,166 iteration 2037 : loss : 0.071197, loss_ce: 0.018845
2021-12-12 23:46:18,607 iteration 2038 : loss : 0.071552, loss_ce: 0.019097
2021-12-12 23:46:20,117 iteration 2039 : loss : 0.078390, loss_ce: 0.024561
2021-12-12 23:46:20,117 Training Data Eval:
2021-12-12 23:46:27,765   Average segmentation loss on training set: 0.0622
2021-12-12 23:46:27,765 Validation Data Eval:
2021-12-12 23:46:30,377   Average segmentation loss on validation set: 0.1067
2021-12-12 23:46:31,831 iteration 2040 : loss : 0.072806, loss_ce: 0.017939
 30%|████████▋                    | 120/400 [56:29<2:16:59, 29.36s/it]2021-12-12 23:46:33,350 iteration 2041 : loss : 0.069624, loss_ce: 0.019729
2021-12-12 23:46:34,821 iteration 2042 : loss : 0.066520, loss_ce: 0.019185
2021-12-12 23:46:36,315 iteration 2043 : loss : 0.067247, loss_ce: 0.021206
2021-12-12 23:46:37,860 iteration 2044 : loss : 0.076722, loss_ce: 0.022322
2021-12-12 23:46:39,438 iteration 2045 : loss : 0.070299, loss_ce: 0.017532
2021-12-12 23:46:40,969 iteration 2046 : loss : 0.075620, loss_ce: 0.021430
2021-12-12 23:46:42,484 iteration 2047 : loss : 0.071364, loss_ce: 0.017874
2021-12-12 23:46:43,898 iteration 2048 : loss : 0.065644, loss_ce: 0.016510
2021-12-12 23:46:45,363 iteration 2049 : loss : 0.068335, loss_ce: 0.017855
2021-12-12 23:46:46,881 iteration 2050 : loss : 0.074856, loss_ce: 0.017853
2021-12-12 23:46:48,409 iteration 2051 : loss : 0.072918, loss_ce: 0.019105
2021-12-12 23:46:49,862 iteration 2052 : loss : 0.074565, loss_ce: 0.025640
2021-12-12 23:46:51,310 iteration 2053 : loss : 0.064955, loss_ce: 0.015693
2021-12-12 23:46:52,806 iteration 2054 : loss : 0.074300, loss_ce: 0.020517
2021-12-12 23:46:54,320 iteration 2055 : loss : 0.081607, loss_ce: 0.029447
2021-12-12 23:46:55,824 iteration 2056 : loss : 0.073814, loss_ce: 0.022633
2021-12-12 23:46:57,349 iteration 2057 : loss : 0.072098, loss_ce: 0.021634
 30%|████████▊                    | 121/400 [56:54<2:11:09, 28.21s/it]2021-12-12 23:46:58,972 iteration 2058 : loss : 0.082140, loss_ce: 0.026949
2021-12-12 23:47:00,502 iteration 2059 : loss : 0.071847, loss_ce: 0.018825
2021-12-12 23:47:02,056 iteration 2060 : loss : 0.073780, loss_ce: 0.018726
2021-12-12 23:47:03,519 iteration 2061 : loss : 0.067181, loss_ce: 0.021311
2021-12-12 23:47:04,941 iteration 2062 : loss : 0.067269, loss_ce: 0.016362
2021-12-12 23:47:06,378 iteration 2063 : loss : 0.069149, loss_ce: 0.023197
2021-12-12 23:47:07,879 iteration 2064 : loss : 0.065628, loss_ce: 0.022023
2021-12-12 23:47:09,387 iteration 2065 : loss : 0.077052, loss_ce: 0.019860
2021-12-12 23:47:10,934 iteration 2066 : loss : 0.083547, loss_ce: 0.020556
2021-12-12 23:47:12,401 iteration 2067 : loss : 0.071769, loss_ce: 0.020525
2021-12-12 23:47:13,958 iteration 2068 : loss : 0.084229, loss_ce: 0.030613
2021-12-12 23:47:15,519 iteration 2069 : loss : 0.080644, loss_ce: 0.018507
2021-12-12 23:47:16,986 iteration 2070 : loss : 0.076757, loss_ce: 0.023518
2021-12-12 23:47:18,462 iteration 2071 : loss : 0.068242, loss_ce: 0.019413
2021-12-12 23:47:19,890 iteration 2072 : loss : 0.068016, loss_ce: 0.020456
2021-12-12 23:47:21,307 iteration 2073 : loss : 0.071395, loss_ce: 0.017332
2021-12-12 23:47:22,795 iteration 2074 : loss : 0.070419, loss_ce: 0.016651
 30%|████████▊                    | 122/400 [57:20<2:06:51, 27.38s/it]2021-12-12 23:47:24,340 iteration 2075 : loss : 0.068439, loss_ce: 0.017352
2021-12-12 23:47:25,902 iteration 2076 : loss : 0.080643, loss_ce: 0.021558
2021-12-12 23:47:27,418 iteration 2077 : loss : 0.074107, loss_ce: 0.021236
2021-12-12 23:47:28,858 iteration 2078 : loss : 0.069757, loss_ce: 0.022334
2021-12-12 23:47:30,322 iteration 2079 : loss : 0.071347, loss_ce: 0.018462
2021-12-12 23:47:31,874 iteration 2080 : loss : 0.073319, loss_ce: 0.019253
2021-12-12 23:47:33,393 iteration 2081 : loss : 0.074569, loss_ce: 0.023490
2021-12-12 23:47:34,893 iteration 2082 : loss : 0.080873, loss_ce: 0.021380
2021-12-12 23:47:36,369 iteration 2083 : loss : 0.067688, loss_ce: 0.017699
2021-12-12 23:47:37,853 iteration 2084 : loss : 0.074641, loss_ce: 0.019096
2021-12-12 23:47:39,308 iteration 2085 : loss : 0.072674, loss_ce: 0.021345
2021-12-12 23:47:40,788 iteration 2086 : loss : 0.067871, loss_ce: 0.021051
2021-12-12 23:47:42,225 iteration 2087 : loss : 0.078299, loss_ce: 0.017064
2021-12-12 23:47:43,836 iteration 2088 : loss : 0.078932, loss_ce: 0.029287
2021-12-12 23:47:45,309 iteration 2089 : loss : 0.068919, loss_ce: 0.018397
2021-12-12 23:47:46,852 iteration 2090 : loss : 0.072452, loss_ce: 0.019649
2021-12-12 23:47:48,340 iteration 2091 : loss : 0.069331, loss_ce: 0.020215
 31%|████████▉                    | 123/400 [57:45<2:03:50, 26.83s/it]2021-12-12 23:47:49,855 iteration 2092 : loss : 0.069027, loss_ce: 0.020008
2021-12-12 23:47:51,446 iteration 2093 : loss : 0.076267, loss_ce: 0.025494
2021-12-12 23:47:52,981 iteration 2094 : loss : 0.073205, loss_ce: 0.021414
2021-12-12 23:47:54,511 iteration 2095 : loss : 0.081757, loss_ce: 0.024672
2021-12-12 23:47:56,067 iteration 2096 : loss : 0.083082, loss_ce: 0.023989
2021-12-12 23:47:57,521 iteration 2097 : loss : 0.068548, loss_ce: 0.024094
2021-12-12 23:47:59,080 iteration 2098 : loss : 0.076326, loss_ce: 0.023669
2021-12-12 23:48:00,626 iteration 2099 : loss : 0.078836, loss_ce: 0.020535
2021-12-12 23:48:02,233 iteration 2100 : loss : 0.072855, loss_ce: 0.022254
2021-12-12 23:48:03,745 iteration 2101 : loss : 0.074123, loss_ce: 0.015916
2021-12-12 23:48:05,212 iteration 2102 : loss : 0.072720, loss_ce: 0.020594
2021-12-12 23:48:06,825 iteration 2103 : loss : 0.079722, loss_ce: 0.023064
2021-12-12 23:48:08,376 iteration 2104 : loss : 0.072817, loss_ce: 0.021706
2021-12-12 23:48:09,850 iteration 2105 : loss : 0.068953, loss_ce: 0.017087
2021-12-12 23:48:11,438 iteration 2106 : loss : 0.079225, loss_ce: 0.016940
2021-12-12 23:48:12,953 iteration 2107 : loss : 0.078169, loss_ce: 0.021667
2021-12-12 23:48:14,475 iteration 2108 : loss : 0.068725, loss_ce: 0.019063
 31%|████████▉                    | 124/400 [58:11<2:02:26, 26.62s/it]2021-12-12 23:48:16,020 iteration 2109 : loss : 0.078842, loss_ce: 0.022329
2021-12-12 23:48:17,507 iteration 2110 : loss : 0.065533, loss_ce: 0.014142
2021-12-12 23:48:19,142 iteration 2111 : loss : 0.091515, loss_ce: 0.025876
2021-12-12 23:48:20,784 iteration 2112 : loss : 0.069759, loss_ce: 0.019300
2021-12-12 23:48:22,258 iteration 2113 : loss : 0.072491, loss_ce: 0.020977
2021-12-12 23:48:23,735 iteration 2114 : loss : 0.071825, loss_ce: 0.017346
2021-12-12 23:48:25,109 iteration 2115 : loss : 0.071517, loss_ce: 0.017957
2021-12-12 23:48:26,628 iteration 2116 : loss : 0.074373, loss_ce: 0.020260
2021-12-12 23:48:28,203 iteration 2117 : loss : 0.078095, loss_ce: 0.019832
2021-12-12 23:48:29,699 iteration 2118 : loss : 0.079840, loss_ce: 0.028888
2021-12-12 23:48:31,227 iteration 2119 : loss : 0.071412, loss_ce: 0.017729
2021-12-12 23:48:32,727 iteration 2120 : loss : 0.079954, loss_ce: 0.023586
2021-12-12 23:48:34,328 iteration 2121 : loss : 0.083701, loss_ce: 0.026630
2021-12-12 23:48:35,779 iteration 2122 : loss : 0.073393, loss_ce: 0.022391
2021-12-12 23:48:37,243 iteration 2123 : loss : 0.075698, loss_ce: 0.029896
2021-12-12 23:48:38,776 iteration 2124 : loss : 0.083027, loss_ce: 0.025941
2021-12-12 23:48:38,776 Training Data Eval:
2021-12-12 23:48:46,433   Average segmentation loss on training set: 0.0624
2021-12-12 23:48:46,434 Validation Data Eval:
2021-12-12 23:48:49,044   Average segmentation loss on validation set: 0.1001
2021-12-12 23:48:55,700 Found new lowest validation loss at iteration 2124! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/TU_3seeds/TU_SGD_best_val_loss_seed100.pth
2021-12-12 23:48:57,112 iteration 2125 : loss : 0.069050, loss_ce: 0.017630
 31%|█████████                    | 125/400 [58:54<2:24:02, 31.43s/it]2021-12-12 23:48:58,525 iteration 2126 : loss : 0.067201, loss_ce: 0.017487
2021-12-12 23:48:59,990 iteration 2127 : loss : 0.070938, loss_ce: 0.020526
2021-12-12 23:49:01,322 iteration 2128 : loss : 0.065119, loss_ce: 0.022332
2021-12-12 23:49:02,798 iteration 2129 : loss : 0.065876, loss_ce: 0.017462
2021-12-12 23:49:04,235 iteration 2130 : loss : 0.068107, loss_ce: 0.016032
2021-12-12 23:49:05,823 iteration 2131 : loss : 0.081374, loss_ce: 0.026147
2021-12-12 23:49:07,409 iteration 2132 : loss : 0.077145, loss_ce: 0.021572
2021-12-12 23:49:08,963 iteration 2133 : loss : 0.079514, loss_ce: 0.024672
2021-12-12 23:49:10,524 iteration 2134 : loss : 0.083424, loss_ce: 0.022855
2021-12-12 23:49:12,045 iteration 2135 : loss : 0.084598, loss_ce: 0.022850
2021-12-12 23:49:13,556 iteration 2136 : loss : 0.083581, loss_ce: 0.025104
2021-12-12 23:49:15,064 iteration 2137 : loss : 0.074777, loss_ce: 0.016181
2021-12-12 23:49:16,529 iteration 2138 : loss : 0.077306, loss_ce: 0.017395
2021-12-12 23:49:17,949 iteration 2139 : loss : 0.070028, loss_ce: 0.021967
2021-12-12 23:49:19,409 iteration 2140 : loss : 0.072385, loss_ce: 0.021433
2021-12-12 23:49:20,916 iteration 2141 : loss : 0.078527, loss_ce: 0.024842
2021-12-12 23:49:22,414 iteration 2142 : loss : 0.087005, loss_ce: 0.032530
 32%|█████████▏                   | 126/400 [59:19<2:15:07, 29.59s/it]2021-12-12 23:49:23,987 iteration 2143 : loss : 0.077334, loss_ce: 0.017367
2021-12-12 23:49:25,477 iteration 2144 : loss : 0.077844, loss_ce: 0.018563
2021-12-12 23:49:26,883 iteration 2145 : loss : 0.065089, loss_ce: 0.022068
2021-12-12 23:49:28,542 iteration 2146 : loss : 0.078283, loss_ce: 0.022127
2021-12-12 23:49:30,065 iteration 2147 : loss : 0.062924, loss_ce: 0.016035
2021-12-12 23:49:31,575 iteration 2148 : loss : 0.075581, loss_ce: 0.023444
2021-12-12 23:49:33,121 iteration 2149 : loss : 0.073263, loss_ce: 0.021896
2021-12-12 23:49:34,689 iteration 2150 : loss : 0.078061, loss_ce: 0.019720
2021-12-12 23:49:36,259 iteration 2151 : loss : 0.081739, loss_ce: 0.023757
2021-12-12 23:49:37,701 iteration 2152 : loss : 0.064187, loss_ce: 0.017610
2021-12-12 23:49:39,169 iteration 2153 : loss : 0.071384, loss_ce: 0.015268
2021-12-12 23:49:40,589 iteration 2154 : loss : 0.067230, loss_ce: 0.022906
2021-12-12 23:49:42,040 iteration 2155 : loss : 0.067887, loss_ce: 0.019409
2021-12-12 23:49:43,574 iteration 2156 : loss : 0.077379, loss_ce: 0.022860
2021-12-12 23:49:45,085 iteration 2157 : loss : 0.072087, loss_ce: 0.019827
2021-12-12 23:49:46,553 iteration 2158 : loss : 0.071125, loss_ce: 0.019537
2021-12-12 23:49:48,103 iteration 2159 : loss : 0.080223, loss_ce: 0.022200
 32%|█████████▏                   | 127/400 [59:45<2:09:18, 28.42s/it]2021-12-12 23:49:49,562 iteration 2160 : loss : 0.064841, loss_ce: 0.016451
2021-12-12 23:49:51,077 iteration 2161 : loss : 0.070117, loss_ce: 0.020452
2021-12-12 23:49:52,576 iteration 2162 : loss : 0.072672, loss_ce: 0.019569
2021-12-12 23:49:53,965 iteration 2163 : loss : 0.063690, loss_ce: 0.013695
2021-12-12 23:49:55,473 iteration 2164 : loss : 0.074357, loss_ce: 0.022596
2021-12-12 23:49:56,969 iteration 2165 : loss : 0.063340, loss_ce: 0.014818
2021-12-12 23:49:58,521 iteration 2166 : loss : 0.081289, loss_ce: 0.032520
2021-12-12 23:50:00,022 iteration 2167 : loss : 0.066205, loss_ce: 0.021665
2021-12-12 23:50:01,512 iteration 2168 : loss : 0.070065, loss_ce: 0.018024
2021-12-12 23:50:03,034 iteration 2169 : loss : 0.069903, loss_ce: 0.019633
2021-12-12 23:50:04,504 iteration 2170 : loss : 0.068085, loss_ce: 0.016353
2021-12-12 23:50:05,988 iteration 2171 : loss : 0.059682, loss_ce: 0.014252
2021-12-12 23:50:07,370 iteration 2172 : loss : 0.065706, loss_ce: 0.021908
2021-12-12 23:50:08,862 iteration 2173 : loss : 0.069370, loss_ce: 0.017260
2021-12-12 23:50:10,394 iteration 2174 : loss : 0.078364, loss_ce: 0.020821
2021-12-12 23:50:11,830 iteration 2175 : loss : 0.074142, loss_ce: 0.017048
2021-12-12 23:50:13,291 iteration 2176 : loss : 0.074815, loss_ce: 0.026144
 32%|████████▋                  | 128/400 [1:00:10<2:04:25, 27.45s/it]2021-12-12 23:50:14,891 iteration 2177 : loss : 0.082698, loss_ce: 0.021896
2021-12-12 23:50:16,327 iteration 2178 : loss : 0.066245, loss_ce: 0.017731
2021-12-12 23:50:17,803 iteration 2179 : loss : 0.068945, loss_ce: 0.022206
2021-12-12 23:50:19,330 iteration 2180 : loss : 0.071202, loss_ce: 0.017990
2021-12-12 23:50:20,802 iteration 2181 : loss : 0.071142, loss_ce: 0.018701
2021-12-12 23:50:22,272 iteration 2182 : loss : 0.068222, loss_ce: 0.018010
2021-12-12 23:50:23,718 iteration 2183 : loss : 0.068724, loss_ce: 0.015859
2021-12-12 23:50:25,142 iteration 2184 : loss : 0.071367, loss_ce: 0.021142
2021-12-12 23:50:26,532 iteration 2185 : loss : 0.069068, loss_ce: 0.019417
2021-12-12 23:50:28,133 iteration 2186 : loss : 0.085293, loss_ce: 0.026964
2021-12-12 23:50:29,555 iteration 2187 : loss : 0.074985, loss_ce: 0.026512
2021-12-12 23:50:31,083 iteration 2188 : loss : 0.074700, loss_ce: 0.022441
2021-12-12 23:50:32,521 iteration 2189 : loss : 0.063465, loss_ce: 0.015902
2021-12-12 23:50:34,066 iteration 2190 : loss : 0.068954, loss_ce: 0.020540
2021-12-12 23:50:35,485 iteration 2191 : loss : 0.066900, loss_ce: 0.022118
2021-12-12 23:50:36,967 iteration 2192 : loss : 0.070744, loss_ce: 0.019596
2021-12-12 23:50:38,527 iteration 2193 : loss : 0.069807, loss_ce: 0.019981
 32%|████████▋                  | 129/400 [1:00:35<2:00:59, 26.79s/it]2021-12-12 23:50:40,027 iteration 2194 : loss : 0.076791, loss_ce: 0.021721
2021-12-12 23:50:41,479 iteration 2195 : loss : 0.068792, loss_ce: 0.016242
2021-12-12 23:50:42,983 iteration 2196 : loss : 0.071635, loss_ce: 0.022975
2021-12-12 23:50:44,509 iteration 2197 : loss : 0.073422, loss_ce: 0.020733
2021-12-12 23:50:45,972 iteration 2198 : loss : 0.067494, loss_ce: 0.023736
2021-12-12 23:50:47,627 iteration 2199 : loss : 0.076568, loss_ce: 0.024688
2021-12-12 23:50:49,114 iteration 2200 : loss : 0.065713, loss_ce: 0.018933
2021-12-12 23:50:50,614 iteration 2201 : loss : 0.074603, loss_ce: 0.018993
2021-12-12 23:50:52,097 iteration 2202 : loss : 0.077018, loss_ce: 0.017343
2021-12-12 23:50:53,629 iteration 2203 : loss : 0.066200, loss_ce: 0.016270
2021-12-12 23:50:55,174 iteration 2204 : loss : 0.065480, loss_ce: 0.017314
2021-12-12 23:50:56,556 iteration 2205 : loss : 0.063365, loss_ce: 0.018010
2021-12-12 23:50:57,968 iteration 2206 : loss : 0.066744, loss_ce: 0.023196
2021-12-12 23:50:59,458 iteration 2207 : loss : 0.077486, loss_ce: 0.022637
2021-12-12 23:51:00,931 iteration 2208 : loss : 0.068392, loss_ce: 0.021405
2021-12-12 23:51:02,484 iteration 2209 : loss : 0.070751, loss_ce: 0.018283
2021-12-12 23:51:02,484 Training Data Eval:
2021-12-12 23:51:10,130   Average segmentation loss on training set: 0.0567
2021-12-12 23:51:10,130 Validation Data Eval:
2021-12-12 23:51:12,727   Average segmentation loss on validation set: 0.1049
2021-12-12 23:51:14,226 iteration 2210 : loss : 0.067221, loss_ce: 0.016743
 32%|████████▊                  | 130/400 [1:01:11<2:12:33, 29.46s/it]2021-12-12 23:51:15,758 iteration 2211 : loss : 0.078610, loss_ce: 0.028792
2021-12-12 23:51:17,158 iteration 2212 : loss : 0.061346, loss_ce: 0.014688
2021-12-12 23:51:18,584 iteration 2213 : loss : 0.063779, loss_ce: 0.017435
2021-12-12 23:51:20,081 iteration 2214 : loss : 0.073334, loss_ce: 0.023305
2021-12-12 23:51:21,545 iteration 2215 : loss : 0.064429, loss_ce: 0.017749
2021-12-12 23:51:22,988 iteration 2216 : loss : 0.063669, loss_ce: 0.020401
2021-12-12 23:51:24,386 iteration 2217 : loss : 0.070667, loss_ce: 0.024887
2021-12-12 23:51:25,884 iteration 2218 : loss : 0.060666, loss_ce: 0.012985
2021-12-12 23:51:27,351 iteration 2219 : loss : 0.069222, loss_ce: 0.020562
2021-12-12 23:51:28,844 iteration 2220 : loss : 0.064904, loss_ce: 0.021412
2021-12-12 23:51:30,400 iteration 2221 : loss : 0.070957, loss_ce: 0.023232
2021-12-12 23:51:31,846 iteration 2222 : loss : 0.079229, loss_ce: 0.019362
2021-12-12 23:51:33,463 iteration 2223 : loss : 0.077894, loss_ce: 0.021378
2021-12-12 23:51:34,922 iteration 2224 : loss : 0.087150, loss_ce: 0.018395
2021-12-12 23:51:36,395 iteration 2225 : loss : 0.067203, loss_ce: 0.018675
2021-12-12 23:51:38,025 iteration 2226 : loss : 0.069323, loss_ce: 0.019294
2021-12-12 23:51:39,572 iteration 2227 : loss : 0.088842, loss_ce: 0.020823
 33%|████████▊                  | 131/400 [1:01:36<2:06:32, 28.23s/it]2021-12-12 23:51:41,159 iteration 2228 : loss : 0.083344, loss_ce: 0.024162
2021-12-12 23:51:42,674 iteration 2229 : loss : 0.072509, loss_ce: 0.020744
2021-12-12 23:51:44,135 iteration 2230 : loss : 0.071505, loss_ce: 0.019811
2021-12-12 23:51:45,571 iteration 2231 : loss : 0.060060, loss_ce: 0.017896
2021-12-12 23:51:47,003 iteration 2232 : loss : 0.067652, loss_ce: 0.017909
2021-12-12 23:51:48,444 iteration 2233 : loss : 0.064139, loss_ce: 0.012926
2021-12-12 23:51:49,959 iteration 2234 : loss : 0.076632, loss_ce: 0.018898
2021-12-12 23:51:51,378 iteration 2235 : loss : 0.068906, loss_ce: 0.020604
2021-12-12 23:51:52,819 iteration 2236 : loss : 0.076027, loss_ce: 0.021915
2021-12-12 23:51:54,295 iteration 2237 : loss : 0.067227, loss_ce: 0.018846
2021-12-12 23:51:55,746 iteration 2238 : loss : 0.073332, loss_ce: 0.018941
2021-12-12 23:51:57,252 iteration 2239 : loss : 0.068692, loss_ce: 0.019108
2021-12-12 23:51:58,660 iteration 2240 : loss : 0.065488, loss_ce: 0.018748
2021-12-12 23:52:00,179 iteration 2241 : loss : 0.071827, loss_ce: 0.022673
2021-12-12 23:52:01,604 iteration 2242 : loss : 0.071111, loss_ce: 0.022865
2021-12-12 23:52:03,031 iteration 2243 : loss : 0.067306, loss_ce: 0.013421
2021-12-12 23:52:04,494 iteration 2244 : loss : 0.068287, loss_ce: 0.018028
 33%|████████▉                  | 132/400 [1:02:01<2:01:38, 27.23s/it]2021-12-12 23:52:05,980 iteration 2245 : loss : 0.070520, loss_ce: 0.021963
2021-12-12 23:52:07,487 iteration 2246 : loss : 0.081098, loss_ce: 0.022905
2021-12-12 23:52:08,972 iteration 2247 : loss : 0.076908, loss_ce: 0.021079
2021-12-12 23:52:10,349 iteration 2248 : loss : 0.061017, loss_ce: 0.017826
2021-12-12 23:52:11,891 iteration 2249 : loss : 0.081658, loss_ce: 0.026142
2021-12-12 23:52:13,503 iteration 2250 : loss : 0.072928, loss_ce: 0.020429
2021-12-12 23:52:14,910 iteration 2251 : loss : 0.068439, loss_ce: 0.020745
2021-12-12 23:52:16,422 iteration 2252 : loss : 0.071304, loss_ce: 0.019654
2021-12-12 23:52:17,855 iteration 2253 : loss : 0.071139, loss_ce: 0.018334
2021-12-12 23:52:19,398 iteration 2254 : loss : 0.077303, loss_ce: 0.024645
2021-12-12 23:52:20,929 iteration 2255 : loss : 0.070487, loss_ce: 0.024050
2021-12-12 23:52:22,511 iteration 2256 : loss : 0.075536, loss_ce: 0.022696
2021-12-12 23:52:23,979 iteration 2257 : loss : 0.070178, loss_ce: 0.018153
2021-12-12 23:52:25,462 iteration 2258 : loss : 0.073233, loss_ce: 0.021233
2021-12-12 23:52:26,992 iteration 2259 : loss : 0.075048, loss_ce: 0.021314
2021-12-12 23:52:28,492 iteration 2260 : loss : 0.085647, loss_ce: 0.023275
2021-12-12 23:52:29,929 iteration 2261 : loss : 0.068015, loss_ce: 0.012946
 33%|████████▉                  | 133/400 [1:02:27<1:58:47, 26.70s/it]2021-12-12 23:52:31,594 iteration 2262 : loss : 0.081333, loss_ce: 0.024617
2021-12-12 23:52:33,105 iteration 2263 : loss : 0.079325, loss_ce: 0.026225
2021-12-12 23:52:34,628 iteration 2264 : loss : 0.070049, loss_ce: 0.016055
2021-12-12 23:52:36,154 iteration 2265 : loss : 0.074640, loss_ce: 0.023987
2021-12-12 23:52:37,639 iteration 2266 : loss : 0.075801, loss_ce: 0.019007
2021-12-12 23:52:39,131 iteration 2267 : loss : 0.074144, loss_ce: 0.019332
2021-12-12 23:52:40,538 iteration 2268 : loss : 0.063047, loss_ce: 0.020415
2021-12-12 23:52:42,041 iteration 2269 : loss : 0.072244, loss_ce: 0.024147
2021-12-12 23:52:43,480 iteration 2270 : loss : 0.075078, loss_ce: 0.018788
2021-12-12 23:52:44,931 iteration 2271 : loss : 0.082127, loss_ce: 0.036293
2021-12-12 23:52:46,361 iteration 2272 : loss : 0.062232, loss_ce: 0.012056
2021-12-12 23:52:47,893 iteration 2273 : loss : 0.074122, loss_ce: 0.018456
2021-12-12 23:52:49,315 iteration 2274 : loss : 0.068376, loss_ce: 0.016182
2021-12-12 23:52:50,776 iteration 2275 : loss : 0.064589, loss_ce: 0.019438
2021-12-12 23:52:52,267 iteration 2276 : loss : 0.085474, loss_ce: 0.022639
2021-12-12 23:52:53,724 iteration 2277 : loss : 0.068953, loss_ce: 0.020268
2021-12-12 23:52:55,204 iteration 2278 : loss : 0.062948, loss_ce: 0.019000
 34%|█████████                  | 134/400 [1:02:52<1:56:27, 26.27s/it]2021-12-12 23:52:56,827 iteration 2279 : loss : 0.079068, loss_ce: 0.027520
2021-12-12 23:52:58,348 iteration 2280 : loss : 0.066711, loss_ce: 0.021906
2021-12-12 23:52:59,751 iteration 2281 : loss : 0.063096, loss_ce: 0.016950
2021-12-12 23:53:01,352 iteration 2282 : loss : 0.087284, loss_ce: 0.027755
2021-12-12 23:53:02,795 iteration 2283 : loss : 0.061803, loss_ce: 0.015178
2021-12-12 23:53:04,387 iteration 2284 : loss : 0.085101, loss_ce: 0.023906
2021-12-12 23:53:05,832 iteration 2285 : loss : 0.065249, loss_ce: 0.018181
2021-12-12 23:53:07,296 iteration 2286 : loss : 0.060525, loss_ce: 0.014118
2021-12-12 23:53:08,821 iteration 2287 : loss : 0.063911, loss_ce: 0.014753
2021-12-12 23:53:10,223 iteration 2288 : loss : 0.076338, loss_ce: 0.023708
2021-12-12 23:53:11,727 iteration 2289 : loss : 0.069468, loss_ce: 0.017993
2021-12-12 23:53:13,223 iteration 2290 : loss : 0.068282, loss_ce: 0.017879
2021-12-12 23:53:14,693 iteration 2291 : loss : 0.066846, loss_ce: 0.017331
2021-12-12 23:53:16,245 iteration 2292 : loss : 0.072911, loss_ce: 0.017862
2021-12-12 23:53:17,657 iteration 2293 : loss : 0.068707, loss_ce: 0.021283
2021-12-12 23:53:19,120 iteration 2294 : loss : 0.071297, loss_ce: 0.024675
2021-12-12 23:53:19,120 Training Data Eval:
2021-12-12 23:53:26,760   Average segmentation loss on training set: 0.0552
2021-12-12 23:53:26,761 Validation Data Eval:
2021-12-12 23:53:29,364   Average segmentation loss on validation set: 0.1073
2021-12-12 23:53:30,843 iteration 2295 : loss : 0.061522, loss_ce: 0.016093
 34%|█████████                  | 135/400 [1:03:28<2:08:25, 29.08s/it]2021-12-12 23:53:32,366 iteration 2296 : loss : 0.078388, loss_ce: 0.025026
2021-12-12 23:53:33,849 iteration 2297 : loss : 0.066033, loss_ce: 0.015575
2021-12-12 23:53:35,456 iteration 2298 : loss : 0.081189, loss_ce: 0.023345
2021-12-12 23:53:36,929 iteration 2299 : loss : 0.072649, loss_ce: 0.014210
2021-12-12 23:53:38,501 iteration 2300 : loss : 0.072949, loss_ce: 0.019121
2021-12-12 23:53:39,981 iteration 2301 : loss : 0.077679, loss_ce: 0.027494
2021-12-12 23:53:41,452 iteration 2302 : loss : 0.065662, loss_ce: 0.021628
2021-12-12 23:53:42,891 iteration 2303 : loss : 0.066881, loss_ce: 0.017309
2021-12-12 23:53:44,288 iteration 2304 : loss : 0.066431, loss_ce: 0.018701
2021-12-12 23:53:45,804 iteration 2305 : loss : 0.073864, loss_ce: 0.017646
2021-12-12 23:53:47,235 iteration 2306 : loss : 0.079265, loss_ce: 0.020256
2021-12-12 23:53:48,651 iteration 2307 : loss : 0.062277, loss_ce: 0.016183
2021-12-12 23:53:50,101 iteration 2308 : loss : 0.067238, loss_ce: 0.022226
2021-12-12 23:53:51,609 iteration 2309 : loss : 0.073743, loss_ce: 0.025690
2021-12-12 23:53:53,122 iteration 2310 : loss : 0.078036, loss_ce: 0.028833
2021-12-12 23:53:54,704 iteration 2311 : loss : 0.068797, loss_ce: 0.018936
2021-12-12 23:53:56,206 iteration 2312 : loss : 0.069602, loss_ce: 0.020651
 34%|█████████▏                 | 136/400 [1:03:53<2:03:02, 27.96s/it]2021-12-12 23:53:57,726 iteration 2313 : loss : 0.068293, loss_ce: 0.023089
2021-12-12 23:53:59,154 iteration 2314 : loss : 0.075531, loss_ce: 0.024193
2021-12-12 23:54:00,642 iteration 2315 : loss : 0.063845, loss_ce: 0.016298
2021-12-12 23:54:02,164 iteration 2316 : loss : 0.072936, loss_ce: 0.025841
2021-12-12 23:54:03,563 iteration 2317 : loss : 0.061237, loss_ce: 0.013327
2021-12-12 23:54:05,111 iteration 2318 : loss : 0.067631, loss_ce: 0.022234
2021-12-12 23:54:06,684 iteration 2319 : loss : 0.086397, loss_ce: 0.022208
2021-12-12 23:54:08,120 iteration 2320 : loss : 0.072010, loss_ce: 0.025866
2021-12-12 23:54:09,601 iteration 2321 : loss : 0.066618, loss_ce: 0.015940
2021-12-12 23:54:11,099 iteration 2322 : loss : 0.075059, loss_ce: 0.022084
2021-12-12 23:54:12,665 iteration 2323 : loss : 0.076723, loss_ce: 0.020368
2021-12-12 23:54:14,172 iteration 2324 : loss : 0.070947, loss_ce: 0.016488
2021-12-12 23:54:15,661 iteration 2325 : loss : 0.074465, loss_ce: 0.028173
2021-12-12 23:54:17,188 iteration 2326 : loss : 0.068259, loss_ce: 0.024974
2021-12-12 23:54:18,657 iteration 2327 : loss : 0.064491, loss_ce: 0.017216
2021-12-12 23:54:20,239 iteration 2328 : loss : 0.069613, loss_ce: 0.017806
2021-12-12 23:54:21,717 iteration 2329 : loss : 0.069287, loss_ce: 0.019146
 34%|█████████▏                 | 137/400 [1:04:18<1:59:21, 27.23s/it]2021-12-12 23:54:23,230 iteration 2330 : loss : 0.064654, loss_ce: 0.018443
2021-12-12 23:54:24,662 iteration 2331 : loss : 0.068727, loss_ce: 0.014117
2021-12-12 23:54:26,236 iteration 2332 : loss : 0.081098, loss_ce: 0.028050
2021-12-12 23:54:27,655 iteration 2333 : loss : 0.068213, loss_ce: 0.026773
2021-12-12 23:54:29,128 iteration 2334 : loss : 0.064877, loss_ce: 0.011737
2021-12-12 23:54:30,728 iteration 2335 : loss : 0.071227, loss_ce: 0.023641
2021-12-12 23:54:32,182 iteration 2336 : loss : 0.066037, loss_ce: 0.023074
2021-12-12 23:54:33,668 iteration 2337 : loss : 0.064042, loss_ce: 0.017316
2021-12-12 23:54:35,195 iteration 2338 : loss : 0.064849, loss_ce: 0.016082
2021-12-12 23:54:36,683 iteration 2339 : loss : 0.081476, loss_ce: 0.027335
2021-12-12 23:54:38,211 iteration 2340 : loss : 0.069690, loss_ce: 0.021554
2021-12-12 23:54:39,702 iteration 2341 : loss : 0.074513, loss_ce: 0.020591
2021-12-12 23:54:41,227 iteration 2342 : loss : 0.073475, loss_ce: 0.019994
2021-12-12 23:54:42,728 iteration 2343 : loss : 0.065471, loss_ce: 0.018961
2021-12-12 23:54:44,127 iteration 2344 : loss : 0.062771, loss_ce: 0.016934
2021-12-12 23:54:45,571 iteration 2345 : loss : 0.068331, loss_ce: 0.020634
2021-12-12 23:54:46,977 iteration 2346 : loss : 0.064595, loss_ce: 0.018034
 34%|█████████▎                 | 138/400 [1:04:44<1:56:18, 26.64s/it]2021-12-12 23:54:48,556 iteration 2347 : loss : 0.079482, loss_ce: 0.022106
2021-12-12 23:54:50,012 iteration 2348 : loss : 0.066910, loss_ce: 0.016951
2021-12-12 23:54:51,519 iteration 2349 : loss : 0.077212, loss_ce: 0.029290
2021-12-12 23:54:52,918 iteration 2350 : loss : 0.073265, loss_ce: 0.027650
2021-12-12 23:54:54,420 iteration 2351 : loss : 0.074636, loss_ce: 0.018213
2021-12-12 23:54:56,004 iteration 2352 : loss : 0.075759, loss_ce: 0.022223
2021-12-12 23:54:57,406 iteration 2353 : loss : 0.067251, loss_ce: 0.023226
2021-12-12 23:54:58,898 iteration 2354 : loss : 0.070623, loss_ce: 0.016479
2021-12-12 23:55:00,322 iteration 2355 : loss : 0.062813, loss_ce: 0.018836
2021-12-12 23:55:01,909 iteration 2356 : loss : 0.088713, loss_ce: 0.028888
2021-12-12 23:55:03,469 iteration 2357 : loss : 0.071875, loss_ce: 0.025316
2021-12-12 23:55:05,050 iteration 2358 : loss : 0.069208, loss_ce: 0.017970
2021-12-12 23:55:06,596 iteration 2359 : loss : 0.089848, loss_ce: 0.015930
2021-12-12 23:55:07,981 iteration 2360 : loss : 0.074012, loss_ce: 0.010719
2021-12-12 23:55:09,396 iteration 2361 : loss : 0.071638, loss_ce: 0.019761
2021-12-12 23:55:10,878 iteration 2362 : loss : 0.070940, loss_ce: 0.022294
2021-12-12 23:55:12,362 iteration 2363 : loss : 0.065334, loss_ce: 0.017682
 35%|█████████▍                 | 139/400 [1:05:09<1:54:14, 26.26s/it]2021-12-12 23:55:13,891 iteration 2364 : loss : 0.075613, loss_ce: 0.024184
2021-12-12 23:55:15,306 iteration 2365 : loss : 0.069144, loss_ce: 0.018047
2021-12-12 23:55:16,805 iteration 2366 : loss : 0.065974, loss_ce: 0.021382
2021-12-12 23:55:18,256 iteration 2367 : loss : 0.071154, loss_ce: 0.016013
2021-12-12 23:55:19,837 iteration 2368 : loss : 0.077245, loss_ce: 0.017580
2021-12-12 23:55:21,310 iteration 2369 : loss : 0.066924, loss_ce: 0.017686
2021-12-12 23:55:22,747 iteration 2370 : loss : 0.058646, loss_ce: 0.014779
2021-12-12 23:55:24,237 iteration 2371 : loss : 0.066401, loss_ce: 0.018163
2021-12-12 23:55:25,649 iteration 2372 : loss : 0.070773, loss_ce: 0.024123
2021-12-12 23:55:27,136 iteration 2373 : loss : 0.059058, loss_ce: 0.014758
2021-12-12 23:55:28,673 iteration 2374 : loss : 0.069331, loss_ce: 0.020562
2021-12-12 23:55:30,100 iteration 2375 : loss : 0.060837, loss_ce: 0.015760
2021-12-12 23:55:31,635 iteration 2376 : loss : 0.065719, loss_ce: 0.019816
2021-12-12 23:55:33,116 iteration 2377 : loss : 0.075903, loss_ce: 0.018816
2021-12-12 23:55:34,713 iteration 2378 : loss : 0.081018, loss_ce: 0.029925
2021-12-12 23:55:36,240 iteration 2379 : loss : 0.078694, loss_ce: 0.017773
2021-12-12 23:55:36,240 Training Data Eval:
2021-12-12 23:55:43,890   Average segmentation loss on training set: 0.0571
2021-12-12 23:55:43,891 Validation Data Eval:
2021-12-12 23:55:46,499   Average segmentation loss on validation set: 0.1016
2021-12-12 23:55:48,109 iteration 2380 : loss : 0.068371, loss_ce: 0.018937
 35%|█████████▍                 | 140/400 [1:05:45<2:06:07, 29.10s/it]2021-12-12 23:55:49,589 iteration 2381 : loss : 0.069483, loss_ce: 0.017846
2021-12-12 23:55:51,130 iteration 2382 : loss : 0.065480, loss_ce: 0.023163
2021-12-12 23:55:52,676 iteration 2383 : loss : 0.071975, loss_ce: 0.023180
2021-12-12 23:55:54,167 iteration 2384 : loss : 0.071353, loss_ce: 0.017276
2021-12-12 23:55:55,674 iteration 2385 : loss : 0.068778, loss_ce: 0.021924
2021-12-12 23:55:57,251 iteration 2386 : loss : 0.067601, loss_ce: 0.017569
2021-12-12 23:55:58,706 iteration 2387 : loss : 0.081310, loss_ce: 0.023387
2021-12-12 23:56:00,218 iteration 2388 : loss : 0.059328, loss_ce: 0.016800
2021-12-12 23:56:01,657 iteration 2389 : loss : 0.063655, loss_ce: 0.017450
2021-12-12 23:56:03,149 iteration 2390 : loss : 0.064018, loss_ce: 0.020300
2021-12-12 23:56:04,591 iteration 2391 : loss : 0.060899, loss_ce: 0.016557
2021-12-12 23:56:06,013 iteration 2392 : loss : 0.060514, loss_ce: 0.017006
2021-12-12 23:56:07,437 iteration 2393 : loss : 0.065468, loss_ce: 0.019932
2021-12-12 23:56:08,930 iteration 2394 : loss : 0.088789, loss_ce: 0.015169
2021-12-12 23:56:10,559 iteration 2395 : loss : 0.073248, loss_ce: 0.019662
2021-12-12 23:56:12,131 iteration 2396 : loss : 0.062225, loss_ce: 0.018090
2021-12-12 23:56:13,617 iteration 2397 : loss : 0.077454, loss_ce: 0.019479
 35%|█████████▌                 | 141/400 [1:06:10<2:00:59, 28.03s/it]2021-12-12 23:56:15,280 iteration 2398 : loss : 0.075762, loss_ce: 0.026723
2021-12-12 23:56:16,699 iteration 2399 : loss : 0.059965, loss_ce: 0.015797
2021-12-12 23:56:18,170 iteration 2400 : loss : 0.071190, loss_ce: 0.015036
2021-12-12 23:56:19,666 iteration 2401 : loss : 0.070996, loss_ce: 0.020770
2021-12-12 23:56:21,129 iteration 2402 : loss : 0.073052, loss_ce: 0.022508
2021-12-12 23:56:22,613 iteration 2403 : loss : 0.064585, loss_ce: 0.018076
2021-12-12 23:56:24,071 iteration 2404 : loss : 0.069230, loss_ce: 0.025019
2021-12-12 23:56:25,555 iteration 2405 : loss : 0.077374, loss_ce: 0.021376
2021-12-12 23:56:27,116 iteration 2406 : loss : 0.062012, loss_ce: 0.018958
2021-12-12 23:56:28,551 iteration 2407 : loss : 0.066247, loss_ce: 0.015695
2021-12-12 23:56:30,031 iteration 2408 : loss : 0.069495, loss_ce: 0.020724
2021-12-12 23:56:31,527 iteration 2409 : loss : 0.071139, loss_ce: 0.019791
2021-12-12 23:56:33,061 iteration 2410 : loss : 0.071018, loss_ce: 0.015534
2021-12-12 23:56:34,493 iteration 2411 : loss : 0.060566, loss_ce: 0.014288
2021-12-12 23:56:36,049 iteration 2412 : loss : 0.078368, loss_ce: 0.022713
2021-12-12 23:56:37,639 iteration 2413 : loss : 0.069107, loss_ce: 0.024162
2021-12-12 23:56:39,193 iteration 2414 : loss : 0.074058, loss_ce: 0.022076
 36%|█████████▌                 | 142/400 [1:06:36<1:57:20, 27.29s/it]2021-12-12 23:56:40,746 iteration 2415 : loss : 0.096796, loss_ce: 0.026473
2021-12-12 23:56:42,236 iteration 2416 : loss : 0.070208, loss_ce: 0.024191
2021-12-12 23:56:43,752 iteration 2417 : loss : 0.065981, loss_ce: 0.016719
2021-12-12 23:56:45,284 iteration 2418 : loss : 0.080771, loss_ce: 0.020963
2021-12-12 23:56:46,789 iteration 2419 : loss : 0.066168, loss_ce: 0.022436
2021-12-12 23:56:48,226 iteration 2420 : loss : 0.066613, loss_ce: 0.019797
2021-12-12 23:56:49,845 iteration 2421 : loss : 0.064693, loss_ce: 0.015729
2021-12-12 23:56:51,382 iteration 2422 : loss : 0.073785, loss_ce: 0.018695
2021-12-12 23:56:52,903 iteration 2423 : loss : 0.072047, loss_ce: 0.025157
2021-12-12 23:56:54,338 iteration 2424 : loss : 0.071326, loss_ce: 0.019997
2021-12-12 23:56:55,918 iteration 2425 : loss : 0.081666, loss_ce: 0.028195
2021-12-12 23:56:57,409 iteration 2426 : loss : 0.066842, loss_ce: 0.019034
2021-12-12 23:56:59,020 iteration 2427 : loss : 0.070813, loss_ce: 0.018054
2021-12-12 23:57:00,608 iteration 2428 : loss : 0.085830, loss_ce: 0.026253
2021-12-12 23:57:02,212 iteration 2429 : loss : 0.073202, loss_ce: 0.021156
2021-12-12 23:57:03,735 iteration 2430 : loss : 0.074632, loss_ce: 0.025061
2021-12-12 23:57:05,187 iteration 2431 : loss : 0.064410, loss_ce: 0.022472
 36%|█████████▋                 | 143/400 [1:07:02<1:55:14, 26.91s/it]2021-12-12 23:57:06,772 iteration 2432 : loss : 0.066609, loss_ce: 0.020506
2021-12-12 23:57:08,288 iteration 2433 : loss : 0.081320, loss_ce: 0.023578
2021-12-12 23:57:09,792 iteration 2434 : loss : 0.060876, loss_ce: 0.014226
2021-12-12 23:57:11,428 iteration 2435 : loss : 0.082719, loss_ce: 0.032096
2021-12-12 23:57:12,835 iteration 2436 : loss : 0.057783, loss_ce: 0.015040
2021-12-12 23:57:14,257 iteration 2437 : loss : 0.057704, loss_ce: 0.015516
2021-12-12 23:57:15,895 iteration 2438 : loss : 0.076910, loss_ce: 0.023166
2021-12-12 23:57:17,415 iteration 2439 : loss : 0.061754, loss_ce: 0.018799
2021-12-12 23:57:18,889 iteration 2440 : loss : 0.069460, loss_ce: 0.019839
2021-12-12 23:57:20,352 iteration 2441 : loss : 0.066178, loss_ce: 0.015203
2021-12-12 23:57:21,794 iteration 2442 : loss : 0.070634, loss_ce: 0.017802
2021-12-12 23:57:23,212 iteration 2443 : loss : 0.063113, loss_ce: 0.021062
2021-12-12 23:57:24,737 iteration 2444 : loss : 0.061070, loss_ce: 0.015493
2021-12-12 23:57:26,123 iteration 2445 : loss : 0.067447, loss_ce: 0.011731
2021-12-12 23:57:27,712 iteration 2446 : loss : 0.065317, loss_ce: 0.017548
2021-12-12 23:57:29,290 iteration 2447 : loss : 0.070910, loss_ce: 0.025106
2021-12-12 23:57:30,793 iteration 2448 : loss : 0.062736, loss_ce: 0.016638
 36%|█████████▋                 | 144/400 [1:07:28<1:53:07, 26.51s/it]2021-12-12 23:57:32,302 iteration 2449 : loss : 0.067979, loss_ce: 0.022808
2021-12-12 23:57:33,875 iteration 2450 : loss : 0.072153, loss_ce: 0.018062
2021-12-12 23:57:35,449 iteration 2451 : loss : 0.076106, loss_ce: 0.022153
2021-12-12 23:57:36,873 iteration 2452 : loss : 0.061180, loss_ce: 0.019312
2021-12-12 23:57:38,353 iteration 2453 : loss : 0.068871, loss_ce: 0.014710
2021-12-12 23:57:39,884 iteration 2454 : loss : 0.068081, loss_ce: 0.022672
2021-12-12 23:57:41,418 iteration 2455 : loss : 0.072557, loss_ce: 0.024742
2021-12-12 23:57:43,055 iteration 2456 : loss : 0.089523, loss_ce: 0.026060
2021-12-12 23:57:44,561 iteration 2457 : loss : 0.063305, loss_ce: 0.016928
2021-12-12 23:57:46,144 iteration 2458 : loss : 0.078040, loss_ce: 0.020419
2021-12-12 23:57:47,594 iteration 2459 : loss : 0.078034, loss_ce: 0.019832
2021-12-12 23:57:49,079 iteration 2460 : loss : 0.072383, loss_ce: 0.016913
2021-12-12 23:57:50,592 iteration 2461 : loss : 0.072055, loss_ce: 0.019267
2021-12-12 23:57:52,062 iteration 2462 : loss : 0.061468, loss_ce: 0.020637
2021-12-12 23:57:53,582 iteration 2463 : loss : 0.063605, loss_ce: 0.018660
2021-12-12 23:57:55,168 iteration 2464 : loss : 0.065528, loss_ce: 0.021508
2021-12-12 23:57:55,168 Training Data Eval:
2021-12-12 23:58:02,823   Average segmentation loss on training set: 0.0568
2021-12-12 23:58:02,824 Validation Data Eval:
2021-12-12 23:58:05,433   Average segmentation loss on validation set: 0.1041
2021-12-12 23:58:06,890 iteration 2465 : loss : 0.062914, loss_ce: 0.016875
 36%|█████████▊                 | 145/400 [1:08:04<2:04:53, 29.39s/it]2021-12-12 23:58:08,394 iteration 2466 : loss : 0.062854, loss_ce: 0.017070
2021-12-12 23:58:09,938 iteration 2467 : loss : 0.073420, loss_ce: 0.029503
2021-12-12 23:58:11,385 iteration 2468 : loss : 0.060153, loss_ce: 0.017979
2021-12-12 23:58:12,802 iteration 2469 : loss : 0.060017, loss_ce: 0.019955
2021-12-12 23:58:14,340 iteration 2470 : loss : 0.085272, loss_ce: 0.024454
2021-12-12 23:58:15,893 iteration 2471 : loss : 0.080837, loss_ce: 0.024630
2021-12-12 23:58:17,493 iteration 2472 : loss : 0.066804, loss_ce: 0.019689
2021-12-12 23:58:18,914 iteration 2473 : loss : 0.067683, loss_ce: 0.018966
2021-12-12 23:58:20,440 iteration 2474 : loss : 0.072218, loss_ce: 0.019413
2021-12-12 23:58:21,873 iteration 2475 : loss : 0.060554, loss_ce: 0.019088
2021-12-12 23:58:23,449 iteration 2476 : loss : 0.080024, loss_ce: 0.016300
2021-12-12 23:58:24,934 iteration 2477 : loss : 0.065975, loss_ce: 0.020470
2021-12-12 23:58:26,411 iteration 2478 : loss : 0.061562, loss_ce: 0.014226
2021-12-12 23:58:28,032 iteration 2479 : loss : 0.068261, loss_ce: 0.016624
2021-12-12 23:58:29,604 iteration 2480 : loss : 0.088485, loss_ce: 0.024103
2021-12-12 23:58:31,077 iteration 2481 : loss : 0.067636, loss_ce: 0.020554
2021-12-12 23:58:32,578 iteration 2482 : loss : 0.069756, loss_ce: 0.019996
 36%|█████████▊                 | 146/400 [1:08:29<1:59:43, 28.28s/it]2021-12-12 23:58:34,124 iteration 2483 : loss : 0.061134, loss_ce: 0.017786
2021-12-12 23:58:35,689 iteration 2484 : loss : 0.077284, loss_ce: 0.019219
2021-12-12 23:58:37,285 iteration 2485 : loss : 0.072155, loss_ce: 0.018765
2021-12-12 23:58:38,737 iteration 2486 : loss : 0.062138, loss_ce: 0.019531
2021-12-12 23:58:40,202 iteration 2487 : loss : 0.064360, loss_ce: 0.017172
2021-12-12 23:58:41,757 iteration 2488 : loss : 0.062427, loss_ce: 0.012858
2021-12-12 23:58:43,317 iteration 2489 : loss : 0.068030, loss_ce: 0.019781
2021-12-12 23:58:44,786 iteration 2490 : loss : 0.067685, loss_ce: 0.019580
2021-12-12 23:58:46,299 iteration 2491 : loss : 0.068902, loss_ce: 0.020981
2021-12-12 23:58:47,756 iteration 2492 : loss : 0.066260, loss_ce: 0.018461
2021-12-12 23:58:49,226 iteration 2493 : loss : 0.060986, loss_ce: 0.015368
2021-12-12 23:58:50,661 iteration 2494 : loss : 0.066532, loss_ce: 0.022252
2021-12-12 23:58:52,138 iteration 2495 : loss : 0.066318, loss_ce: 0.015343
2021-12-12 23:58:53,682 iteration 2496 : loss : 0.081919, loss_ce: 0.021043
2021-12-12 23:58:55,169 iteration 2497 : loss : 0.077719, loss_ce: 0.024464
2021-12-12 23:58:56,570 iteration 2498 : loss : 0.069799, loss_ce: 0.022925
2021-12-12 23:58:58,130 iteration 2499 : loss : 0.072178, loss_ce: 0.026885
 37%|█████████▉                 | 147/400 [1:08:55<1:55:46, 27.46s/it]2021-12-12 23:58:59,694 iteration 2500 : loss : 0.069734, loss_ce: 0.018924
2021-12-12 23:59:01,134 iteration 2501 : loss : 0.062003, loss_ce: 0.016089
2021-12-12 23:59:02,616 iteration 2502 : loss : 0.064636, loss_ce: 0.018636
2021-12-12 23:59:04,116 iteration 2503 : loss : 0.066851, loss_ce: 0.022848
2021-12-12 23:59:05,646 iteration 2504 : loss : 0.074978, loss_ce: 0.020533
2021-12-12 23:59:07,077 iteration 2505 : loss : 0.058234, loss_ce: 0.015766
2021-12-12 23:59:08,529 iteration 2506 : loss : 0.063023, loss_ce: 0.012310
2021-12-12 23:59:10,066 iteration 2507 : loss : 0.059499, loss_ce: 0.016174
2021-12-12 23:59:11,537 iteration 2508 : loss : 0.066574, loss_ce: 0.023443
2021-12-12 23:59:13,092 iteration 2509 : loss : 0.075896, loss_ce: 0.022953
2021-12-12 23:59:14,563 iteration 2510 : loss : 0.064396, loss_ce: 0.016875
2021-12-12 23:59:15,992 iteration 2511 : loss : 0.059629, loss_ce: 0.019406
2021-12-12 23:59:17,540 iteration 2512 : loss : 0.071165, loss_ce: 0.017314
2021-12-12 23:59:18,998 iteration 2513 : loss : 0.060542, loss_ce: 0.017750
2021-12-12 23:59:20,530 iteration 2514 : loss : 0.067778, loss_ce: 0.015925
2021-12-12 23:59:21,993 iteration 2515 : loss : 0.068440, loss_ce: 0.022696
2021-12-12 23:59:23,660 iteration 2516 : loss : 0.075048, loss_ce: 0.023474
 37%|█████████▉                 | 148/400 [1:09:20<1:52:54, 26.88s/it]2021-12-12 23:59:25,220 iteration 2517 : loss : 0.067141, loss_ce: 0.017625
2021-12-12 23:59:26,759 iteration 2518 : loss : 0.068534, loss_ce: 0.015192
2021-12-12 23:59:28,187 iteration 2519 : loss : 0.057836, loss_ce: 0.013987
2021-12-12 23:59:29,722 iteration 2520 : loss : 0.070167, loss_ce: 0.025183
2021-12-12 23:59:31,230 iteration 2521 : loss : 0.067587, loss_ce: 0.023747
2021-12-12 23:59:32,752 iteration 2522 : loss : 0.065841, loss_ce: 0.019748
2021-12-12 23:59:34,255 iteration 2523 : loss : 0.059949, loss_ce: 0.013583
2021-12-12 23:59:35,803 iteration 2524 : loss : 0.069118, loss_ce: 0.022659
2021-12-12 23:59:37,264 iteration 2525 : loss : 0.062944, loss_ce: 0.014166
2021-12-12 23:59:38,693 iteration 2526 : loss : 0.065757, loss_ce: 0.017525
2021-12-12 23:59:40,205 iteration 2527 : loss : 0.070579, loss_ce: 0.023545
2021-12-12 23:59:41,689 iteration 2528 : loss : 0.065204, loss_ce: 0.022603
2021-12-12 23:59:43,211 iteration 2529 : loss : 0.069192, loss_ce: 0.020670
2021-12-12 23:59:44,630 iteration 2530 : loss : 0.063571, loss_ce: 0.017864
2021-12-12 23:59:46,143 iteration 2531 : loss : 0.069154, loss_ce: 0.017000
2021-12-12 23:59:47,729 iteration 2532 : loss : 0.072336, loss_ce: 0.026196
2021-12-12 23:59:49,156 iteration 2533 : loss : 0.063884, loss_ce: 0.012200
 37%|██████████                 | 149/400 [1:09:46<1:50:42, 26.46s/it]2021-12-12 23:59:50,697 iteration 2534 : loss : 0.058762, loss_ce: 0.015255
2021-12-12 23:59:52,084 iteration 2535 : loss : 0.067385, loss_ce: 0.020674
2021-12-12 23:59:53,522 iteration 2536 : loss : 0.055463, loss_ce: 0.016173
2021-12-12 23:59:55,110 iteration 2537 : loss : 0.071123, loss_ce: 0.025914
2021-12-12 23:59:56,467 iteration 2538 : loss : 0.063167, loss_ce: 0.017640
2021-12-12 23:59:57,874 iteration 2539 : loss : 0.060827, loss_ce: 0.018004
2021-12-12 23:59:59,350 iteration 2540 : loss : 0.064694, loss_ce: 0.022741
2021-12-13 00:00:00,909 iteration 2541 : loss : 0.075521, loss_ce: 0.022843
2021-12-13 00:00:02,397 iteration 2542 : loss : 0.068706, loss_ce: 0.015392
2021-12-13 00:00:03,980 iteration 2543 : loss : 0.095636, loss_ce: 0.024581
2021-12-13 00:00:05,561 iteration 2544 : loss : 0.086297, loss_ce: 0.022992
2021-12-13 00:00:07,005 iteration 2545 : loss : 0.059856, loss_ce: 0.019086
2021-12-13 00:00:08,697 iteration 2546 : loss : 0.073760, loss_ce: 0.020740
2021-12-13 00:00:10,254 iteration 2547 : loss : 0.077569, loss_ce: 0.023857
2021-12-13 00:00:11,843 iteration 2548 : loss : 0.075838, loss_ce: 0.024981
2021-12-13 00:00:13,330 iteration 2549 : loss : 0.071480, loss_ce: 0.019037
2021-12-13 00:00:13,331 Training Data Eval:
2021-12-13 00:00:20,986   Average segmentation loss on training set: 0.0554
2021-12-13 00:00:20,987 Validation Data Eval:
2021-12-13 00:00:23,585   Average segmentation loss on validation set: 0.1081
2021-12-13 00:00:25,070 iteration 2550 : loss : 0.070363, loss_ce: 0.012092
 38%|██████████▏                | 150/400 [1:10:22<2:02:04, 29.30s/it]2021-12-13 00:00:26,605 iteration 2551 : loss : 0.075061, loss_ce: 0.021515
2021-12-13 00:00:28,114 iteration 2552 : loss : 0.057395, loss_ce: 0.017821
2021-12-13 00:00:29,629 iteration 2553 : loss : 0.070247, loss_ce: 0.019748
2021-12-13 00:00:31,108 iteration 2554 : loss : 0.065150, loss_ce: 0.019955
2021-12-13 00:00:32,566 iteration 2555 : loss : 0.068477, loss_ce: 0.019031
2021-12-13 00:00:34,057 iteration 2556 : loss : 0.063799, loss_ce: 0.019006
2021-12-13 00:00:35,540 iteration 2557 : loss : 0.072843, loss_ce: 0.021669
2021-12-13 00:00:37,003 iteration 2558 : loss : 0.065469, loss_ce: 0.016073
2021-12-13 00:00:38,423 iteration 2559 : loss : 0.082024, loss_ce: 0.029125
2021-12-13 00:00:40,011 iteration 2560 : loss : 0.071534, loss_ce: 0.018931
2021-12-13 00:00:41,499 iteration 2561 : loss : 0.071130, loss_ce: 0.017462
2021-12-13 00:00:43,019 iteration 2562 : loss : 0.060579, loss_ce: 0.017779
2021-12-13 00:00:44,431 iteration 2563 : loss : 0.065199, loss_ce: 0.021565
2021-12-13 00:00:45,868 iteration 2564 : loss : 0.060697, loss_ce: 0.018885
2021-12-13 00:00:47,305 iteration 2565 : loss : 0.061189, loss_ce: 0.017899
2021-12-13 00:00:48,709 iteration 2566 : loss : 0.060952, loss_ce: 0.016429
2021-12-13 00:00:50,169 iteration 2567 : loss : 0.081846, loss_ce: 0.019638
 38%|██████████▏                | 151/400 [1:10:47<1:56:22, 28.04s/it]2021-12-13 00:00:51,673 iteration 2568 : loss : 0.060262, loss_ce: 0.018593
2021-12-13 00:00:53,144 iteration 2569 : loss : 0.058224, loss_ce: 0.016647
2021-12-13 00:00:54,722 iteration 2570 : loss : 0.069542, loss_ce: 0.018305
2021-12-13 00:00:56,206 iteration 2571 : loss : 0.062702, loss_ce: 0.014525
2021-12-13 00:00:57,612 iteration 2572 : loss : 0.061940, loss_ce: 0.015713
2021-12-13 00:00:59,070 iteration 2573 : loss : 0.061722, loss_ce: 0.018431
2021-12-13 00:01:00,505 iteration 2574 : loss : 0.061018, loss_ce: 0.017628
2021-12-13 00:01:01,912 iteration 2575 : loss : 0.062476, loss_ce: 0.015142
2021-12-13 00:01:03,322 iteration 2576 : loss : 0.062151, loss_ce: 0.013592
2021-12-13 00:01:04,836 iteration 2577 : loss : 0.074405, loss_ce: 0.027564
2021-12-13 00:01:06,298 iteration 2578 : loss : 0.070010, loss_ce: 0.019292
2021-12-13 00:01:07,793 iteration 2579 : loss : 0.072175, loss_ce: 0.023337
2021-12-13 00:01:09,217 iteration 2580 : loss : 0.066106, loss_ce: 0.014060
2021-12-13 00:01:10,722 iteration 2581 : loss : 0.064510, loss_ce: 0.021676
2021-12-13 00:01:12,276 iteration 2582 : loss : 0.058955, loss_ce: 0.019509
2021-12-13 00:01:13,725 iteration 2583 : loss : 0.066582, loss_ce: 0.023335
2021-12-13 00:01:15,187 iteration 2584 : loss : 0.066264, loss_ce: 0.017899
 38%|██████████▎                | 152/400 [1:11:12<1:52:09, 27.13s/it]2021-12-13 00:01:16,710 iteration 2585 : loss : 0.067129, loss_ce: 0.018331
2021-12-13 00:01:18,389 iteration 2586 : loss : 0.076866, loss_ce: 0.018678
2021-12-13 00:01:19,967 iteration 2587 : loss : 0.067897, loss_ce: 0.017436
2021-12-13 00:01:21,467 iteration 2588 : loss : 0.074651, loss_ce: 0.021577
2021-12-13 00:01:22,882 iteration 2589 : loss : 0.073303, loss_ce: 0.016623
2021-12-13 00:01:24,419 iteration 2590 : loss : 0.064824, loss_ce: 0.015498
2021-12-13 00:01:25,913 iteration 2591 : loss : 0.068473, loss_ce: 0.021571
2021-12-13 00:01:27,358 iteration 2592 : loss : 0.061821, loss_ce: 0.022581
2021-12-13 00:01:28,743 iteration 2593 : loss : 0.062843, loss_ce: 0.021416
2021-12-13 00:01:30,227 iteration 2594 : loss : 0.074036, loss_ce: 0.025844
2021-12-13 00:01:31,648 iteration 2595 : loss : 0.063407, loss_ce: 0.016504
2021-12-13 00:01:33,099 iteration 2596 : loss : 0.070911, loss_ce: 0.015787
2021-12-13 00:01:34,570 iteration 2597 : loss : 0.070555, loss_ce: 0.019860
2021-12-13 00:01:36,064 iteration 2598 : loss : 0.064227, loss_ce: 0.019707
2021-12-13 00:01:37,559 iteration 2599 : loss : 0.069108, loss_ce: 0.021989
2021-12-13 00:01:39,006 iteration 2600 : loss : 0.070320, loss_ce: 0.021276
2021-12-13 00:01:40,427 iteration 2601 : loss : 0.069387, loss_ce: 0.018202
 38%|██████████▎                | 153/400 [1:11:37<1:49:21, 26.57s/it]2021-12-13 00:01:41,925 iteration 2602 : loss : 0.066304, loss_ce: 0.019038
2021-12-13 00:01:43,428 iteration 2603 : loss : 0.075199, loss_ce: 0.013770
2021-12-13 00:01:44,917 iteration 2604 : loss : 0.057205, loss_ce: 0.014056
2021-12-13 00:01:46,396 iteration 2605 : loss : 0.066848, loss_ce: 0.016502
2021-12-13 00:01:47,891 iteration 2606 : loss : 0.064639, loss_ce: 0.017008
2021-12-13 00:01:49,395 iteration 2607 : loss : 0.064486, loss_ce: 0.021563
2021-12-13 00:01:50,828 iteration 2608 : loss : 0.067731, loss_ce: 0.018211
2021-12-13 00:01:52,352 iteration 2609 : loss : 0.061341, loss_ce: 0.017378
2021-12-13 00:01:53,869 iteration 2610 : loss : 0.070096, loss_ce: 0.019723
2021-12-13 00:01:55,423 iteration 2611 : loss : 0.070708, loss_ce: 0.024227
2021-12-13 00:01:56,953 iteration 2612 : loss : 0.073553, loss_ce: 0.019463
2021-12-13 00:01:58,523 iteration 2613 : loss : 0.071673, loss_ce: 0.021029
2021-12-13 00:02:00,067 iteration 2614 : loss : 0.076641, loss_ce: 0.025117
2021-12-13 00:02:01,579 iteration 2615 : loss : 0.083032, loss_ce: 0.031181
2021-12-13 00:02:03,135 iteration 2616 : loss : 0.064970, loss_ce: 0.017854
2021-12-13 00:02:04,648 iteration 2617 : loss : 0.068401, loss_ce: 0.018812
2021-12-13 00:02:06,191 iteration 2618 : loss : 0.061299, loss_ce: 0.015061
 38%|██████████▍                | 154/400 [1:12:03<1:47:56, 26.33s/it]2021-12-13 00:02:07,862 iteration 2619 : loss : 0.067896, loss_ce: 0.021181
2021-12-13 00:02:09,393 iteration 2620 : loss : 0.068322, loss_ce: 0.019436
2021-12-13 00:02:10,775 iteration 2621 : loss : 0.060035, loss_ce: 0.018673
2021-12-13 00:02:12,287 iteration 2622 : loss : 0.075847, loss_ce: 0.019654
2021-12-13 00:02:13,737 iteration 2623 : loss : 0.060656, loss_ce: 0.018510
2021-12-13 00:02:15,282 iteration 2624 : loss : 0.072803, loss_ce: 0.022339
2021-12-13 00:02:16,832 iteration 2625 : loss : 0.071168, loss_ce: 0.023235
2021-12-13 00:02:18,354 iteration 2626 : loss : 0.064536, loss_ce: 0.019240
2021-12-13 00:02:19,767 iteration 2627 : loss : 0.062505, loss_ce: 0.022870
2021-12-13 00:02:21,199 iteration 2628 : loss : 0.062422, loss_ce: 0.018240
2021-12-13 00:02:22,591 iteration 2629 : loss : 0.070876, loss_ce: 0.014946
2021-12-13 00:02:24,111 iteration 2630 : loss : 0.091613, loss_ce: 0.012474
2021-12-13 00:02:25,524 iteration 2631 : loss : 0.064137, loss_ce: 0.024695
2021-12-13 00:02:26,957 iteration 2632 : loss : 0.060972, loss_ce: 0.014065
2021-12-13 00:02:28,457 iteration 2633 : loss : 0.065596, loss_ce: 0.018114
2021-12-13 00:02:29,920 iteration 2634 : loss : 0.066190, loss_ce: 0.017665
2021-12-13 00:02:29,920 Training Data Eval:
2021-12-13 00:02:37,578   Average segmentation loss on training set: 0.0514
2021-12-13 00:02:37,578 Validation Data Eval:
2021-12-13 00:02:40,182   Average segmentation loss on validation set: 0.1011
2021-12-13 00:02:41,631 iteration 2635 : loss : 0.069100, loss_ce: 0.021897
 39%|██████████▍                | 155/400 [1:12:38<1:58:39, 29.06s/it]2021-12-13 00:02:43,103 iteration 2636 : loss : 0.059653, loss_ce: 0.015775
2021-12-13 00:02:44,565 iteration 2637 : loss : 0.061263, loss_ce: 0.020652
2021-12-13 00:02:46,136 iteration 2638 : loss : 0.083217, loss_ce: 0.023008
2021-12-13 00:02:47,610 iteration 2639 : loss : 0.062204, loss_ce: 0.018514
2021-12-13 00:02:49,113 iteration 2640 : loss : 0.065613, loss_ce: 0.016085
2021-12-13 00:02:50,587 iteration 2641 : loss : 0.060181, loss_ce: 0.016688
2021-12-13 00:02:52,098 iteration 2642 : loss : 0.062897, loss_ce: 0.015355
2021-12-13 00:02:53,562 iteration 2643 : loss : 0.059455, loss_ce: 0.016393
2021-12-13 00:02:55,070 iteration 2644 : loss : 0.064915, loss_ce: 0.020123
2021-12-13 00:02:56,495 iteration 2645 : loss : 0.063335, loss_ce: 0.020924
2021-12-13 00:02:57,926 iteration 2646 : loss : 0.065676, loss_ce: 0.016124
2021-12-13 00:02:59,485 iteration 2647 : loss : 0.066824, loss_ce: 0.021535
2021-12-13 00:03:00,938 iteration 2648 : loss : 0.073740, loss_ce: 0.018671
2021-12-13 00:03:02,426 iteration 2649 : loss : 0.081509, loss_ce: 0.021926
2021-12-13 00:03:04,037 iteration 2650 : loss : 0.073684, loss_ce: 0.021424
2021-12-13 00:03:05,501 iteration 2651 : loss : 0.063560, loss_ce: 0.018868
2021-12-13 00:03:07,063 iteration 2652 : loss : 0.058117, loss_ce: 0.017472
 39%|██████████▌                | 156/400 [1:13:04<1:53:44, 27.97s/it]2021-12-13 00:03:08,652 iteration 2653 : loss : 0.074644, loss_ce: 0.021608
2021-12-13 00:03:10,072 iteration 2654 : loss : 0.061187, loss_ce: 0.018171
2021-12-13 00:03:11,534 iteration 2655 : loss : 0.061318, loss_ce: 0.020347
2021-12-13 00:03:12,996 iteration 2656 : loss : 0.065271, loss_ce: 0.024430
2021-12-13 00:03:14,507 iteration 2657 : loss : 0.069766, loss_ce: 0.022705
2021-12-13 00:03:16,056 iteration 2658 : loss : 0.076341, loss_ce: 0.018931
2021-12-13 00:03:17,571 iteration 2659 : loss : 0.065468, loss_ce: 0.018740
2021-12-13 00:03:19,155 iteration 2660 : loss : 0.066319, loss_ce: 0.020817
2021-12-13 00:03:20,550 iteration 2661 : loss : 0.059308, loss_ce: 0.014623
2021-12-13 00:03:22,108 iteration 2662 : loss : 0.059295, loss_ce: 0.015839
2021-12-13 00:03:23,647 iteration 2663 : loss : 0.081361, loss_ce: 0.026814
2021-12-13 00:03:25,068 iteration 2664 : loss : 0.066716, loss_ce: 0.020121
2021-12-13 00:03:26,515 iteration 2665 : loss : 0.056335, loss_ce: 0.016304
2021-12-13 00:03:28,084 iteration 2666 : loss : 0.070606, loss_ce: 0.019226
2021-12-13 00:03:29,527 iteration 2667 : loss : 0.064529, loss_ce: 0.017775
2021-12-13 00:03:31,091 iteration 2668 : loss : 0.069481, loss_ce: 0.023107
2021-12-13 00:03:32,565 iteration 2669 : loss : 0.060646, loss_ce: 0.013132
 39%|██████████▌                | 157/400 [1:13:29<1:50:17, 27.23s/it]2021-12-13 00:03:34,014 iteration 2670 : loss : 0.055495, loss_ce: 0.015219
2021-12-13 00:03:35,423 iteration 2671 : loss : 0.059857, loss_ce: 0.017460
2021-12-13 00:03:36,863 iteration 2672 : loss : 0.063557, loss_ce: 0.016129
2021-12-13 00:03:38,305 iteration 2673 : loss : 0.061092, loss_ce: 0.013227
2021-12-13 00:03:39,815 iteration 2674 : loss : 0.072034, loss_ce: 0.020005
2021-12-13 00:03:41,353 iteration 2675 : loss : 0.058616, loss_ce: 0.013826
2021-12-13 00:03:42,817 iteration 2676 : loss : 0.058541, loss_ce: 0.015067
2021-12-13 00:03:44,292 iteration 2677 : loss : 0.060703, loss_ce: 0.017618
2021-12-13 00:03:45,696 iteration 2678 : loss : 0.063368, loss_ce: 0.022398
2021-12-13 00:03:47,190 iteration 2679 : loss : 0.061633, loss_ce: 0.020297
2021-12-13 00:03:48,752 iteration 2680 : loss : 0.063755, loss_ce: 0.017421
2021-12-13 00:03:50,251 iteration 2681 : loss : 0.063409, loss_ce: 0.016792
2021-12-13 00:03:51,787 iteration 2682 : loss : 0.060059, loss_ce: 0.017512
2021-12-13 00:03:53,244 iteration 2683 : loss : 0.069948, loss_ce: 0.025069
2021-12-13 00:03:54,756 iteration 2684 : loss : 0.059502, loss_ce: 0.019913
2021-12-13 00:03:56,295 iteration 2685 : loss : 0.066652, loss_ce: 0.022697
2021-12-13 00:03:57,830 iteration 2686 : loss : 0.087116, loss_ce: 0.016368
 40%|██████████▋                | 158/400 [1:13:55<1:47:27, 26.64s/it]2021-12-13 00:03:59,324 iteration 2687 : loss : 0.064647, loss_ce: 0.019614
2021-12-13 00:04:00,789 iteration 2688 : loss : 0.065175, loss_ce: 0.017539
2021-12-13 00:04:02,227 iteration 2689 : loss : 0.059637, loss_ce: 0.013563
2021-12-13 00:04:03,770 iteration 2690 : loss : 0.081646, loss_ce: 0.014284
2021-12-13 00:04:05,206 iteration 2691 : loss : 0.065661, loss_ce: 0.022514
2021-12-13 00:04:06,644 iteration 2692 : loss : 0.065356, loss_ce: 0.018784
2021-12-13 00:04:08,074 iteration 2693 : loss : 0.056666, loss_ce: 0.015414
2021-12-13 00:04:09,577 iteration 2694 : loss : 0.059644, loss_ce: 0.016375
2021-12-13 00:04:11,055 iteration 2695 : loss : 0.061976, loss_ce: 0.022652
2021-12-13 00:04:12,534 iteration 2696 : loss : 0.061611, loss_ce: 0.018939
2021-12-13 00:04:13,978 iteration 2697 : loss : 0.065081, loss_ce: 0.021777
2021-12-13 00:04:15,415 iteration 2698 : loss : 0.059180, loss_ce: 0.013206
2021-12-13 00:04:16,882 iteration 2699 : loss : 0.061502, loss_ce: 0.020391
2021-12-13 00:04:18,319 iteration 2700 : loss : 0.057758, loss_ce: 0.015016
2021-12-13 00:04:19,930 iteration 2701 : loss : 0.066970, loss_ce: 0.017597
2021-12-13 00:04:21,465 iteration 2702 : loss : 0.076751, loss_ce: 0.032042
2021-12-13 00:04:23,031 iteration 2703 : loss : 0.070915, loss_ce: 0.023887
 40%|██████████▋                | 159/400 [1:14:20<1:45:16, 26.21s/it]2021-12-13 00:04:24,642 iteration 2704 : loss : 0.073582, loss_ce: 0.019661
2021-12-13 00:04:26,082 iteration 2705 : loss : 0.059223, loss_ce: 0.020051
2021-12-13 00:04:27,625 iteration 2706 : loss : 0.072232, loss_ce: 0.017925
2021-12-13 00:04:29,094 iteration 2707 : loss : 0.060705, loss_ce: 0.021187
2021-12-13 00:04:30,626 iteration 2708 : loss : 0.067050, loss_ce: 0.021361
2021-12-13 00:04:32,111 iteration 2709 : loss : 0.059300, loss_ce: 0.018060
2021-12-13 00:04:33,530 iteration 2710 : loss : 0.057795, loss_ce: 0.018419
2021-12-13 00:04:35,041 iteration 2711 : loss : 0.062252, loss_ce: 0.017772
2021-12-13 00:04:36,571 iteration 2712 : loss : 0.058589, loss_ce: 0.012728
2021-12-13 00:04:38,012 iteration 2713 : loss : 0.059235, loss_ce: 0.021717
2021-12-13 00:04:39,572 iteration 2714 : loss : 0.065591, loss_ce: 0.019751
2021-12-13 00:04:41,112 iteration 2715 : loss : 0.072997, loss_ce: 0.018340
2021-12-13 00:04:42,653 iteration 2716 : loss : 0.063973, loss_ce: 0.017149
2021-12-13 00:04:44,138 iteration 2717 : loss : 0.066820, loss_ce: 0.020964
2021-12-13 00:04:45,683 iteration 2718 : loss : 0.071136, loss_ce: 0.017956
2021-12-13 00:04:47,150 iteration 2719 : loss : 0.065855, loss_ce: 0.019304
2021-12-13 00:04:47,150 Training Data Eval:
2021-12-13 00:04:54,805   Average segmentation loss on training set: 0.0503
2021-12-13 00:04:54,805 Validation Data Eval:
2021-12-13 00:04:57,405   Average segmentation loss on validation set: 0.0989
2021-12-13 00:05:03,783 Found new lowest validation loss at iteration 2719! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/TU_3seeds/TU_SGD_best_val_loss_seed100.pth
2021-12-13 00:05:05,101 iteration 2720 : loss : 0.055818, loss_ce: 0.014634
 40%|██████████▊                | 160/400 [1:15:02<2:03:51, 30.96s/it]2021-12-13 00:05:06,536 iteration 2721 : loss : 0.062126, loss_ce: 0.020445
2021-12-13 00:05:07,920 iteration 2722 : loss : 0.065847, loss_ce: 0.019773
2021-12-13 00:05:09,334 iteration 2723 : loss : 0.059426, loss_ce: 0.015857
2021-12-13 00:05:10,809 iteration 2724 : loss : 0.070503, loss_ce: 0.020085
2021-12-13 00:05:12,275 iteration 2725 : loss : 0.068172, loss_ce: 0.018517
2021-12-13 00:05:13,913 iteration 2726 : loss : 0.086078, loss_ce: 0.021012
2021-12-13 00:05:15,461 iteration 2727 : loss : 0.070051, loss_ce: 0.023106
2021-12-13 00:05:16,957 iteration 2728 : loss : 0.062060, loss_ce: 0.018374
2021-12-13 00:05:18,441 iteration 2729 : loss : 0.062590, loss_ce: 0.019157
2021-12-13 00:05:19,855 iteration 2730 : loss : 0.061779, loss_ce: 0.016724
2021-12-13 00:05:21,353 iteration 2731 : loss : 0.063046, loss_ce: 0.016575
2021-12-13 00:05:22,917 iteration 2732 : loss : 0.070857, loss_ce: 0.026139
2021-12-13 00:05:24,434 iteration 2733 : loss : 0.068277, loss_ce: 0.018717
2021-12-13 00:05:25,929 iteration 2734 : loss : 0.062178, loss_ce: 0.015892
2021-12-13 00:05:27,523 iteration 2735 : loss : 0.077748, loss_ce: 0.018226
2021-12-13 00:05:29,007 iteration 2736 : loss : 0.058085, loss_ce: 0.018622
2021-12-13 00:05:30,582 iteration 2737 : loss : 0.068624, loss_ce: 0.020642
 40%|██████████▊                | 161/400 [1:15:27<1:56:47, 29.32s/it]2021-12-13 00:05:32,053 iteration 2738 : loss : 0.053238, loss_ce: 0.016018
2021-12-13 00:05:33,616 iteration 2739 : loss : 0.069748, loss_ce: 0.021811
2021-12-13 00:05:35,201 iteration 2740 : loss : 0.068415, loss_ce: 0.020707
2021-12-13 00:05:36,667 iteration 2741 : loss : 0.063384, loss_ce: 0.019423
2021-12-13 00:05:38,086 iteration 2742 : loss : 0.065615, loss_ce: 0.015920
2021-12-13 00:05:39,601 iteration 2743 : loss : 0.058806, loss_ce: 0.015567
2021-12-13 00:05:41,247 iteration 2744 : loss : 0.086986, loss_ce: 0.027639
2021-12-13 00:05:42,734 iteration 2745 : loss : 0.059398, loss_ce: 0.014766
2021-12-13 00:05:44,207 iteration 2746 : loss : 0.055502, loss_ce: 0.013664
2021-12-13 00:05:45,819 iteration 2747 : loss : 0.068916, loss_ce: 0.020609
2021-12-13 00:05:47,290 iteration 2748 : loss : 0.066126, loss_ce: 0.020867
2021-12-13 00:05:48,797 iteration 2749 : loss : 0.065528, loss_ce: 0.021547
2021-12-13 00:05:50,270 iteration 2750 : loss : 0.062731, loss_ce: 0.016696
2021-12-13 00:05:51,742 iteration 2751 : loss : 0.061093, loss_ce: 0.012321
2021-12-13 00:05:53,192 iteration 2752 : loss : 0.059662, loss_ce: 0.018102
2021-12-13 00:05:54,662 iteration 2753 : loss : 0.070144, loss_ce: 0.024222
2021-12-13 00:05:56,167 iteration 2754 : loss : 0.073440, loss_ce: 0.020526
 40%|██████████▉                | 162/400 [1:15:53<1:51:51, 28.20s/it]2021-12-13 00:05:57,704 iteration 2755 : loss : 0.061750, loss_ce: 0.016024
2021-12-13 00:05:59,215 iteration 2756 : loss : 0.071768, loss_ce: 0.022826
2021-12-13 00:06:00,662 iteration 2757 : loss : 0.062271, loss_ce: 0.017698
2021-12-13 00:06:02,140 iteration 2758 : loss : 0.061362, loss_ce: 0.020385
2021-12-13 00:06:03,615 iteration 2759 : loss : 0.065292, loss_ce: 0.018601
2021-12-13 00:06:05,153 iteration 2760 : loss : 0.092904, loss_ce: 0.019313
2021-12-13 00:06:06,645 iteration 2761 : loss : 0.058955, loss_ce: 0.016016
2021-12-13 00:06:08,124 iteration 2762 : loss : 0.065024, loss_ce: 0.019475
2021-12-13 00:06:09,601 iteration 2763 : loss : 0.062986, loss_ce: 0.019513
2021-12-13 00:06:11,020 iteration 2764 : loss : 0.057441, loss_ce: 0.019515
2021-12-13 00:06:12,497 iteration 2765 : loss : 0.074488, loss_ce: 0.026177
2021-12-13 00:06:14,040 iteration 2766 : loss : 0.063826, loss_ce: 0.018660
2021-12-13 00:06:15,455 iteration 2767 : loss : 0.059790, loss_ce: 0.012980
2021-12-13 00:06:17,068 iteration 2768 : loss : 0.061410, loss_ce: 0.016945
2021-12-13 00:06:18,535 iteration 2769 : loss : 0.055408, loss_ce: 0.012931
2021-12-13 00:06:19,961 iteration 2770 : loss : 0.057707, loss_ce: 0.020446
2021-12-13 00:06:21,432 iteration 2771 : loss : 0.063703, loss_ce: 0.023415
 41%|███████████                | 163/400 [1:16:18<1:47:55, 27.32s/it]2021-12-13 00:06:23,032 iteration 2772 : loss : 0.065814, loss_ce: 0.017026
2021-12-13 00:06:24,449 iteration 2773 : loss : 0.063860, loss_ce: 0.015593
2021-12-13 00:06:25,907 iteration 2774 : loss : 0.061264, loss_ce: 0.019895
2021-12-13 00:06:27,393 iteration 2775 : loss : 0.065592, loss_ce: 0.019785
2021-12-13 00:06:28,880 iteration 2776 : loss : 0.065266, loss_ce: 0.024352
2021-12-13 00:06:30,408 iteration 2777 : loss : 0.069985, loss_ce: 0.023039
2021-12-13 00:06:31,818 iteration 2778 : loss : 0.055837, loss_ce: 0.016760
2021-12-13 00:06:33,348 iteration 2779 : loss : 0.065317, loss_ce: 0.018234
2021-12-13 00:06:34,921 iteration 2780 : loss : 0.073963, loss_ce: 0.023536
2021-12-13 00:06:36,322 iteration 2781 : loss : 0.060920, loss_ce: 0.018885
2021-12-13 00:06:37,840 iteration 2782 : loss : 0.073970, loss_ce: 0.023371
2021-12-13 00:06:39,404 iteration 2783 : loss : 0.071943, loss_ce: 0.020105
2021-12-13 00:06:41,009 iteration 2784 : loss : 0.063591, loss_ce: 0.022289
2021-12-13 00:06:42,434 iteration 2785 : loss : 0.058705, loss_ce: 0.014674
2021-12-13 00:06:43,943 iteration 2786 : loss : 0.066971, loss_ce: 0.018484
2021-12-13 00:06:45,520 iteration 2787 : loss : 0.069029, loss_ce: 0.015646
2021-12-13 00:06:47,028 iteration 2788 : loss : 0.066097, loss_ce: 0.015384
 41%|███████████                | 164/400 [1:16:44<1:45:25, 26.80s/it]2021-12-13 00:06:48,569 iteration 2789 : loss : 0.056706, loss_ce: 0.020842
2021-12-13 00:06:50,031 iteration 2790 : loss : 0.068683, loss_ce: 0.015771
2021-12-13 00:06:51,547 iteration 2791 : loss : 0.054509, loss_ce: 0.012525
2021-12-13 00:06:53,096 iteration 2792 : loss : 0.071164, loss_ce: 0.015430
2021-12-13 00:06:54,595 iteration 2793 : loss : 0.064614, loss_ce: 0.021485
2021-12-13 00:06:56,104 iteration 2794 : loss : 0.060403, loss_ce: 0.016299
2021-12-13 00:06:57,668 iteration 2795 : loss : 0.069678, loss_ce: 0.019465
2021-12-13 00:06:59,265 iteration 2796 : loss : 0.065726, loss_ce: 0.019148
2021-12-13 00:07:00,805 iteration 2797 : loss : 0.072678, loss_ce: 0.025021
2021-12-13 00:07:02,393 iteration 2798 : loss : 0.071264, loss_ce: 0.018238
2021-12-13 00:07:03,948 iteration 2799 : loss : 0.066639, loss_ce: 0.021193
2021-12-13 00:07:05,484 iteration 2800 : loss : 0.059055, loss_ce: 0.019836
2021-12-13 00:07:06,963 iteration 2801 : loss : 0.065486, loss_ce: 0.023678
2021-12-13 00:07:08,391 iteration 2802 : loss : 0.054807, loss_ce: 0.015610
2021-12-13 00:07:09,989 iteration 2803 : loss : 0.068679, loss_ce: 0.017255
2021-12-13 00:07:11,492 iteration 2804 : loss : 0.066104, loss_ce: 0.019005
2021-12-13 00:07:11,492 Training Data Eval:
2021-12-13 00:07:19,127   Average segmentation loss on training set: 0.0539
2021-12-13 00:07:19,127 Validation Data Eval:
2021-12-13 00:07:21,730   Average segmentation loss on validation set: 0.0985
2021-12-13 00:07:28,119 Found new lowest validation loss at iteration 2804! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/TU_3seeds/TU_SGD_best_val_loss_seed100.pth
2021-12-13 00:07:29,493 iteration 2805 : loss : 0.063151, loss_ce: 0.016331
 41%|███████████▏               | 165/400 [1:17:26<2:03:22, 31.50s/it]2021-12-13 00:07:30,940 iteration 2806 : loss : 0.083017, loss_ce: 0.019653
2021-12-13 00:07:32,478 iteration 2807 : loss : 0.071027, loss_ce: 0.018533
2021-12-13 00:07:33,777 iteration 2808 : loss : 0.058276, loss_ce: 0.021104
2021-12-13 00:07:35,265 iteration 2809 : loss : 0.060212, loss_ce: 0.014990
2021-12-13 00:07:36,800 iteration 2810 : loss : 0.066526, loss_ce: 0.019444
2021-12-13 00:07:38,263 iteration 2811 : loss : 0.058464, loss_ce: 0.018486
2021-12-13 00:07:39,755 iteration 2812 : loss : 0.057181, loss_ce: 0.018395
2021-12-13 00:07:41,284 iteration 2813 : loss : 0.061272, loss_ce: 0.015473
2021-12-13 00:07:42,789 iteration 2814 : loss : 0.064427, loss_ce: 0.021157
2021-12-13 00:07:44,236 iteration 2815 : loss : 0.057514, loss_ce: 0.018146
2021-12-13 00:07:45,680 iteration 2816 : loss : 0.068341, loss_ce: 0.023725
2021-12-13 00:07:47,131 iteration 2817 : loss : 0.075661, loss_ce: 0.021450
2021-12-13 00:07:48,702 iteration 2818 : loss : 0.062803, loss_ce: 0.013477
2021-12-13 00:07:50,243 iteration 2819 : loss : 0.059722, loss_ce: 0.018754
2021-12-13 00:07:51,861 iteration 2820 : loss : 0.070081, loss_ce: 0.022522
2021-12-13 00:07:53,358 iteration 2821 : loss : 0.062561, loss_ce: 0.020564
2021-12-13 00:07:54,818 iteration 2822 : loss : 0.056273, loss_ce: 0.014587
 42%|███████████▏               | 166/400 [1:17:52<1:55:37, 29.65s/it]2021-12-13 00:07:56,444 iteration 2823 : loss : 0.068887, loss_ce: 0.018109
2021-12-13 00:07:57,885 iteration 2824 : loss : 0.050155, loss_ce: 0.011602
2021-12-13 00:07:59,407 iteration 2825 : loss : 0.071806, loss_ce: 0.026195
2021-12-13 00:08:00,857 iteration 2826 : loss : 0.057704, loss_ce: 0.018160
2021-12-13 00:08:02,382 iteration 2827 : loss : 0.068533, loss_ce: 0.027829
2021-12-13 00:08:03,892 iteration 2828 : loss : 0.063542, loss_ce: 0.021764
2021-12-13 00:08:05,509 iteration 2829 : loss : 0.074290, loss_ce: 0.023728
2021-12-13 00:08:06,938 iteration 2830 : loss : 0.060954, loss_ce: 0.020095
2021-12-13 00:08:08,430 iteration 2831 : loss : 0.060253, loss_ce: 0.020495
2021-12-13 00:08:09,974 iteration 2832 : loss : 0.080670, loss_ce: 0.021380
2021-12-13 00:08:11,534 iteration 2833 : loss : 0.068268, loss_ce: 0.018637
2021-12-13 00:08:13,019 iteration 2834 : loss : 0.071049, loss_ce: 0.017982
2021-12-13 00:08:14,548 iteration 2835 : loss : 0.099299, loss_ce: 0.022798
2021-12-13 00:08:16,107 iteration 2836 : loss : 0.069488, loss_ce: 0.023303
2021-12-13 00:08:17,603 iteration 2837 : loss : 0.065623, loss_ce: 0.014488
2021-12-13 00:08:19,036 iteration 2838 : loss : 0.056520, loss_ce: 0.014623
2021-12-13 00:08:20,564 iteration 2839 : loss : 0.064439, loss_ce: 0.019468
 42%|███████████▎               | 167/400 [1:18:17<1:50:35, 28.48s/it]2021-12-13 00:08:22,084 iteration 2840 : loss : 0.076070, loss_ce: 0.030907
2021-12-13 00:08:23,593 iteration 2841 : loss : 0.063276, loss_ce: 0.020778
2021-12-13 00:08:25,097 iteration 2842 : loss : 0.069408, loss_ce: 0.018205
2021-12-13 00:08:26,509 iteration 2843 : loss : 0.061128, loss_ce: 0.016133
2021-12-13 00:08:27,900 iteration 2844 : loss : 0.053790, loss_ce: 0.014570
2021-12-13 00:08:29,363 iteration 2845 : loss : 0.058271, loss_ce: 0.016877
2021-12-13 00:08:30,969 iteration 2846 : loss : 0.070485, loss_ce: 0.017022
2021-12-13 00:08:32,514 iteration 2847 : loss : 0.080498, loss_ce: 0.020290
2021-12-13 00:08:34,012 iteration 2848 : loss : 0.067460, loss_ce: 0.015302
2021-12-13 00:08:35,540 iteration 2849 : loss : 0.061406, loss_ce: 0.019434
2021-12-13 00:08:37,036 iteration 2850 : loss : 0.062881, loss_ce: 0.022553
2021-12-13 00:08:38,620 iteration 2851 : loss : 0.064920, loss_ce: 0.020533
2021-12-13 00:08:40,076 iteration 2852 : loss : 0.064638, loss_ce: 0.016642
2021-12-13 00:08:41,595 iteration 2853 : loss : 0.061625, loss_ce: 0.013703
2021-12-13 00:08:43,022 iteration 2854 : loss : 0.068061, loss_ce: 0.016788
2021-12-13 00:08:44,475 iteration 2855 : loss : 0.070135, loss_ce: 0.021615
2021-12-13 00:08:45,874 iteration 2856 : loss : 0.058024, loss_ce: 0.021327
 42%|███████████▎               | 168/400 [1:18:43<1:46:26, 27.53s/it]2021-12-13 00:08:47,411 iteration 2857 : loss : 0.070621, loss_ce: 0.023576
2021-12-13 00:08:49,032 iteration 2858 : loss : 0.068745, loss_ce: 0.015606
2021-12-13 00:08:50,573 iteration 2859 : loss : 0.065554, loss_ce: 0.019987
2021-12-13 00:08:52,110 iteration 2860 : loss : 0.062622, loss_ce: 0.017347
2021-12-13 00:08:53,625 iteration 2861 : loss : 0.063289, loss_ce: 0.016243
2021-12-13 00:08:55,162 iteration 2862 : loss : 0.068179, loss_ce: 0.016370
2021-12-13 00:08:56,613 iteration 2863 : loss : 0.057551, loss_ce: 0.017594
2021-12-13 00:08:58,093 iteration 2864 : loss : 0.062205, loss_ce: 0.016042
2021-12-13 00:08:59,574 iteration 2865 : loss : 0.057100, loss_ce: 0.016725
2021-12-13 00:09:01,051 iteration 2866 : loss : 0.054946, loss_ce: 0.013300
2021-12-13 00:09:02,639 iteration 2867 : loss : 0.072380, loss_ce: 0.019752
2021-12-13 00:09:04,178 iteration 2868 : loss : 0.071517, loss_ce: 0.021644
2021-12-13 00:09:05,727 iteration 2869 : loss : 0.070943, loss_ce: 0.020416
2021-12-13 00:09:07,326 iteration 2870 : loss : 0.065237, loss_ce: 0.017512
2021-12-13 00:09:08,756 iteration 2871 : loss : 0.068609, loss_ce: 0.024443
2021-12-13 00:09:10,213 iteration 2872 : loss : 0.058998, loss_ce: 0.020725
2021-12-13 00:09:11,689 iteration 2873 : loss : 0.060299, loss_ce: 0.020728
 42%|███████████▍               | 169/400 [1:19:08<1:44:00, 27.02s/it]2021-12-13 00:09:13,216 iteration 2874 : loss : 0.059751, loss_ce: 0.017215
2021-12-13 00:09:14,799 iteration 2875 : loss : 0.060771, loss_ce: 0.018953
2021-12-13 00:09:16,257 iteration 2876 : loss : 0.074541, loss_ce: 0.020950
2021-12-13 00:09:17,673 iteration 2877 : loss : 0.057948, loss_ce: 0.014979
2021-12-13 00:09:19,148 iteration 2878 : loss : 0.056464, loss_ce: 0.012505
2021-12-13 00:09:20,550 iteration 2879 : loss : 0.057834, loss_ce: 0.020728
2021-12-13 00:09:22,065 iteration 2880 : loss : 0.074779, loss_ce: 0.031181
2021-12-13 00:09:23,471 iteration 2881 : loss : 0.055162, loss_ce: 0.016310
2021-12-13 00:09:24,918 iteration 2882 : loss : 0.068937, loss_ce: 0.014982
2021-12-13 00:09:26,440 iteration 2883 : loss : 0.073357, loss_ce: 0.019009
2021-12-13 00:09:27,975 iteration 2884 : loss : 0.082167, loss_ce: 0.024684
2021-12-13 00:09:29,475 iteration 2885 : loss : 0.070915, loss_ce: 0.022912
2021-12-13 00:09:30,924 iteration 2886 : loss : 0.060027, loss_ce: 0.017049
2021-12-13 00:09:32,372 iteration 2887 : loss : 0.061135, loss_ce: 0.017299
2021-12-13 00:09:33,903 iteration 2888 : loss : 0.061748, loss_ce: 0.017982
2021-12-13 00:09:35,395 iteration 2889 : loss : 0.063883, loss_ce: 0.017382
2021-12-13 00:09:35,396 Training Data Eval:
2021-12-13 00:09:43,061   Average segmentation loss on training set: 0.0494
2021-12-13 00:09:43,062 Validation Data Eval:
2021-12-13 00:09:45,675   Average segmentation loss on validation set: 0.0948
2021-12-13 00:09:52,045 Found new lowest validation loss at iteration 2889! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/TU_3seeds/TU_SGD_best_val_loss_seed100.pth
2021-12-13 00:09:53,421 iteration 2890 : loss : 0.066039, loss_ce: 0.020464
 42%|███████████▍               | 170/400 [1:19:50<2:00:28, 31.43s/it]2021-12-13 00:09:54,817 iteration 2891 : loss : 0.060846, loss_ce: 0.021162
2021-12-13 00:09:56,218 iteration 2892 : loss : 0.068510, loss_ce: 0.021158
2021-12-13 00:09:57,603 iteration 2893 : loss : 0.056851, loss_ce: 0.016813
2021-12-13 00:09:59,041 iteration 2894 : loss : 0.051446, loss_ce: 0.013625
2021-12-13 00:10:00,479 iteration 2895 : loss : 0.063308, loss_ce: 0.016985
2021-12-13 00:10:01,951 iteration 2896 : loss : 0.058303, loss_ce: 0.014386
2021-12-13 00:10:03,561 iteration 2897 : loss : 0.085237, loss_ce: 0.015837
2021-12-13 00:10:05,076 iteration 2898 : loss : 0.073148, loss_ce: 0.020222
2021-12-13 00:10:06,569 iteration 2899 : loss : 0.072577, loss_ce: 0.024981
2021-12-13 00:10:08,193 iteration 2900 : loss : 0.079143, loss_ce: 0.027234
2021-12-13 00:10:09,729 iteration 2901 : loss : 0.059435, loss_ce: 0.017908
2021-12-13 00:10:11,197 iteration 2902 : loss : 0.062875, loss_ce: 0.020944
2021-12-13 00:10:12,697 iteration 2903 : loss : 0.057078, loss_ce: 0.014815
2021-12-13 00:10:14,173 iteration 2904 : loss : 0.055650, loss_ce: 0.018396
2021-12-13 00:10:15,702 iteration 2905 : loss : 0.062127, loss_ce: 0.021235
2021-12-13 00:10:17,147 iteration 2906 : loss : 0.071466, loss_ce: 0.022649
2021-12-13 00:10:18,636 iteration 2907 : loss : 0.066457, loss_ce: 0.020179
 43%|███████████▌               | 171/400 [1:20:15<1:52:49, 29.56s/it]2021-12-13 00:10:20,193 iteration 2908 : loss : 0.062873, loss_ce: 0.017714
2021-12-13 00:10:21,738 iteration 2909 : loss : 0.073030, loss_ce: 0.018877
2021-12-13 00:10:23,272 iteration 2910 : loss : 0.055246, loss_ce: 0.016392
2021-12-13 00:10:24,829 iteration 2911 : loss : 0.065195, loss_ce: 0.018615
2021-12-13 00:10:26,286 iteration 2912 : loss : 0.053943, loss_ce: 0.015141
2021-12-13 00:10:27,793 iteration 2913 : loss : 0.069132, loss_ce: 0.017321
2021-12-13 00:10:29,326 iteration 2914 : loss : 0.067983, loss_ce: 0.021359
2021-12-13 00:10:30,827 iteration 2915 : loss : 0.068912, loss_ce: 0.025696
2021-12-13 00:10:32,320 iteration 2916 : loss : 0.078950, loss_ce: 0.024548
2021-12-13 00:10:33,816 iteration 2917 : loss : 0.061605, loss_ce: 0.018691
2021-12-13 00:10:35,229 iteration 2918 : loss : 0.058917, loss_ce: 0.020886
2021-12-13 00:10:36,728 iteration 2919 : loss : 0.069382, loss_ce: 0.016156
2021-12-13 00:10:38,196 iteration 2920 : loss : 0.060674, loss_ce: 0.019877
2021-12-13 00:10:39,677 iteration 2921 : loss : 0.062787, loss_ce: 0.022080
2021-12-13 00:10:41,143 iteration 2922 : loss : 0.061782, loss_ce: 0.014554
2021-12-13 00:10:42,711 iteration 2923 : loss : 0.069888, loss_ce: 0.015214
2021-12-13 00:10:44,248 iteration 2924 : loss : 0.059170, loss_ce: 0.019047
 43%|███████████▌               | 172/400 [1:20:41<1:47:50, 28.38s/it]2021-12-13 00:10:45,798 iteration 2925 : loss : 0.063805, loss_ce: 0.022998
2021-12-13 00:10:47,223 iteration 2926 : loss : 0.054434, loss_ce: 0.014971
2021-12-13 00:10:48,718 iteration 2927 : loss : 0.058466, loss_ce: 0.013899
2021-12-13 00:10:50,246 iteration 2928 : loss : 0.061597, loss_ce: 0.018498
2021-12-13 00:10:51,689 iteration 2929 : loss : 0.058746, loss_ce: 0.013424
2021-12-13 00:10:53,125 iteration 2930 : loss : 0.059288, loss_ce: 0.020067
2021-12-13 00:10:54,608 iteration 2931 : loss : 0.089356, loss_ce: 0.014083
2021-12-13 00:10:56,180 iteration 2932 : loss : 0.069333, loss_ce: 0.021023
2021-12-13 00:10:57,715 iteration 2933 : loss : 0.075886, loss_ce: 0.027431
2021-12-13 00:10:59,284 iteration 2934 : loss : 0.062688, loss_ce: 0.023504
2021-12-13 00:11:00,753 iteration 2935 : loss : 0.057933, loss_ce: 0.015794
2021-12-13 00:11:02,242 iteration 2936 : loss : 0.058798, loss_ce: 0.019326
2021-12-13 00:11:03,760 iteration 2937 : loss : 0.074208, loss_ce: 0.030142
2021-12-13 00:11:05,288 iteration 2938 : loss : 0.064680, loss_ce: 0.019058
2021-12-13 00:11:06,684 iteration 2939 : loss : 0.056967, loss_ce: 0.012662
2021-12-13 00:11:08,149 iteration 2940 : loss : 0.058400, loss_ce: 0.016820
2021-12-13 00:11:09,616 iteration 2941 : loss : 0.066130, loss_ce: 0.023144
 43%|███████████▋               | 173/400 [1:21:06<1:43:56, 27.47s/it]2021-12-13 00:11:11,239 iteration 2942 : loss : 0.071245, loss_ce: 0.024570
2021-12-13 00:11:12,782 iteration 2943 : loss : 0.060641, loss_ce: 0.018423
2021-12-13 00:11:14,297 iteration 2944 : loss : 0.064002, loss_ce: 0.018014
2021-12-13 00:11:15,797 iteration 2945 : loss : 0.078909, loss_ce: 0.028988
2021-12-13 00:11:17,341 iteration 2946 : loss : 0.063434, loss_ce: 0.018744
2021-12-13 00:11:18,806 iteration 2947 : loss : 0.050833, loss_ce: 0.014084
2021-12-13 00:11:20,329 iteration 2948 : loss : 0.058069, loss_ce: 0.018050
2021-12-13 00:11:21,798 iteration 2949 : loss : 0.061064, loss_ce: 0.017677
2021-12-13 00:11:23,206 iteration 2950 : loss : 0.057312, loss_ce: 0.017458
2021-12-13 00:11:24,688 iteration 2951 : loss : 0.063871, loss_ce: 0.016570
2021-12-13 00:11:26,088 iteration 2952 : loss : 0.077238, loss_ce: 0.010202
2021-12-13 00:11:27,648 iteration 2953 : loss : 0.072706, loss_ce: 0.022307
2021-12-13 00:11:29,147 iteration 2954 : loss : 0.061938, loss_ce: 0.019793
2021-12-13 00:11:30,651 iteration 2955 : loss : 0.071282, loss_ce: 0.012366
2021-12-13 00:11:32,082 iteration 2956 : loss : 0.064029, loss_ce: 0.023828
2021-12-13 00:11:33,509 iteration 2957 : loss : 0.066245, loss_ce: 0.017447
2021-12-13 00:11:35,004 iteration 2958 : loss : 0.059688, loss_ce: 0.017901
 44%|███████████▋               | 174/400 [1:21:32<1:41:08, 26.85s/it]2021-12-13 00:11:36,574 iteration 2959 : loss : 0.060146, loss_ce: 0.019324
2021-12-13 00:11:38,065 iteration 2960 : loss : 0.063745, loss_ce: 0.021817
2021-12-13 00:11:39,606 iteration 2961 : loss : 0.069103, loss_ce: 0.021298
2021-12-13 00:11:41,142 iteration 2962 : loss : 0.065082, loss_ce: 0.019360
2021-12-13 00:11:42,605 iteration 2963 : loss : 0.060547, loss_ce: 0.021228
2021-12-13 00:11:44,055 iteration 2964 : loss : 0.074408, loss_ce: 0.022373
2021-12-13 00:11:45,465 iteration 2965 : loss : 0.056732, loss_ce: 0.013804
2021-12-13 00:11:46,862 iteration 2966 : loss : 0.056891, loss_ce: 0.015404
2021-12-13 00:11:48,374 iteration 2967 : loss : 0.062332, loss_ce: 0.021483
2021-12-13 00:11:49,981 iteration 2968 : loss : 0.064023, loss_ce: 0.011849
2021-12-13 00:11:51,526 iteration 2969 : loss : 0.054004, loss_ce: 0.018244
2021-12-13 00:11:53,000 iteration 2970 : loss : 0.062240, loss_ce: 0.017642
2021-12-13 00:11:54,428 iteration 2971 : loss : 0.062653, loss_ce: 0.020169
2021-12-13 00:11:55,963 iteration 2972 : loss : 0.067851, loss_ce: 0.019550
2021-12-13 00:11:57,420 iteration 2973 : loss : 0.068539, loss_ce: 0.022174
2021-12-13 00:11:58,977 iteration 2974 : loss : 0.054198, loss_ce: 0.014271
2021-12-13 00:11:58,977 Training Data Eval:
2021-12-13 00:12:06,644   Average segmentation loss on training set: 0.0473
2021-12-13 00:12:06,645 Validation Data Eval:
2021-12-13 00:12:09,257   Average segmentation loss on validation set: 0.0935
2021-12-13 00:12:15,655 Found new lowest validation loss at iteration 2974! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/TU_3seeds/TU_SGD_best_val_loss_seed100.pth
2021-12-13 00:12:17,071 iteration 2975 : loss : 0.060355, loss_ce: 0.013578
 44%|███████████▊               | 175/400 [1:22:14<1:57:48, 31.42s/it]2021-12-13 00:12:18,579 iteration 2976 : loss : 0.074254, loss_ce: 0.027248
2021-12-13 00:12:19,884 iteration 2977 : loss : 0.060605, loss_ce: 0.019624
2021-12-13 00:12:21,183 iteration 2978 : loss : 0.054288, loss_ce: 0.015092
2021-12-13 00:12:22,699 iteration 2979 : loss : 0.057021, loss_ce: 0.014566
2021-12-13 00:12:24,325 iteration 2980 : loss : 0.059649, loss_ce: 0.017869
2021-12-13 00:12:25,837 iteration 2981 : loss : 0.062449, loss_ce: 0.015130
2021-12-13 00:12:27,423 iteration 2982 : loss : 0.071626, loss_ce: 0.024617
2021-12-13 00:12:28,903 iteration 2983 : loss : 0.066029, loss_ce: 0.023628
2021-12-13 00:12:30,459 iteration 2984 : loss : 0.062034, loss_ce: 0.017060
2021-12-13 00:12:31,978 iteration 2985 : loss : 0.068452, loss_ce: 0.016441
2021-12-13 00:12:33,532 iteration 2986 : loss : 0.067937, loss_ce: 0.022847
2021-12-13 00:12:34,966 iteration 2987 : loss : 0.060157, loss_ce: 0.022204
2021-12-13 00:12:36,428 iteration 2988 : loss : 0.053068, loss_ce: 0.013259
2021-12-13 00:12:37,938 iteration 2989 : loss : 0.056284, loss_ce: 0.012364
2021-12-13 00:12:39,461 iteration 2990 : loss : 0.068581, loss_ce: 0.019985
2021-12-13 00:12:41,042 iteration 2991 : loss : 0.055257, loss_ce: 0.014829
2021-12-13 00:12:42,691 iteration 2992 : loss : 0.069420, loss_ce: 0.022464
 44%|███████████▉               | 176/400 [1:22:39<1:50:47, 29.67s/it]2021-12-13 00:12:44,156 iteration 2993 : loss : 0.055839, loss_ce: 0.013220
2021-12-13 00:12:45,639 iteration 2994 : loss : 0.064214, loss_ce: 0.020159
2021-12-13 00:12:47,234 iteration 2995 : loss : 0.065121, loss_ce: 0.021827
2021-12-13 00:12:48,728 iteration 2996 : loss : 0.062072, loss_ce: 0.017517
2021-12-13 00:12:50,218 iteration 2997 : loss : 0.063032, loss_ce: 0.019377
2021-12-13 00:12:51,697 iteration 2998 : loss : 0.055067, loss_ce: 0.011862
2021-12-13 00:12:53,234 iteration 2999 : loss : 0.062319, loss_ce: 0.018910
2021-12-13 00:12:54,717 iteration 3000 : loss : 0.077803, loss_ce: 0.020610
2021-12-13 00:12:56,260 iteration 3001 : loss : 0.058937, loss_ce: 0.019106
2021-12-13 00:12:57,667 iteration 3002 : loss : 0.055850, loss_ce: 0.017554
2021-12-13 00:12:59,215 iteration 3003 : loss : 0.067237, loss_ce: 0.019416
2021-12-13 00:13:00,615 iteration 3004 : loss : 0.055585, loss_ce: 0.017297
2021-12-13 00:13:02,130 iteration 3005 : loss : 0.072851, loss_ce: 0.021872
2021-12-13 00:13:03,655 iteration 3006 : loss : 0.066017, loss_ce: 0.020501
2021-12-13 00:13:05,119 iteration 3007 : loss : 0.055120, loss_ce: 0.015317
2021-12-13 00:13:06,734 iteration 3008 : loss : 0.064235, loss_ce: 0.021323
2021-12-13 00:13:08,214 iteration 3009 : loss : 0.057775, loss_ce: 0.021398
 44%|███████████▉               | 177/400 [1:23:05<1:45:39, 28.43s/it]2021-12-13 00:13:09,737 iteration 3010 : loss : 0.054517, loss_ce: 0.015578
2021-12-13 00:13:11,217 iteration 3011 : loss : 0.069259, loss_ce: 0.019660
2021-12-13 00:13:12,668 iteration 3012 : loss : 0.062877, loss_ce: 0.015713
2021-12-13 00:13:14,114 iteration 3013 : loss : 0.055612, loss_ce: 0.017549
2021-12-13 00:13:15,676 iteration 3014 : loss : 0.067542, loss_ce: 0.016474
2021-12-13 00:13:17,178 iteration 3015 : loss : 0.062728, loss_ce: 0.016197
2021-12-13 00:13:18,661 iteration 3016 : loss : 0.055326, loss_ce: 0.014112
2021-12-13 00:13:20,132 iteration 3017 : loss : 0.070810, loss_ce: 0.017110
2021-12-13 00:13:21,564 iteration 3018 : loss : 0.058197, loss_ce: 0.013616
2021-12-13 00:13:23,026 iteration 3019 : loss : 0.067761, loss_ce: 0.020470
2021-12-13 00:13:24,379 iteration 3020 : loss : 0.054474, loss_ce: 0.020009
2021-12-13 00:13:25,763 iteration 3021 : loss : 0.058491, loss_ce: 0.017203
2021-12-13 00:13:27,215 iteration 3022 : loss : 0.062044, loss_ce: 0.014736
2021-12-13 00:13:28,681 iteration 3023 : loss : 0.059982, loss_ce: 0.023843
2021-12-13 00:13:30,166 iteration 3024 : loss : 0.060965, loss_ce: 0.016932
2021-12-13 00:13:31,545 iteration 3025 : loss : 0.058890, loss_ce: 0.018176
2021-12-13 00:13:33,062 iteration 3026 : loss : 0.085181, loss_ce: 0.033447
 44%|████████████               | 178/400 [1:23:30<1:41:13, 27.36s/it]2021-12-13 00:13:34,589 iteration 3027 : loss : 0.064224, loss_ce: 0.022570
2021-12-13 00:13:36,065 iteration 3028 : loss : 0.064616, loss_ce: 0.022798
2021-12-13 00:13:37,558 iteration 3029 : loss : 0.057566, loss_ce: 0.019843
2021-12-13 00:13:39,150 iteration 3030 : loss : 0.068229, loss_ce: 0.018753
2021-12-13 00:13:40,538 iteration 3031 : loss : 0.054999, loss_ce: 0.016860
2021-12-13 00:13:42,036 iteration 3032 : loss : 0.057040, loss_ce: 0.013779
2021-12-13 00:13:43,545 iteration 3033 : loss : 0.060972, loss_ce: 0.020958
2021-12-13 00:13:45,008 iteration 3034 : loss : 0.056589, loss_ce: 0.015370
2021-12-13 00:13:46,579 iteration 3035 : loss : 0.071516, loss_ce: 0.018654
2021-12-13 00:13:48,084 iteration 3036 : loss : 0.060536, loss_ce: 0.017998
2021-12-13 00:13:49,630 iteration 3037 : loss : 0.056425, loss_ce: 0.016858
2021-12-13 00:13:51,039 iteration 3038 : loss : 0.057418, loss_ce: 0.011748
2021-12-13 00:13:52,694 iteration 3039 : loss : 0.063766, loss_ce: 0.018152
2021-12-13 00:13:54,125 iteration 3040 : loss : 0.056881, loss_ce: 0.019088
2021-12-13 00:13:55,576 iteration 3041 : loss : 0.065235, loss_ce: 0.019571
2021-12-13 00:13:57,017 iteration 3042 : loss : 0.055133, loss_ce: 0.017852
2021-12-13 00:13:58,622 iteration 3043 : loss : 0.057234, loss_ce: 0.016800
 45%|████████████               | 179/400 [1:23:55<1:38:46, 26.82s/it]2021-12-13 00:14:00,191 iteration 3044 : loss : 0.068097, loss_ce: 0.022160
2021-12-13 00:14:01,666 iteration 3045 : loss : 0.053929, loss_ce: 0.016187
2021-12-13 00:14:03,119 iteration 3046 : loss : 0.069979, loss_ce: 0.022197
2021-12-13 00:14:04,737 iteration 3047 : loss : 0.062208, loss_ce: 0.019781
2021-12-13 00:14:06,298 iteration 3048 : loss : 0.058686, loss_ce: 0.019911
2021-12-13 00:14:07,754 iteration 3049 : loss : 0.059121, loss_ce: 0.018199
2021-12-13 00:14:09,346 iteration 3050 : loss : 0.066169, loss_ce: 0.023591
2021-12-13 00:14:10,884 iteration 3051 : loss : 0.068807, loss_ce: 0.014286
2021-12-13 00:14:12,468 iteration 3052 : loss : 0.075794, loss_ce: 0.023406
2021-12-13 00:14:13,992 iteration 3053 : loss : 0.053102, loss_ce: 0.015445
2021-12-13 00:14:15,508 iteration 3054 : loss : 0.069353, loss_ce: 0.019496
2021-12-13 00:14:17,032 iteration 3055 : loss : 0.059905, loss_ce: 0.017076
2021-12-13 00:14:18,523 iteration 3056 : loss : 0.058989, loss_ce: 0.015554
2021-12-13 00:14:19,963 iteration 3057 : loss : 0.048207, loss_ce: 0.012424
2021-12-13 00:14:21,532 iteration 3058 : loss : 0.089076, loss_ce: 0.023772
2021-12-13 00:14:23,110 iteration 3059 : loss : 0.065112, loss_ce: 0.020154
2021-12-13 00:14:23,110 Training Data Eval:
2021-12-13 00:14:30,763   Average segmentation loss on training set: 0.0477
2021-12-13 00:14:30,763 Validation Data Eval:
2021-12-13 00:14:33,376   Average segmentation loss on validation set: 0.0940
2021-12-13 00:14:34,857 iteration 3060 : loss : 0.057187, loss_ce: 0.018137
 45%|████████████▏              | 180/400 [1:24:32<1:48:41, 29.64s/it]2021-12-13 00:14:36,352 iteration 3061 : loss : 0.061840, loss_ce: 0.018990
2021-12-13 00:14:37,874 iteration 3062 : loss : 0.064654, loss_ce: 0.017131
2021-12-13 00:14:39,337 iteration 3063 : loss : 0.061080, loss_ce: 0.014700
2021-12-13 00:14:40,888 iteration 3064 : loss : 0.065280, loss_ce: 0.025707
2021-12-13 00:14:42,373 iteration 3065 : loss : 0.066421, loss_ce: 0.014478
2021-12-13 00:14:43,847 iteration 3066 : loss : 0.056416, loss_ce: 0.010838
2021-12-13 00:14:45,353 iteration 3067 : loss : 0.061255, loss_ce: 0.019941
2021-12-13 00:14:46,919 iteration 3068 : loss : 0.052670, loss_ce: 0.013449
2021-12-13 00:14:48,479 iteration 3069 : loss : 0.054376, loss_ce: 0.010492
2021-12-13 00:14:49,904 iteration 3070 : loss : 0.056255, loss_ce: 0.017048
2021-12-13 00:14:51,471 iteration 3071 : loss : 0.063713, loss_ce: 0.019683
2021-12-13 00:14:52,986 iteration 3072 : loss : 0.053110, loss_ce: 0.015432
2021-12-13 00:14:54,437 iteration 3073 : loss : 0.063718, loss_ce: 0.017424
2021-12-13 00:14:56,041 iteration 3074 : loss : 0.071030, loss_ce: 0.025156
2021-12-13 00:14:57,595 iteration 3075 : loss : 0.056477, loss_ce: 0.013757
2021-12-13 00:14:59,073 iteration 3076 : loss : 0.064187, loss_ce: 0.024039
2021-12-13 00:15:00,657 iteration 3077 : loss : 0.072513, loss_ce: 0.027720
 45%|████████████▏              | 181/400 [1:24:57<1:43:59, 28.49s/it]2021-12-13 00:15:02,079 iteration 3078 : loss : 0.052385, loss_ce: 0.016418
2021-12-13 00:15:03,627 iteration 3079 : loss : 0.068669, loss_ce: 0.019426
2021-12-13 00:15:05,123 iteration 3080 : loss : 0.062879, loss_ce: 0.019806
2021-12-13 00:15:06,590 iteration 3081 : loss : 0.066645, loss_ce: 0.015064
2021-12-13 00:15:08,092 iteration 3082 : loss : 0.055116, loss_ce: 0.016254
2021-12-13 00:15:09,596 iteration 3083 : loss : 0.060029, loss_ce: 0.022005
2021-12-13 00:15:11,075 iteration 3084 : loss : 0.065508, loss_ce: 0.021652
2021-12-13 00:15:12,572 iteration 3085 : loss : 0.066573, loss_ce: 0.011054
2021-12-13 00:15:14,019 iteration 3086 : loss : 0.057169, loss_ce: 0.016443
2021-12-13 00:15:15,457 iteration 3087 : loss : 0.054757, loss_ce: 0.013907
2021-12-13 00:15:16,891 iteration 3088 : loss : 0.053800, loss_ce: 0.014710
2021-12-13 00:15:18,322 iteration 3089 : loss : 0.056339, loss_ce: 0.016579
2021-12-13 00:15:19,831 iteration 3090 : loss : 0.054916, loss_ce: 0.014356
2021-12-13 00:15:21,275 iteration 3091 : loss : 0.065483, loss_ce: 0.019414
2021-12-13 00:15:22,678 iteration 3092 : loss : 0.056786, loss_ce: 0.020374
2021-12-13 00:15:24,150 iteration 3093 : loss : 0.055128, loss_ce: 0.012880
2021-12-13 00:15:25,592 iteration 3094 : loss : 0.064917, loss_ce: 0.023743
 46%|████████████▎              | 182/400 [1:25:22<1:39:37, 27.42s/it]2021-12-13 00:15:27,183 iteration 3095 : loss : 0.054202, loss_ce: 0.013156
2021-12-13 00:15:28,649 iteration 3096 : loss : 0.054896, loss_ce: 0.017031
2021-12-13 00:15:30,113 iteration 3097 : loss : 0.062554, loss_ce: 0.018927
2021-12-13 00:15:31,547 iteration 3098 : loss : 0.057614, loss_ce: 0.015308
2021-12-13 00:15:33,095 iteration 3099 : loss : 0.070882, loss_ce: 0.019450
2021-12-13 00:15:34,528 iteration 3100 : loss : 0.050893, loss_ce: 0.012510
2021-12-13 00:15:36,017 iteration 3101 : loss : 0.057318, loss_ce: 0.019719
2021-12-13 00:15:37,551 iteration 3102 : loss : 0.060819, loss_ce: 0.015553
2021-12-13 00:15:39,013 iteration 3103 : loss : 0.059227, loss_ce: 0.012268
2021-12-13 00:15:40,497 iteration 3104 : loss : 0.054541, loss_ce: 0.011783
2021-12-13 00:15:41,951 iteration 3105 : loss : 0.054212, loss_ce: 0.015186
2021-12-13 00:15:43,594 iteration 3106 : loss : 0.080229, loss_ce: 0.024633
2021-12-13 00:15:45,078 iteration 3107 : loss : 0.057260, loss_ce: 0.017521
2021-12-13 00:15:46,489 iteration 3108 : loss : 0.052590, loss_ce: 0.012659
2021-12-13 00:15:48,039 iteration 3109 : loss : 0.066490, loss_ce: 0.029114
2021-12-13 00:15:49,589 iteration 3110 : loss : 0.063400, loss_ce: 0.023150
2021-12-13 00:15:51,040 iteration 3111 : loss : 0.058860, loss_ce: 0.013739
 46%|████████████▎              | 183/400 [1:25:48<1:37:01, 26.83s/it]2021-12-13 00:15:52,522 iteration 3112 : loss : 0.050673, loss_ce: 0.015485
2021-12-13 00:15:54,004 iteration 3113 : loss : 0.057088, loss_ce: 0.018223
2021-12-13 00:15:55,619 iteration 3114 : loss : 0.075925, loss_ce: 0.017130
2021-12-13 00:15:57,167 iteration 3115 : loss : 0.065434, loss_ce: 0.022770
2021-12-13 00:15:58,678 iteration 3116 : loss : 0.060288, loss_ce: 0.020004
2021-12-13 00:16:00,160 iteration 3117 : loss : 0.053519, loss_ce: 0.016454
2021-12-13 00:16:01,714 iteration 3118 : loss : 0.064394, loss_ce: 0.019505
2021-12-13 00:16:03,248 iteration 3119 : loss : 0.072506, loss_ce: 0.022064
2021-12-13 00:16:04,752 iteration 3120 : loss : 0.056990, loss_ce: 0.015632
2021-12-13 00:16:06,273 iteration 3121 : loss : 0.063505, loss_ce: 0.015506
2021-12-13 00:16:07,846 iteration 3122 : loss : 0.067816, loss_ce: 0.015517
2021-12-13 00:16:09,353 iteration 3123 : loss : 0.054150, loss_ce: 0.017195
2021-12-13 00:16:10,878 iteration 3124 : loss : 0.062450, loss_ce: 0.018871
2021-12-13 00:16:12,396 iteration 3125 : loss : 0.065431, loss_ce: 0.023727
2021-12-13 00:16:13,825 iteration 3126 : loss : 0.061845, loss_ce: 0.019187
2021-12-13 00:16:15,338 iteration 3127 : loss : 0.062918, loss_ce: 0.018626
2021-12-13 00:16:16,750 iteration 3128 : loss : 0.054679, loss_ce: 0.015284
 46%|████████████▍              | 184/400 [1:26:14<1:35:22, 26.49s/it]2021-12-13 00:16:18,327 iteration 3129 : loss : 0.053418, loss_ce: 0.013654
2021-12-13 00:16:19,854 iteration 3130 : loss : 0.055903, loss_ce: 0.014798
2021-12-13 00:16:21,417 iteration 3131 : loss : 0.067783, loss_ce: 0.021310
2021-12-13 00:16:22,894 iteration 3132 : loss : 0.058985, loss_ce: 0.018955
2021-12-13 00:16:24,316 iteration 3133 : loss : 0.054318, loss_ce: 0.014521
2021-12-13 00:16:25,804 iteration 3134 : loss : 0.052334, loss_ce: 0.014261
2021-12-13 00:16:27,293 iteration 3135 : loss : 0.064318, loss_ce: 0.017493
2021-12-13 00:16:28,848 iteration 3136 : loss : 0.079021, loss_ce: 0.022545
2021-12-13 00:16:30,361 iteration 3137 : loss : 0.071422, loss_ce: 0.026153
2021-12-13 00:16:31,895 iteration 3138 : loss : 0.067156, loss_ce: 0.026624
2021-12-13 00:16:33,468 iteration 3139 : loss : 0.072558, loss_ce: 0.020619
2021-12-13 00:16:35,014 iteration 3140 : loss : 0.059659, loss_ce: 0.019260
2021-12-13 00:16:36,555 iteration 3141 : loss : 0.063657, loss_ce: 0.018805
2021-12-13 00:16:38,048 iteration 3142 : loss : 0.057493, loss_ce: 0.015629
2021-12-13 00:16:39,615 iteration 3143 : loss : 0.059526, loss_ce: 0.019330
2021-12-13 00:16:41,189 iteration 3144 : loss : 0.060222, loss_ce: 0.021021
2021-12-13 00:16:41,189 Training Data Eval:
2021-12-13 00:16:48,859   Average segmentation loss on training set: 0.0505
2021-12-13 00:16:48,859 Validation Data Eval:
2021-12-13 00:16:51,467   Average segmentation loss on validation set: 0.0964
2021-12-13 00:16:52,839 iteration 3145 : loss : 0.052899, loss_ce: 0.013921
 46%|████████████▍              | 185/400 [1:26:50<1:45:15, 29.37s/it]2021-12-13 00:16:54,268 iteration 3146 : loss : 0.051776, loss_ce: 0.014726
2021-12-13 00:16:55,728 iteration 3147 : loss : 0.058602, loss_ce: 0.020165
2021-12-13 00:16:57,199 iteration 3148 : loss : 0.056933, loss_ce: 0.014534
2021-12-13 00:16:58,653 iteration 3149 : loss : 0.053566, loss_ce: 0.014148
2021-12-13 00:17:00,122 iteration 3150 : loss : 0.057944, loss_ce: 0.019424
2021-12-13 00:17:01,565 iteration 3151 : loss : 0.059493, loss_ce: 0.011662
2021-12-13 00:17:03,028 iteration 3152 : loss : 0.048154, loss_ce: 0.013441
2021-12-13 00:17:04,526 iteration 3153 : loss : 0.059545, loss_ce: 0.015213
2021-12-13 00:17:05,977 iteration 3154 : loss : 0.056488, loss_ce: 0.018900
2021-12-13 00:17:07,440 iteration 3155 : loss : 0.057401, loss_ce: 0.015974
2021-12-13 00:17:08,992 iteration 3156 : loss : 0.065441, loss_ce: 0.020909
2021-12-13 00:17:10,517 iteration 3157 : loss : 0.057919, loss_ce: 0.017991
2021-12-13 00:17:11,987 iteration 3158 : loss : 0.063130, loss_ce: 0.022278
2021-12-13 00:17:13,467 iteration 3159 : loss : 0.057289, loss_ce: 0.016485
2021-12-13 00:17:14,950 iteration 3160 : loss : 0.057981, loss_ce: 0.018430
2021-12-13 00:17:16,377 iteration 3161 : loss : 0.054826, loss_ce: 0.014287
2021-12-13 00:17:17,938 iteration 3162 : loss : 0.058971, loss_ce: 0.017663
 46%|████████████▌              | 186/400 [1:27:15<1:40:11, 28.09s/it]2021-12-13 00:17:19,537 iteration 3163 : loss : 0.068648, loss_ce: 0.024708
2021-12-13 00:17:21,069 iteration 3164 : loss : 0.049937, loss_ce: 0.013096
2021-12-13 00:17:22,541 iteration 3165 : loss : 0.048075, loss_ce: 0.012434
2021-12-13 00:17:24,099 iteration 3166 : loss : 0.063108, loss_ce: 0.015748
2021-12-13 00:17:25,589 iteration 3167 : loss : 0.048961, loss_ce: 0.013855
2021-12-13 00:17:27,214 iteration 3168 : loss : 0.065527, loss_ce: 0.023514
2021-12-13 00:17:28,805 iteration 3169 : loss : 0.063703, loss_ce: 0.022065
2021-12-13 00:17:30,278 iteration 3170 : loss : 0.054533, loss_ce: 0.018141
2021-12-13 00:17:31,691 iteration 3171 : loss : 0.058735, loss_ce: 0.020286
2021-12-13 00:17:33,206 iteration 3172 : loss : 0.059092, loss_ce: 0.016182
2021-12-13 00:17:34,753 iteration 3173 : loss : 0.060041, loss_ce: 0.018418
2021-12-13 00:17:36,243 iteration 3174 : loss : 0.062204, loss_ce: 0.022084
2021-12-13 00:17:37,833 iteration 3175 : loss : 0.077483, loss_ce: 0.018699
2021-12-13 00:17:39,324 iteration 3176 : loss : 0.064043, loss_ce: 0.018199
2021-12-13 00:17:40,872 iteration 3177 : loss : 0.072581, loss_ce: 0.021254
2021-12-13 00:17:42,300 iteration 3178 : loss : 0.060522, loss_ce: 0.017115
2021-12-13 00:17:43,766 iteration 3179 : loss : 0.068130, loss_ce: 0.018996
 47%|████████████▌              | 187/400 [1:27:41<1:37:18, 27.41s/it]2021-12-13 00:17:45,356 iteration 3180 : loss : 0.064838, loss_ce: 0.021165
2021-12-13 00:17:46,812 iteration 3181 : loss : 0.053133, loss_ce: 0.014089
2021-12-13 00:17:48,324 iteration 3182 : loss : 0.061267, loss_ce: 0.017355
2021-12-13 00:17:49,790 iteration 3183 : loss : 0.057875, loss_ce: 0.013104
2021-12-13 00:17:51,289 iteration 3184 : loss : 0.063316, loss_ce: 0.019569
2021-12-13 00:17:52,826 iteration 3185 : loss : 0.060967, loss_ce: 0.018374
2021-12-13 00:17:54,237 iteration 3186 : loss : 0.054323, loss_ce: 0.018238
2021-12-13 00:17:55,740 iteration 3187 : loss : 0.051059, loss_ce: 0.015081
2021-12-13 00:17:57,213 iteration 3188 : loss : 0.060391, loss_ce: 0.019885
2021-12-13 00:17:58,729 iteration 3189 : loss : 0.066447, loss_ce: 0.022568
2021-12-13 00:18:00,217 iteration 3190 : loss : 0.056981, loss_ce: 0.016358
2021-12-13 00:18:01,836 iteration 3191 : loss : 0.063309, loss_ce: 0.013114
2021-12-13 00:18:03,288 iteration 3192 : loss : 0.060056, loss_ce: 0.014630
2021-12-13 00:18:04,831 iteration 3193 : loss : 0.063594, loss_ce: 0.022475
2021-12-13 00:18:06,373 iteration 3194 : loss : 0.061490, loss_ce: 0.015289
2021-12-13 00:18:07,880 iteration 3195 : loss : 0.063093, loss_ce: 0.022057
2021-12-13 00:18:09,370 iteration 3196 : loss : 0.065301, loss_ce: 0.022462
 47%|████████████▋              | 188/400 [1:28:06<1:34:56, 26.87s/it]2021-12-13 00:18:10,834 iteration 3197 : loss : 0.052150, loss_ce: 0.016757
2021-12-13 00:18:12,308 iteration 3198 : loss : 0.054861, loss_ce: 0.016671
2021-12-13 00:18:13,799 iteration 3199 : loss : 0.060741, loss_ce: 0.019341
2021-12-13 00:18:15,398 iteration 3200 : loss : 0.060572, loss_ce: 0.021037
2021-12-13 00:18:16,921 iteration 3201 : loss : 0.067556, loss_ce: 0.013159
2021-12-13 00:18:18,418 iteration 3202 : loss : 0.062081, loss_ce: 0.017454
2021-12-13 00:18:20,036 iteration 3203 : loss : 0.056232, loss_ce: 0.018427
2021-12-13 00:18:21,528 iteration 3204 : loss : 0.060109, loss_ce: 0.014451
2021-12-13 00:18:23,092 iteration 3205 : loss : 0.057248, loss_ce: 0.014212
2021-12-13 00:18:24,602 iteration 3206 : loss : 0.052923, loss_ce: 0.016038
2021-12-13 00:18:26,203 iteration 3207 : loss : 0.065646, loss_ce: 0.020781
2021-12-13 00:18:27,644 iteration 3208 : loss : 0.056031, loss_ce: 0.013245
2021-12-13 00:18:29,091 iteration 3209 : loss : 0.059230, loss_ce: 0.023898
2021-12-13 00:18:30,479 iteration 3210 : loss : 0.062337, loss_ce: 0.022825
2021-12-13 00:18:31,953 iteration 3211 : loss : 0.063864, loss_ce: 0.013887
2021-12-13 00:18:33,416 iteration 3212 : loss : 0.056818, loss_ce: 0.017072
2021-12-13 00:18:34,908 iteration 3213 : loss : 0.063681, loss_ce: 0.012789
 47%|████████████▊              | 189/400 [1:28:32<1:33:05, 26.47s/it]2021-12-13 00:18:36,454 iteration 3214 : loss : 0.055548, loss_ce: 0.018086
2021-12-13 00:18:38,040 iteration 3215 : loss : 0.069918, loss_ce: 0.022803
2021-12-13 00:18:39,593 iteration 3216 : loss : 0.057295, loss_ce: 0.016224
2021-12-13 00:18:41,155 iteration 3217 : loss : 0.070556, loss_ce: 0.015904
2021-12-13 00:18:42,675 iteration 3218 : loss : 0.060692, loss_ce: 0.018472
2021-12-13 00:18:44,149 iteration 3219 : loss : 0.054393, loss_ce: 0.015891
2021-12-13 00:18:45,688 iteration 3220 : loss : 0.060902, loss_ce: 0.015638
2021-12-13 00:18:47,147 iteration 3221 : loss : 0.054391, loss_ce: 0.016742
2021-12-13 00:18:48,648 iteration 3222 : loss : 0.063837, loss_ce: 0.027927
2021-12-13 00:18:50,156 iteration 3223 : loss : 0.065905, loss_ce: 0.017744
2021-12-13 00:18:51,714 iteration 3224 : loss : 0.059034, loss_ce: 0.016442
2021-12-13 00:18:53,197 iteration 3225 : loss : 0.062534, loss_ce: 0.015850
2021-12-13 00:18:54,795 iteration 3226 : loss : 0.066710, loss_ce: 0.018375
2021-12-13 00:18:56,320 iteration 3227 : loss : 0.053031, loss_ce: 0.014602
2021-12-13 00:18:57,742 iteration 3228 : loss : 0.053576, loss_ce: 0.018866
2021-12-13 00:18:59,323 iteration 3229 : loss : 0.058180, loss_ce: 0.018692
2021-12-13 00:18:59,323 Training Data Eval:
2021-12-13 00:19:06,970   Average segmentation loss on training set: 0.0458
2021-12-13 00:19:06,971 Validation Data Eval:
2021-12-13 00:19:09,578   Average segmentation loss on validation set: 0.0956
2021-12-13 00:19:11,176 iteration 3230 : loss : 0.073003, loss_ce: 0.022588
 48%|████████████▊              | 190/400 [1:29:08<1:42:55, 29.41s/it]2021-12-13 00:19:12,690 iteration 3231 : loss : 0.060520, loss_ce: 0.017244
2021-12-13 00:19:14,187 iteration 3232 : loss : 0.054751, loss_ce: 0.018531
2021-12-13 00:19:15,669 iteration 3233 : loss : 0.058505, loss_ce: 0.015917
2021-12-13 00:19:17,106 iteration 3234 : loss : 0.056236, loss_ce: 0.015841
2021-12-13 00:19:18,557 iteration 3235 : loss : 0.050099, loss_ce: 0.011635
2021-12-13 00:19:20,078 iteration 3236 : loss : 0.051887, loss_ce: 0.015403
2021-12-13 00:19:21,615 iteration 3237 : loss : 0.056495, loss_ce: 0.019113
2021-12-13 00:19:23,129 iteration 3238 : loss : 0.064207, loss_ce: 0.013902
2021-12-13 00:19:24,618 iteration 3239 : loss : 0.057813, loss_ce: 0.017797
2021-12-13 00:19:26,132 iteration 3240 : loss : 0.063598, loss_ce: 0.018775
2021-12-13 00:19:27,683 iteration 3241 : loss : 0.061861, loss_ce: 0.019493
2021-12-13 00:19:29,288 iteration 3242 : loss : 0.063128, loss_ce: 0.021142
2021-12-13 00:19:30,749 iteration 3243 : loss : 0.055188, loss_ce: 0.019082
2021-12-13 00:19:32,251 iteration 3244 : loss : 0.072392, loss_ce: 0.016435
2021-12-13 00:19:33,726 iteration 3245 : loss : 0.057123, loss_ce: 0.016988
2021-12-13 00:19:35,291 iteration 3246 : loss : 0.071999, loss_ce: 0.024695
2021-12-13 00:19:36,819 iteration 3247 : loss : 0.059390, loss_ce: 0.014845
 48%|████████████▉              | 191/400 [1:29:34<1:38:30, 28.28s/it]2021-12-13 00:19:38,538 iteration 3248 : loss : 0.068598, loss_ce: 0.026221
2021-12-13 00:19:39,962 iteration 3249 : loss : 0.052072, loss_ce: 0.012321
2021-12-13 00:19:41,495 iteration 3250 : loss : 0.064859, loss_ce: 0.019961
2021-12-13 00:19:43,003 iteration 3251 : loss : 0.099032, loss_ce: 0.015960
2021-12-13 00:19:44,448 iteration 3252 : loss : 0.070088, loss_ce: 0.024706
2021-12-13 00:19:45,976 iteration 3253 : loss : 0.058030, loss_ce: 0.019151
2021-12-13 00:19:47,484 iteration 3254 : loss : 0.060515, loss_ce: 0.015802
2021-12-13 00:19:49,033 iteration 3255 : loss : 0.056731, loss_ce: 0.017417
2021-12-13 00:19:50,474 iteration 3256 : loss : 0.059301, loss_ce: 0.025193
2021-12-13 00:19:51,955 iteration 3257 : loss : 0.065560, loss_ce: 0.018824
2021-12-13 00:19:53,525 iteration 3258 : loss : 0.062449, loss_ce: 0.018978
2021-12-13 00:19:55,014 iteration 3259 : loss : 0.054166, loss_ce: 0.016207
2021-12-13 00:19:56,474 iteration 3260 : loss : 0.053645, loss_ce: 0.016641
2021-12-13 00:19:58,027 iteration 3261 : loss : 0.069079, loss_ce: 0.022454
2021-12-13 00:19:59,513 iteration 3262 : loss : 0.062318, loss_ce: 0.020206
2021-12-13 00:20:00,992 iteration 3263 : loss : 0.071589, loss_ce: 0.014253
2021-12-13 00:20:02,460 iteration 3264 : loss : 0.058256, loss_ce: 0.018519
 48%|████████████▉              | 192/400 [1:29:59<1:35:17, 27.49s/it]2021-12-13 00:20:03,870 iteration 3265 : loss : 0.049984, loss_ce: 0.014442
2021-12-13 00:20:05,388 iteration 3266 : loss : 0.053606, loss_ce: 0.016904
2021-12-13 00:20:06,830 iteration 3267 : loss : 0.052688, loss_ce: 0.015496
2021-12-13 00:20:08,434 iteration 3268 : loss : 0.065853, loss_ce: 0.020977
2021-12-13 00:20:09,864 iteration 3269 : loss : 0.050588, loss_ce: 0.013897
2021-12-13 00:20:11,300 iteration 3270 : loss : 0.055495, loss_ce: 0.016375
2021-12-13 00:20:12,858 iteration 3271 : loss : 0.067306, loss_ce: 0.020987
2021-12-13 00:20:14,321 iteration 3272 : loss : 0.058585, loss_ce: 0.017920
2021-12-13 00:20:15,769 iteration 3273 : loss : 0.063253, loss_ce: 0.018436
2021-12-13 00:20:17,295 iteration 3274 : loss : 0.058645, loss_ce: 0.015918
2021-12-13 00:20:18,731 iteration 3275 : loss : 0.056116, loss_ce: 0.016737
2021-12-13 00:20:20,214 iteration 3276 : loss : 0.052522, loss_ce: 0.013049
2021-12-13 00:20:21,769 iteration 3277 : loss : 0.054907, loss_ce: 0.018636
2021-12-13 00:20:23,254 iteration 3278 : loss : 0.060552, loss_ce: 0.018075
2021-12-13 00:20:24,798 iteration 3279 : loss : 0.064790, loss_ce: 0.018886
2021-12-13 00:20:26,309 iteration 3280 : loss : 0.061937, loss_ce: 0.020789
2021-12-13 00:20:27,807 iteration 3281 : loss : 0.051840, loss_ce: 0.011773
 48%|█████████████              | 193/400 [1:30:25<1:32:37, 26.85s/it]2021-12-13 00:20:29,302 iteration 3282 : loss : 0.053564, loss_ce: 0.017192
2021-12-13 00:20:30,902 iteration 3283 : loss : 0.068271, loss_ce: 0.018353
2021-12-13 00:20:32,361 iteration 3284 : loss : 0.050765, loss_ce: 0.017426
2021-12-13 00:20:33,760 iteration 3285 : loss : 0.049380, loss_ce: 0.013587
2021-12-13 00:20:35,312 iteration 3286 : loss : 0.061424, loss_ce: 0.011783
2021-12-13 00:20:36,844 iteration 3287 : loss : 0.062608, loss_ce: 0.022454
2021-12-13 00:20:38,343 iteration 3288 : loss : 0.054986, loss_ce: 0.014279
2021-12-13 00:20:39,852 iteration 3289 : loss : 0.057050, loss_ce: 0.012759
2021-12-13 00:20:41,290 iteration 3290 : loss : 0.050117, loss_ce: 0.011718
2021-12-13 00:20:42,840 iteration 3291 : loss : 0.060564, loss_ce: 0.016731
2021-12-13 00:20:44,307 iteration 3292 : loss : 0.058348, loss_ce: 0.020791
2021-12-13 00:20:45,728 iteration 3293 : loss : 0.053806, loss_ce: 0.016263
2021-12-13 00:20:47,167 iteration 3294 : loss : 0.052639, loss_ce: 0.015865
2021-12-13 00:20:48,650 iteration 3295 : loss : 0.054214, loss_ce: 0.018224
2021-12-13 00:20:50,157 iteration 3296 : loss : 0.060682, loss_ce: 0.017639
2021-12-13 00:20:51,622 iteration 3297 : loss : 0.065762, loss_ce: 0.022535
2021-12-13 00:20:53,037 iteration 3298 : loss : 0.054820, loss_ce: 0.016372
 48%|█████████████              | 194/400 [1:30:50<1:30:30, 26.36s/it]2021-12-13 00:20:54,517 iteration 3299 : loss : 0.052854, loss_ce: 0.017421
2021-12-13 00:20:55,981 iteration 3300 : loss : 0.057335, loss_ce: 0.019035
2021-12-13 00:20:57,510 iteration 3301 : loss : 0.057742, loss_ce: 0.014114
2021-12-13 00:20:58,935 iteration 3302 : loss : 0.055701, loss_ce: 0.013538
2021-12-13 00:21:00,484 iteration 3303 : loss : 0.064809, loss_ce: 0.018999
2021-12-13 00:21:02,000 iteration 3304 : loss : 0.055789, loss_ce: 0.019818
2021-12-13 00:21:03,538 iteration 3305 : loss : 0.060731, loss_ce: 0.016261
2021-12-13 00:21:04,990 iteration 3306 : loss : 0.049037, loss_ce: 0.015011
2021-12-13 00:21:06,565 iteration 3307 : loss : 0.060961, loss_ce: 0.018582
2021-12-13 00:21:08,135 iteration 3308 : loss : 0.068872, loss_ce: 0.020279
2021-12-13 00:21:09,651 iteration 3309 : loss : 0.049483, loss_ce: 0.014153
2021-12-13 00:21:11,219 iteration 3310 : loss : 0.064139, loss_ce: 0.015099
2021-12-13 00:21:12,698 iteration 3311 : loss : 0.061869, loss_ce: 0.014537
2021-12-13 00:21:14,125 iteration 3312 : loss : 0.052448, loss_ce: 0.017050
2021-12-13 00:21:15,663 iteration 3313 : loss : 0.063711, loss_ce: 0.020370
2021-12-13 00:21:17,034 iteration 3314 : loss : 0.054550, loss_ce: 0.018402
2021-12-13 00:21:17,035 Training Data Eval:
2021-12-13 00:21:24,681   Average segmentation loss on training set: 0.0444
2021-12-13 00:21:24,681 Validation Data Eval:
2021-12-13 00:21:27,281   Average segmentation loss on validation set: 0.0931
2021-12-13 00:21:33,757 Found new lowest validation loss at iteration 3314! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/TU_3seeds/TU_SGD_best_val_loss_seed100.pth
2021-12-13 00:21:35,096 iteration 3315 : loss : 0.057716, loss_ce: 0.017809
 49%|█████████████▏             | 195/400 [1:31:32<1:46:09, 31.07s/it]2021-12-13 00:21:36,502 iteration 3316 : loss : 0.052524, loss_ce: 0.011080
2021-12-13 00:21:37,892 iteration 3317 : loss : 0.057411, loss_ce: 0.016799
2021-12-13 00:21:39,297 iteration 3318 : loss : 0.063328, loss_ce: 0.021190
2021-12-13 00:21:40,724 iteration 3319 : loss : 0.052446, loss_ce: 0.014884
2021-12-13 00:21:42,320 iteration 3320 : loss : 0.064352, loss_ce: 0.017949
2021-12-13 00:21:43,742 iteration 3321 : loss : 0.053704, loss_ce: 0.018304
2021-12-13 00:21:45,130 iteration 3322 : loss : 0.052582, loss_ce: 0.012835
2021-12-13 00:21:46,641 iteration 3323 : loss : 0.059191, loss_ce: 0.018847
2021-12-13 00:21:48,137 iteration 3324 : loss : 0.056324, loss_ce: 0.015265
2021-12-13 00:21:49,695 iteration 3325 : loss : 0.063596, loss_ce: 0.017259
2021-12-13 00:21:51,203 iteration 3326 : loss : 0.050946, loss_ce: 0.013764
2021-12-13 00:21:52,854 iteration 3327 : loss : 0.069283, loss_ce: 0.016596
2021-12-13 00:21:54,310 iteration 3328 : loss : 0.058736, loss_ce: 0.023635
2021-12-13 00:21:55,769 iteration 3329 : loss : 0.060494, loss_ce: 0.015853
2021-12-13 00:21:57,281 iteration 3330 : loss : 0.059221, loss_ce: 0.021255
2021-12-13 00:21:58,677 iteration 3331 : loss : 0.053305, loss_ce: 0.014809
2021-12-13 00:22:00,054 iteration 3332 : loss : 0.052618, loss_ce: 0.014206
 49%|█████████████▏             | 196/400 [1:31:57<1:39:24, 29.24s/it]2021-12-13 00:22:01,570 iteration 3333 : loss : 0.057501, loss_ce: 0.017163
2021-12-13 00:22:03,067 iteration 3334 : loss : 0.056632, loss_ce: 0.019285
2021-12-13 00:22:04,621 iteration 3335 : loss : 0.059468, loss_ce: 0.018664
2021-12-13 00:22:06,061 iteration 3336 : loss : 0.054355, loss_ce: 0.015503
2021-12-13 00:22:07,487 iteration 3337 : loss : 0.055928, loss_ce: 0.016051
2021-12-13 00:22:09,010 iteration 3338 : loss : 0.054869, loss_ce: 0.015684
2021-12-13 00:22:10,548 iteration 3339 : loss : 0.069125, loss_ce: 0.016969
2021-12-13 00:22:12,047 iteration 3340 : loss : 0.059816, loss_ce: 0.017259
2021-12-13 00:22:13,499 iteration 3341 : loss : 0.058089, loss_ce: 0.016622
2021-12-13 00:22:14,944 iteration 3342 : loss : 0.056671, loss_ce: 0.022120
2021-12-13 00:22:16,414 iteration 3343 : loss : 0.060785, loss_ce: 0.020078
2021-12-13 00:22:17,961 iteration 3344 : loss : 0.070587, loss_ce: 0.020172
2021-12-13 00:22:19,514 iteration 3345 : loss : 0.059691, loss_ce: 0.021913
2021-12-13 00:22:21,013 iteration 3346 : loss : 0.069490, loss_ce: 0.018419
2021-12-13 00:22:22,484 iteration 3347 : loss : 0.057975, loss_ce: 0.016828
2021-12-13 00:22:24,050 iteration 3348 : loss : 0.058519, loss_ce: 0.019313
2021-12-13 00:22:25,487 iteration 3349 : loss : 0.057287, loss_ce: 0.011610
 49%|█████████████▎             | 197/400 [1:32:22<1:35:03, 28.10s/it]2021-12-13 00:22:26,960 iteration 3350 : loss : 0.054340, loss_ce: 0.018852
2021-12-13 00:22:28,481 iteration 3351 : loss : 0.059840, loss_ce: 0.017553
2021-12-13 00:22:29,927 iteration 3352 : loss : 0.056892, loss_ce: 0.015320
2021-12-13 00:22:31,414 iteration 3353 : loss : 0.054491, loss_ce: 0.013900
2021-12-13 00:22:32,861 iteration 3354 : loss : 0.059317, loss_ce: 0.011315
2021-12-13 00:22:34,358 iteration 3355 : loss : 0.053267, loss_ce: 0.017240
2021-12-13 00:22:35,794 iteration 3356 : loss : 0.060058, loss_ce: 0.019271
2021-12-13 00:22:37,228 iteration 3357 : loss : 0.051846, loss_ce: 0.016511
2021-12-13 00:22:38,702 iteration 3358 : loss : 0.052144, loss_ce: 0.012829
2021-12-13 00:22:40,162 iteration 3359 : loss : 0.059293, loss_ce: 0.018464
2021-12-13 00:22:41,693 iteration 3360 : loss : 0.053222, loss_ce: 0.013159
2021-12-13 00:22:43,190 iteration 3361 : loss : 0.062623, loss_ce: 0.019897
2021-12-13 00:22:44,639 iteration 3362 : loss : 0.057466, loss_ce: 0.015636
2021-12-13 00:22:46,025 iteration 3363 : loss : 0.057421, loss_ce: 0.016492
2021-12-13 00:22:47,561 iteration 3364 : loss : 0.058877, loss_ce: 0.017597
2021-12-13 00:22:49,033 iteration 3365 : loss : 0.055580, loss_ce: 0.016824
2021-12-13 00:22:50,501 iteration 3366 : loss : 0.053985, loss_ce: 0.017026
 50%|█████████████▎             | 198/400 [1:32:47<1:31:28, 27.17s/it]2021-12-13 00:22:52,142 iteration 3367 : loss : 0.060546, loss_ce: 0.019815
2021-12-13 00:22:53,625 iteration 3368 : loss : 0.057436, loss_ce: 0.022142
2021-12-13 00:22:55,130 iteration 3369 : loss : 0.056152, loss_ce: 0.018289
2021-12-13 00:22:56,561 iteration 3370 : loss : 0.061816, loss_ce: 0.013553
2021-12-13 00:22:58,037 iteration 3371 : loss : 0.051602, loss_ce: 0.015309
2021-12-13 00:22:59,578 iteration 3372 : loss : 0.069447, loss_ce: 0.017085
2021-12-13 00:23:01,085 iteration 3373 : loss : 0.059346, loss_ce: 0.018218
2021-12-13 00:23:02,659 iteration 3374 : loss : 0.053856, loss_ce: 0.017414
2021-12-13 00:23:04,261 iteration 3375 : loss : 0.067141, loss_ce: 0.015753
2021-12-13 00:23:05,670 iteration 3376 : loss : 0.048528, loss_ce: 0.012748
2021-12-13 00:23:07,211 iteration 3377 : loss : 0.057227, loss_ce: 0.017488
2021-12-13 00:23:08,660 iteration 3378 : loss : 0.056142, loss_ce: 0.012835
2021-12-13 00:23:10,181 iteration 3379 : loss : 0.055005, loss_ce: 0.014325
2021-12-13 00:23:11,617 iteration 3380 : loss : 0.056764, loss_ce: 0.014137
2021-12-13 00:23:13,118 iteration 3381 : loss : 0.058136, loss_ce: 0.022156
2021-12-13 00:23:14,645 iteration 3382 : loss : 0.061647, loss_ce: 0.017098
2021-12-13 00:23:16,122 iteration 3383 : loss : 0.055592, loss_ce: 0.016514
 50%|█████████████▍             | 199/400 [1:33:13<1:29:27, 26.71s/it]2021-12-13 00:23:17,655 iteration 3384 : loss : 0.050031, loss_ce: 0.013364
2021-12-13 00:23:19,111 iteration 3385 : loss : 0.063090, loss_ce: 0.021770
2021-12-13 00:23:20,552 iteration 3386 : loss : 0.053953, loss_ce: 0.014505
2021-12-13 00:23:21,981 iteration 3387 : loss : 0.062215, loss_ce: 0.016204
2021-12-13 00:23:23,521 iteration 3388 : loss : 0.059732, loss_ce: 0.017514
2021-12-13 00:23:24,954 iteration 3389 : loss : 0.057322, loss_ce: 0.014779
2021-12-13 00:23:26,452 iteration 3390 : loss : 0.052065, loss_ce: 0.016141
2021-12-13 00:23:28,041 iteration 3391 : loss : 0.055730, loss_ce: 0.021410
2021-12-13 00:23:29,575 iteration 3392 : loss : 0.059723, loss_ce: 0.017402
2021-12-13 00:23:30,998 iteration 3393 : loss : 0.057522, loss_ce: 0.020197
2021-12-13 00:23:32,465 iteration 3394 : loss : 0.057579, loss_ce: 0.016789
2021-12-13 00:23:33,960 iteration 3395 : loss : 0.051725, loss_ce: 0.016570
2021-12-13 00:23:35,449 iteration 3396 : loss : 0.059307, loss_ce: 0.016747
2021-12-13 00:23:36,870 iteration 3397 : loss : 0.058075, loss_ce: 0.016787
2021-12-13 00:23:38,498 iteration 3398 : loss : 0.071834, loss_ce: 0.019438
2021-12-13 00:23:40,005 iteration 3399 : loss : 0.069849, loss_ce: 0.031334
2021-12-13 00:23:40,005 Training Data Eval:
2021-12-13 00:23:47,642   Average segmentation loss on training set: 0.0438
2021-12-13 00:23:47,642 Validation Data Eval:
2021-12-13 00:23:50,241   Average segmentation loss on validation set: 0.0969
2021-12-13 00:23:51,731 iteration 3400 : loss : 0.061571, loss_ce: 0.014564
 50%|█████████████▌             | 200/400 [1:33:48<1:37:54, 29.37s/it]2021-12-13 00:23:53,300 iteration 3401 : loss : 0.074456, loss_ce: 0.023425
2021-12-13 00:23:54,814 iteration 3402 : loss : 0.058962, loss_ce: 0.019810
2021-12-13 00:23:56,398 iteration 3403 : loss : 0.060736, loss_ce: 0.017922
2021-12-13 00:23:57,839 iteration 3404 : loss : 0.053953, loss_ce: 0.012755
2021-12-13 00:23:59,309 iteration 3405 : loss : 0.056297, loss_ce: 0.013004
2021-12-13 00:24:00,718 iteration 3406 : loss : 0.055405, loss_ce: 0.019499
2021-12-13 00:24:02,158 iteration 3407 : loss : 0.052559, loss_ce: 0.013720
2021-12-13 00:24:03,642 iteration 3408 : loss : 0.053041, loss_ce: 0.015598
2021-12-13 00:24:05,119 iteration 3409 : loss : 0.060432, loss_ce: 0.011844
2021-12-13 00:24:06,598 iteration 3410 : loss : 0.053874, loss_ce: 0.016118
2021-12-13 00:24:08,127 iteration 3411 : loss : 0.074178, loss_ce: 0.017632
2021-12-13 00:24:09,682 iteration 3412 : loss : 0.053112, loss_ce: 0.017783
2021-12-13 00:24:11,272 iteration 3413 : loss : 0.059649, loss_ce: 0.022329
2021-12-13 00:24:12,828 iteration 3414 : loss : 0.073471, loss_ce: 0.017298
2021-12-13 00:24:14,215 iteration 3415 : loss : 0.052352, loss_ce: 0.016716
2021-12-13 00:24:15,730 iteration 3416 : loss : 0.053579, loss_ce: 0.015693
2021-12-13 00:24:17,198 iteration 3417 : loss : 0.054416, loss_ce: 0.017103
 50%|█████████████▌             | 201/400 [1:34:14<1:33:32, 28.21s/it]2021-12-13 00:24:18,831 iteration 3418 : loss : 0.073118, loss_ce: 0.016786
2021-12-13 00:24:20,345 iteration 3419 : loss : 0.067257, loss_ce: 0.028012
2021-12-13 00:24:21,774 iteration 3420 : loss : 0.056214, loss_ce: 0.022718
2021-12-13 00:24:23,270 iteration 3421 : loss : 0.066340, loss_ce: 0.015937
2021-12-13 00:24:24,754 iteration 3422 : loss : 0.053773, loss_ce: 0.014268
2021-12-13 00:24:26,302 iteration 3423 : loss : 0.058834, loss_ce: 0.018575
2021-12-13 00:24:27,726 iteration 3424 : loss : 0.052754, loss_ce: 0.013360
2021-12-13 00:24:29,229 iteration 3425 : loss : 0.056209, loss_ce: 0.020871
2021-12-13 00:24:30,695 iteration 3426 : loss : 0.050458, loss_ce: 0.010558
2021-12-13 00:24:32,301 iteration 3427 : loss : 0.066866, loss_ce: 0.019582
2021-12-13 00:24:33,807 iteration 3428 : loss : 0.056780, loss_ce: 0.017485
2021-12-13 00:24:35,322 iteration 3429 : loss : 0.056817, loss_ce: 0.020158
2021-12-13 00:24:36,835 iteration 3430 : loss : 0.049783, loss_ce: 0.012241
2021-12-13 00:24:38,391 iteration 3431 : loss : 0.063665, loss_ce: 0.018704
2021-12-13 00:24:39,843 iteration 3432 : loss : 0.056960, loss_ce: 0.014021
2021-12-13 00:24:41,338 iteration 3433 : loss : 0.053828, loss_ce: 0.012543
2021-12-13 00:24:42,800 iteration 3434 : loss : 0.064776, loss_ce: 0.015550
 50%|█████████████▋             | 202/400 [1:34:40<1:30:29, 27.42s/it]2021-12-13 00:24:44,263 iteration 3435 : loss : 0.051637, loss_ce: 0.018648
2021-12-13 00:24:45,725 iteration 3436 : loss : 0.057735, loss_ce: 0.016691
2021-12-13 00:24:47,160 iteration 3437 : loss : 0.054867, loss_ce: 0.014951
2021-12-13 00:24:48,632 iteration 3438 : loss : 0.056425, loss_ce: 0.021583
2021-12-13 00:24:50,196 iteration 3439 : loss : 0.057824, loss_ce: 0.016962
2021-12-13 00:24:51,758 iteration 3440 : loss : 0.067213, loss_ce: 0.016849
2021-12-13 00:24:53,133 iteration 3441 : loss : 0.049542, loss_ce: 0.018404
2021-12-13 00:24:54,581 iteration 3442 : loss : 0.047074, loss_ce: 0.012587
2021-12-13 00:24:56,099 iteration 3443 : loss : 0.052965, loss_ce: 0.015762
2021-12-13 00:24:57,552 iteration 3444 : loss : 0.059570, loss_ce: 0.017145
2021-12-13 00:24:59,075 iteration 3445 : loss : 0.054143, loss_ce: 0.015953
2021-12-13 00:25:00,558 iteration 3446 : loss : 0.065593, loss_ce: 0.019611
2021-12-13 00:25:02,045 iteration 3447 : loss : 0.057080, loss_ce: 0.022439
2021-12-13 00:25:03,446 iteration 3448 : loss : 0.056007, loss_ce: 0.015702
2021-12-13 00:25:04,943 iteration 3449 : loss : 0.059940, loss_ce: 0.015853
2021-12-13 00:25:06,479 iteration 3450 : loss : 0.055491, loss_ce: 0.014281
2021-12-13 00:25:07,910 iteration 3451 : loss : 0.050921, loss_ce: 0.017172
 51%|█████████████▋             | 203/400 [1:35:05<1:27:45, 26.73s/it]2021-12-13 00:25:09,427 iteration 3452 : loss : 0.049368, loss_ce: 0.012326
2021-12-13 00:25:10,965 iteration 3453 : loss : 0.052067, loss_ce: 0.014079
2021-12-13 00:25:12,521 iteration 3454 : loss : 0.064792, loss_ce: 0.017142
2021-12-13 00:25:14,040 iteration 3455 : loss : 0.056965, loss_ce: 0.019245
2021-12-13 00:25:15,553 iteration 3456 : loss : 0.064094, loss_ce: 0.016560
2021-12-13 00:25:17,075 iteration 3457 : loss : 0.057962, loss_ce: 0.018126
2021-12-13 00:25:18,607 iteration 3458 : loss : 0.049548, loss_ce: 0.017799
2021-12-13 00:25:20,190 iteration 3459 : loss : 0.058665, loss_ce: 0.020517
2021-12-13 00:25:21,724 iteration 3460 : loss : 0.056608, loss_ce: 0.015955
2021-12-13 00:25:23,224 iteration 3461 : loss : 0.056782, loss_ce: 0.018771
2021-12-13 00:25:24,665 iteration 3462 : loss : 0.053614, loss_ce: 0.016660
2021-12-13 00:25:26,067 iteration 3463 : loss : 0.048716, loss_ce: 0.012488
2021-12-13 00:25:27,538 iteration 3464 : loss : 0.057189, loss_ce: 0.016359
2021-12-13 00:25:28,967 iteration 3465 : loss : 0.053833, loss_ce: 0.014412
2021-12-13 00:25:30,480 iteration 3466 : loss : 0.054627, loss_ce: 0.016834
2021-12-13 00:25:32,064 iteration 3467 : loss : 0.069565, loss_ce: 0.022672
2021-12-13 00:25:33,692 iteration 3468 : loss : 0.057467, loss_ce: 0.015266
 51%|█████████████▊             | 204/400 [1:35:30<1:26:23, 26.45s/it]2021-12-13 00:25:35,150 iteration 3469 : loss : 0.051504, loss_ce: 0.015580
2021-12-13 00:25:36,803 iteration 3470 : loss : 0.060029, loss_ce: 0.016666
2021-12-13 00:25:38,348 iteration 3471 : loss : 0.059148, loss_ce: 0.016567
2021-12-13 00:25:39,906 iteration 3472 : loss : 0.056745, loss_ce: 0.020158
2021-12-13 00:25:41,472 iteration 3473 : loss : 0.057484, loss_ce: 0.021819
2021-12-13 00:25:42,939 iteration 3474 : loss : 0.053094, loss_ce: 0.015810
2021-12-13 00:25:44,524 iteration 3475 : loss : 0.056911, loss_ce: 0.016047
2021-12-13 00:25:46,013 iteration 3476 : loss : 0.061181, loss_ce: 0.018523
2021-12-13 00:25:47,556 iteration 3477 : loss : 0.059867, loss_ce: 0.016473
2021-12-13 00:25:49,068 iteration 3478 : loss : 0.066397, loss_ce: 0.018159
2021-12-13 00:25:50,543 iteration 3479 : loss : 0.054844, loss_ce: 0.018341
2021-12-13 00:25:51,991 iteration 3480 : loss : 0.051695, loss_ce: 0.015449
2021-12-13 00:25:53,486 iteration 3481 : loss : 0.071354, loss_ce: 0.031976
2021-12-13 00:25:54,897 iteration 3482 : loss : 0.059953, loss_ce: 0.017584
2021-12-13 00:25:56,450 iteration 3483 : loss : 0.057092, loss_ce: 0.016174
2021-12-13 00:25:57,946 iteration 3484 : loss : 0.049198, loss_ce: 0.011097
2021-12-13 00:25:57,946 Training Data Eval:
2021-12-13 00:26:05,591   Average segmentation loss on training set: 0.0447
2021-12-13 00:26:05,591 Validation Data Eval:
2021-12-13 00:26:08,196   Average segmentation loss on validation set: 0.0953
2021-12-13 00:26:09,680 iteration 3485 : loss : 0.051509, loss_ce: 0.014886
 51%|█████████████▊             | 205/400 [1:36:06<1:35:15, 29.31s/it]2021-12-13 00:26:11,170 iteration 3486 : loss : 0.051112, loss_ce: 0.013323
2021-12-13 00:26:12,661 iteration 3487 : loss : 0.056352, loss_ce: 0.017733
2021-12-13 00:26:14,201 iteration 3488 : loss : 0.057092, loss_ce: 0.025119
2021-12-13 00:26:15,648 iteration 3489 : loss : 0.060296, loss_ce: 0.015446
2021-12-13 00:26:17,215 iteration 3490 : loss : 0.059970, loss_ce: 0.017497
2021-12-13 00:26:18,747 iteration 3491 : loss : 0.063231, loss_ce: 0.023611
2021-12-13 00:26:20,146 iteration 3492 : loss : 0.047712, loss_ce: 0.011384
2021-12-13 00:26:21,722 iteration 3493 : loss : 0.059452, loss_ce: 0.013518
2021-12-13 00:26:23,242 iteration 3494 : loss : 0.055010, loss_ce: 0.020875
2021-12-13 00:26:24,645 iteration 3495 : loss : 0.052201, loss_ce: 0.018085
2021-12-13 00:26:26,162 iteration 3496 : loss : 0.064414, loss_ce: 0.015083
2021-12-13 00:26:27,730 iteration 3497 : loss : 0.056880, loss_ce: 0.019418
2021-12-13 00:26:29,233 iteration 3498 : loss : 0.054294, loss_ce: 0.017399
2021-12-13 00:26:30,798 iteration 3499 : loss : 0.064480, loss_ce: 0.015478
2021-12-13 00:26:32,271 iteration 3500 : loss : 0.055567, loss_ce: 0.013420
2021-12-13 00:26:33,793 iteration 3501 : loss : 0.056520, loss_ce: 0.016099
2021-12-13 00:26:35,301 iteration 3502 : loss : 0.054072, loss_ce: 0.014705
 52%|█████████████▉             | 206/400 [1:36:32<1:31:11, 28.20s/it]2021-12-13 00:26:36,764 iteration 3503 : loss : 0.048247, loss_ce: 0.016240
2021-12-13 00:26:38,273 iteration 3504 : loss : 0.062131, loss_ce: 0.016626
2021-12-13 00:26:39,766 iteration 3505 : loss : 0.058362, loss_ce: 0.023333
2021-12-13 00:26:41,364 iteration 3506 : loss : 0.063976, loss_ce: 0.018000
2021-12-13 00:26:42,832 iteration 3507 : loss : 0.050144, loss_ce: 0.015953
2021-12-13 00:26:44,244 iteration 3508 : loss : 0.050380, loss_ce: 0.014072
2021-12-13 00:26:45,716 iteration 3509 : loss : 0.052170, loss_ce: 0.016724
2021-12-13 00:26:47,323 iteration 3510 : loss : 0.058227, loss_ce: 0.016470
2021-12-13 00:26:48,799 iteration 3511 : loss : 0.060863, loss_ce: 0.020989
2021-12-13 00:26:50,312 iteration 3512 : loss : 0.066007, loss_ce: 0.017184
2021-12-13 00:26:51,741 iteration 3513 : loss : 0.059097, loss_ce: 0.015053
2021-12-13 00:26:53,147 iteration 3514 : loss : 0.051158, loss_ce: 0.016732
2021-12-13 00:26:54,610 iteration 3515 : loss : 0.050104, loss_ce: 0.013037
2021-12-13 00:26:56,144 iteration 3516 : loss : 0.062968, loss_ce: 0.018622
2021-12-13 00:26:57,643 iteration 3517 : loss : 0.054950, loss_ce: 0.015291
2021-12-13 00:26:59,075 iteration 3518 : loss : 0.047484, loss_ce: 0.014433
2021-12-13 00:27:00,526 iteration 3519 : loss : 0.059398, loss_ce: 0.015224
 52%|█████████████▉             | 207/400 [1:36:57<1:27:50, 27.31s/it]2021-12-13 00:27:02,043 iteration 3520 : loss : 0.054527, loss_ce: 0.014506
2021-12-13 00:27:03,570 iteration 3521 : loss : 0.052759, loss_ce: 0.016092
2021-12-13 00:27:05,105 iteration 3522 : loss : 0.060200, loss_ce: 0.019822
2021-12-13 00:27:06,717 iteration 3523 : loss : 0.059849, loss_ce: 0.018879
2021-12-13 00:27:08,198 iteration 3524 : loss : 0.054806, loss_ce: 0.012930
2021-12-13 00:27:09,731 iteration 3525 : loss : 0.060293, loss_ce: 0.020583
2021-12-13 00:27:11,260 iteration 3526 : loss : 0.055666, loss_ce: 0.020627
2021-12-13 00:27:12,791 iteration 3527 : loss : 0.052456, loss_ce: 0.012957
2021-12-13 00:27:14,240 iteration 3528 : loss : 0.048118, loss_ce: 0.016567
2021-12-13 00:27:15,749 iteration 3529 : loss : 0.052594, loss_ce: 0.013166
2021-12-13 00:27:17,230 iteration 3530 : loss : 0.059720, loss_ce: 0.015497
2021-12-13 00:27:18,752 iteration 3531 : loss : 0.053452, loss_ce: 0.016521
2021-12-13 00:27:20,213 iteration 3532 : loss : 0.050913, loss_ce: 0.014114
2021-12-13 00:27:21,647 iteration 3533 : loss : 0.058242, loss_ce: 0.016608
2021-12-13 00:27:23,093 iteration 3534 : loss : 0.054305, loss_ce: 0.021447
2021-12-13 00:27:24,513 iteration 3535 : loss : 0.048399, loss_ce: 0.012604
2021-12-13 00:27:25,935 iteration 3536 : loss : 0.048706, loss_ce: 0.012171
 52%|██████████████             | 208/400 [1:37:23<1:25:33, 26.74s/it]2021-12-13 00:27:27,428 iteration 3537 : loss : 0.055988, loss_ce: 0.016906
2021-12-13 00:27:28,912 iteration 3538 : loss : 0.058363, loss_ce: 0.014909
2021-12-13 00:27:30,438 iteration 3539 : loss : 0.058203, loss_ce: 0.021557
2021-12-13 00:27:31,872 iteration 3540 : loss : 0.054848, loss_ce: 0.009797
2021-12-13 00:27:33,426 iteration 3541 : loss : 0.054630, loss_ce: 0.017962
2021-12-13 00:27:34,883 iteration 3542 : loss : 0.048875, loss_ce: 0.011037
2021-12-13 00:27:36,400 iteration 3543 : loss : 0.055088, loss_ce: 0.020152
2021-12-13 00:27:37,938 iteration 3544 : loss : 0.054989, loss_ce: 0.015382
2021-12-13 00:27:39,388 iteration 3545 : loss : 0.056857, loss_ce: 0.016835
2021-12-13 00:27:40,783 iteration 3546 : loss : 0.051795, loss_ce: 0.018373
2021-12-13 00:27:42,314 iteration 3547 : loss : 0.061244, loss_ce: 0.018872
2021-12-13 00:27:43,841 iteration 3548 : loss : 0.062012, loss_ce: 0.017334
2021-12-13 00:27:45,332 iteration 3549 : loss : 0.053460, loss_ce: 0.017662
2021-12-13 00:27:46,801 iteration 3550 : loss : 0.053269, loss_ce: 0.014509
2021-12-13 00:27:48,358 iteration 3551 : loss : 0.074440, loss_ce: 0.017142
2021-12-13 00:27:49,912 iteration 3552 : loss : 0.065731, loss_ce: 0.022773
2021-12-13 00:27:51,470 iteration 3553 : loss : 0.062111, loss_ce: 0.017578
 52%|██████████████             | 209/400 [1:37:48<1:23:57, 26.38s/it]2021-12-13 00:27:52,994 iteration 3554 : loss : 0.059463, loss_ce: 0.019758
2021-12-13 00:27:54,530 iteration 3555 : loss : 0.061084, loss_ce: 0.022483
2021-12-13 00:27:56,007 iteration 3556 : loss : 0.070412, loss_ce: 0.018773
2021-12-13 00:27:57,467 iteration 3557 : loss : 0.054102, loss_ce: 0.018748
2021-12-13 00:27:58,883 iteration 3558 : loss : 0.053111, loss_ce: 0.015444
2021-12-13 00:28:00,434 iteration 3559 : loss : 0.059102, loss_ce: 0.016686
2021-12-13 00:28:01,908 iteration 3560 : loss : 0.062801, loss_ce: 0.014303
2021-12-13 00:28:03,430 iteration 3561 : loss : 0.053545, loss_ce: 0.018650
2021-12-13 00:28:04,918 iteration 3562 : loss : 0.058559, loss_ce: 0.015705
2021-12-13 00:28:06,408 iteration 3563 : loss : 0.047889, loss_ce: 0.013244
2021-12-13 00:28:07,927 iteration 3564 : loss : 0.053943, loss_ce: 0.013154
2021-12-13 00:28:09,356 iteration 3565 : loss : 0.050306, loss_ce: 0.015220
2021-12-13 00:28:10,728 iteration 3566 : loss : 0.049992, loss_ce: 0.015745
2021-12-13 00:28:12,140 iteration 3567 : loss : 0.052890, loss_ce: 0.012881
2021-12-13 00:28:13,657 iteration 3568 : loss : 0.060252, loss_ce: 0.017889
2021-12-13 00:28:15,093 iteration 3569 : loss : 0.052575, loss_ce: 0.016816
2021-12-13 00:28:15,093 Training Data Eval:
2021-12-13 00:28:22,736   Average segmentation loss on training set: 0.0441
2021-12-13 00:28:22,736 Validation Data Eval:
2021-12-13 00:28:25,341   Average segmentation loss on validation set: 0.0966
2021-12-13 00:28:26,793 iteration 3570 : loss : 0.051308, loss_ce: 0.012567
 52%|██████████████▏            | 210/400 [1:38:24<1:32:01, 29.06s/it]2021-12-13 00:28:28,343 iteration 3571 : loss : 0.059256, loss_ce: 0.019782
2021-12-13 00:28:29,903 iteration 3572 : loss : 0.077059, loss_ce: 0.021239
2021-12-13 00:28:31,415 iteration 3573 : loss : 0.058234, loss_ce: 0.017399
2021-12-13 00:28:32,868 iteration 3574 : loss : 0.048997, loss_ce: 0.015856
2021-12-13 00:28:34,305 iteration 3575 : loss : 0.053201, loss_ce: 0.016703
2021-12-13 00:28:35,755 iteration 3576 : loss : 0.052302, loss_ce: 0.013426
2021-12-13 00:28:37,270 iteration 3577 : loss : 0.050426, loss_ce: 0.015872
2021-12-13 00:28:38,733 iteration 3578 : loss : 0.052036, loss_ce: 0.016829
2021-12-13 00:28:40,225 iteration 3579 : loss : 0.053767, loss_ce: 0.014034
2021-12-13 00:28:41,756 iteration 3580 : loss : 0.057084, loss_ce: 0.021902
2021-12-13 00:28:43,290 iteration 3581 : loss : 0.050073, loss_ce: 0.015541
2021-12-13 00:28:44,798 iteration 3582 : loss : 0.055915, loss_ce: 0.014680
2021-12-13 00:28:46,274 iteration 3583 : loss : 0.062898, loss_ce: 0.011975
2021-12-13 00:28:47,675 iteration 3584 : loss : 0.051594, loss_ce: 0.013979
2021-12-13 00:28:49,126 iteration 3585 : loss : 0.047399, loss_ce: 0.014601
2021-12-13 00:28:50,542 iteration 3586 : loss : 0.048821, loss_ce: 0.012265
2021-12-13 00:28:52,024 iteration 3587 : loss : 0.051313, loss_ce: 0.018472
 53%|██████████████▏            | 211/400 [1:38:49<1:27:55, 27.91s/it]2021-12-13 00:28:53,603 iteration 3588 : loss : 0.051960, loss_ce: 0.016061
2021-12-13 00:28:55,167 iteration 3589 : loss : 0.059819, loss_ce: 0.013129
2021-12-13 00:28:56,662 iteration 3590 : loss : 0.059718, loss_ce: 0.014168
2021-12-13 00:28:58,094 iteration 3591 : loss : 0.048999, loss_ce: 0.012786
2021-12-13 00:28:59,567 iteration 3592 : loss : 0.053673, loss_ce: 0.017265
2021-12-13 00:29:01,085 iteration 3593 : loss : 0.051082, loss_ce: 0.018722
2021-12-13 00:29:02,574 iteration 3594 : loss : 0.057275, loss_ce: 0.016285
2021-12-13 00:29:04,044 iteration 3595 : loss : 0.064091, loss_ce: 0.019155
2021-12-13 00:29:05,491 iteration 3596 : loss : 0.052920, loss_ce: 0.018151
2021-12-13 00:29:06,985 iteration 3597 : loss : 0.054989, loss_ce: 0.017557
2021-12-13 00:29:08,504 iteration 3598 : loss : 0.055188, loss_ce: 0.018989
2021-12-13 00:29:10,022 iteration 3599 : loss : 0.051302, loss_ce: 0.014573
2021-12-13 00:29:11,450 iteration 3600 : loss : 0.051547, loss_ce: 0.018872
2021-12-13 00:29:12,960 iteration 3601 : loss : 0.053921, loss_ce: 0.014662
2021-12-13 00:29:14,485 iteration 3602 : loss : 0.066065, loss_ce: 0.020601
2021-12-13 00:29:15,947 iteration 3603 : loss : 0.055411, loss_ce: 0.020100
2021-12-13 00:29:17,423 iteration 3604 : loss : 0.068801, loss_ce: 0.015096
 53%|██████████████▎            | 212/400 [1:39:14<1:25:05, 27.16s/it]2021-12-13 00:29:19,064 iteration 3605 : loss : 0.062150, loss_ce: 0.018795
2021-12-13 00:29:20,487 iteration 3606 : loss : 0.051317, loss_ce: 0.010103
2021-12-13 00:29:21,871 iteration 3607 : loss : 0.046031, loss_ce: 0.012271
2021-12-13 00:29:23,411 iteration 3608 : loss : 0.067440, loss_ce: 0.020416
2021-12-13 00:29:24,945 iteration 3609 : loss : 0.071669, loss_ce: 0.023039
2021-12-13 00:29:26,366 iteration 3610 : loss : 0.045657, loss_ce: 0.012221
2021-12-13 00:29:27,839 iteration 3611 : loss : 0.051115, loss_ce: 0.014369
2021-12-13 00:29:29,267 iteration 3612 : loss : 0.052962, loss_ce: 0.017301
2021-12-13 00:29:30,760 iteration 3613 : loss : 0.051235, loss_ce: 0.014414
2021-12-13 00:29:32,204 iteration 3614 : loss : 0.057088, loss_ce: 0.017510
2021-12-13 00:29:33,671 iteration 3615 : loss : 0.056873, loss_ce: 0.013527
2021-12-13 00:29:35,135 iteration 3616 : loss : 0.055541, loss_ce: 0.021583
2021-12-13 00:29:36,642 iteration 3617 : loss : 0.051228, loss_ce: 0.017668
2021-12-13 00:29:38,155 iteration 3618 : loss : 0.056074, loss_ce: 0.014912
2021-12-13 00:29:39,622 iteration 3619 : loss : 0.050871, loss_ce: 0.014085
2021-12-13 00:29:41,116 iteration 3620 : loss : 0.049982, loss_ce: 0.011067
2021-12-13 00:29:42,583 iteration 3621 : loss : 0.049986, loss_ce: 0.018755
 53%|██████████████▍            | 213/400 [1:39:39<1:22:46, 26.56s/it]2021-12-13 00:29:44,131 iteration 3622 : loss : 0.053739, loss_ce: 0.013178
2021-12-13 00:29:45,635 iteration 3623 : loss : 0.059350, loss_ce: 0.015750
2021-12-13 00:29:47,144 iteration 3624 : loss : 0.059670, loss_ce: 0.019595
2021-12-13 00:29:48,541 iteration 3625 : loss : 0.047928, loss_ce: 0.015399
2021-12-13 00:29:50,036 iteration 3626 : loss : 0.058358, loss_ce: 0.015155
2021-12-13 00:29:51,566 iteration 3627 : loss : 0.061154, loss_ce: 0.021130
2021-12-13 00:29:53,089 iteration 3628 : loss : 0.063770, loss_ce: 0.018787
2021-12-13 00:29:54,655 iteration 3629 : loss : 0.058152, loss_ce: 0.018809
2021-12-13 00:29:56,161 iteration 3630 : loss : 0.058677, loss_ce: 0.021675
2021-12-13 00:29:57,651 iteration 3631 : loss : 0.053154, loss_ce: 0.014186
2021-12-13 00:29:59,177 iteration 3632 : loss : 0.056449, loss_ce: 0.022072
2021-12-13 00:30:00,710 iteration 3633 : loss : 0.055507, loss_ce: 0.013965
2021-12-13 00:30:02,154 iteration 3634 : loss : 0.052255, loss_ce: 0.013570
2021-12-13 00:30:03,765 iteration 3635 : loss : 0.057394, loss_ce: 0.023505
2021-12-13 00:30:05,169 iteration 3636 : loss : 0.047641, loss_ce: 0.017220
2021-12-13 00:30:06,649 iteration 3637 : loss : 0.052093, loss_ce: 0.014978
2021-12-13 00:30:08,156 iteration 3638 : loss : 0.052922, loss_ce: 0.014170
 54%|██████████████▍            | 214/400 [1:40:05<1:21:25, 26.26s/it]2021-12-13 00:30:09,699 iteration 3639 : loss : 0.057121, loss_ce: 0.014570
2021-12-13 00:30:11,234 iteration 3640 : loss : 0.049400, loss_ce: 0.012093
2021-12-13 00:30:12,764 iteration 3641 : loss : 0.051017, loss_ce: 0.015212
2021-12-13 00:30:14,222 iteration 3642 : loss : 0.056724, loss_ce: 0.020226
2021-12-13 00:30:15,736 iteration 3643 : loss : 0.055373, loss_ce: 0.014051
2021-12-13 00:30:17,257 iteration 3644 : loss : 0.053110, loss_ce: 0.013965
2021-12-13 00:30:18,835 iteration 3645 : loss : 0.066745, loss_ce: 0.012654
2021-12-13 00:30:20,351 iteration 3646 : loss : 0.054254, loss_ce: 0.019133
2021-12-13 00:30:21,817 iteration 3647 : loss : 0.057534, loss_ce: 0.019660
2021-12-13 00:30:23,338 iteration 3648 : loss : 0.059762, loss_ce: 0.015899
2021-12-13 00:30:24,720 iteration 3649 : loss : 0.050955, loss_ce: 0.014930
2021-12-13 00:30:26,218 iteration 3650 : loss : 0.053043, loss_ce: 0.017966
2021-12-13 00:30:27,659 iteration 3651 : loss : 0.055094, loss_ce: 0.017290
2021-12-13 00:30:29,074 iteration 3652 : loss : 0.060971, loss_ce: 0.022047
2021-12-13 00:30:30,616 iteration 3653 : loss : 0.060410, loss_ce: 0.019029
2021-12-13 00:30:32,071 iteration 3654 : loss : 0.048467, loss_ce: 0.014188
2021-12-13 00:30:32,071 Training Data Eval:
2021-12-13 00:30:39,711   Average segmentation loss on training set: 0.0430
2021-12-13 00:30:39,711 Validation Data Eval:
2021-12-13 00:30:42,315   Average segmentation loss on validation set: 0.0928
2021-12-13 00:30:48,908 Found new lowest validation loss at iteration 3654! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/TU_3seeds/TU_SGD_best_val_loss_seed100.pth
2021-12-13 00:30:50,463 iteration 3655 : loss : 0.065986, loss_ce: 0.019755
 54%|██████████████▌            | 215/400 [1:40:47<1:35:48, 31.07s/it]2021-12-13 00:30:51,869 iteration 3656 : loss : 0.051556, loss_ce: 0.013892
2021-12-13 00:30:53,294 iteration 3657 : loss : 0.049209, loss_ce: 0.016829
2021-12-13 00:30:54,671 iteration 3658 : loss : 0.052315, loss_ce: 0.018045
2021-12-13 00:30:56,064 iteration 3659 : loss : 0.049989, loss_ce: 0.013970
2021-12-13 00:30:57,632 iteration 3660 : loss : 0.062272, loss_ce: 0.020923
2021-12-13 00:30:59,106 iteration 3661 : loss : 0.056557, loss_ce: 0.018858
2021-12-13 00:31:00,526 iteration 3662 : loss : 0.053966, loss_ce: 0.019258
2021-12-13 00:31:01,999 iteration 3663 : loss : 0.050904, loss_ce: 0.015613
2021-12-13 00:31:03,510 iteration 3664 : loss : 0.053895, loss_ce: 0.013723
2021-12-13 00:31:05,116 iteration 3665 : loss : 0.066818, loss_ce: 0.017165
2021-12-13 00:31:06,658 iteration 3666 : loss : 0.051253, loss_ce: 0.016943
2021-12-13 00:31:08,123 iteration 3667 : loss : 0.054142, loss_ce: 0.016318
2021-12-13 00:31:09,590 iteration 3668 : loss : 0.063198, loss_ce: 0.018296
2021-12-13 00:31:11,096 iteration 3669 : loss : 0.064168, loss_ce: 0.018010
2021-12-13 00:31:12,509 iteration 3670 : loss : 0.051929, loss_ce: 0.018466
2021-12-13 00:31:13,977 iteration 3671 : loss : 0.064393, loss_ce: 0.011942
2021-12-13 00:31:15,440 iteration 3672 : loss : 0.057425, loss_ce: 0.012331
 54%|██████████████▌            | 216/400 [1:41:12<1:29:41, 29.25s/it]2021-12-13 00:31:16,943 iteration 3673 : loss : 0.052946, loss_ce: 0.017589
2021-12-13 00:31:18,508 iteration 3674 : loss : 0.051802, loss_ce: 0.018861
2021-12-13 00:31:20,075 iteration 3675 : loss : 0.072482, loss_ce: 0.025394
2021-12-13 00:31:21,624 iteration 3676 : loss : 0.058004, loss_ce: 0.013544
2021-12-13 00:31:23,141 iteration 3677 : loss : 0.053841, loss_ce: 0.018339
2021-12-13 00:31:24,716 iteration 3678 : loss : 0.061932, loss_ce: 0.015396
2021-12-13 00:31:26,120 iteration 3679 : loss : 0.052375, loss_ce: 0.014570
2021-12-13 00:31:27,553 iteration 3680 : loss : 0.056558, loss_ce: 0.019078
2021-12-13 00:31:29,037 iteration 3681 : loss : 0.052354, loss_ce: 0.013825
2021-12-13 00:31:30,431 iteration 3682 : loss : 0.047286, loss_ce: 0.012071
2021-12-13 00:31:31,903 iteration 3683 : loss : 0.053984, loss_ce: 0.018901
2021-12-13 00:31:33,320 iteration 3684 : loss : 0.051678, loss_ce: 0.015429
2021-12-13 00:31:34,780 iteration 3685 : loss : 0.051824, loss_ce: 0.013650
2021-12-13 00:31:36,339 iteration 3686 : loss : 0.055814, loss_ce: 0.017290
2021-12-13 00:31:37,911 iteration 3687 : loss : 0.063498, loss_ce: 0.020065
2021-12-13 00:31:39,427 iteration 3688 : loss : 0.061831, loss_ce: 0.019351
2021-12-13 00:31:40,953 iteration 3689 : loss : 0.059914, loss_ce: 0.013765
 54%|██████████████▋            | 217/400 [1:41:38<1:25:47, 28.13s/it]2021-12-13 00:31:42,610 iteration 3690 : loss : 0.054918, loss_ce: 0.019141
2021-12-13 00:31:44,083 iteration 3691 : loss : 0.060129, loss_ce: 0.014310
2021-12-13 00:31:45,607 iteration 3692 : loss : 0.069944, loss_ce: 0.018210
2021-12-13 00:31:47,045 iteration 3693 : loss : 0.050008, loss_ce: 0.011199
2021-12-13 00:31:48,565 iteration 3694 : loss : 0.056570, loss_ce: 0.014386
2021-12-13 00:31:50,072 iteration 3695 : loss : 0.048024, loss_ce: 0.013246
2021-12-13 00:31:51,509 iteration 3696 : loss : 0.048742, loss_ce: 0.015738
2021-12-13 00:31:52,946 iteration 3697 : loss : 0.067427, loss_ce: 0.019950
2021-12-13 00:31:54,525 iteration 3698 : loss : 0.068856, loss_ce: 0.020749
2021-12-13 00:31:56,016 iteration 3699 : loss : 0.052275, loss_ce: 0.020017
2021-12-13 00:31:57,498 iteration 3700 : loss : 0.050499, loss_ce: 0.014477
2021-12-13 00:31:58,909 iteration 3701 : loss : 0.056519, loss_ce: 0.020447
2021-12-13 00:32:00,428 iteration 3702 : loss : 0.063796, loss_ce: 0.021107
2021-12-13 00:32:01,921 iteration 3703 : loss : 0.050745, loss_ce: 0.013942
2021-12-13 00:32:03,405 iteration 3704 : loss : 0.049584, loss_ce: 0.012528
2021-12-13 00:32:04,906 iteration 3705 : loss : 0.060620, loss_ce: 0.022583
2021-12-13 00:32:06,329 iteration 3706 : loss : 0.056505, loss_ce: 0.014416
 55%|██████████████▋            | 218/400 [1:42:03<1:22:48, 27.30s/it]2021-12-13 00:32:07,882 iteration 3707 : loss : 0.104706, loss_ce: 0.015062
2021-12-13 00:32:09,497 iteration 3708 : loss : 0.073895, loss_ce: 0.018573
2021-12-13 00:32:10,992 iteration 3709 : loss : 0.053401, loss_ce: 0.013466
2021-12-13 00:32:12,393 iteration 3710 : loss : 0.050319, loss_ce: 0.016093
2021-12-13 00:32:13,998 iteration 3711 : loss : 0.054756, loss_ce: 0.018410
2021-12-13 00:32:15,438 iteration 3712 : loss : 0.065418, loss_ce: 0.018736
2021-12-13 00:32:16,859 iteration 3713 : loss : 0.051672, loss_ce: 0.015925
2021-12-13 00:32:18,381 iteration 3714 : loss : 0.074894, loss_ce: 0.028650
2021-12-13 00:32:19,836 iteration 3715 : loss : 0.059688, loss_ce: 0.016766
2021-12-13 00:32:21,321 iteration 3716 : loss : 0.060692, loss_ce: 0.021561
2021-12-13 00:32:22,823 iteration 3717 : loss : 0.053774, loss_ce: 0.018952
2021-12-13 00:32:24,358 iteration 3718 : loss : 0.057828, loss_ce: 0.017274
2021-12-13 00:32:25,957 iteration 3719 : loss : 0.058322, loss_ce: 0.017994
2021-12-13 00:32:27,530 iteration 3720 : loss : 0.058734, loss_ce: 0.019613
2021-12-13 00:32:29,030 iteration 3721 : loss : 0.066365, loss_ce: 0.022906
2021-12-13 00:32:30,591 iteration 3722 : loss : 0.054290, loss_ce: 0.012561
2021-12-13 00:32:32,064 iteration 3723 : loss : 0.057279, loss_ce: 0.017110
 55%|██████████████▊            | 219/400 [1:42:29<1:20:55, 26.83s/it]2021-12-13 00:32:33,588 iteration 3724 : loss : 0.053851, loss_ce: 0.020859
2021-12-13 00:32:35,127 iteration 3725 : loss : 0.070028, loss_ce: 0.021814
2021-12-13 00:32:36,603 iteration 3726 : loss : 0.059881, loss_ce: 0.022987
2021-12-13 00:32:38,103 iteration 3727 : loss : 0.046796, loss_ce: 0.013253
2021-12-13 00:32:39,501 iteration 3728 : loss : 0.057972, loss_ce: 0.022560
2021-12-13 00:32:41,029 iteration 3729 : loss : 0.067406, loss_ce: 0.014230
2021-12-13 00:32:42,479 iteration 3730 : loss : 0.049694, loss_ce: 0.014437
2021-12-13 00:32:44,023 iteration 3731 : loss : 0.058091, loss_ce: 0.018377
2021-12-13 00:32:45,511 iteration 3732 : loss : 0.052492, loss_ce: 0.019143
2021-12-13 00:32:46,928 iteration 3733 : loss : 0.046994, loss_ce: 0.011097
2021-12-13 00:32:48,452 iteration 3734 : loss : 0.056384, loss_ce: 0.016708
2021-12-13 00:32:50,015 iteration 3735 : loss : 0.057597, loss_ce: 0.016302
2021-12-13 00:32:51,533 iteration 3736 : loss : 0.058167, loss_ce: 0.018222
2021-12-13 00:32:53,091 iteration 3737 : loss : 0.060969, loss_ce: 0.021097
2021-12-13 00:32:54,578 iteration 3738 : loss : 0.063005, loss_ce: 0.015858
2021-12-13 00:32:56,068 iteration 3739 : loss : 0.055627, loss_ce: 0.013480
2021-12-13 00:32:56,068 Training Data Eval:
2021-12-13 00:33:03,709   Average segmentation loss on training set: 0.0421
2021-12-13 00:33:03,709 Validation Data Eval:
2021-12-13 00:33:06,314   Average segmentation loss on validation set: 0.0960
2021-12-13 00:33:07,747 iteration 3740 : loss : 0.052461, loss_ce: 0.013510
 55%|██████████████▊            | 220/400 [1:43:05<1:28:27, 29.49s/it]2021-12-13 00:33:09,285 iteration 3741 : loss : 0.055876, loss_ce: 0.018706
2021-12-13 00:33:10,760 iteration 3742 : loss : 0.054930, loss_ce: 0.015497
2021-12-13 00:33:12,216 iteration 3743 : loss : 0.046859, loss_ce: 0.011229
2021-12-13 00:33:13,725 iteration 3744 : loss : 0.046507, loss_ce: 0.013072
2021-12-13 00:33:15,304 iteration 3745 : loss : 0.065862, loss_ce: 0.029051
2021-12-13 00:33:16,773 iteration 3746 : loss : 0.048297, loss_ce: 0.009371
2021-12-13 00:33:18,307 iteration 3747 : loss : 0.065397, loss_ce: 0.021874
2021-12-13 00:33:19,857 iteration 3748 : loss : 0.055877, loss_ce: 0.017275
2021-12-13 00:33:21,353 iteration 3749 : loss : 0.054149, loss_ce: 0.014402
2021-12-13 00:33:22,952 iteration 3750 : loss : 0.066349, loss_ce: 0.020373
2021-12-13 00:33:24,428 iteration 3751 : loss : 0.053964, loss_ce: 0.018697
2021-12-13 00:33:25,881 iteration 3752 : loss : 0.051073, loss_ce: 0.013819
2021-12-13 00:33:27,349 iteration 3753 : loss : 0.060635, loss_ce: 0.016940
2021-12-13 00:33:28,755 iteration 3754 : loss : 0.050192, loss_ce: 0.020137
2021-12-13 00:33:30,248 iteration 3755 : loss : 0.055589, loss_ce: 0.015118
2021-12-13 00:33:31,698 iteration 3756 : loss : 0.048297, loss_ce: 0.012935
2021-12-13 00:33:33,181 iteration 3757 : loss : 0.053875, loss_ce: 0.017005
 55%|██████████████▉            | 221/400 [1:43:30<1:24:20, 28.27s/it]2021-12-13 00:33:34,756 iteration 3758 : loss : 0.057269, loss_ce: 0.017824
2021-12-13 00:33:36,293 iteration 3759 : loss : 0.054201, loss_ce: 0.017554
2021-12-13 00:33:37,822 iteration 3760 : loss : 0.064881, loss_ce: 0.019760
2021-12-13 00:33:39,322 iteration 3761 : loss : 0.049155, loss_ce: 0.017659
2021-12-13 00:33:40,868 iteration 3762 : loss : 0.064290, loss_ce: 0.018950
2021-12-13 00:33:42,415 iteration 3763 : loss : 0.065931, loss_ce: 0.023403
2021-12-13 00:33:43,869 iteration 3764 : loss : 0.054727, loss_ce: 0.009775
2021-12-13 00:33:45,272 iteration 3765 : loss : 0.066397, loss_ce: 0.021264
2021-12-13 00:33:46,710 iteration 3766 : loss : 0.051079, loss_ce: 0.013934
2021-12-13 00:33:48,201 iteration 3767 : loss : 0.053265, loss_ce: 0.013886
2021-12-13 00:33:49,769 iteration 3768 : loss : 0.059311, loss_ce: 0.019268
2021-12-13 00:33:51,264 iteration 3769 : loss : 0.059263, loss_ce: 0.018528
2021-12-13 00:33:52,815 iteration 3770 : loss : 0.049520, loss_ce: 0.013993
2021-12-13 00:33:54,337 iteration 3771 : loss : 0.058730, loss_ce: 0.020753
2021-12-13 00:33:55,885 iteration 3772 : loss : 0.063412, loss_ce: 0.012881
2021-12-13 00:33:57,392 iteration 3773 : loss : 0.057100, loss_ce: 0.016863
2021-12-13 00:33:58,912 iteration 3774 : loss : 0.058545, loss_ce: 0.022270
 56%|██████████████▉            | 222/400 [1:43:56<1:21:37, 27.51s/it]2021-12-13 00:34:00,504 iteration 3775 : loss : 0.058509, loss_ce: 0.017952
2021-12-13 00:34:02,011 iteration 3776 : loss : 0.051605, loss_ce: 0.016470
2021-12-13 00:34:03,508 iteration 3777 : loss : 0.066312, loss_ce: 0.019161
2021-12-13 00:34:05,068 iteration 3778 : loss : 0.056791, loss_ce: 0.017904
2021-12-13 00:34:06,519 iteration 3779 : loss : 0.059904, loss_ce: 0.019301
2021-12-13 00:34:08,079 iteration 3780 : loss : 0.055711, loss_ce: 0.019970
2021-12-13 00:34:09,624 iteration 3781 : loss : 0.056122, loss_ce: 0.016963
2021-12-13 00:34:11,197 iteration 3782 : loss : 0.062665, loss_ce: 0.018570
2021-12-13 00:34:12,751 iteration 3783 : loss : 0.052829, loss_ce: 0.019928
2021-12-13 00:34:14,190 iteration 3784 : loss : 0.048290, loss_ce: 0.009430
2021-12-13 00:34:15,688 iteration 3785 : loss : 0.064120, loss_ce: 0.015917
2021-12-13 00:34:17,181 iteration 3786 : loss : 0.054875, loss_ce: 0.016996
2021-12-13 00:34:18,621 iteration 3787 : loss : 0.054142, loss_ce: 0.016637
2021-12-13 00:34:20,134 iteration 3788 : loss : 0.059949, loss_ce: 0.016925
2021-12-13 00:34:21,630 iteration 3789 : loss : 0.051246, loss_ce: 0.015883
2021-12-13 00:34:23,091 iteration 3790 : loss : 0.057918, loss_ce: 0.016070
2021-12-13 00:34:24,641 iteration 3791 : loss : 0.055131, loss_ce: 0.015102
 56%|███████████████            | 223/400 [1:44:21<1:19:34, 26.97s/it]2021-12-13 00:34:26,137 iteration 3792 : loss : 0.051244, loss_ce: 0.015541
2021-12-13 00:34:27,549 iteration 3793 : loss : 0.041515, loss_ce: 0.009513
2021-12-13 00:34:29,020 iteration 3794 : loss : 0.052486, loss_ce: 0.016564
2021-12-13 00:34:30,480 iteration 3795 : loss : 0.056740, loss_ce: 0.013764
2021-12-13 00:34:32,046 iteration 3796 : loss : 0.075462, loss_ce: 0.017778
2021-12-13 00:34:33,574 iteration 3797 : loss : 0.052220, loss_ce: 0.013226
2021-12-13 00:34:35,017 iteration 3798 : loss : 0.054802, loss_ce: 0.015908
2021-12-13 00:34:36,606 iteration 3799 : loss : 0.057336, loss_ce: 0.014311
2021-12-13 00:34:38,269 iteration 3800 : loss : 0.071945, loss_ce: 0.027878
2021-12-13 00:34:39,866 iteration 3801 : loss : 0.060751, loss_ce: 0.016546
2021-12-13 00:34:41,436 iteration 3802 : loss : 0.056829, loss_ce: 0.015862
2021-12-13 00:34:42,929 iteration 3803 : loss : 0.052462, loss_ce: 0.015067
2021-12-13 00:34:44,425 iteration 3804 : loss : 0.057123, loss_ce: 0.015479
2021-12-13 00:34:45,920 iteration 3805 : loss : 0.055980, loss_ce: 0.017172
2021-12-13 00:34:47,324 iteration 3806 : loss : 0.050638, loss_ce: 0.018297
2021-12-13 00:34:48,837 iteration 3807 : loss : 0.056901, loss_ce: 0.016437
2021-12-13 00:34:50,305 iteration 3808 : loss : 0.057070, loss_ce: 0.020186
 56%|███████████████            | 224/400 [1:44:47<1:17:58, 26.58s/it]2021-12-13 00:34:51,860 iteration 3809 : loss : 0.055497, loss_ce: 0.017922
2021-12-13 00:34:53,398 iteration 3810 : loss : 0.051616, loss_ce: 0.014049
2021-12-13 00:34:54,849 iteration 3811 : loss : 0.050651, loss_ce: 0.015003
2021-12-13 00:34:56,358 iteration 3812 : loss : 0.050133, loss_ce: 0.014079
2021-12-13 00:34:57,758 iteration 3813 : loss : 0.049569, loss_ce: 0.017742
2021-12-13 00:34:59,301 iteration 3814 : loss : 0.053026, loss_ce: 0.019241
2021-12-13 00:35:00,897 iteration 3815 : loss : 0.053349, loss_ce: 0.017040
2021-12-13 00:35:02,344 iteration 3816 : loss : 0.049516, loss_ce: 0.015238
2021-12-13 00:35:03,720 iteration 3817 : loss : 0.047733, loss_ce: 0.014420
2021-12-13 00:35:05,177 iteration 3818 : loss : 0.048877, loss_ce: 0.012442
2021-12-13 00:35:06,709 iteration 3819 : loss : 0.065626, loss_ce: 0.022699
2021-12-13 00:35:08,124 iteration 3820 : loss : 0.045949, loss_ce: 0.014476
2021-12-13 00:35:09,685 iteration 3821 : loss : 0.055543, loss_ce: 0.019324
2021-12-13 00:35:11,154 iteration 3822 : loss : 0.051837, loss_ce: 0.012887
2021-12-13 00:35:12,602 iteration 3823 : loss : 0.053174, loss_ce: 0.013695
2021-12-13 00:35:14,124 iteration 3824 : loss : 0.054683, loss_ce: 0.015926
2021-12-13 00:35:14,124 Training Data Eval:
2021-12-13 00:35:21,779   Average segmentation loss on training set: 0.0406
2021-12-13 00:35:21,779 Validation Data Eval:
2021-12-13 00:35:24,379   Average segmentation loss on validation set: 0.0941
2021-12-13 00:35:25,774 iteration 3825 : loss : 0.049866, loss_ce: 0.013314
 56%|███████████████▏           | 225/400 [1:45:23<1:25:18, 29.25s/it]2021-12-13 00:35:27,378 iteration 3826 : loss : 0.056029, loss_ce: 0.017919
2021-12-13 00:35:28,939 iteration 3827 : loss : 0.054153, loss_ce: 0.017878
2021-12-13 00:35:30,375 iteration 3828 : loss : 0.049877, loss_ce: 0.017374
2021-12-13 00:35:31,815 iteration 3829 : loss : 0.049613, loss_ce: 0.013737
2021-12-13 00:35:33,384 iteration 3830 : loss : 0.050542, loss_ce: 0.011790
2021-12-13 00:35:34,932 iteration 3831 : loss : 0.052335, loss_ce: 0.016833
2021-12-13 00:35:36,445 iteration 3832 : loss : 0.057915, loss_ce: 0.022283
2021-12-13 00:35:37,959 iteration 3833 : loss : 0.064656, loss_ce: 0.015376
2021-12-13 00:35:39,398 iteration 3834 : loss : 0.053219, loss_ce: 0.013092
2021-12-13 00:35:40,912 iteration 3835 : loss : 0.079003, loss_ce: 0.012905
2021-12-13 00:35:42,327 iteration 3836 : loss : 0.046502, loss_ce: 0.014098
2021-12-13 00:35:43,714 iteration 3837 : loss : 0.047286, loss_ce: 0.016277
2021-12-13 00:35:45,242 iteration 3838 : loss : 0.053019, loss_ce: 0.017668
2021-12-13 00:35:46,674 iteration 3839 : loss : 0.051461, loss_ce: 0.011160
2021-12-13 00:35:48,167 iteration 3840 : loss : 0.046410, loss_ce: 0.014280
2021-12-13 00:35:49,564 iteration 3841 : loss : 0.053106, loss_ce: 0.017054
2021-12-13 00:35:51,079 iteration 3842 : loss : 0.065362, loss_ce: 0.012091
 56%|███████████████▎           | 226/400 [1:45:48<1:21:23, 28.07s/it]2021-12-13 00:35:52,596 iteration 3843 : loss : 0.059528, loss_ce: 0.016012
2021-12-13 00:35:54,140 iteration 3844 : loss : 0.053839, loss_ce: 0.016498
2021-12-13 00:35:55,528 iteration 3845 : loss : 0.050219, loss_ce: 0.015543
2021-12-13 00:35:56,986 iteration 3846 : loss : 0.054741, loss_ce: 0.018348
2021-12-13 00:35:58,583 iteration 3847 : loss : 0.057550, loss_ce: 0.017917
2021-12-13 00:36:00,044 iteration 3848 : loss : 0.052799, loss_ce: 0.015420
2021-12-13 00:36:01,491 iteration 3849 : loss : 0.048487, loss_ce: 0.015133
2021-12-13 00:36:03,046 iteration 3850 : loss : 0.053295, loss_ce: 0.014116
2021-12-13 00:36:04,625 iteration 3851 : loss : 0.073830, loss_ce: 0.014748
2021-12-13 00:36:06,161 iteration 3852 : loss : 0.059015, loss_ce: 0.017157
2021-12-13 00:36:07,645 iteration 3853 : loss : 0.051981, loss_ce: 0.015815
2021-12-13 00:36:09,041 iteration 3854 : loss : 0.047784, loss_ce: 0.014651
2021-12-13 00:36:10,505 iteration 3855 : loss : 0.050259, loss_ce: 0.011892
2021-12-13 00:36:12,103 iteration 3856 : loss : 0.052736, loss_ce: 0.018028
2021-12-13 00:36:13,566 iteration 3857 : loss : 0.060242, loss_ce: 0.016360
2021-12-13 00:36:15,036 iteration 3858 : loss : 0.051405, loss_ce: 0.017114
2021-12-13 00:36:16,472 iteration 3859 : loss : 0.050620, loss_ce: 0.016196
 57%|███████████████▎           | 227/400 [1:46:13<1:18:36, 27.26s/it]2021-12-13 00:36:17,945 iteration 3860 : loss : 0.047374, loss_ce: 0.018043
2021-12-13 00:36:19,396 iteration 3861 : loss : 0.053444, loss_ce: 0.019340
2021-12-13 00:36:20,820 iteration 3862 : loss : 0.066007, loss_ce: 0.017070
2021-12-13 00:36:22,315 iteration 3863 : loss : 0.053697, loss_ce: 0.019428
2021-12-13 00:36:23,711 iteration 3864 : loss : 0.045931, loss_ce: 0.012296
2021-12-13 00:36:25,214 iteration 3865 : loss : 0.056931, loss_ce: 0.019825
2021-12-13 00:36:26,687 iteration 3866 : loss : 0.047464, loss_ce: 0.015199
2021-12-13 00:36:28,210 iteration 3867 : loss : 0.064027, loss_ce: 0.019768
2021-12-13 00:36:29,616 iteration 3868 : loss : 0.048635, loss_ce: 0.015623
2021-12-13 00:36:31,177 iteration 3869 : loss : 0.057410, loss_ce: 0.012569
2021-12-13 00:36:32,635 iteration 3870 : loss : 0.049499, loss_ce: 0.014412
2021-12-13 00:36:34,166 iteration 3871 : loss : 0.056872, loss_ce: 0.016799
2021-12-13 00:36:35,640 iteration 3872 : loss : 0.062963, loss_ce: 0.017721
2021-12-13 00:36:37,100 iteration 3873 : loss : 0.047862, loss_ce: 0.011874
2021-12-13 00:36:38,639 iteration 3874 : loss : 0.057439, loss_ce: 0.016611
2021-12-13 00:36:40,155 iteration 3875 : loss : 0.054572, loss_ce: 0.013004
2021-12-13 00:36:41,738 iteration 3876 : loss : 0.052260, loss_ce: 0.015912
 57%|███████████████▍           | 228/400 [1:46:39<1:16:26, 26.66s/it]2021-12-13 00:36:43,228 iteration 3877 : loss : 0.051303, loss_ce: 0.016788
2021-12-13 00:36:44,749 iteration 3878 : loss : 0.057439, loss_ce: 0.015228
2021-12-13 00:36:46,302 iteration 3879 : loss : 0.051468, loss_ce: 0.020801
2021-12-13 00:36:47,794 iteration 3880 : loss : 0.052839, loss_ce: 0.014825
2021-12-13 00:36:49,300 iteration 3881 : loss : 0.055408, loss_ce: 0.013530
2021-12-13 00:36:50,839 iteration 3882 : loss : 0.053752, loss_ce: 0.019080
2021-12-13 00:36:52,337 iteration 3883 : loss : 0.051249, loss_ce: 0.015163
2021-12-13 00:36:53,832 iteration 3884 : loss : 0.048045, loss_ce: 0.012731
2021-12-13 00:36:55,269 iteration 3885 : loss : 0.044376, loss_ce: 0.010756
2021-12-13 00:36:56,800 iteration 3886 : loss : 0.053230, loss_ce: 0.017536
2021-12-13 00:36:58,312 iteration 3887 : loss : 0.056903, loss_ce: 0.017317
2021-12-13 00:36:59,870 iteration 3888 : loss : 0.061030, loss_ce: 0.023942
2021-12-13 00:37:01,325 iteration 3889 : loss : 0.052147, loss_ce: 0.012524
2021-12-13 00:37:02,795 iteration 3890 : loss : 0.056524, loss_ce: 0.014227
2021-12-13 00:37:04,326 iteration 3891 : loss : 0.054245, loss_ce: 0.018791
2021-12-13 00:37:05,818 iteration 3892 : loss : 0.057023, loss_ce: 0.014925
2021-12-13 00:37:07,270 iteration 3893 : loss : 0.071219, loss_ce: 0.018552
 57%|███████████████▍           | 229/400 [1:47:04<1:15:01, 26.32s/it]2021-12-13 00:37:08,797 iteration 3894 : loss : 0.057607, loss_ce: 0.016466
2021-12-13 00:37:10,327 iteration 3895 : loss : 0.053114, loss_ce: 0.020115
2021-12-13 00:37:11,754 iteration 3896 : loss : 0.043531, loss_ce: 0.011367
2021-12-13 00:37:13,299 iteration 3897 : loss : 0.055885, loss_ce: 0.013877
2021-12-13 00:37:14,816 iteration 3898 : loss : 0.055706, loss_ce: 0.012675
2021-12-13 00:37:16,317 iteration 3899 : loss : 0.057990, loss_ce: 0.014154
2021-12-13 00:37:17,828 iteration 3900 : loss : 0.049948, loss_ce: 0.018246
2021-12-13 00:37:19,218 iteration 3901 : loss : 0.049871, loss_ce: 0.017406
2021-12-13 00:37:20,707 iteration 3902 : loss : 0.045975, loss_ce: 0.012449
2021-12-13 00:37:22,137 iteration 3903 : loss : 0.050551, loss_ce: 0.014089
2021-12-13 00:37:23,639 iteration 3904 : loss : 0.049834, loss_ce: 0.015855
2021-12-13 00:37:25,126 iteration 3905 : loss : 0.061846, loss_ce: 0.027455
2021-12-13 00:37:26,614 iteration 3906 : loss : 0.047695, loss_ce: 0.012998
2021-12-13 00:37:28,098 iteration 3907 : loss : 0.052013, loss_ce: 0.014504
2021-12-13 00:37:29,491 iteration 3908 : loss : 0.051039, loss_ce: 0.013540
2021-12-13 00:37:30,894 iteration 3909 : loss : 0.044896, loss_ce: 0.012960
2021-12-13 00:37:30,894 Training Data Eval:
2021-12-13 00:37:38,537   Average segmentation loss on training set: 0.0418
2021-12-13 00:37:38,537 Validation Data Eval:
2021-12-13 00:37:41,146   Average segmentation loss on validation set: 0.0919
2021-12-13 00:37:47,836 Found new lowest validation loss at iteration 3909! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/TU_3seeds/TU_SGD_best_val_loss_seed100.pth
2021-12-13 00:37:49,224 iteration 3910 : loss : 0.056500, loss_ce: 0.019519
 57%|███████████████▌           | 230/400 [1:47:46<1:27:52, 31.01s/it]2021-12-13 00:37:50,544 iteration 3911 : loss : 0.046910, loss_ce: 0.015520
2021-12-13 00:37:51,892 iteration 3912 : loss : 0.058501, loss_ce: 0.017301
2021-12-13 00:37:53,285 iteration 3913 : loss : 0.057202, loss_ce: 0.021348
2021-12-13 00:37:54,621 iteration 3914 : loss : 0.044662, loss_ce: 0.015569
2021-12-13 00:37:56,129 iteration 3915 : loss : 0.056292, loss_ce: 0.013700
2021-12-13 00:37:57,634 iteration 3916 : loss : 0.049438, loss_ce: 0.015239
2021-12-13 00:37:59,155 iteration 3917 : loss : 0.049622, loss_ce: 0.013133
2021-12-13 00:38:00,633 iteration 3918 : loss : 0.055700, loss_ce: 0.013403
2021-12-13 00:38:02,112 iteration 3919 : loss : 0.047834, loss_ce: 0.011553
2021-12-13 00:38:03,629 iteration 3920 : loss : 0.068551, loss_ce: 0.009101
2021-12-13 00:38:05,114 iteration 3921 : loss : 0.050712, loss_ce: 0.012157
2021-12-13 00:38:06,628 iteration 3922 : loss : 0.053855, loss_ce: 0.011833
2021-12-13 00:38:08,217 iteration 3923 : loss : 0.065486, loss_ce: 0.022840
2021-12-13 00:38:09,673 iteration 3924 : loss : 0.054561, loss_ce: 0.015209
2021-12-13 00:38:11,077 iteration 3925 : loss : 0.055621, loss_ce: 0.018811
2021-12-13 00:38:12,711 iteration 3926 : loss : 0.060149, loss_ce: 0.018743
2021-12-13 00:38:14,229 iteration 3927 : loss : 0.059949, loss_ce: 0.023626
 58%|███████████████▌           | 231/400 [1:48:11<1:22:16, 29.21s/it]2021-12-13 00:38:15,740 iteration 3928 : loss : 0.052357, loss_ce: 0.015862
2021-12-13 00:38:17,178 iteration 3929 : loss : 0.054711, loss_ce: 0.025432
2021-12-13 00:38:18,661 iteration 3930 : loss : 0.051578, loss_ce: 0.012726
2021-12-13 00:38:20,064 iteration 3931 : loss : 0.052898, loss_ce: 0.011441
2021-12-13 00:38:21,561 iteration 3932 : loss : 0.057052, loss_ce: 0.014228
2021-12-13 00:38:23,027 iteration 3933 : loss : 0.054377, loss_ce: 0.019714
2021-12-13 00:38:24,540 iteration 3934 : loss : 0.054050, loss_ce: 0.014499
2021-12-13 00:38:26,014 iteration 3935 : loss : 0.051799, loss_ce: 0.015293
2021-12-13 00:38:27,449 iteration 3936 : loss : 0.049613, loss_ce: 0.012354
2021-12-13 00:38:28,930 iteration 3937 : loss : 0.057403, loss_ce: 0.014634
2021-12-13 00:38:30,410 iteration 3938 : loss : 0.054387, loss_ce: 0.012914
2021-12-13 00:38:31,958 iteration 3939 : loss : 0.053896, loss_ce: 0.011128
2021-12-13 00:38:33,444 iteration 3940 : loss : 0.052802, loss_ce: 0.017230
2021-12-13 00:38:34,929 iteration 3941 : loss : 0.049478, loss_ce: 0.011879
2021-12-13 00:38:36,373 iteration 3942 : loss : 0.049010, loss_ce: 0.020911
2021-12-13 00:38:37,814 iteration 3943 : loss : 0.054131, loss_ce: 0.017742
2021-12-13 00:38:39,271 iteration 3944 : loss : 0.056899, loss_ce: 0.014606
 58%|███████████████▋           | 232/400 [1:48:36<1:18:17, 27.96s/it]2021-12-13 00:38:40,850 iteration 3945 : loss : 0.057679, loss_ce: 0.016018
2021-12-13 00:38:42,312 iteration 3946 : loss : 0.049478, loss_ce: 0.017159
2021-12-13 00:38:43,753 iteration 3947 : loss : 0.053056, loss_ce: 0.016547
2021-12-13 00:38:45,279 iteration 3948 : loss : 0.064892, loss_ce: 0.023126
2021-12-13 00:38:46,834 iteration 3949 : loss : 0.058622, loss_ce: 0.013880
2021-12-13 00:38:48,290 iteration 3950 : loss : 0.052499, loss_ce: 0.020370
2021-12-13 00:38:49,712 iteration 3951 : loss : 0.048369, loss_ce: 0.009453
2021-12-13 00:38:51,301 iteration 3952 : loss : 0.058418, loss_ce: 0.018979
2021-12-13 00:38:52,795 iteration 3953 : loss : 0.049142, loss_ce: 0.015993
2021-12-13 00:38:54,227 iteration 3954 : loss : 0.046956, loss_ce: 0.014333
2021-12-13 00:38:55,740 iteration 3955 : loss : 0.048380, loss_ce: 0.015841
2021-12-13 00:38:57,275 iteration 3956 : loss : 0.065184, loss_ce: 0.027333
2021-12-13 00:38:58,825 iteration 3957 : loss : 0.070074, loss_ce: 0.016631
2021-12-13 00:39:00,375 iteration 3958 : loss : 0.048403, loss_ce: 0.016896
2021-12-13 00:39:01,771 iteration 3959 : loss : 0.046306, loss_ce: 0.009881
2021-12-13 00:39:03,297 iteration 3960 : loss : 0.056344, loss_ce: 0.016633
2021-12-13 00:39:04,753 iteration 3961 : loss : 0.056685, loss_ce: 0.012211
 58%|███████████████▋           | 233/400 [1:49:02<1:15:45, 27.22s/it]2021-12-13 00:39:06,296 iteration 3962 : loss : 0.058186, loss_ce: 0.018520
2021-12-13 00:39:07,847 iteration 3963 : loss : 0.059580, loss_ce: 0.019655
2021-12-13 00:39:09,280 iteration 3964 : loss : 0.046045, loss_ce: 0.011408
2021-12-13 00:39:10,785 iteration 3965 : loss : 0.048513, loss_ce: 0.017443
2021-12-13 00:39:12,371 iteration 3966 : loss : 0.056253, loss_ce: 0.021428
2021-12-13 00:39:13,858 iteration 3967 : loss : 0.057611, loss_ce: 0.011178
2021-12-13 00:39:15,341 iteration 3968 : loss : 0.047714, loss_ce: 0.012485
2021-12-13 00:39:16,786 iteration 3969 : loss : 0.049569, loss_ce: 0.013289
2021-12-13 00:39:18,272 iteration 3970 : loss : 0.054532, loss_ce: 0.010982
2021-12-13 00:39:19,749 iteration 3971 : loss : 0.058385, loss_ce: 0.014247
2021-12-13 00:39:21,239 iteration 3972 : loss : 0.064632, loss_ce: 0.022962
2021-12-13 00:39:22,796 iteration 3973 : loss : 0.054357, loss_ce: 0.017113
2021-12-13 00:39:24,349 iteration 3974 : loss : 0.054755, loss_ce: 0.020319
2021-12-13 00:39:25,834 iteration 3975 : loss : 0.056542, loss_ce: 0.013246
2021-12-13 00:39:27,306 iteration 3976 : loss : 0.050791, loss_ce: 0.014969
2021-12-13 00:39:28,824 iteration 3977 : loss : 0.055161, loss_ce: 0.020069
2021-12-13 00:39:30,325 iteration 3978 : loss : 0.052521, loss_ce: 0.017358
 58%|███████████████▊           | 234/400 [1:49:27<1:13:55, 26.72s/it]2021-12-13 00:39:31,931 iteration 3979 : loss : 0.066871, loss_ce: 0.026798
2021-12-13 00:39:33,339 iteration 3980 : loss : 0.052849, loss_ce: 0.016559
2021-12-13 00:39:34,823 iteration 3981 : loss : 0.052999, loss_ce: 0.012737
2021-12-13 00:39:36,325 iteration 3982 : loss : 0.049627, loss_ce: 0.017871
2021-12-13 00:39:37,834 iteration 3983 : loss : 0.063159, loss_ce: 0.014259
2021-12-13 00:39:39,359 iteration 3984 : loss : 0.057731, loss_ce: 0.021999
2021-12-13 00:39:40,894 iteration 3985 : loss : 0.048479, loss_ce: 0.012066
2021-12-13 00:39:42,440 iteration 3986 : loss : 0.062634, loss_ce: 0.016282
2021-12-13 00:39:43,880 iteration 3987 : loss : 0.050790, loss_ce: 0.015343
2021-12-13 00:39:45,355 iteration 3988 : loss : 0.053421, loss_ce: 0.014095
2021-12-13 00:39:46,867 iteration 3989 : loss : 0.048916, loss_ce: 0.014499
2021-12-13 00:39:48,373 iteration 3990 : loss : 0.051081, loss_ce: 0.017171
2021-12-13 00:39:49,898 iteration 3991 : loss : 0.058805, loss_ce: 0.021044
2021-12-13 00:39:51,427 iteration 3992 : loss : 0.047824, loss_ce: 0.013983
2021-12-13 00:39:52,925 iteration 3993 : loss : 0.055646, loss_ce: 0.019133
2021-12-13 00:39:54,462 iteration 3994 : loss : 0.047578, loss_ce: 0.012891
2021-12-13 00:39:54,462 Training Data Eval:
2021-12-13 00:40:02,124   Average segmentation loss on training set: 0.0419
2021-12-13 00:40:02,124 Validation Data Eval:
2021-12-13 00:40:04,732   Average segmentation loss on validation set: 0.0908
2021-12-13 00:40:11,150 Found new lowest validation loss at iteration 3994! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/TU_3seeds/TU_SGD_best_val_loss_seed100.pth
2021-12-13 00:40:12,661 iteration 3995 : loss : 0.059773, loss_ce: 0.018377
 59%|███████████████▊           | 235/400 [1:50:09<1:26:21, 31.40s/it]2021-12-13 00:40:14,059 iteration 3996 : loss : 0.053308, loss_ce: 0.020470
2021-12-13 00:40:15,378 iteration 3997 : loss : 0.057960, loss_ce: 0.020333
2021-12-13 00:40:16,696 iteration 3998 : loss : 0.048161, loss_ce: 0.010687
2021-12-13 00:40:18,235 iteration 3999 : loss : 0.062029, loss_ce: 0.015481
2021-12-13 00:40:19,722 iteration 4000 : loss : 0.047922, loss_ce: 0.015927
2021-12-13 00:40:21,248 iteration 4001 : loss : 0.049925, loss_ce: 0.020053
2021-12-13 00:40:22,642 iteration 4002 : loss : 0.050835, loss_ce: 0.013095
2021-12-13 00:40:24,182 iteration 4003 : loss : 0.058973, loss_ce: 0.017538
2021-12-13 00:40:25,717 iteration 4004 : loss : 0.062375, loss_ce: 0.021301
2021-12-13 00:40:27,226 iteration 4005 : loss : 0.051591, loss_ce: 0.015907
2021-12-13 00:40:28,655 iteration 4006 : loss : 0.058412, loss_ce: 0.013412
2021-12-13 00:40:30,133 iteration 4007 : loss : 0.070411, loss_ce: 0.019443
2021-12-13 00:40:31,595 iteration 4008 : loss : 0.053546, loss_ce: 0.020078
2021-12-13 00:40:33,083 iteration 4009 : loss : 0.046716, loss_ce: 0.012185
2021-12-13 00:40:34,583 iteration 4010 : loss : 0.051518, loss_ce: 0.016128
2021-12-13 00:40:36,048 iteration 4011 : loss : 0.057796, loss_ce: 0.016296
2021-12-13 00:40:37,549 iteration 4012 : loss : 0.059434, loss_ce: 0.022210
 59%|███████████████▉           | 236/400 [1:50:34<1:20:30, 29.45s/it]2021-12-13 00:40:38,957 iteration 4013 : loss : 0.043792, loss_ce: 0.015664
2021-12-13 00:40:40,498 iteration 4014 : loss : 0.051023, loss_ce: 0.016134
2021-12-13 00:40:41,926 iteration 4015 : loss : 0.054632, loss_ce: 0.021207
2021-12-13 00:40:43,477 iteration 4016 : loss : 0.054605, loss_ce: 0.017128
2021-12-13 00:40:44,982 iteration 4017 : loss : 0.059169, loss_ce: 0.013351
2021-12-13 00:40:46,393 iteration 4018 : loss : 0.050538, loss_ce: 0.014240
2021-12-13 00:40:47,891 iteration 4019 : loss : 0.104117, loss_ce: 0.025564
2021-12-13 00:40:49,355 iteration 4020 : loss : 0.050028, loss_ce: 0.012385
2021-12-13 00:40:50,863 iteration 4021 : loss : 0.060248, loss_ce: 0.014784
2021-12-13 00:40:52,445 iteration 4022 : loss : 0.056352, loss_ce: 0.016594
2021-12-13 00:40:53,867 iteration 4023 : loss : 0.048051, loss_ce: 0.017307
2021-12-13 00:40:55,403 iteration 4024 : loss : 0.051925, loss_ce: 0.012147
2021-12-13 00:40:56,904 iteration 4025 : loss : 0.058595, loss_ce: 0.016725
2021-12-13 00:40:58,451 iteration 4026 : loss : 0.052893, loss_ce: 0.013106
2021-12-13 00:40:59,852 iteration 4027 : loss : 0.049695, loss_ce: 0.012989
2021-12-13 00:41:01,474 iteration 4028 : loss : 0.063291, loss_ce: 0.022411
2021-12-13 00:41:03,076 iteration 4029 : loss : 0.056936, loss_ce: 0.016513
 59%|███████████████▉           | 237/400 [1:51:00<1:16:48, 28.28s/it]2021-12-13 00:41:04,592 iteration 4030 : loss : 0.050004, loss_ce: 0.012973
2021-12-13 00:41:06,068 iteration 4031 : loss : 0.052432, loss_ce: 0.019305
2021-12-13 00:41:07,474 iteration 4032 : loss : 0.043866, loss_ce: 0.010887
2021-12-13 00:41:08,909 iteration 4033 : loss : 0.048491, loss_ce: 0.015682
2021-12-13 00:41:10,434 iteration 4034 : loss : 0.057618, loss_ce: 0.020668
2021-12-13 00:41:11,960 iteration 4035 : loss : 0.052686, loss_ce: 0.014197
2021-12-13 00:41:13,463 iteration 4036 : loss : 0.047366, loss_ce: 0.014854
2021-12-13 00:41:14,903 iteration 4037 : loss : 0.050435, loss_ce: 0.020945
2021-12-13 00:41:16,383 iteration 4038 : loss : 0.054838, loss_ce: 0.018754
2021-12-13 00:41:17,881 iteration 4039 : loss : 0.055487, loss_ce: 0.012639
2021-12-13 00:41:19,350 iteration 4040 : loss : 0.051722, loss_ce: 0.012658
2021-12-13 00:41:20,892 iteration 4041 : loss : 0.056462, loss_ce: 0.016617
2021-12-13 00:41:22,417 iteration 4042 : loss : 0.055481, loss_ce: 0.017087
2021-12-13 00:41:23,918 iteration 4043 : loss : 0.047826, loss_ce: 0.014514
2021-12-13 00:41:25,402 iteration 4044 : loss : 0.056491, loss_ce: 0.018094
2021-12-13 00:41:26,884 iteration 4045 : loss : 0.050726, loss_ce: 0.013152
2021-12-13 00:41:28,438 iteration 4046 : loss : 0.053771, loss_ce: 0.016546
 60%|████████████████           | 238/400 [1:51:25<1:13:58, 27.40s/it]2021-12-13 00:41:30,003 iteration 4047 : loss : 0.047134, loss_ce: 0.013876
2021-12-13 00:41:31,484 iteration 4048 : loss : 0.053983, loss_ce: 0.013319
2021-12-13 00:41:32,967 iteration 4049 : loss : 0.044146, loss_ce: 0.010553
2021-12-13 00:41:34,393 iteration 4050 : loss : 0.051135, loss_ce: 0.012133
2021-12-13 00:41:35,874 iteration 4051 : loss : 0.047299, loss_ce: 0.015683
2021-12-13 00:41:37,307 iteration 4052 : loss : 0.048143, loss_ce: 0.015195
2021-12-13 00:41:38,816 iteration 4053 : loss : 0.059196, loss_ce: 0.009543
2021-12-13 00:41:40,349 iteration 4054 : loss : 0.052897, loss_ce: 0.015814
2021-12-13 00:41:41,854 iteration 4055 : loss : 0.061622, loss_ce: 0.023563
2021-12-13 00:41:43,346 iteration 4056 : loss : 0.050178, loss_ce: 0.018583
2021-12-13 00:41:44,770 iteration 4057 : loss : 0.050383, loss_ce: 0.011290
2021-12-13 00:41:46,214 iteration 4058 : loss : 0.045054, loss_ce: 0.013324
2021-12-13 00:41:47,738 iteration 4059 : loss : 0.056888, loss_ce: 0.017285
2021-12-13 00:41:49,194 iteration 4060 : loss : 0.058645, loss_ce: 0.021203
2021-12-13 00:41:50,592 iteration 4061 : loss : 0.049618, loss_ce: 0.017575
2021-12-13 00:41:52,050 iteration 4062 : loss : 0.047768, loss_ce: 0.016729
2021-12-13 00:41:53,587 iteration 4063 : loss : 0.056824, loss_ce: 0.016167
 60%|████████████████▏          | 239/400 [1:51:50<1:11:42, 26.72s/it]2021-12-13 00:41:55,116 iteration 4064 : loss : 0.046226, loss_ce: 0.012814
2021-12-13 00:41:56,570 iteration 4065 : loss : 0.055287, loss_ce: 0.011269
2021-12-13 00:41:58,058 iteration 4066 : loss : 0.047113, loss_ce: 0.013439
2021-12-13 00:41:59,537 iteration 4067 : loss : 0.050286, loss_ce: 0.012832
2021-12-13 00:42:01,051 iteration 4068 : loss : 0.061222, loss_ce: 0.019263
2021-12-13 00:42:02,572 iteration 4069 : loss : 0.061661, loss_ce: 0.023526
2021-12-13 00:42:04,017 iteration 4070 : loss : 0.056525, loss_ce: 0.013138
2021-12-13 00:42:05,508 iteration 4071 : loss : 0.047020, loss_ce: 0.012933
2021-12-13 00:42:07,006 iteration 4072 : loss : 0.049171, loss_ce: 0.015578
2021-12-13 00:42:08,529 iteration 4073 : loss : 0.057210, loss_ce: 0.017670
2021-12-13 00:42:10,026 iteration 4074 : loss : 0.049219, loss_ce: 0.015893
2021-12-13 00:42:11,622 iteration 4075 : loss : 0.057063, loss_ce: 0.016129
2021-12-13 00:42:13,142 iteration 4076 : loss : 0.045358, loss_ce: 0.014850
2021-12-13 00:42:14,702 iteration 4077 : loss : 0.053222, loss_ce: 0.017315
2021-12-13 00:42:16,263 iteration 4078 : loss : 0.054628, loss_ce: 0.014194
2021-12-13 00:42:17,730 iteration 4079 : loss : 0.053396, loss_ce: 0.016341
2021-12-13 00:42:17,730 Training Data Eval:
2021-12-13 00:42:25,388   Average segmentation loss on training set: 0.0400
2021-12-13 00:42:25,388 Validation Data Eval:
2021-12-13 00:42:27,993   Average segmentation loss on validation set: 0.0905
2021-12-13 00:42:34,339 Found new lowest validation loss at iteration 4079! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/TU_3seeds/TU_SGD_best_val_loss_seed100.pth
2021-12-13 00:42:35,819 iteration 4080 : loss : 0.065063, loss_ce: 0.017757
 60%|████████████████▏          | 240/400 [1:52:33<1:23:40, 31.38s/it]2021-12-13 00:42:37,190 iteration 4081 : loss : 0.046695, loss_ce: 0.015521
2021-12-13 00:42:38,553 iteration 4082 : loss : 0.053891, loss_ce: 0.015881
2021-12-13 00:42:39,942 iteration 4083 : loss : 0.048264, loss_ce: 0.017458
2021-12-13 00:42:41,459 iteration 4084 : loss : 0.054265, loss_ce: 0.013689
2021-12-13 00:42:43,028 iteration 4085 : loss : 0.052481, loss_ce: 0.014056
2021-12-13 00:42:44,578 iteration 4086 : loss : 0.057012, loss_ce: 0.022172
2021-12-13 00:42:46,051 iteration 4087 : loss : 0.057877, loss_ce: 0.013224
2021-12-13 00:42:47,525 iteration 4088 : loss : 0.050236, loss_ce: 0.016436
2021-12-13 00:42:49,041 iteration 4089 : loss : 0.063535, loss_ce: 0.018375
2021-12-13 00:42:50,559 iteration 4090 : loss : 0.047184, loss_ce: 0.013328
2021-12-13 00:42:52,018 iteration 4091 : loss : 0.051347, loss_ce: 0.013433
2021-12-13 00:42:53,503 iteration 4092 : loss : 0.043897, loss_ce: 0.011572
2021-12-13 00:42:54,978 iteration 4093 : loss : 0.051210, loss_ce: 0.017394
2021-12-13 00:42:56,423 iteration 4094 : loss : 0.043200, loss_ce: 0.010324
2021-12-13 00:42:57,890 iteration 4095 : loss : 0.047500, loss_ce: 0.014194
2021-12-13 00:42:59,366 iteration 4096 : loss : 0.049521, loss_ce: 0.016403
2021-12-13 00:43:00,864 iteration 4097 : loss : 0.059600, loss_ce: 0.021677
 60%|████████████████▎          | 241/400 [1:52:58<1:18:06, 29.47s/it]2021-12-13 00:43:02,430 iteration 4098 : loss : 0.057168, loss_ce: 0.017247
2021-12-13 00:43:03,893 iteration 4099 : loss : 0.054741, loss_ce: 0.012597
2021-12-13 00:43:05,318 iteration 4100 : loss : 0.049552, loss_ce: 0.017992
2021-12-13 00:43:06,766 iteration 4101 : loss : 0.049851, loss_ce: 0.015483
2021-12-13 00:43:08,265 iteration 4102 : loss : 0.056519, loss_ce: 0.017701
2021-12-13 00:43:09,719 iteration 4103 : loss : 0.047892, loss_ce: 0.015073
2021-12-13 00:43:11,145 iteration 4104 : loss : 0.047080, loss_ce: 0.015803
2021-12-13 00:43:12,640 iteration 4105 : loss : 0.056523, loss_ce: 0.019602
2021-12-13 00:43:14,069 iteration 4106 : loss : 0.054679, loss_ce: 0.009919
2021-12-13 00:43:15,656 iteration 4107 : loss : 0.057347, loss_ce: 0.022658
2021-12-13 00:43:17,187 iteration 4108 : loss : 0.062437, loss_ce: 0.020860
2021-12-13 00:43:18,617 iteration 4109 : loss : 0.050754, loss_ce: 0.010917
2021-12-13 00:43:20,117 iteration 4110 : loss : 0.055287, loss_ce: 0.018290
2021-12-13 00:43:21,667 iteration 4111 : loss : 0.057807, loss_ce: 0.018703
2021-12-13 00:43:23,125 iteration 4112 : loss : 0.057180, loss_ce: 0.015852
2021-12-13 00:43:24,630 iteration 4113 : loss : 0.054927, loss_ce: 0.018634
2021-12-13 00:43:26,202 iteration 4114 : loss : 0.058400, loss_ce: 0.017800
 60%|████████████████▎          | 242/400 [1:53:23<1:14:21, 28.24s/it]2021-12-13 00:43:27,692 iteration 4115 : loss : 0.046774, loss_ce: 0.010581
2021-12-13 00:43:29,110 iteration 4116 : loss : 0.049860, loss_ce: 0.017045
2021-12-13 00:43:30,737 iteration 4117 : loss : 0.054901, loss_ce: 0.014632
2021-12-13 00:43:32,352 iteration 4118 : loss : 0.057651, loss_ce: 0.017472
2021-12-13 00:43:33,877 iteration 4119 : loss : 0.047584, loss_ce: 0.014525
2021-12-13 00:43:35,384 iteration 4120 : loss : 0.058729, loss_ce: 0.010486
2021-12-13 00:43:36,886 iteration 4121 : loss : 0.054117, loss_ce: 0.020135
2021-12-13 00:43:38,348 iteration 4122 : loss : 0.057868, loss_ce: 0.017039
2021-12-13 00:43:39,907 iteration 4123 : loss : 0.056855, loss_ce: 0.016355
2021-12-13 00:43:41,330 iteration 4124 : loss : 0.054005, loss_ce: 0.015159
2021-12-13 00:43:42,884 iteration 4125 : loss : 0.055484, loss_ce: 0.013559
2021-12-13 00:43:44,384 iteration 4126 : loss : 0.050165, loss_ce: 0.016935
2021-12-13 00:43:45,913 iteration 4127 : loss : 0.053153, loss_ce: 0.016961
2021-12-13 00:43:47,365 iteration 4128 : loss : 0.053031, loss_ce: 0.022366
2021-12-13 00:43:48,911 iteration 4129 : loss : 0.053932, loss_ce: 0.016914
2021-12-13 00:43:50,392 iteration 4130 : loss : 0.052886, loss_ce: 0.021121
2021-12-13 00:43:51,791 iteration 4131 : loss : 0.040462, loss_ce: 0.009751
 61%|████████████████▍          | 243/400 [1:53:49<1:11:48, 27.44s/it]2021-12-13 00:43:53,372 iteration 4132 : loss : 0.060868, loss_ce: 0.016802
2021-12-13 00:43:54,921 iteration 4133 : loss : 0.069558, loss_ce: 0.021450
2021-12-13 00:43:56,326 iteration 4134 : loss : 0.046673, loss_ce: 0.015685
2021-12-13 00:43:57,868 iteration 4135 : loss : 0.054936, loss_ce: 0.015631
2021-12-13 00:43:59,356 iteration 4136 : loss : 0.049590, loss_ce: 0.018575
2021-12-13 00:44:00,812 iteration 4137 : loss : 0.050873, loss_ce: 0.019814
2021-12-13 00:44:02,250 iteration 4138 : loss : 0.050214, loss_ce: 0.019430
2021-12-13 00:44:03,756 iteration 4139 : loss : 0.062164, loss_ce: 0.016651
2021-12-13 00:44:05,300 iteration 4140 : loss : 0.065832, loss_ce: 0.016856
2021-12-13 00:44:06,786 iteration 4141 : loss : 0.052027, loss_ce: 0.013087
2021-12-13 00:44:08,295 iteration 4142 : loss : 0.060499, loss_ce: 0.016442
2021-12-13 00:44:09,838 iteration 4143 : loss : 0.065002, loss_ce: 0.015875
2021-12-13 00:44:11,308 iteration 4144 : loss : 0.052021, loss_ce: 0.016314
2021-12-13 00:44:12,768 iteration 4145 : loss : 0.048957, loss_ce: 0.011525
2021-12-13 00:44:14,253 iteration 4146 : loss : 0.059779, loss_ce: 0.014781
2021-12-13 00:44:15,712 iteration 4147 : loss : 0.053752, loss_ce: 0.017195
2021-12-13 00:44:17,103 iteration 4148 : loss : 0.042443, loss_ce: 0.011959
 61%|████████████████▍          | 244/400 [1:54:14<1:09:41, 26.80s/it]2021-12-13 00:44:18,749 iteration 4149 : loss : 0.066211, loss_ce: 0.019634
2021-12-13 00:44:20,155 iteration 4150 : loss : 0.046500, loss_ce: 0.010887
2021-12-13 00:44:21,676 iteration 4151 : loss : 0.051358, loss_ce: 0.019357
2021-12-13 00:44:23,064 iteration 4152 : loss : 0.045030, loss_ce: 0.013798
2021-12-13 00:44:24,582 iteration 4153 : loss : 0.061545, loss_ce: 0.016163
2021-12-13 00:44:26,092 iteration 4154 : loss : 0.055216, loss_ce: 0.018862
2021-12-13 00:44:27,649 iteration 4155 : loss : 0.058533, loss_ce: 0.016649
2021-12-13 00:44:29,037 iteration 4156 : loss : 0.041971, loss_ce: 0.011716
2021-12-13 00:44:30,550 iteration 4157 : loss : 0.052073, loss_ce: 0.013307
2021-12-13 00:44:32,032 iteration 4158 : loss : 0.057969, loss_ce: 0.018662
2021-12-13 00:44:33,555 iteration 4159 : loss : 0.049173, loss_ce: 0.015778
2021-12-13 00:44:35,043 iteration 4160 : loss : 0.056663, loss_ce: 0.020757
2021-12-13 00:44:36,525 iteration 4161 : loss : 0.044374, loss_ce: 0.013145
2021-12-13 00:44:37,981 iteration 4162 : loss : 0.047247, loss_ce: 0.013262
2021-12-13 00:44:39,505 iteration 4163 : loss : 0.061090, loss_ce: 0.014552
2021-12-13 00:44:41,031 iteration 4164 : loss : 0.055247, loss_ce: 0.020812
2021-12-13 00:44:41,031 Training Data Eval:
2021-12-13 00:44:48,673   Average segmentation loss on training set: 0.0401
2021-12-13 00:44:48,673 Validation Data Eval:
2021-12-13 00:44:51,277   Average segmentation loss on validation set: 0.0883
2021-12-13 00:44:57,627 Found new lowest validation loss at iteration 4164! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/TU_3seeds/TU_SGD_best_val_loss_seed100.pth
2021-12-13 00:44:59,040 iteration 4165 : loss : 0.050137, loss_ce: 0.015635
 61%|████████████████▌          | 245/400 [1:54:56<1:20:58, 31.34s/it]2021-12-13 00:45:00,397 iteration 4166 : loss : 0.049588, loss_ce: 0.014731
2021-12-13 00:45:01,796 iteration 4167 : loss : 0.051877, loss_ce: 0.012366
2021-12-13 00:45:03,287 iteration 4168 : loss : 0.052240, loss_ce: 0.017503
2021-12-13 00:45:04,796 iteration 4169 : loss : 0.055497, loss_ce: 0.015856
2021-12-13 00:45:06,327 iteration 4170 : loss : 0.059593, loss_ce: 0.020961
2021-12-13 00:45:07,776 iteration 4171 : loss : 0.050648, loss_ce: 0.017748
2021-12-13 00:45:09,240 iteration 4172 : loss : 0.059439, loss_ce: 0.018918
2021-12-13 00:45:10,635 iteration 4173 : loss : 0.046481, loss_ce: 0.012037
2021-12-13 00:45:12,164 iteration 4174 : loss : 0.052510, loss_ce: 0.014489
2021-12-13 00:45:13,671 iteration 4175 : loss : 0.045120, loss_ce: 0.010007
2021-12-13 00:45:15,201 iteration 4176 : loss : 0.058795, loss_ce: 0.016509
2021-12-13 00:45:16,619 iteration 4177 : loss : 0.045472, loss_ce: 0.017310
2021-12-13 00:45:18,090 iteration 4178 : loss : 0.047915, loss_ce: 0.016301
2021-12-13 00:45:19,616 iteration 4179 : loss : 0.047174, loss_ce: 0.013426
2021-12-13 00:45:21,185 iteration 4180 : loss : 0.056514, loss_ce: 0.016754
2021-12-13 00:45:22,692 iteration 4181 : loss : 0.053207, loss_ce: 0.016699
2021-12-13 00:45:24,233 iteration 4182 : loss : 0.049625, loss_ce: 0.014623
 62%|████████████████▌          | 246/400 [1:55:21<1:15:42, 29.50s/it]2021-12-13 00:45:25,747 iteration 4183 : loss : 0.052678, loss_ce: 0.011825
2021-12-13 00:45:27,211 iteration 4184 : loss : 0.050834, loss_ce: 0.014124
2021-12-13 00:45:28,727 iteration 4185 : loss : 0.071949, loss_ce: 0.019518
2021-12-13 00:45:30,184 iteration 4186 : loss : 0.046177, loss_ce: 0.014030
2021-12-13 00:45:31,620 iteration 4187 : loss : 0.051540, loss_ce: 0.016543
2021-12-13 00:45:33,086 iteration 4188 : loss : 0.054592, loss_ce: 0.018478
2021-12-13 00:45:34,492 iteration 4189 : loss : 0.054127, loss_ce: 0.014847
2021-12-13 00:45:36,075 iteration 4190 : loss : 0.058086, loss_ce: 0.020068
2021-12-13 00:45:37,611 iteration 4191 : loss : 0.047912, loss_ce: 0.009364
2021-12-13 00:45:39,030 iteration 4192 : loss : 0.046626, loss_ce: 0.018819
2021-12-13 00:45:40,426 iteration 4193 : loss : 0.047797, loss_ce: 0.013446
2021-12-13 00:45:41,894 iteration 4194 : loss : 0.055553, loss_ce: 0.015936
2021-12-13 00:45:43,331 iteration 4195 : loss : 0.047092, loss_ce: 0.015441
2021-12-13 00:45:44,764 iteration 4196 : loss : 0.045176, loss_ce: 0.013258
2021-12-13 00:45:46,194 iteration 4197 : loss : 0.052061, loss_ce: 0.017195
2021-12-13 00:45:47,609 iteration 4198 : loss : 0.056226, loss_ce: 0.009710
2021-12-13 00:45:49,127 iteration 4199 : loss : 0.054951, loss_ce: 0.018944
 62%|████████████████▋          | 247/400 [1:55:46<1:11:41, 28.12s/it]2021-12-13 00:45:50,639 iteration 4200 : loss : 0.053133, loss_ce: 0.015540
2021-12-13 00:45:52,165 iteration 4201 : loss : 0.059504, loss_ce: 0.022274
2021-12-13 00:45:53,731 iteration 4202 : loss : 0.048525, loss_ce: 0.016306
2021-12-13 00:45:55,295 iteration 4203 : loss : 0.063315, loss_ce: 0.017336
2021-12-13 00:45:56,823 iteration 4204 : loss : 0.043986, loss_ce: 0.013678
2021-12-13 00:45:58,152 iteration 4205 : loss : 0.041263, loss_ce: 0.010824
2021-12-13 00:45:59,598 iteration 4206 : loss : 0.050626, loss_ce: 0.012661
2021-12-13 00:46:01,039 iteration 4207 : loss : 0.046850, loss_ce: 0.016319
2021-12-13 00:46:02,565 iteration 4208 : loss : 0.060612, loss_ce: 0.020504
2021-12-13 00:46:04,025 iteration 4209 : loss : 0.049325, loss_ce: 0.012893
2021-12-13 00:46:05,481 iteration 4210 : loss : 0.043276, loss_ce: 0.011877
2021-12-13 00:46:06,977 iteration 4211 : loss : 0.045550, loss_ce: 0.014462
2021-12-13 00:46:08,424 iteration 4212 : loss : 0.046299, loss_ce: 0.016381
2021-12-13 00:46:09,974 iteration 4213 : loss : 0.053250, loss_ce: 0.015098
2021-12-13 00:46:11,528 iteration 4214 : loss : 0.059404, loss_ce: 0.022555
2021-12-13 00:46:13,056 iteration 4215 : loss : 0.055369, loss_ce: 0.011200
2021-12-13 00:46:14,539 iteration 4216 : loss : 0.062628, loss_ce: 0.019131
 62%|████████████████▋          | 248/400 [1:56:11<1:09:10, 27.31s/it]2021-12-13 00:46:16,146 iteration 4217 : loss : 0.063329, loss_ce: 0.014096
2021-12-13 00:46:17,725 iteration 4218 : loss : 0.051529, loss_ce: 0.019545
2021-12-13 00:46:19,239 iteration 4219 : loss : 0.048361, loss_ce: 0.015652
2021-12-13 00:46:20,730 iteration 4220 : loss : 0.052212, loss_ce: 0.014120
2021-12-13 00:46:22,249 iteration 4221 : loss : 0.065854, loss_ce: 0.017728
2021-12-13 00:46:23,788 iteration 4222 : loss : 0.047568, loss_ce: 0.016999
2021-12-13 00:46:25,414 iteration 4223 : loss : 0.065897, loss_ce: 0.015022
2021-12-13 00:46:26,903 iteration 4224 : loss : 0.046937, loss_ce: 0.012097
2021-12-13 00:46:28,442 iteration 4225 : loss : 0.050579, loss_ce: 0.017045
2021-12-13 00:46:30,032 iteration 4226 : loss : 0.060897, loss_ce: 0.022197
2021-12-13 00:46:31,499 iteration 4227 : loss : 0.050134, loss_ce: 0.013384
2021-12-13 00:46:33,022 iteration 4228 : loss : 0.055191, loss_ce: 0.020802
2021-12-13 00:46:34,462 iteration 4229 : loss : 0.051914, loss_ce: 0.016676
2021-12-13 00:46:36,003 iteration 4230 : loss : 0.048405, loss_ce: 0.011592
2021-12-13 00:46:37,521 iteration 4231 : loss : 0.052080, loss_ce: 0.014487
2021-12-13 00:46:39,070 iteration 4232 : loss : 0.056264, loss_ce: 0.017076
2021-12-13 00:46:40,498 iteration 4233 : loss : 0.050591, loss_ce: 0.014529
 62%|████████████████▊          | 249/400 [1:56:37<1:07:42, 26.90s/it]2021-12-13 00:46:42,076 iteration 4234 : loss : 0.052243, loss_ce: 0.017642
2021-12-13 00:46:43,516 iteration 4235 : loss : 0.045720, loss_ce: 0.014096
2021-12-13 00:46:45,062 iteration 4236 : loss : 0.058080, loss_ce: 0.023160
2021-12-13 00:46:46,546 iteration 4237 : loss : 0.053092, loss_ce: 0.022616
2021-12-13 00:46:48,117 iteration 4238 : loss : 0.061463, loss_ce: 0.014967
2021-12-13 00:46:49,529 iteration 4239 : loss : 0.049762, loss_ce: 0.012906
2021-12-13 00:46:51,100 iteration 4240 : loss : 0.053340, loss_ce: 0.016172
2021-12-13 00:46:52,573 iteration 4241 : loss : 0.047590, loss_ce: 0.013228
2021-12-13 00:46:54,024 iteration 4242 : loss : 0.049030, loss_ce: 0.012783
2021-12-13 00:46:55,495 iteration 4243 : loss : 0.050823, loss_ce: 0.014204
2021-12-13 00:46:57,035 iteration 4244 : loss : 0.050034, loss_ce: 0.014909
2021-12-13 00:46:58,540 iteration 4245 : loss : 0.058559, loss_ce: 0.011378
2021-12-13 00:47:00,010 iteration 4246 : loss : 0.051279, loss_ce: 0.019432
2021-12-13 00:47:01,469 iteration 4247 : loss : 0.054049, loss_ce: 0.011491
2021-12-13 00:47:02,999 iteration 4248 : loss : 0.055506, loss_ce: 0.017360
2021-12-13 00:47:04,490 iteration 4249 : loss : 0.059786, loss_ce: 0.022887
2021-12-13 00:47:04,491 Training Data Eval:
2021-12-13 00:47:12,126   Average segmentation loss on training set: 0.0393
2021-12-13 00:47:12,127 Validation Data Eval:
2021-12-13 00:47:14,727   Average segmentation loss on validation set: 0.0899
2021-12-13 00:47:16,246 iteration 4250 : loss : 0.053001, loss_ce: 0.014495
 62%|████████████████▉          | 250/400 [1:57:13<1:13:53, 29.56s/it]2021-12-13 00:47:17,832 iteration 4251 : loss : 0.049848, loss_ce: 0.013707
2021-12-13 00:47:19,360 iteration 4252 : loss : 0.058254, loss_ce: 0.020395
2021-12-13 00:47:20,882 iteration 4253 : loss : 0.051288, loss_ce: 0.013888
2021-12-13 00:47:22,468 iteration 4254 : loss : 0.050666, loss_ce: 0.017026
2021-12-13 00:47:23,863 iteration 4255 : loss : 0.043002, loss_ce: 0.012603
2021-12-13 00:47:25,342 iteration 4256 : loss : 0.046186, loss_ce: 0.010366
2021-12-13 00:47:26,913 iteration 4257 : loss : 0.054798, loss_ce: 0.017401
2021-12-13 00:47:28,374 iteration 4258 : loss : 0.045498, loss_ce: 0.012208
2021-12-13 00:47:29,868 iteration 4259 : loss : 0.050131, loss_ce: 0.015443
2021-12-13 00:47:31,365 iteration 4260 : loss : 0.047766, loss_ce: 0.013182
2021-12-13 00:47:32,867 iteration 4261 : loss : 0.051775, loss_ce: 0.015387
2021-12-13 00:47:34,456 iteration 4262 : loss : 0.050758, loss_ce: 0.018803
2021-12-13 00:47:35,952 iteration 4263 : loss : 0.044631, loss_ce: 0.012863
2021-12-13 00:47:37,378 iteration 4264 : loss : 0.052196, loss_ce: 0.014587
2021-12-13 00:47:38,848 iteration 4265 : loss : 0.049259, loss_ce: 0.011519
2021-12-13 00:47:40,333 iteration 4266 : loss : 0.049206, loss_ce: 0.014481
2021-12-13 00:47:41,823 iteration 4267 : loss : 0.052060, loss_ce: 0.019476
 63%|████████████████▉          | 251/400 [1:57:39<1:10:25, 28.36s/it]2021-12-13 00:47:43,349 iteration 4268 : loss : 0.056230, loss_ce: 0.019212
2021-12-13 00:47:44,782 iteration 4269 : loss : 0.046947, loss_ce: 0.012201
2021-12-13 00:47:46,230 iteration 4270 : loss : 0.046930, loss_ce: 0.005914
2021-12-13 00:47:47,662 iteration 4271 : loss : 0.049458, loss_ce: 0.017967
2021-12-13 00:47:49,187 iteration 4272 : loss : 0.053232, loss_ce: 0.016889
2021-12-13 00:47:50,688 iteration 4273 : loss : 0.057988, loss_ce: 0.017973
2021-12-13 00:47:52,214 iteration 4274 : loss : 0.051279, loss_ce: 0.016402
2021-12-13 00:47:53,568 iteration 4275 : loss : 0.044240, loss_ce: 0.014879
2021-12-13 00:47:54,996 iteration 4276 : loss : 0.051034, loss_ce: 0.014510
2021-12-13 00:47:56,433 iteration 4277 : loss : 0.046745, loss_ce: 0.014337
2021-12-13 00:47:57,885 iteration 4278 : loss : 0.055264, loss_ce: 0.013879
2021-12-13 00:47:59,290 iteration 4279 : loss : 0.044409, loss_ce: 0.009217
2021-12-13 00:48:00,710 iteration 4280 : loss : 0.051626, loss_ce: 0.012539
2021-12-13 00:48:02,314 iteration 4281 : loss : 0.054552, loss_ce: 0.021026
2021-12-13 00:48:03,809 iteration 4282 : loss : 0.049965, loss_ce: 0.016727
2021-12-13 00:48:05,195 iteration 4283 : loss : 0.044799, loss_ce: 0.014233
2021-12-13 00:48:06,651 iteration 4284 : loss : 0.050636, loss_ce: 0.018766
 63%|█████████████████          | 252/400 [1:58:03<1:07:20, 27.30s/it]2021-12-13 00:48:08,225 iteration 4285 : loss : 0.055873, loss_ce: 0.017578
2021-12-13 00:48:09,743 iteration 4286 : loss : 0.050905, loss_ce: 0.012115
2021-12-13 00:48:11,287 iteration 4287 : loss : 0.056950, loss_ce: 0.017204
2021-12-13 00:48:12,772 iteration 4288 : loss : 0.054176, loss_ce: 0.012490
2021-12-13 00:48:14,276 iteration 4289 : loss : 0.049795, loss_ce: 0.011526
2021-12-13 00:48:15,816 iteration 4290 : loss : 0.076527, loss_ce: 0.023750
2021-12-13 00:48:17,229 iteration 4291 : loss : 0.047501, loss_ce: 0.013014
2021-12-13 00:48:18,783 iteration 4292 : loss : 0.047941, loss_ce: 0.014957
2021-12-13 00:48:20,290 iteration 4293 : loss : 0.053326, loss_ce: 0.019688
2021-12-13 00:48:21,782 iteration 4294 : loss : 0.055981, loss_ce: 0.022527
2021-12-13 00:48:23,313 iteration 4295 : loss : 0.048906, loss_ce: 0.016689
2021-12-13 00:48:24,725 iteration 4296 : loss : 0.044614, loss_ce: 0.014955
2021-12-13 00:48:26,222 iteration 4297 : loss : 0.047647, loss_ce: 0.011289
2021-12-13 00:48:27,671 iteration 4298 : loss : 0.046318, loss_ce: 0.012359
2021-12-13 00:48:29,241 iteration 4299 : loss : 0.060054, loss_ce: 0.022763
2021-12-13 00:48:30,780 iteration 4300 : loss : 0.055673, loss_ce: 0.014458
2021-12-13 00:48:32,254 iteration 4301 : loss : 0.045242, loss_ce: 0.014632
 63%|█████████████████          | 253/400 [1:58:29<1:05:38, 26.79s/it]2021-12-13 00:48:33,799 iteration 4302 : loss : 0.047660, loss_ce: 0.016059
2021-12-13 00:48:35,334 iteration 4303 : loss : 0.065474, loss_ce: 0.017812
2021-12-13 00:48:36,760 iteration 4304 : loss : 0.046200, loss_ce: 0.014100
2021-12-13 00:48:38,268 iteration 4305 : loss : 0.046818, loss_ce: 0.011449
2021-12-13 00:48:39,722 iteration 4306 : loss : 0.047468, loss_ce: 0.012613
2021-12-13 00:48:41,187 iteration 4307 : loss : 0.048270, loss_ce: 0.013060
2021-12-13 00:48:42,728 iteration 4308 : loss : 0.058318, loss_ce: 0.018327
2021-12-13 00:48:44,180 iteration 4309 : loss : 0.048433, loss_ce: 0.012927
2021-12-13 00:48:45,693 iteration 4310 : loss : 0.052343, loss_ce: 0.017670
2021-12-13 00:48:47,192 iteration 4311 : loss : 0.051888, loss_ce: 0.016402
2021-12-13 00:48:48,795 iteration 4312 : loss : 0.056897, loss_ce: 0.017267
2021-12-13 00:48:50,287 iteration 4313 : loss : 0.055539, loss_ce: 0.017793
2021-12-13 00:48:51,770 iteration 4314 : loss : 0.051036, loss_ce: 0.014091
2021-12-13 00:48:53,220 iteration 4315 : loss : 0.050248, loss_ce: 0.013695
2021-12-13 00:48:54,766 iteration 4316 : loss : 0.054250, loss_ce: 0.015705
2021-12-13 00:48:56,230 iteration 4317 : loss : 0.049147, loss_ce: 0.013197
2021-12-13 00:48:57,691 iteration 4318 : loss : 0.050433, loss_ce: 0.018847
 64%|█████████████████▏         | 254/400 [1:58:54<1:04:12, 26.38s/it]2021-12-13 00:48:59,256 iteration 4319 : loss : 0.054259, loss_ce: 0.013577
2021-12-13 00:49:00,811 iteration 4320 : loss : 0.057068, loss_ce: 0.018961
2021-12-13 00:49:02,377 iteration 4321 : loss : 0.055962, loss_ce: 0.020041
2021-12-13 00:49:03,897 iteration 4322 : loss : 0.052435, loss_ce: 0.014156
2021-12-13 00:49:05,354 iteration 4323 : loss : 0.043137, loss_ce: 0.011273
2021-12-13 00:49:06,872 iteration 4324 : loss : 0.052917, loss_ce: 0.015939
2021-12-13 00:49:08,489 iteration 4325 : loss : 0.064089, loss_ce: 0.012824
2021-12-13 00:49:09,994 iteration 4326 : loss : 0.051776, loss_ce: 0.022881
2021-12-13 00:49:11,456 iteration 4327 : loss : 0.049211, loss_ce: 0.016357
2021-12-13 00:49:12,933 iteration 4328 : loss : 0.051542, loss_ce: 0.015979
2021-12-13 00:49:14,465 iteration 4329 : loss : 0.051423, loss_ce: 0.016265
2021-12-13 00:49:15,914 iteration 4330 : loss : 0.042994, loss_ce: 0.012546
2021-12-13 00:49:17,551 iteration 4331 : loss : 0.058152, loss_ce: 0.014758
2021-12-13 00:49:19,099 iteration 4332 : loss : 0.051476, loss_ce: 0.016416
2021-12-13 00:49:20,521 iteration 4333 : loss : 0.044365, loss_ce: 0.010395
2021-12-13 00:49:21,959 iteration 4334 : loss : 0.048786, loss_ce: 0.016932
2021-12-13 00:49:21,959 Training Data Eval:
2021-12-13 00:49:29,595   Average segmentation loss on training set: 0.0387
2021-12-13 00:49:29,595 Validation Data Eval:
2021-12-13 00:49:32,194   Average segmentation loss on validation set: 0.0899
2021-12-13 00:49:33,759 iteration 4335 : loss : 0.053484, loss_ce: 0.022872
 64%|█████████████████▏         | 255/400 [1:59:31<1:10:47, 29.29s/it]2021-12-13 00:49:35,285 iteration 4336 : loss : 0.048295, loss_ce: 0.015768
2021-12-13 00:49:36,809 iteration 4337 : loss : 0.055359, loss_ce: 0.017649
2021-12-13 00:49:38,402 iteration 4338 : loss : 0.057921, loss_ce: 0.017502
2021-12-13 00:49:39,861 iteration 4339 : loss : 0.056035, loss_ce: 0.014439
2021-12-13 00:49:41,371 iteration 4340 : loss : 0.049652, loss_ce: 0.014639
2021-12-13 00:49:42,893 iteration 4341 : loss : 0.058146, loss_ce: 0.020193
2021-12-13 00:49:44,374 iteration 4342 : loss : 0.062268, loss_ce: 0.017186
2021-12-13 00:49:45,874 iteration 4343 : loss : 0.042460, loss_ce: 0.009640
2021-12-13 00:49:47,328 iteration 4344 : loss : 0.045546, loss_ce: 0.013608
2021-12-13 00:49:48,812 iteration 4345 : loss : 0.051484, loss_ce: 0.013714
2021-12-13 00:49:50,262 iteration 4346 : loss : 0.044795, loss_ce: 0.013953
2021-12-13 00:49:51,799 iteration 4347 : loss : 0.057793, loss_ce: 0.015618
2021-12-13 00:49:53,298 iteration 4348 : loss : 0.055063, loss_ce: 0.023304
2021-12-13 00:49:54,798 iteration 4349 : loss : 0.052663, loss_ce: 0.011741
2021-12-13 00:49:56,258 iteration 4350 : loss : 0.046968, loss_ce: 0.013476
2021-12-13 00:49:57,676 iteration 4351 : loss : 0.044323, loss_ce: 0.014079
2021-12-13 00:49:59,118 iteration 4352 : loss : 0.044858, loss_ce: 0.014172
 64%|█████████████████▎         | 256/400 [1:59:56<1:07:28, 28.11s/it]2021-12-13 00:50:00,670 iteration 4353 : loss : 0.047654, loss_ce: 0.012014
2021-12-13 00:50:02,246 iteration 4354 : loss : 0.055586, loss_ce: 0.013164
2021-12-13 00:50:03,681 iteration 4355 : loss : 0.044962, loss_ce: 0.012392
2021-12-13 00:50:05,206 iteration 4356 : loss : 0.052164, loss_ce: 0.020611
2021-12-13 00:50:06,703 iteration 4357 : loss : 0.054873, loss_ce: 0.017806
2021-12-13 00:50:08,159 iteration 4358 : loss : 0.048415, loss_ce: 0.014802
2021-12-13 00:50:09,627 iteration 4359 : loss : 0.043951, loss_ce: 0.013272
2021-12-13 00:50:11,093 iteration 4360 : loss : 0.063575, loss_ce: 0.020238
2021-12-13 00:50:12,637 iteration 4361 : loss : 0.058002, loss_ce: 0.019709
2021-12-13 00:50:14,100 iteration 4362 : loss : 0.044139, loss_ce: 0.011664
2021-12-13 00:50:15,508 iteration 4363 : loss : 0.052464, loss_ce: 0.019801
2021-12-13 00:50:17,047 iteration 4364 : loss : 0.050795, loss_ce: 0.013667
2021-12-13 00:50:18,527 iteration 4365 : loss : 0.058788, loss_ce: 0.019716
2021-12-13 00:50:19,897 iteration 4366 : loss : 0.043288, loss_ce: 0.014351
2021-12-13 00:50:21,370 iteration 4367 : loss : 0.056154, loss_ce: 0.016422
2021-12-13 00:50:22,926 iteration 4368 : loss : 0.053841, loss_ce: 0.017000
2021-12-13 00:50:24,352 iteration 4369 : loss : 0.058101, loss_ce: 0.013740
 64%|█████████████████▎         | 257/400 [2:00:21<1:04:56, 27.25s/it]2021-12-13 00:50:25,899 iteration 4370 : loss : 0.059333, loss_ce: 0.012336
2021-12-13 00:50:27,395 iteration 4371 : loss : 0.054705, loss_ce: 0.021251
2021-12-13 00:50:28,802 iteration 4372 : loss : 0.044104, loss_ce: 0.014668
2021-12-13 00:50:30,327 iteration 4373 : loss : 0.042855, loss_ce: 0.012041
2021-12-13 00:50:31,797 iteration 4374 : loss : 0.046055, loss_ce: 0.014859
2021-12-13 00:50:33,213 iteration 4375 : loss : 0.046230, loss_ce: 0.014495
2021-12-13 00:50:34,670 iteration 4376 : loss : 0.048304, loss_ce: 0.013337
2021-12-13 00:50:36,251 iteration 4377 : loss : 0.054374, loss_ce: 0.016966
2021-12-13 00:50:37,781 iteration 4378 : loss : 0.051505, loss_ce: 0.015563
2021-12-13 00:50:39,379 iteration 4379 : loss : 0.063916, loss_ce: 0.028019
2021-12-13 00:50:40,901 iteration 4380 : loss : 0.043612, loss_ce: 0.012449
2021-12-13 00:50:42,437 iteration 4381 : loss : 0.064444, loss_ce: 0.022566
2021-12-13 00:50:43,914 iteration 4382 : loss : 0.056933, loss_ce: 0.016937
2021-12-13 00:50:45,397 iteration 4383 : loss : 0.048703, loss_ce: 0.011253
2021-12-13 00:50:46,893 iteration 4384 : loss : 0.052270, loss_ce: 0.015310
2021-12-13 00:50:48,285 iteration 4385 : loss : 0.048978, loss_ce: 0.017497
2021-12-13 00:50:49,780 iteration 4386 : loss : 0.053380, loss_ce: 0.018423
 64%|█████████████████▍         | 258/400 [2:00:47<1:03:11, 26.70s/it]2021-12-13 00:50:51,295 iteration 4387 : loss : 0.041820, loss_ce: 0.011331
2021-12-13 00:50:52,814 iteration 4388 : loss : 0.048332, loss_ce: 0.016063
2021-12-13 00:50:54,237 iteration 4389 : loss : 0.041820, loss_ce: 0.014376
2021-12-13 00:50:55,652 iteration 4390 : loss : 0.048651, loss_ce: 0.014626
2021-12-13 00:50:57,124 iteration 4391 : loss : 0.051269, loss_ce: 0.017236
2021-12-13 00:50:58,644 iteration 4392 : loss : 0.055497, loss_ce: 0.018963
2021-12-13 00:51:00,041 iteration 4393 : loss : 0.046927, loss_ce: 0.013338
2021-12-13 00:51:01,600 iteration 4394 : loss : 0.054590, loss_ce: 0.018386
2021-12-13 00:51:03,135 iteration 4395 : loss : 0.053029, loss_ce: 0.014960
2021-12-13 00:51:04,700 iteration 4396 : loss : 0.047322, loss_ce: 0.014454
2021-12-13 00:51:06,180 iteration 4397 : loss : 0.051131, loss_ce: 0.013781
2021-12-13 00:51:07,633 iteration 4398 : loss : 0.057581, loss_ce: 0.014431
2021-12-13 00:51:09,108 iteration 4399 : loss : 0.042800, loss_ce: 0.011810
2021-12-13 00:51:10,548 iteration 4400 : loss : 0.045138, loss_ce: 0.015489
2021-12-13 00:51:11,999 iteration 4401 : loss : 0.049999, loss_ce: 0.018219
2021-12-13 00:51:13,416 iteration 4402 : loss : 0.047661, loss_ce: 0.014055
2021-12-13 00:51:14,894 iteration 4403 : loss : 0.051631, loss_ce: 0.013396
 65%|█████████████████▍         | 259/400 [2:01:12<1:01:37, 26.22s/it]2021-12-13 00:51:16,360 iteration 4404 : loss : 0.043596, loss_ce: 0.016080
2021-12-13 00:51:17,876 iteration 4405 : loss : 0.046178, loss_ce: 0.011448
2021-12-13 00:51:19,379 iteration 4406 : loss : 0.051316, loss_ce: 0.013735
2021-12-13 00:51:20,805 iteration 4407 : loss : 0.051024, loss_ce: 0.015425
2021-12-13 00:51:22,195 iteration 4408 : loss : 0.049555, loss_ce: 0.010384
2021-12-13 00:51:23,682 iteration 4409 : loss : 0.053254, loss_ce: 0.017127
2021-12-13 00:51:25,171 iteration 4410 : loss : 0.052715, loss_ce: 0.014603
2021-12-13 00:51:26,721 iteration 4411 : loss : 0.047358, loss_ce: 0.017457
2021-12-13 00:51:28,147 iteration 4412 : loss : 0.054403, loss_ce: 0.017535
2021-12-13 00:51:29,702 iteration 4413 : loss : 0.054977, loss_ce: 0.013806
2021-12-13 00:51:31,090 iteration 4414 : loss : 0.046432, loss_ce: 0.013510
2021-12-13 00:51:32,583 iteration 4415 : loss : 0.051019, loss_ce: 0.016911
2021-12-13 00:51:34,026 iteration 4416 : loss : 0.050710, loss_ce: 0.010640
2021-12-13 00:51:35,504 iteration 4417 : loss : 0.049216, loss_ce: 0.013152
2021-12-13 00:51:37,005 iteration 4418 : loss : 0.054729, loss_ce: 0.020014
2021-12-13 00:51:38,514 iteration 4419 : loss : 0.052203, loss_ce: 0.014321
2021-12-13 00:51:38,514 Training Data Eval:
2021-12-13 00:51:46,161   Average segmentation loss on training set: 0.0387
2021-12-13 00:51:46,161 Validation Data Eval:
2021-12-13 00:51:48,761   Average segmentation loss on validation set: 0.0894
2021-12-13 00:51:50,185 iteration 4420 : loss : 0.044409, loss_ce: 0.015466
 65%|█████████████████▌         | 260/400 [2:01:47<1:07:32, 28.94s/it]2021-12-13 00:51:51,731 iteration 4421 : loss : 0.051599, loss_ce: 0.014043
2021-12-13 00:51:53,269 iteration 4422 : loss : 0.055903, loss_ce: 0.016443
2021-12-13 00:51:54,871 iteration 4423 : loss : 0.053647, loss_ce: 0.018380
2021-12-13 00:51:56,431 iteration 4424 : loss : 0.067349, loss_ce: 0.022385
2021-12-13 00:51:57,854 iteration 4425 : loss : 0.046092, loss_ce: 0.013274
2021-12-13 00:51:59,390 iteration 4426 : loss : 0.048668, loss_ce: 0.013550
2021-12-13 00:52:00,919 iteration 4427 : loss : 0.060860, loss_ce: 0.012556
2021-12-13 00:52:02,465 iteration 4428 : loss : 0.058238, loss_ce: 0.020073
2021-12-13 00:52:04,023 iteration 4429 : loss : 0.053580, loss_ce: 0.016160
2021-12-13 00:52:05,496 iteration 4430 : loss : 0.050516, loss_ce: 0.014209
2021-12-13 00:52:06,960 iteration 4431 : loss : 0.050094, loss_ce: 0.014430
2021-12-13 00:52:08,427 iteration 4432 : loss : 0.043039, loss_ce: 0.013991
2021-12-13 00:52:09,899 iteration 4433 : loss : 0.052558, loss_ce: 0.016802
2021-12-13 00:52:11,367 iteration 4434 : loss : 0.047415, loss_ce: 0.013154
2021-12-13 00:52:12,891 iteration 4435 : loss : 0.068038, loss_ce: 0.020173
2021-12-13 00:52:14,507 iteration 4436 : loss : 0.069593, loss_ce: 0.026162
2021-12-13 00:52:15,938 iteration 4437 : loss : 0.046828, loss_ce: 0.013983
 65%|█████████████████▌         | 261/400 [2:02:13<1:04:50, 27.99s/it]2021-12-13 00:52:17,528 iteration 4438 : loss : 0.050267, loss_ce: 0.012364
2021-12-13 00:52:18,978 iteration 4439 : loss : 0.050512, loss_ce: 0.013998
2021-12-13 00:52:20,510 iteration 4440 : loss : 0.055025, loss_ce: 0.016645
2021-12-13 00:52:22,041 iteration 4441 : loss : 0.053510, loss_ce: 0.020391
2021-12-13 00:52:23,479 iteration 4442 : loss : 0.044107, loss_ce: 0.015402
2021-12-13 00:52:24,975 iteration 4443 : loss : 0.051468, loss_ce: 0.016890
2021-12-13 00:52:26,421 iteration 4444 : loss : 0.045943, loss_ce: 0.014211
2021-12-13 00:52:27,913 iteration 4445 : loss : 0.052183, loss_ce: 0.014924
2021-12-13 00:52:29,431 iteration 4446 : loss : 0.050950, loss_ce: 0.015998
2021-12-13 00:52:30,945 iteration 4447 : loss : 0.053740, loss_ce: 0.015757
2021-12-13 00:52:32,419 iteration 4448 : loss : 0.050513, loss_ce: 0.011099
2021-12-13 00:52:33,916 iteration 4449 : loss : 0.052265, loss_ce: 0.020309
2021-12-13 00:52:35,458 iteration 4450 : loss : 0.045668, loss_ce: 0.011963
2021-12-13 00:52:36,869 iteration 4451 : loss : 0.048899, loss_ce: 0.013380
2021-12-13 00:52:38,358 iteration 4452 : loss : 0.049945, loss_ce: 0.015740
2021-12-13 00:52:39,942 iteration 4453 : loss : 0.051299, loss_ce: 0.015664
2021-12-13 00:52:41,458 iteration 4454 : loss : 0.053712, loss_ce: 0.019814
 66%|█████████████████▋         | 262/400 [2:02:38<1:02:40, 27.25s/it]2021-12-13 00:52:43,067 iteration 4455 : loss : 0.052793, loss_ce: 0.014286
2021-12-13 00:52:44,563 iteration 4456 : loss : 0.065545, loss_ce: 0.026799
2021-12-13 00:52:45,961 iteration 4457 : loss : 0.044035, loss_ce: 0.017208
2021-12-13 00:52:47,475 iteration 4458 : loss : 0.053015, loss_ce: 0.020090
2021-12-13 00:52:48,896 iteration 4459 : loss : 0.051180, loss_ce: 0.015290
2021-12-13 00:52:50,380 iteration 4460 : loss : 0.045361, loss_ce: 0.012371
2021-12-13 00:52:51,804 iteration 4461 : loss : 0.048734, loss_ce: 0.017806
2021-12-13 00:52:53,242 iteration 4462 : loss : 0.045861, loss_ce: 0.013890
2021-12-13 00:52:54,670 iteration 4463 : loss : 0.047444, loss_ce: 0.013766
2021-12-13 00:52:56,091 iteration 4464 : loss : 0.043992, loss_ce: 0.012557
2021-12-13 00:52:57,604 iteration 4465 : loss : 0.056296, loss_ce: 0.020744
2021-12-13 00:52:59,121 iteration 4466 : loss : 0.057756, loss_ce: 0.013981
2021-12-13 00:53:00,606 iteration 4467 : loss : 0.045270, loss_ce: 0.013756
2021-12-13 00:53:02,074 iteration 4468 : loss : 0.046763, loss_ce: 0.013065
2021-12-13 00:53:03,493 iteration 4469 : loss : 0.050247, loss_ce: 0.009885
2021-12-13 00:53:05,006 iteration 4470 : loss : 0.057719, loss_ce: 0.014796
2021-12-13 00:53:06,537 iteration 4471 : loss : 0.053549, loss_ce: 0.012890
 66%|█████████████████▊         | 263/400 [2:03:03<1:00:43, 26.60s/it]2021-12-13 00:53:08,025 iteration 4472 : loss : 0.054565, loss_ce: 0.018473
2021-12-13 00:53:09,488 iteration 4473 : loss : 0.045843, loss_ce: 0.014830
2021-12-13 00:53:10,995 iteration 4474 : loss : 0.046469, loss_ce: 0.012592
2021-12-13 00:53:12,439 iteration 4475 : loss : 0.048138, loss_ce: 0.016141
2021-12-13 00:53:13,820 iteration 4476 : loss : 0.047026, loss_ce: 0.014292
2021-12-13 00:53:15,300 iteration 4477 : loss : 0.047573, loss_ce: 0.013497
2021-12-13 00:53:16,793 iteration 4478 : loss : 0.053659, loss_ce: 0.014209
2021-12-13 00:53:18,280 iteration 4479 : loss : 0.046951, loss_ce: 0.010168
2021-12-13 00:53:19,707 iteration 4480 : loss : 0.043075, loss_ce: 0.013732
2021-12-13 00:53:21,130 iteration 4481 : loss : 0.043082, loss_ce: 0.014612
2021-12-13 00:53:22,674 iteration 4482 : loss : 0.049207, loss_ce: 0.012633
2021-12-13 00:53:24,258 iteration 4483 : loss : 0.059576, loss_ce: 0.020847
2021-12-13 00:53:25,805 iteration 4484 : loss : 0.052434, loss_ce: 0.014088
2021-12-13 00:53:27,257 iteration 4485 : loss : 0.041214, loss_ce: 0.010284
2021-12-13 00:53:28,860 iteration 4486 : loss : 0.058690, loss_ce: 0.020835
2021-12-13 00:53:30,306 iteration 4487 : loss : 0.049546, loss_ce: 0.015999
2021-12-13 00:53:31,774 iteration 4488 : loss : 0.046566, loss_ce: 0.015913
 66%|███████████████████▏         | 264/400 [2:03:29<59:22, 26.19s/it]2021-12-13 00:53:33,337 iteration 4489 : loss : 0.064076, loss_ce: 0.020214
2021-12-13 00:53:34,830 iteration 4490 : loss : 0.043198, loss_ce: 0.012595
2021-12-13 00:53:36,395 iteration 4491 : loss : 0.055789, loss_ce: 0.017236
2021-12-13 00:53:37,909 iteration 4492 : loss : 0.048123, loss_ce: 0.014839
2021-12-13 00:53:39,469 iteration 4493 : loss : 0.063912, loss_ce: 0.018236
2021-12-13 00:53:40,861 iteration 4494 : loss : 0.048175, loss_ce: 0.014442
2021-12-13 00:53:42,385 iteration 4495 : loss : 0.050464, loss_ce: 0.018335
2021-12-13 00:53:43,908 iteration 4496 : loss : 0.050268, loss_ce: 0.011760
2021-12-13 00:53:45,335 iteration 4497 : loss : 0.045584, loss_ce: 0.011611
2021-12-13 00:53:46,905 iteration 4498 : loss : 0.053033, loss_ce: 0.018248
2021-12-13 00:53:48,534 iteration 4499 : loss : 0.061239, loss_ce: 0.022453
2021-12-13 00:53:50,009 iteration 4500 : loss : 0.049568, loss_ce: 0.015980
2021-12-13 00:53:51,441 iteration 4501 : loss : 0.050582, loss_ce: 0.014715
2021-12-13 00:53:52,871 iteration 4502 : loss : 0.048706, loss_ce: 0.014619
2021-12-13 00:53:54,419 iteration 4503 : loss : 0.056169, loss_ce: 0.020924
2021-12-13 00:53:55,902 iteration 4504 : loss : 0.060018, loss_ce: 0.015058
2021-12-13 00:53:55,903 Training Data Eval:
2021-12-13 00:54:03,533   Average segmentation loss on training set: 0.0375
2021-12-13 00:54:03,534 Validation Data Eval:
2021-12-13 00:54:06,134   Average segmentation loss on validation set: 0.0875
2021-12-13 00:54:12,467 Found new lowest validation loss at iteration 4504! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/TU_3seeds/TU_SGD_best_val_loss_seed100.pth
2021-12-13 00:54:13,762 iteration 4505 : loss : 0.044255, loss_ce: 0.014840
 66%|█████████████████▉         | 265/400 [2:04:11<1:09:35, 30.93s/it]2021-12-13 00:54:15,224 iteration 4506 : loss : 0.057688, loss_ce: 0.014516
2021-12-13 00:54:16,554 iteration 4507 : loss : 0.048682, loss_ce: 0.013718
2021-12-13 00:54:17,980 iteration 4508 : loss : 0.050045, loss_ce: 0.012872
2021-12-13 00:54:19,464 iteration 4509 : loss : 0.048374, loss_ce: 0.012904
2021-12-13 00:54:20,956 iteration 4510 : loss : 0.045486, loss_ce: 0.012797
2021-12-13 00:54:22,404 iteration 4511 : loss : 0.047227, loss_ce: 0.014235
2021-12-13 00:54:23,877 iteration 4512 : loss : 0.041176, loss_ce: 0.012354
2021-12-13 00:54:25,381 iteration 4513 : loss : 0.044816, loss_ce: 0.017387
2021-12-13 00:54:26,796 iteration 4514 : loss : 0.045606, loss_ce: 0.014388
2021-12-13 00:54:28,262 iteration 4515 : loss : 0.055201, loss_ce: 0.016834
2021-12-13 00:54:29,868 iteration 4516 : loss : 0.049542, loss_ce: 0.015443
2021-12-13 00:54:31,349 iteration 4517 : loss : 0.049474, loss_ce: 0.013999
2021-12-13 00:54:32,867 iteration 4518 : loss : 0.050836, loss_ce: 0.013714
2021-12-13 00:54:34,363 iteration 4519 : loss : 0.051121, loss_ce: 0.019061
2021-12-13 00:54:35,801 iteration 4520 : loss : 0.042784, loss_ce: 0.011026
2021-12-13 00:54:37,308 iteration 4521 : loss : 0.061572, loss_ce: 0.016385
2021-12-13 00:54:38,735 iteration 4522 : loss : 0.043513, loss_ce: 0.016093
 66%|█████████████████▉         | 266/400 [2:04:36<1:05:05, 29.14s/it]2021-12-13 00:54:40,331 iteration 4523 : loss : 0.045161, loss_ce: 0.013728
2021-12-13 00:54:41,741 iteration 4524 : loss : 0.044901, loss_ce: 0.013408
2021-12-13 00:54:43,294 iteration 4525 : loss : 0.054440, loss_ce: 0.011311
2021-12-13 00:54:44,800 iteration 4526 : loss : 0.044831, loss_ce: 0.014167
2021-12-13 00:54:46,294 iteration 4527 : loss : 0.047979, loss_ce: 0.015980
2021-12-13 00:54:47,838 iteration 4528 : loss : 0.050828, loss_ce: 0.016064
2021-12-13 00:54:49,312 iteration 4529 : loss : 0.059614, loss_ce: 0.017283
2021-12-13 00:54:50,826 iteration 4530 : loss : 0.050856, loss_ce: 0.017479
2021-12-13 00:54:52,263 iteration 4531 : loss : 0.048689, loss_ce: 0.016847
2021-12-13 00:54:53,795 iteration 4532 : loss : 0.051265, loss_ce: 0.016893
2021-12-13 00:54:55,364 iteration 4533 : loss : 0.053909, loss_ce: 0.018301
2021-12-13 00:54:56,994 iteration 4534 : loss : 0.052557, loss_ce: 0.015505
2021-12-13 00:54:58,407 iteration 4535 : loss : 0.045866, loss_ce: 0.011774
2021-12-13 00:54:59,926 iteration 4536 : loss : 0.046623, loss_ce: 0.015820
2021-12-13 00:55:01,505 iteration 4537 : loss : 0.054403, loss_ce: 0.014638
2021-12-13 00:55:02,982 iteration 4538 : loss : 0.052452, loss_ce: 0.015185
2021-12-13 00:55:04,507 iteration 4539 : loss : 0.062604, loss_ce: 0.018776
 67%|██████████████████         | 267/400 [2:05:01<1:02:21, 28.13s/it]2021-12-13 00:55:06,091 iteration 4540 : loss : 0.050898, loss_ce: 0.011770
2021-12-13 00:55:07,507 iteration 4541 : loss : 0.047162, loss_ce: 0.014104
2021-12-13 00:55:08,996 iteration 4542 : loss : 0.052706, loss_ce: 0.009555
2021-12-13 00:55:10,386 iteration 4543 : loss : 0.048132, loss_ce: 0.015190
2021-12-13 00:55:11,904 iteration 4544 : loss : 0.050314, loss_ce: 0.017851
2021-12-13 00:55:13,465 iteration 4545 : loss : 0.065024, loss_ce: 0.025817
2021-12-13 00:55:14,982 iteration 4546 : loss : 0.058258, loss_ce: 0.017744
2021-12-13 00:55:16,489 iteration 4547 : loss : 0.045828, loss_ce: 0.015749
2021-12-13 00:55:17,975 iteration 4548 : loss : 0.049242, loss_ce: 0.014276
2021-12-13 00:55:19,547 iteration 4549 : loss : 0.050449, loss_ce: 0.017553
2021-12-13 00:55:21,106 iteration 4550 : loss : 0.060254, loss_ce: 0.015336
2021-12-13 00:55:22,656 iteration 4551 : loss : 0.059821, loss_ce: 0.012640
2021-12-13 00:55:24,057 iteration 4552 : loss : 0.043224, loss_ce: 0.013871
2021-12-13 00:55:25,488 iteration 4553 : loss : 0.055997, loss_ce: 0.017704
2021-12-13 00:55:26,950 iteration 4554 : loss : 0.045872, loss_ce: 0.015128
2021-12-13 00:55:28,487 iteration 4555 : loss : 0.056683, loss_ce: 0.018011
2021-12-13 00:55:29,967 iteration 4556 : loss : 0.051553, loss_ce: 0.019455
 67%|██████████████████         | 268/400 [2:05:27<1:00:07, 27.33s/it]2021-12-13 00:55:31,533 iteration 4557 : loss : 0.046335, loss_ce: 0.016540
2021-12-13 00:55:33,102 iteration 4558 : loss : 0.057493, loss_ce: 0.021643
2021-12-13 00:55:34,578 iteration 4559 : loss : 0.062313, loss_ce: 0.016143
2021-12-13 00:55:36,036 iteration 4560 : loss : 0.050270, loss_ce: 0.012233
2021-12-13 00:55:37,521 iteration 4561 : loss : 0.045503, loss_ce: 0.009440
2021-12-13 00:55:39,004 iteration 4562 : loss : 0.048267, loss_ce: 0.013719
2021-12-13 00:55:40,444 iteration 4563 : loss : 0.042517, loss_ce: 0.011607
2021-12-13 00:55:41,930 iteration 4564 : loss : 0.056913, loss_ce: 0.015385
2021-12-13 00:55:43,375 iteration 4565 : loss : 0.048355, loss_ce: 0.016349
2021-12-13 00:55:44,791 iteration 4566 : loss : 0.045679, loss_ce: 0.016228
2021-12-13 00:55:46,287 iteration 4567 : loss : 0.051803, loss_ce: 0.017021
2021-12-13 00:55:47,760 iteration 4568 : loss : 0.046505, loss_ce: 0.011990
2021-12-13 00:55:49,250 iteration 4569 : loss : 0.049480, loss_ce: 0.015643
2021-12-13 00:55:50,686 iteration 4570 : loss : 0.051175, loss_ce: 0.018664
2021-12-13 00:55:52,133 iteration 4571 : loss : 0.044679, loss_ce: 0.016361
2021-12-13 00:55:53,594 iteration 4572 : loss : 0.047312, loss_ce: 0.011994
2021-12-13 00:55:55,079 iteration 4573 : loss : 0.050300, loss_ce: 0.016669
 67%|███████████████████▌         | 269/400 [2:05:52<58:13, 26.66s/it]2021-12-13 00:55:56,593 iteration 4574 : loss : 0.043652, loss_ce: 0.010866
2021-12-13 00:55:58,033 iteration 4575 : loss : 0.045070, loss_ce: 0.012167
2021-12-13 00:55:59,549 iteration 4576 : loss : 0.046972, loss_ce: 0.018062
2021-12-13 00:56:01,096 iteration 4577 : loss : 0.055142, loss_ce: 0.016484
2021-12-13 00:56:02,592 iteration 4578 : loss : 0.063791, loss_ce: 0.022373
2021-12-13 00:56:04,065 iteration 4579 : loss : 0.052319, loss_ce: 0.018578
2021-12-13 00:56:05,531 iteration 4580 : loss : 0.045840, loss_ce: 0.016109
2021-12-13 00:56:07,118 iteration 4581 : loss : 0.046438, loss_ce: 0.012158
2021-12-13 00:56:08,615 iteration 4582 : loss : 0.047395, loss_ce: 0.016387
2021-12-13 00:56:10,111 iteration 4583 : loss : 0.045250, loss_ce: 0.013290
2021-12-13 00:56:11,564 iteration 4584 : loss : 0.052653, loss_ce: 0.017841
2021-12-13 00:56:13,072 iteration 4585 : loss : 0.051457, loss_ce: 0.013111
2021-12-13 00:56:14,526 iteration 4586 : loss : 0.045435, loss_ce: 0.014480
2021-12-13 00:56:16,107 iteration 4587 : loss : 0.058133, loss_ce: 0.016511
2021-12-13 00:56:17,564 iteration 4588 : loss : 0.046381, loss_ce: 0.008897
2021-12-13 00:56:19,043 iteration 4589 : loss : 0.048839, loss_ce: 0.017081
2021-12-13 00:56:19,043 Training Data Eval:
2021-12-13 00:56:26,686   Average segmentation loss on training set: 0.0370
2021-12-13 00:56:26,687 Validation Data Eval:
2021-12-13 00:56:29,292   Average segmentation loss on validation set: 0.0890
2021-12-13 00:56:30,906 iteration 4590 : loss : 0.055720, loss_ce: 0.015616
 68%|██████████████████▏        | 270/400 [2:06:28<1:03:43, 29.41s/it]2021-12-13 00:56:32,354 iteration 4591 : loss : 0.041900, loss_ce: 0.010353
2021-12-13 00:56:33,848 iteration 4592 : loss : 0.052965, loss_ce: 0.012809
2021-12-13 00:56:35,251 iteration 4593 : loss : 0.047670, loss_ce: 0.010865
2021-12-13 00:56:36,709 iteration 4594 : loss : 0.047445, loss_ce: 0.016969
2021-12-13 00:56:38,212 iteration 4595 : loss : 0.050935, loss_ce: 0.009823
2021-12-13 00:56:39,783 iteration 4596 : loss : 0.049658, loss_ce: 0.014526
2021-12-13 00:56:41,272 iteration 4597 : loss : 0.047383, loss_ce: 0.016794
2021-12-13 00:56:42,671 iteration 4598 : loss : 0.048239, loss_ce: 0.016838
2021-12-13 00:56:44,115 iteration 4599 : loss : 0.044375, loss_ce: 0.009494
2021-12-13 00:56:45,663 iteration 4600 : loss : 0.048799, loss_ce: 0.020159
2021-12-13 00:56:47,262 iteration 4601 : loss : 0.056372, loss_ce: 0.025167
2021-12-13 00:56:48,655 iteration 4602 : loss : 0.042615, loss_ce: 0.013364
2021-12-13 00:56:50,068 iteration 4603 : loss : 0.046020, loss_ce: 0.016319
2021-12-13 00:56:51,576 iteration 4604 : loss : 0.051234, loss_ce: 0.013799
2021-12-13 00:56:53,085 iteration 4605 : loss : 0.053182, loss_ce: 0.011564
2021-12-13 00:56:54,636 iteration 4606 : loss : 0.054873, loss_ce: 0.013841
2021-12-13 00:56:56,053 iteration 4607 : loss : 0.048223, loss_ce: 0.020002
 68%|██████████████████▎        | 271/400 [2:06:53<1:00:29, 28.14s/it]2021-12-13 00:56:57,625 iteration 4608 : loss : 0.050950, loss_ce: 0.013303
2021-12-13 00:56:59,189 iteration 4609 : loss : 0.051422, loss_ce: 0.019074
2021-12-13 00:57:00,723 iteration 4610 : loss : 0.051061, loss_ce: 0.011887
2021-12-13 00:57:02,211 iteration 4611 : loss : 0.049716, loss_ce: 0.016221
2021-12-13 00:57:03,622 iteration 4612 : loss : 0.042465, loss_ce: 0.011581
2021-12-13 00:57:05,137 iteration 4613 : loss : 0.054439, loss_ce: 0.018758
2021-12-13 00:57:06,663 iteration 4614 : loss : 0.047670, loss_ce: 0.014193
2021-12-13 00:57:08,195 iteration 4615 : loss : 0.069276, loss_ce: 0.023677
2021-12-13 00:57:09,769 iteration 4616 : loss : 0.051747, loss_ce: 0.014244
2021-12-13 00:57:11,200 iteration 4617 : loss : 0.049298, loss_ce: 0.017701
2021-12-13 00:57:12,739 iteration 4618 : loss : 0.049139, loss_ce: 0.013637
2021-12-13 00:57:14,280 iteration 4619 : loss : 0.054652, loss_ce: 0.016081
2021-12-13 00:57:15,822 iteration 4620 : loss : 0.047348, loss_ce: 0.013540
2021-12-13 00:57:17,388 iteration 4621 : loss : 0.052755, loss_ce: 0.016364
2021-12-13 00:57:18,882 iteration 4622 : loss : 0.046871, loss_ce: 0.013375
2021-12-13 00:57:20,352 iteration 4623 : loss : 0.052681, loss_ce: 0.016386
2021-12-13 00:57:21,840 iteration 4624 : loss : 0.052897, loss_ce: 0.017764
 68%|███████████████████▋         | 272/400 [2:07:19<58:30, 27.43s/it]2021-12-13 00:57:23,367 iteration 4625 : loss : 0.048059, loss_ce: 0.017153
2021-12-13 00:57:24,824 iteration 4626 : loss : 0.043209, loss_ce: 0.011389
2021-12-13 00:57:26,313 iteration 4627 : loss : 0.048445, loss_ce: 0.013699
2021-12-13 00:57:27,761 iteration 4628 : loss : 0.045844, loss_ce: 0.013692
2021-12-13 00:57:29,201 iteration 4629 : loss : 0.046661, loss_ce: 0.015630
2021-12-13 00:57:30,667 iteration 4630 : loss : 0.048103, loss_ce: 0.012062
2021-12-13 00:57:32,231 iteration 4631 : loss : 0.051235, loss_ce: 0.014193
2021-12-13 00:57:33,733 iteration 4632 : loss : 0.040584, loss_ce: 0.010961
2021-12-13 00:57:35,230 iteration 4633 : loss : 0.051186, loss_ce: 0.015714
2021-12-13 00:57:36,622 iteration 4634 : loss : 0.040779, loss_ce: 0.014206
2021-12-13 00:57:38,139 iteration 4635 : loss : 0.052824, loss_ce: 0.014328
2021-12-13 00:57:39,625 iteration 4636 : loss : 0.055279, loss_ce: 0.015867
2021-12-13 00:57:41,104 iteration 4637 : loss : 0.044378, loss_ce: 0.012784
2021-12-13 00:57:42,632 iteration 4638 : loss : 0.060154, loss_ce: 0.021638
2021-12-13 00:57:44,209 iteration 4639 : loss : 0.051269, loss_ce: 0.013766
2021-12-13 00:57:45,738 iteration 4640 : loss : 0.052752, loss_ce: 0.016559
2021-12-13 00:57:47,194 iteration 4641 : loss : 0.046928, loss_ce: 0.015620
 68%|███████████████████▊         | 273/400 [2:07:44<56:44, 26.81s/it]2021-12-13 00:57:48,773 iteration 4642 : loss : 0.051365, loss_ce: 0.013307
2021-12-13 00:57:50,304 iteration 4643 : loss : 0.059454, loss_ce: 0.019274
2021-12-13 00:57:51,822 iteration 4644 : loss : 0.046908, loss_ce: 0.014871
2021-12-13 00:57:53,483 iteration 4645 : loss : 0.053306, loss_ce: 0.018038
2021-12-13 00:57:54,947 iteration 4646 : loss : 0.048258, loss_ce: 0.015697
2021-12-13 00:57:56,405 iteration 4647 : loss : 0.048930, loss_ce: 0.014897
2021-12-13 00:57:57,793 iteration 4648 : loss : 0.047307, loss_ce: 0.016412
2021-12-13 00:57:59,302 iteration 4649 : loss : 0.052119, loss_ce: 0.016622
2021-12-13 00:58:00,773 iteration 4650 : loss : 0.048480, loss_ce: 0.013215
2021-12-13 00:58:02,269 iteration 4651 : loss : 0.059603, loss_ce: 0.020857
2021-12-13 00:58:03,769 iteration 4652 : loss : 0.055090, loss_ce: 0.019839
2021-12-13 00:58:05,257 iteration 4653 : loss : 0.046580, loss_ce: 0.013210
2021-12-13 00:58:06,754 iteration 4654 : loss : 0.047248, loss_ce: 0.017494
2021-12-13 00:58:08,260 iteration 4655 : loss : 0.048998, loss_ce: 0.013702
2021-12-13 00:58:09,826 iteration 4656 : loss : 0.062272, loss_ce: 0.011321
2021-12-13 00:58:11,307 iteration 4657 : loss : 0.066748, loss_ce: 0.008722
2021-12-13 00:58:12,754 iteration 4658 : loss : 0.051815, loss_ce: 0.011018
 68%|███████████████████▊         | 274/400 [2:08:10<55:30, 26.43s/it]2021-12-13 00:58:14,278 iteration 4659 : loss : 0.049878, loss_ce: 0.014170
2021-12-13 00:58:15,809 iteration 4660 : loss : 0.050989, loss_ce: 0.010176
2021-12-13 00:58:17,307 iteration 4661 : loss : 0.058977, loss_ce: 0.024376
2021-12-13 00:58:18,923 iteration 4662 : loss : 0.056808, loss_ce: 0.023070
2021-12-13 00:58:20,359 iteration 4663 : loss : 0.043032, loss_ce: 0.010848
2021-12-13 00:58:21,863 iteration 4664 : loss : 0.057850, loss_ce: 0.014002
2021-12-13 00:58:23,518 iteration 4665 : loss : 0.066213, loss_ce: 0.018782
2021-12-13 00:58:24,956 iteration 4666 : loss : 0.045708, loss_ce: 0.015058
2021-12-13 00:58:26,433 iteration 4667 : loss : 0.057702, loss_ce: 0.020864
2021-12-13 00:58:27,909 iteration 4668 : loss : 0.055173, loss_ce: 0.011227
2021-12-13 00:58:29,474 iteration 4669 : loss : 0.045599, loss_ce: 0.014832
2021-12-13 00:58:31,041 iteration 4670 : loss : 0.048358, loss_ce: 0.016161
2021-12-13 00:58:32,466 iteration 4671 : loss : 0.046945, loss_ce: 0.011567
2021-12-13 00:58:34,093 iteration 4672 : loss : 0.058005, loss_ce: 0.020396
2021-12-13 00:58:35,628 iteration 4673 : loss : 0.052581, loss_ce: 0.015439
2021-12-13 00:58:37,204 iteration 4674 : loss : 0.052741, loss_ce: 0.023023
2021-12-13 00:58:37,204 Training Data Eval:
2021-12-13 00:58:44,845   Average segmentation loss on training set: 0.0410
2021-12-13 00:58:44,846 Validation Data Eval:
2021-12-13 00:58:47,448   Average segmentation loss on validation set: 0.0989
2021-12-13 00:58:48,900 iteration 4675 : loss : 0.049576, loss_ce: 0.015538
 69%|██████████████████▌        | 275/400 [2:08:46<1:01:08, 29.35s/it]2021-12-13 00:58:50,506 iteration 4676 : loss : 0.057462, loss_ce: 0.017624
2021-12-13 00:58:51,976 iteration 4677 : loss : 0.049635, loss_ce: 0.015928
2021-12-13 00:58:53,459 iteration 4678 : loss : 0.056842, loss_ce: 0.016387
2021-12-13 00:58:54,957 iteration 4679 : loss : 0.052144, loss_ce: 0.023987
2021-12-13 00:58:56,404 iteration 4680 : loss : 0.045889, loss_ce: 0.013406
2021-12-13 00:58:57,907 iteration 4681 : loss : 0.054172, loss_ce: 0.013775
2021-12-13 00:58:59,334 iteration 4682 : loss : 0.052453, loss_ce: 0.009803
2021-12-13 00:59:00,816 iteration 4683 : loss : 0.047307, loss_ce: 0.015771
2021-12-13 00:59:02,350 iteration 4684 : loss : 0.054881, loss_ce: 0.018581
2021-12-13 00:59:03,853 iteration 4685 : loss : 0.047720, loss_ce: 0.016195
2021-12-13 00:59:05,252 iteration 4686 : loss : 0.049627, loss_ce: 0.014588
2021-12-13 00:59:06,795 iteration 4687 : loss : 0.047159, loss_ce: 0.014345
2021-12-13 00:59:08,359 iteration 4688 : loss : 0.057814, loss_ce: 0.016225
2021-12-13 00:59:09,939 iteration 4689 : loss : 0.054819, loss_ce: 0.016110
2021-12-13 00:59:11,415 iteration 4690 : loss : 0.043551, loss_ce: 0.009368
2021-12-13 00:59:12,855 iteration 4691 : loss : 0.047600, loss_ce: 0.013201
2021-12-13 00:59:14,328 iteration 4692 : loss : 0.049557, loss_ce: 0.017359
 69%|████████████████████         | 276/400 [2:09:11<58:13, 28.17s/it]2021-12-13 00:59:15,887 iteration 4693 : loss : 0.052386, loss_ce: 0.016135
2021-12-13 00:59:17,342 iteration 4694 : loss : 0.044409, loss_ce: 0.011019
2021-12-13 00:59:18,896 iteration 4695 : loss : 0.064894, loss_ce: 0.023908
2021-12-13 00:59:20,411 iteration 4696 : loss : 0.054145, loss_ce: 0.017524
2021-12-13 00:59:21,966 iteration 4697 : loss : 0.053046, loss_ce: 0.019582
2021-12-13 00:59:23,355 iteration 4698 : loss : 0.046958, loss_ce: 0.015231
2021-12-13 00:59:24,831 iteration 4699 : loss : 0.055857, loss_ce: 0.015674
2021-12-13 00:59:26,332 iteration 4700 : loss : 0.058084, loss_ce: 0.014671
2021-12-13 00:59:27,835 iteration 4701 : loss : 0.050055, loss_ce: 0.017929
2021-12-13 00:59:29,236 iteration 4702 : loss : 0.047139, loss_ce: 0.014463
2021-12-13 00:59:30,759 iteration 4703 : loss : 0.052486, loss_ce: 0.013001
2021-12-13 00:59:32,260 iteration 4704 : loss : 0.074713, loss_ce: 0.014999
2021-12-13 00:59:33,690 iteration 4705 : loss : 0.041554, loss_ce: 0.011560
2021-12-13 00:59:35,139 iteration 4706 : loss : 0.048524, loss_ce: 0.016586
2021-12-13 00:59:36,700 iteration 4707 : loss : 0.049015, loss_ce: 0.016887
2021-12-13 00:59:38,181 iteration 4708 : loss : 0.069596, loss_ce: 0.014871
2021-12-13 00:59:39,648 iteration 4709 : loss : 0.044183, loss_ce: 0.013727
 69%|████████████████████         | 277/400 [2:09:36<55:59, 27.31s/it]2021-12-13 00:59:41,256 iteration 4710 : loss : 0.057107, loss_ce: 0.016877
2021-12-13 00:59:42,712 iteration 4711 : loss : 0.058553, loss_ce: 0.022383
2021-12-13 00:59:44,203 iteration 4712 : loss : 0.049407, loss_ce: 0.015568
2021-12-13 00:59:45,657 iteration 4713 : loss : 0.041597, loss_ce: 0.011682
2021-12-13 00:59:47,105 iteration 4714 : loss : 0.042241, loss_ce: 0.013818
2021-12-13 00:59:48,677 iteration 4715 : loss : 0.062803, loss_ce: 0.018582
2021-12-13 00:59:50,170 iteration 4716 : loss : 0.044895, loss_ce: 0.015507
2021-12-13 00:59:51,663 iteration 4717 : loss : 0.049750, loss_ce: 0.012306
2021-12-13 00:59:53,137 iteration 4718 : loss : 0.046243, loss_ce: 0.014229
2021-12-13 00:59:54,589 iteration 4719 : loss : 0.046415, loss_ce: 0.014125
2021-12-13 00:59:56,141 iteration 4720 : loss : 0.044984, loss_ce: 0.012506
2021-12-13 00:59:57,628 iteration 4721 : loss : 0.047489, loss_ce: 0.012542
2021-12-13 00:59:59,113 iteration 4722 : loss : 0.051887, loss_ce: 0.017814
2021-12-13 01:00:00,643 iteration 4723 : loss : 0.055387, loss_ce: 0.013008
2021-12-13 01:00:02,076 iteration 4724 : loss : 0.049277, loss_ce: 0.020923
2021-12-13 01:00:03,541 iteration 4725 : loss : 0.052960, loss_ce: 0.015843
2021-12-13 01:00:04,987 iteration 4726 : loss : 0.046256, loss_ce: 0.011957
 70%|████████████████████▏        | 278/400 [2:10:02<54:20, 26.72s/it]2021-12-13 01:00:06,512 iteration 4727 : loss : 0.050243, loss_ce: 0.015790
2021-12-13 01:00:08,028 iteration 4728 : loss : 0.061880, loss_ce: 0.022043
2021-12-13 01:00:09,552 iteration 4729 : loss : 0.043069, loss_ce: 0.011915
2021-12-13 01:00:11,052 iteration 4730 : loss : 0.054311, loss_ce: 0.017085
2021-12-13 01:00:12,541 iteration 4731 : loss : 0.060365, loss_ce: 0.013822
2021-12-13 01:00:13,958 iteration 4732 : loss : 0.046059, loss_ce: 0.015927
2021-12-13 01:00:15,550 iteration 4733 : loss : 0.067687, loss_ce: 0.026391
2021-12-13 01:00:17,048 iteration 4734 : loss : 0.055325, loss_ce: 0.017153
2021-12-13 01:00:18,470 iteration 4735 : loss : 0.043725, loss_ce: 0.013802
2021-12-13 01:00:20,012 iteration 4736 : loss : 0.055561, loss_ce: 0.018689
2021-12-13 01:00:21,525 iteration 4737 : loss : 0.048446, loss_ce: 0.017409
2021-12-13 01:00:23,155 iteration 4738 : loss : 0.071268, loss_ce: 0.019304
2021-12-13 01:00:24,688 iteration 4739 : loss : 0.043932, loss_ce: 0.012119
2021-12-13 01:00:26,200 iteration 4740 : loss : 0.043336, loss_ce: 0.009903
2021-12-13 01:00:27,692 iteration 4741 : loss : 0.054265, loss_ce: 0.021306
2021-12-13 01:00:29,211 iteration 4742 : loss : 0.053820, loss_ce: 0.018623
2021-12-13 01:00:30,886 iteration 4743 : loss : 0.055216, loss_ce: 0.019184
 70%|████████████████████▏        | 279/400 [2:10:28<53:23, 26.47s/it]2021-12-13 01:00:32,489 iteration 4744 : loss : 0.051990, loss_ce: 0.015351
2021-12-13 01:00:34,012 iteration 4745 : loss : 0.052879, loss_ce: 0.017760
2021-12-13 01:00:35,508 iteration 4746 : loss : 0.051056, loss_ce: 0.012698
2021-12-13 01:00:36,928 iteration 4747 : loss : 0.043604, loss_ce: 0.015000
2021-12-13 01:00:38,520 iteration 4748 : loss : 0.052716, loss_ce: 0.016897
2021-12-13 01:00:39,951 iteration 4749 : loss : 0.051992, loss_ce: 0.018557
2021-12-13 01:00:41,461 iteration 4750 : loss : 0.051082, loss_ce: 0.011608
2021-12-13 01:00:42,982 iteration 4751 : loss : 0.055454, loss_ce: 0.020906
2021-12-13 01:00:44,473 iteration 4752 : loss : 0.050032, loss_ce: 0.011798
2021-12-13 01:00:45,939 iteration 4753 : loss : 0.039578, loss_ce: 0.010642
2021-12-13 01:00:47,446 iteration 4754 : loss : 0.046725, loss_ce: 0.015676
2021-12-13 01:00:49,032 iteration 4755 : loss : 0.066894, loss_ce: 0.011356
2021-12-13 01:00:50,563 iteration 4756 : loss : 0.051436, loss_ce: 0.012759
2021-12-13 01:00:52,071 iteration 4757 : loss : 0.048485, loss_ce: 0.019669
2021-12-13 01:00:53,649 iteration 4758 : loss : 0.056688, loss_ce: 0.014594
2021-12-13 01:00:55,143 iteration 4759 : loss : 0.060901, loss_ce: 0.030346
2021-12-13 01:00:55,143 Training Data Eval:
2021-12-13 01:01:02,792   Average segmentation loss on training set: 0.0360
2021-12-13 01:01:02,793 Validation Data Eval:
2021-12-13 01:01:05,399   Average segmentation loss on validation set: 0.0933
2021-12-13 01:01:06,910 iteration 4760 : loss : 0.046646, loss_ce: 0.012093
 70%|████████████████████▎        | 280/400 [2:11:04<58:40, 29.34s/it]2021-12-13 01:01:08,383 iteration 4761 : loss : 0.045555, loss_ce: 0.012541
2021-12-13 01:01:09,970 iteration 4762 : loss : 0.054982, loss_ce: 0.015816
2021-12-13 01:01:11,489 iteration 4763 : loss : 0.049045, loss_ce: 0.016969
2021-12-13 01:01:12,918 iteration 4764 : loss : 0.041019, loss_ce: 0.012012
2021-12-13 01:01:14,386 iteration 4765 : loss : 0.047954, loss_ce: 0.017419
2021-12-13 01:01:15,834 iteration 4766 : loss : 0.046297, loss_ce: 0.011885
2021-12-13 01:01:17,372 iteration 4767 : loss : 0.062409, loss_ce: 0.019392
2021-12-13 01:01:18,969 iteration 4768 : loss : 0.059209, loss_ce: 0.020790
2021-12-13 01:01:20,550 iteration 4769 : loss : 0.053256, loss_ce: 0.011171
2021-12-13 01:01:22,054 iteration 4770 : loss : 0.041112, loss_ce: 0.014253
2021-12-13 01:01:23,512 iteration 4771 : loss : 0.046645, loss_ce: 0.016315
2021-12-13 01:01:24,991 iteration 4772 : loss : 0.040905, loss_ce: 0.010054
2021-12-13 01:01:26,451 iteration 4773 : loss : 0.047498, loss_ce: 0.014550
2021-12-13 01:01:27,901 iteration 4774 : loss : 0.046869, loss_ce: 0.015985
2021-12-13 01:01:29,406 iteration 4775 : loss : 0.059324, loss_ce: 0.020151
2021-12-13 01:01:30,891 iteration 4776 : loss : 0.055153, loss_ce: 0.016995
2021-12-13 01:01:32,364 iteration 4777 : loss : 0.046897, loss_ce: 0.013007
 70%|████████████████████▎        | 281/400 [2:11:29<55:52, 28.17s/it]2021-12-13 01:01:33,961 iteration 4778 : loss : 0.048970, loss_ce: 0.013593
2021-12-13 01:01:35,410 iteration 4779 : loss : 0.043768, loss_ce: 0.011955
2021-12-13 01:01:36,860 iteration 4780 : loss : 0.058368, loss_ce: 0.012184
2021-12-13 01:01:38,315 iteration 4781 : loss : 0.044402, loss_ce: 0.016107
2021-12-13 01:01:39,859 iteration 4782 : loss : 0.053910, loss_ce: 0.012967
2021-12-13 01:01:41,279 iteration 4783 : loss : 0.044493, loss_ce: 0.010232
2021-12-13 01:01:42,831 iteration 4784 : loss : 0.048594, loss_ce: 0.014143
2021-12-13 01:01:44,409 iteration 4785 : loss : 0.057705, loss_ce: 0.022968
2021-12-13 01:01:45,893 iteration 4786 : loss : 0.047506, loss_ce: 0.015696
2021-12-13 01:01:47,423 iteration 4787 : loss : 0.063967, loss_ce: 0.011645
2021-12-13 01:01:48,932 iteration 4788 : loss : 0.045170, loss_ce: 0.014125
2021-12-13 01:01:50,438 iteration 4789 : loss : 0.045258, loss_ce: 0.013692
2021-12-13 01:01:51,929 iteration 4790 : loss : 0.046899, loss_ce: 0.015642
2021-12-13 01:01:53,387 iteration 4791 : loss : 0.047795, loss_ce: 0.015443
2021-12-13 01:01:54,892 iteration 4792 : loss : 0.055915, loss_ce: 0.021622
2021-12-13 01:01:56,421 iteration 4793 : loss : 0.047873, loss_ce: 0.018051
2021-12-13 01:01:57,899 iteration 4794 : loss : 0.058774, loss_ce: 0.008991
 70%|████████████████████▍        | 282/400 [2:11:55<53:50, 27.38s/it]2021-12-13 01:01:59,367 iteration 4795 : loss : 0.044441, loss_ce: 0.011965
2021-12-13 01:02:00,792 iteration 4796 : loss : 0.043468, loss_ce: 0.013885
2021-12-13 01:02:02,233 iteration 4797 : loss : 0.046428, loss_ce: 0.014720
2021-12-13 01:02:03,790 iteration 4798 : loss : 0.054874, loss_ce: 0.019738
2021-12-13 01:02:05,283 iteration 4799 : loss : 0.059446, loss_ce: 0.013048
2021-12-13 01:02:06,790 iteration 4800 : loss : 0.054830, loss_ce: 0.020153
2021-12-13 01:02:08,292 iteration 4801 : loss : 0.042622, loss_ce: 0.012053
2021-12-13 01:02:09,688 iteration 4802 : loss : 0.041820, loss_ce: 0.011370
2021-12-13 01:02:11,183 iteration 4803 : loss : 0.052533, loss_ce: 0.018083
2021-12-13 01:02:12,642 iteration 4804 : loss : 0.049472, loss_ce: 0.011787
2021-12-13 01:02:14,158 iteration 4805 : loss : 0.048188, loss_ce: 0.014479
2021-12-13 01:02:15,607 iteration 4806 : loss : 0.052025, loss_ce: 0.013438
2021-12-13 01:02:17,191 iteration 4807 : loss : 0.053348, loss_ce: 0.017993
2021-12-13 01:02:18,907 iteration 4808 : loss : 0.058648, loss_ce: 0.019272
2021-12-13 01:02:20,308 iteration 4809 : loss : 0.046120, loss_ce: 0.016314
2021-12-13 01:02:21,849 iteration 4810 : loss : 0.048923, loss_ce: 0.015828
2021-12-13 01:02:23,320 iteration 4811 : loss : 0.044573, loss_ce: 0.013262
 71%|████████████████████▌        | 283/400 [2:12:20<52:14, 26.79s/it]2021-12-13 01:02:24,818 iteration 4812 : loss : 0.042926, loss_ce: 0.013078
2021-12-13 01:02:26,327 iteration 4813 : loss : 0.048245, loss_ce: 0.013058
2021-12-13 01:02:27,771 iteration 4814 : loss : 0.048512, loss_ce: 0.018233
2021-12-13 01:02:29,272 iteration 4815 : loss : 0.054428, loss_ce: 0.017004
2021-12-13 01:02:30,722 iteration 4816 : loss : 0.043137, loss_ce: 0.012271
2021-12-13 01:02:32,080 iteration 4817 : loss : 0.044898, loss_ce: 0.013232
2021-12-13 01:02:33,603 iteration 4818 : loss : 0.057816, loss_ce: 0.023375
2021-12-13 01:02:35,110 iteration 4819 : loss : 0.057981, loss_ce: 0.015629
2021-12-13 01:02:36,663 iteration 4820 : loss : 0.045672, loss_ce: 0.015275
2021-12-13 01:02:38,133 iteration 4821 : loss : 0.045819, loss_ce: 0.015399
2021-12-13 01:02:39,645 iteration 4822 : loss : 0.053138, loss_ce: 0.014667
2021-12-13 01:02:41,099 iteration 4823 : loss : 0.055378, loss_ce: 0.017513
2021-12-13 01:02:42,646 iteration 4824 : loss : 0.046688, loss_ce: 0.013852
2021-12-13 01:02:44,121 iteration 4825 : loss : 0.047916, loss_ce: 0.012472
2021-12-13 01:02:45,577 iteration 4826 : loss : 0.050318, loss_ce: 0.013274
2021-12-13 01:02:47,180 iteration 4827 : loss : 0.049684, loss_ce: 0.013467
2021-12-13 01:02:48,737 iteration 4828 : loss : 0.048188, loss_ce: 0.015235
 71%|████████████████████▌        | 284/400 [2:12:46<51:00, 26.38s/it]2021-12-13 01:02:50,297 iteration 4829 : loss : 0.048083, loss_ce: 0.013067
2021-12-13 01:02:51,733 iteration 4830 : loss : 0.046620, loss_ce: 0.012813
2021-12-13 01:02:53,314 iteration 4831 : loss : 0.060296, loss_ce: 0.017437
2021-12-13 01:02:54,802 iteration 4832 : loss : 0.054700, loss_ce: 0.018199
2021-12-13 01:02:56,362 iteration 4833 : loss : 0.054451, loss_ce: 0.012214
2021-12-13 01:02:57,962 iteration 4834 : loss : 0.047007, loss_ce: 0.012813
2021-12-13 01:02:59,444 iteration 4835 : loss : 0.052082, loss_ce: 0.015771
2021-12-13 01:03:00,937 iteration 4836 : loss : 0.046189, loss_ce: 0.012602
2021-12-13 01:03:02,380 iteration 4837 : loss : 0.048852, loss_ce: 0.010474
2021-12-13 01:03:03,910 iteration 4838 : loss : 0.055075, loss_ce: 0.020920
2021-12-13 01:03:05,436 iteration 4839 : loss : 0.055302, loss_ce: 0.024175
2021-12-13 01:03:06,943 iteration 4840 : loss : 0.052582, loss_ce: 0.016207
2021-12-13 01:03:08,519 iteration 4841 : loss : 0.053563, loss_ce: 0.015974
2021-12-13 01:03:10,040 iteration 4842 : loss : 0.050764, loss_ce: 0.016669
2021-12-13 01:03:11,546 iteration 4843 : loss : 0.053763, loss_ce: 0.018221
2021-12-13 01:03:12,975 iteration 4844 : loss : 0.041938, loss_ce: 0.015767
2021-12-13 01:03:12,975 Training Data Eval:
2021-12-13 01:03:20,624   Average segmentation loss on training set: 0.0398
2021-12-13 01:03:20,624 Validation Data Eval:
2021-12-13 01:03:23,234   Average segmentation loss on validation set: 0.0879
2021-12-13 01:03:24,682 iteration 4845 : loss : 0.050633, loss_ce: 0.014127
 71%|████████████████████▋        | 285/400 [2:13:21<56:03, 29.25s/it]2021-12-13 01:03:26,231 iteration 4846 : loss : 0.049871, loss_ce: 0.013977
2021-12-13 01:03:27,698 iteration 4847 : loss : 0.054160, loss_ce: 0.014639
2021-12-13 01:03:29,299 iteration 4848 : loss : 0.055209, loss_ce: 0.020130
2021-12-13 01:03:30,781 iteration 4849 : loss : 0.053314, loss_ce: 0.020536
2021-12-13 01:03:32,307 iteration 4850 : loss : 0.085923, loss_ce: 0.013101
2021-12-13 01:03:33,827 iteration 4851 : loss : 0.044050, loss_ce: 0.015370
2021-12-13 01:03:35,287 iteration 4852 : loss : 0.050154, loss_ce: 0.016549
2021-12-13 01:03:36,747 iteration 4853 : loss : 0.049469, loss_ce: 0.015721
2021-12-13 01:03:38,155 iteration 4854 : loss : 0.040821, loss_ce: 0.012162
2021-12-13 01:03:39,647 iteration 4855 : loss : 0.046951, loss_ce: 0.015880
2021-12-13 01:03:41,162 iteration 4856 : loss : 0.052927, loss_ce: 0.016569
2021-12-13 01:03:42,690 iteration 4857 : loss : 0.053534, loss_ce: 0.021756
2021-12-13 01:03:44,186 iteration 4858 : loss : 0.050964, loss_ce: 0.015402
2021-12-13 01:03:45,615 iteration 4859 : loss : 0.036858, loss_ce: 0.010058
2021-12-13 01:03:47,073 iteration 4860 : loss : 0.043890, loss_ce: 0.009623
2021-12-13 01:03:48,481 iteration 4861 : loss : 0.056206, loss_ce: 0.012208
2021-12-13 01:03:50,050 iteration 4862 : loss : 0.052969, loss_ce: 0.015484
 72%|████████████████████▋        | 286/400 [2:13:47<53:21, 28.09s/it]2021-12-13 01:03:51,613 iteration 4863 : loss : 0.047157, loss_ce: 0.012083
2021-12-13 01:03:53,136 iteration 4864 : loss : 0.057606, loss_ce: 0.014712
2021-12-13 01:03:54,644 iteration 4865 : loss : 0.047536, loss_ce: 0.014946
2021-12-13 01:03:56,142 iteration 4866 : loss : 0.047277, loss_ce: 0.016535
2021-12-13 01:03:57,669 iteration 4867 : loss : 0.061532, loss_ce: 0.017163
2021-12-13 01:03:59,172 iteration 4868 : loss : 0.051729, loss_ce: 0.020358
2021-12-13 01:04:00,628 iteration 4869 : loss : 0.047274, loss_ce: 0.013608
2021-12-13 01:04:02,009 iteration 4870 : loss : 0.042086, loss_ce: 0.012040
2021-12-13 01:04:03,443 iteration 4871 : loss : 0.050184, loss_ce: 0.013221
2021-12-13 01:04:04,929 iteration 4872 : loss : 0.050551, loss_ce: 0.019436
2021-12-13 01:04:06,418 iteration 4873 : loss : 0.048138, loss_ce: 0.013388
2021-12-13 01:04:07,917 iteration 4874 : loss : 0.046122, loss_ce: 0.013917
2021-12-13 01:04:09,378 iteration 4875 : loss : 0.048982, loss_ce: 0.012114
2021-12-13 01:04:10,776 iteration 4876 : loss : 0.045354, loss_ce: 0.016107
2021-12-13 01:04:12,238 iteration 4877 : loss : 0.043737, loss_ce: 0.012909
2021-12-13 01:04:13,732 iteration 4878 : loss : 0.059646, loss_ce: 0.017814
2021-12-13 01:04:15,273 iteration 4879 : loss : 0.048719, loss_ce: 0.011827
 72%|████████████████████▊        | 287/400 [2:14:12<51:16, 27.23s/it]2021-12-13 01:04:16,838 iteration 4880 : loss : 0.049234, loss_ce: 0.014536
2021-12-13 01:04:18,266 iteration 4881 : loss : 0.040920, loss_ce: 0.011073
2021-12-13 01:04:19,691 iteration 4882 : loss : 0.043213, loss_ce: 0.008437
2021-12-13 01:04:21,208 iteration 4883 : loss : 0.059413, loss_ce: 0.021660
2021-12-13 01:04:22,648 iteration 4884 : loss : 0.046760, loss_ce: 0.015611
2021-12-13 01:04:24,138 iteration 4885 : loss : 0.047112, loss_ce: 0.011982
2021-12-13 01:04:25,667 iteration 4886 : loss : 0.045980, loss_ce: 0.012650
2021-12-13 01:04:27,130 iteration 4887 : loss : 0.049447, loss_ce: 0.015879
2021-12-13 01:04:28,637 iteration 4888 : loss : 0.051670, loss_ce: 0.010648
2021-12-13 01:04:30,112 iteration 4889 : loss : 0.045794, loss_ce: 0.015475
2021-12-13 01:04:31,615 iteration 4890 : loss : 0.051731, loss_ce: 0.015393
2021-12-13 01:04:33,115 iteration 4891 : loss : 0.057170, loss_ce: 0.020972
2021-12-13 01:04:34,618 iteration 4892 : loss : 0.047862, loss_ce: 0.018223
2021-12-13 01:04:36,291 iteration 4893 : loss : 0.051691, loss_ce: 0.016995
2021-12-13 01:04:37,794 iteration 4894 : loss : 0.059078, loss_ce: 0.021110
2021-12-13 01:04:39,275 iteration 4895 : loss : 0.049278, loss_ce: 0.017825
2021-12-13 01:04:40,745 iteration 4896 : loss : 0.052164, loss_ce: 0.016560
 72%|████████████████████▉        | 288/400 [2:14:38<49:50, 26.70s/it]2021-12-13 01:04:42,202 iteration 4897 : loss : 0.042571, loss_ce: 0.014810
2021-12-13 01:04:43,669 iteration 4898 : loss : 0.042411, loss_ce: 0.012108
2021-12-13 01:04:45,100 iteration 4899 : loss : 0.046064, loss_ce: 0.016544
2021-12-13 01:04:46,661 iteration 4900 : loss : 0.054439, loss_ce: 0.019420
2021-12-13 01:04:48,165 iteration 4901 : loss : 0.053618, loss_ce: 0.013978
2021-12-13 01:04:49,613 iteration 4902 : loss : 0.043376, loss_ce: 0.008138
2021-12-13 01:04:51,117 iteration 4903 : loss : 0.042561, loss_ce: 0.013036
2021-12-13 01:04:52,598 iteration 4904 : loss : 0.055174, loss_ce: 0.024013
2021-12-13 01:04:54,062 iteration 4905 : loss : 0.051383, loss_ce: 0.019242
2021-12-13 01:04:55,528 iteration 4906 : loss : 0.045250, loss_ce: 0.012837
2021-12-13 01:04:57,021 iteration 4907 : loss : 0.044140, loss_ce: 0.013422
2021-12-13 01:04:58,403 iteration 4908 : loss : 0.047342, loss_ce: 0.010594
2021-12-13 01:04:59,889 iteration 4909 : loss : 0.050333, loss_ce: 0.013824
2021-12-13 01:05:01,349 iteration 4910 : loss : 0.048507, loss_ce: 0.010567
2021-12-13 01:05:02,921 iteration 4911 : loss : 0.053521, loss_ce: 0.017980
2021-12-13 01:05:04,477 iteration 4912 : loss : 0.058377, loss_ce: 0.014871
2021-12-13 01:05:05,935 iteration 4913 : loss : 0.048903, loss_ce: 0.012514
 72%|████████████████████▉        | 289/400 [2:15:03<48:33, 26.25s/it]2021-12-13 01:05:07,389 iteration 4914 : loss : 0.045372, loss_ce: 0.012134
2021-12-13 01:05:08,907 iteration 4915 : loss : 0.051736, loss_ce: 0.017157
2021-12-13 01:05:10,416 iteration 4916 : loss : 0.063760, loss_ce: 0.016884
2021-12-13 01:05:11,901 iteration 4917 : loss : 0.042888, loss_ce: 0.012613
2021-12-13 01:05:13,347 iteration 4918 : loss : 0.043709, loss_ce: 0.015802
2021-12-13 01:05:14,783 iteration 4919 : loss : 0.046338, loss_ce: 0.014727
2021-12-13 01:05:16,294 iteration 4920 : loss : 0.045746, loss_ce: 0.016154
2021-12-13 01:05:17,762 iteration 4921 : loss : 0.045295, loss_ce: 0.009342
2021-12-13 01:05:19,238 iteration 4922 : loss : 0.055992, loss_ce: 0.009638
2021-12-13 01:05:20,727 iteration 4923 : loss : 0.051227, loss_ce: 0.017007
2021-12-13 01:05:22,235 iteration 4924 : loss : 0.047865, loss_ce: 0.013070
2021-12-13 01:05:23,858 iteration 4925 : loss : 0.055567, loss_ce: 0.017324
2021-12-13 01:05:25,320 iteration 4926 : loss : 0.046216, loss_ce: 0.014082
2021-12-13 01:05:26,874 iteration 4927 : loss : 0.048078, loss_ce: 0.015873
2021-12-13 01:05:28,446 iteration 4928 : loss : 0.055405, loss_ce: 0.022575
2021-12-13 01:05:29,997 iteration 4929 : loss : 0.086142, loss_ce: 0.023560
2021-12-13 01:05:29,997 Training Data Eval:
2021-12-13 01:05:37,650   Average segmentation loss on training set: 0.0363
2021-12-13 01:05:37,650 Validation Data Eval:
2021-12-13 01:05:40,262   Average segmentation loss on validation set: 0.0857
2021-12-13 01:05:46,629 Found new lowest validation loss at iteration 4929! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/TU_3seeds/TU_SGD_best_val_loss_seed100.pth
2021-12-13 01:05:48,034 iteration 4930 : loss : 0.042636, loss_ce: 0.014467
 72%|█████████████████████        | 290/400 [2:15:45<56:50, 31.00s/it]2021-12-13 01:05:49,492 iteration 4931 : loss : 0.048205, loss_ce: 0.017202
2021-12-13 01:05:50,836 iteration 4932 : loss : 0.042803, loss_ce: 0.013566
2021-12-13 01:05:52,163 iteration 4933 : loss : 0.045848, loss_ce: 0.013135
2021-12-13 01:05:53,630 iteration 4934 : loss : 0.047242, loss_ce: 0.017186
2021-12-13 01:05:55,153 iteration 4935 : loss : 0.052799, loss_ce: 0.013802
2021-12-13 01:05:56,653 iteration 4936 : loss : 0.039924, loss_ce: 0.011614
2021-12-13 01:05:58,166 iteration 4937 : loss : 0.045221, loss_ce: 0.012915
2021-12-13 01:05:59,657 iteration 4938 : loss : 0.053871, loss_ce: 0.014810
2021-12-13 01:06:01,211 iteration 4939 : loss : 0.049484, loss_ce: 0.017035
2021-12-13 01:06:02,658 iteration 4940 : loss : 0.053018, loss_ce: 0.012138
2021-12-13 01:06:04,145 iteration 4941 : loss : 0.047737, loss_ce: 0.014279
2021-12-13 01:06:05,679 iteration 4942 : loss : 0.046439, loss_ce: 0.015533
2021-12-13 01:06:07,129 iteration 4943 : loss : 0.038331, loss_ce: 0.009598
2021-12-13 01:06:08,619 iteration 4944 : loss : 0.058894, loss_ce: 0.020011
2021-12-13 01:06:10,199 iteration 4945 : loss : 0.060247, loss_ce: 0.016145
2021-12-13 01:06:11,747 iteration 4946 : loss : 0.054415, loss_ce: 0.021270
2021-12-13 01:06:13,348 iteration 4947 : loss : 0.058201, loss_ce: 0.017510
 73%|█████████████████████        | 291/400 [2:16:10<53:13, 29.30s/it]2021-12-13 01:06:14,945 iteration 4948 : loss : 0.052964, loss_ce: 0.017080
2021-12-13 01:06:16,413 iteration 4949 : loss : 0.045457, loss_ce: 0.014605
2021-12-13 01:06:17,859 iteration 4950 : loss : 0.055935, loss_ce: 0.017609
2021-12-13 01:06:19,328 iteration 4951 : loss : 0.051662, loss_ce: 0.014097
2021-12-13 01:06:20,727 iteration 4952 : loss : 0.047686, loss_ce: 0.015750
2021-12-13 01:06:22,253 iteration 4953 : loss : 0.044356, loss_ce: 0.014167
2021-12-13 01:06:23,678 iteration 4954 : loss : 0.051002, loss_ce: 0.014904
2021-12-13 01:06:25,143 iteration 4955 : loss : 0.043871, loss_ce: 0.014655
2021-12-13 01:06:26,614 iteration 4956 : loss : 0.062190, loss_ce: 0.016601
2021-12-13 01:06:28,092 iteration 4957 : loss : 0.048116, loss_ce: 0.015288
2021-12-13 01:06:29,593 iteration 4958 : loss : 0.047043, loss_ce: 0.015639
2021-12-13 01:06:30,990 iteration 4959 : loss : 0.042971, loss_ce: 0.011961
2021-12-13 01:06:32,473 iteration 4960 : loss : 0.046523, loss_ce: 0.011569
2021-12-13 01:06:33,910 iteration 4961 : loss : 0.045028, loss_ce: 0.014183
2021-12-13 01:06:35,442 iteration 4962 : loss : 0.049226, loss_ce: 0.017505
2021-12-13 01:06:36,872 iteration 4963 : loss : 0.044860, loss_ce: 0.015087
2021-12-13 01:06:38,446 iteration 4964 : loss : 0.054797, loss_ce: 0.014853
 73%|█████████████████████▏       | 292/400 [2:16:35<50:27, 28.04s/it]2021-12-13 01:06:40,049 iteration 4965 : loss : 0.055406, loss_ce: 0.016024
2021-12-13 01:06:41,698 iteration 4966 : loss : 0.053536, loss_ce: 0.013030
2021-12-13 01:06:43,132 iteration 4967 : loss : 0.043559, loss_ce: 0.014620
2021-12-13 01:06:44,626 iteration 4968 : loss : 0.044675, loss_ce: 0.014545
2021-12-13 01:06:46,091 iteration 4969 : loss : 0.047279, loss_ce: 0.009158
2021-12-13 01:06:47,531 iteration 4970 : loss : 0.038516, loss_ce: 0.010177
2021-12-13 01:06:49,066 iteration 4971 : loss : 0.054337, loss_ce: 0.017130
2021-12-13 01:06:50,626 iteration 4972 : loss : 0.050886, loss_ce: 0.013398
2021-12-13 01:06:52,093 iteration 4973 : loss : 0.045151, loss_ce: 0.014810
2021-12-13 01:06:53,551 iteration 4974 : loss : 0.047219, loss_ce: 0.014265
2021-12-13 01:06:55,011 iteration 4975 : loss : 0.054246, loss_ce: 0.014907
2021-12-13 01:06:56,472 iteration 4976 : loss : 0.052433, loss_ce: 0.018080
2021-12-13 01:06:58,062 iteration 4977 : loss : 0.050925, loss_ce: 0.021079
2021-12-13 01:06:59,548 iteration 4978 : loss : 0.061972, loss_ce: 0.015541
2021-12-13 01:07:01,045 iteration 4979 : loss : 0.046517, loss_ce: 0.017785
2021-12-13 01:07:02,436 iteration 4980 : loss : 0.040200, loss_ce: 0.011876
2021-12-13 01:07:03,921 iteration 4981 : loss : 0.062828, loss_ce: 0.015930
 73%|█████████████████████▏       | 293/400 [2:17:01<48:37, 27.27s/it]2021-12-13 01:07:05,436 iteration 4982 : loss : 0.045663, loss_ce: 0.014480
2021-12-13 01:07:06,894 iteration 4983 : loss : 0.042767, loss_ce: 0.011728
2021-12-13 01:07:08,449 iteration 4984 : loss : 0.057127, loss_ce: 0.017865
2021-12-13 01:07:09,963 iteration 4985 : loss : 0.044592, loss_ce: 0.012552
2021-12-13 01:07:11,553 iteration 4986 : loss : 0.049660, loss_ce: 0.017675
2021-12-13 01:07:13,018 iteration 4987 : loss : 0.046958, loss_ce: 0.018471
2021-12-13 01:07:14,492 iteration 4988 : loss : 0.049663, loss_ce: 0.017841
2021-12-13 01:07:15,961 iteration 4989 : loss : 0.053828, loss_ce: 0.015767
2021-12-13 01:07:17,609 iteration 4990 : loss : 0.051308, loss_ce: 0.013619
2021-12-13 01:07:19,111 iteration 4991 : loss : 0.044368, loss_ce: 0.011545
2021-12-13 01:07:20,605 iteration 4992 : loss : 0.048016, loss_ce: 0.016311
2021-12-13 01:07:22,163 iteration 4993 : loss : 0.051994, loss_ce: 0.019222
2021-12-13 01:07:23,751 iteration 4994 : loss : 0.056647, loss_ce: 0.014678
2021-12-13 01:07:25,248 iteration 4995 : loss : 0.055414, loss_ce: 0.017764
2021-12-13 01:07:26,736 iteration 4996 : loss : 0.052650, loss_ce: 0.016039
2021-12-13 01:07:28,313 iteration 4997 : loss : 0.048963, loss_ce: 0.016404
2021-12-13 01:07:29,755 iteration 4998 : loss : 0.043498, loss_ce: 0.013252
 74%|█████████████████████▎       | 294/400 [2:17:27<47:24, 26.84s/it]2021-12-13 01:07:31,330 iteration 4999 : loss : 0.047391, loss_ce: 0.015885
2021-12-13 01:07:32,758 iteration 5000 : loss : 0.044514, loss_ce: 0.012427
2021-12-13 01:07:34,218 iteration 5001 : loss : 0.055227, loss_ce: 0.015982
2021-12-13 01:07:35,741 iteration 5002 : loss : 0.054021, loss_ce: 0.017701
2021-12-13 01:07:37,196 iteration 5003 : loss : 0.052202, loss_ce: 0.011731
2021-12-13 01:07:38,761 iteration 5004 : loss : 0.056905, loss_ce: 0.015883
2021-12-13 01:07:40,212 iteration 5005 : loss : 0.049428, loss_ce: 0.014047
2021-12-13 01:07:41,743 iteration 5006 : loss : 0.050957, loss_ce: 0.010489
2021-12-13 01:07:43,206 iteration 5007 : loss : 0.044422, loss_ce: 0.015056
2021-12-13 01:07:44,625 iteration 5008 : loss : 0.047243, loss_ce: 0.013051
2021-12-13 01:07:46,103 iteration 5009 : loss : 0.049326, loss_ce: 0.014364
2021-12-13 01:07:47,703 iteration 5010 : loss : 0.060576, loss_ce: 0.018685
2021-12-13 01:07:49,210 iteration 5011 : loss : 0.047750, loss_ce: 0.022011
2021-12-13 01:07:50,772 iteration 5012 : loss : 0.065033, loss_ce: 0.021317
2021-12-13 01:07:52,444 iteration 5013 : loss : 0.050838, loss_ce: 0.017535
2021-12-13 01:07:53,845 iteration 5014 : loss : 0.043356, loss_ce: 0.012146
2021-12-13 01:07:53,845 Training Data Eval:
2021-12-13 01:08:01,501   Average segmentation loss on training set: 0.0368
2021-12-13 01:08:01,502 Validation Data Eval:
2021-12-13 01:08:04,119   Average segmentation loss on validation set: 0.0892
2021-12-13 01:08:05,590 iteration 5015 : loss : 0.040997, loss_ce: 0.008071
 74%|█████████████████████▍       | 295/400 [2:18:02<51:41, 29.53s/it]2021-12-13 01:08:07,065 iteration 5016 : loss : 0.052470, loss_ce: 0.017940
2021-12-13 01:08:08,525 iteration 5017 : loss : 0.047645, loss_ce: 0.011451
2021-12-13 01:08:10,182 iteration 5018 : loss : 0.058736, loss_ce: 0.018778
2021-12-13 01:08:11,628 iteration 5019 : loss : 0.045700, loss_ce: 0.010628
2021-12-13 01:08:13,145 iteration 5020 : loss : 0.041499, loss_ce: 0.012405
2021-12-13 01:08:14,717 iteration 5021 : loss : 0.046296, loss_ce: 0.011563
2021-12-13 01:08:16,272 iteration 5022 : loss : 0.060743, loss_ce: 0.020767
2021-12-13 01:08:17,798 iteration 5023 : loss : 0.046315, loss_ce: 0.016891
2021-12-13 01:08:19,262 iteration 5024 : loss : 0.039064, loss_ce: 0.012077
2021-12-13 01:08:20,707 iteration 5025 : loss : 0.049113, loss_ce: 0.016545
2021-12-13 01:08:22,204 iteration 5026 : loss : 0.056008, loss_ce: 0.020679
2021-12-13 01:08:23,696 iteration 5027 : loss : 0.043488, loss_ce: 0.011499
2021-12-13 01:08:25,205 iteration 5028 : loss : 0.048859, loss_ce: 0.015328
2021-12-13 01:08:26,649 iteration 5029 : loss : 0.044680, loss_ce: 0.013961
2021-12-13 01:08:28,163 iteration 5030 : loss : 0.056531, loss_ce: 0.017293
2021-12-13 01:08:29,729 iteration 5031 : loss : 0.058199, loss_ce: 0.022039
2021-12-13 01:08:31,302 iteration 5032 : loss : 0.046859, loss_ce: 0.013120
 74%|█████████████████████▍       | 296/400 [2:18:28<49:12, 28.39s/it]2021-12-13 01:08:32,847 iteration 5033 : loss : 0.047554, loss_ce: 0.018184
2021-12-13 01:08:34,340 iteration 5034 : loss : 0.054088, loss_ce: 0.011403
2021-12-13 01:08:35,810 iteration 5035 : loss : 0.043851, loss_ce: 0.012735
2021-12-13 01:08:37,225 iteration 5036 : loss : 0.040722, loss_ce: 0.012104
2021-12-13 01:08:38,681 iteration 5037 : loss : 0.042631, loss_ce: 0.012251
2021-12-13 01:08:40,315 iteration 5038 : loss : 0.049725, loss_ce: 0.017149
2021-12-13 01:08:41,782 iteration 5039 : loss : 0.051265, loss_ce: 0.012749
2021-12-13 01:08:43,315 iteration 5040 : loss : 0.052190, loss_ce: 0.017334
2021-12-13 01:08:44,804 iteration 5041 : loss : 0.050735, loss_ce: 0.015284
2021-12-13 01:08:46,290 iteration 5042 : loss : 0.046750, loss_ce: 0.014407
2021-12-13 01:08:47,745 iteration 5043 : loss : 0.044408, loss_ce: 0.012411
2021-12-13 01:08:49,243 iteration 5044 : loss : 0.045563, loss_ce: 0.012977
2021-12-13 01:08:50,636 iteration 5045 : loss : 0.048192, loss_ce: 0.011907
2021-12-13 01:08:52,136 iteration 5046 : loss : 0.047297, loss_ce: 0.012925
2021-12-13 01:08:53,596 iteration 5047 : loss : 0.048501, loss_ce: 0.015824
2021-12-13 01:08:55,026 iteration 5048 : loss : 0.039021, loss_ce: 0.011720
2021-12-13 01:08:56,591 iteration 5049 : loss : 0.053606, loss_ce: 0.019403
 74%|█████████████████████▌       | 297/400 [2:18:53<47:08, 27.46s/it]2021-12-13 01:08:58,117 iteration 5050 : loss : 0.049497, loss_ce: 0.011355
2021-12-13 01:08:59,547 iteration 5051 : loss : 0.044568, loss_ce: 0.014422
2021-12-13 01:09:01,073 iteration 5052 : loss : 0.044898, loss_ce: 0.017517
2021-12-13 01:09:02,506 iteration 5053 : loss : 0.043469, loss_ce: 0.012345
2021-12-13 01:09:04,014 iteration 5054 : loss : 0.045758, loss_ce: 0.015502
2021-12-13 01:09:05,467 iteration 5055 : loss : 0.042024, loss_ce: 0.011897
2021-12-13 01:09:06,930 iteration 5056 : loss : 0.046350, loss_ce: 0.012246
2021-12-13 01:09:08,405 iteration 5057 : loss : 0.045055, loss_ce: 0.016002
2021-12-13 01:09:09,855 iteration 5058 : loss : 0.045942, loss_ce: 0.012256
2021-12-13 01:09:11,343 iteration 5059 : loss : 0.048746, loss_ce: 0.020022
2021-12-13 01:09:12,887 iteration 5060 : loss : 0.050294, loss_ce: 0.012211
2021-12-13 01:09:14,383 iteration 5061 : loss : 0.047183, loss_ce: 0.017768
2021-12-13 01:09:15,939 iteration 5062 : loss : 0.042759, loss_ce: 0.011631
2021-12-13 01:09:17,393 iteration 5063 : loss : 0.047043, loss_ce: 0.015244
2021-12-13 01:09:18,890 iteration 5064 : loss : 0.053248, loss_ce: 0.012370
2021-12-13 01:09:20,448 iteration 5065 : loss : 0.055761, loss_ce: 0.014853
2021-12-13 01:09:21,822 iteration 5066 : loss : 0.042015, loss_ce: 0.013388
 74%|█████████████████████▌       | 298/400 [2:19:19<45:32, 26.79s/it]2021-12-13 01:09:23,471 iteration 5067 : loss : 0.046886, loss_ce: 0.011772
2021-12-13 01:09:24,947 iteration 5068 : loss : 0.043027, loss_ce: 0.011469
2021-12-13 01:09:26,397 iteration 5069 : loss : 0.047941, loss_ce: 0.013817
2021-12-13 01:09:27,867 iteration 5070 : loss : 0.047362, loss_ce: 0.018598
2021-12-13 01:09:29,417 iteration 5071 : loss : 0.045760, loss_ce: 0.015879
2021-12-13 01:09:30,934 iteration 5072 : loss : 0.047741, loss_ce: 0.014589
2021-12-13 01:09:32,487 iteration 5073 : loss : 0.054549, loss_ce: 0.011666
2021-12-13 01:09:34,026 iteration 5074 : loss : 0.049130, loss_ce: 0.015876
2021-12-13 01:09:35,434 iteration 5075 : loss : 0.047703, loss_ce: 0.018062
2021-12-13 01:09:36,848 iteration 5076 : loss : 0.044108, loss_ce: 0.014025
2021-12-13 01:09:38,396 iteration 5077 : loss : 0.058678, loss_ce: 0.019739
2021-12-13 01:09:39,896 iteration 5078 : loss : 0.047845, loss_ce: 0.013267
2021-12-13 01:09:41,366 iteration 5079 : loss : 0.038989, loss_ce: 0.012060
2021-12-13 01:09:42,879 iteration 5080 : loss : 0.051815, loss_ce: 0.016840
2021-12-13 01:09:44,397 iteration 5081 : loss : 0.047666, loss_ce: 0.017368
2021-12-13 01:09:45,859 iteration 5082 : loss : 0.051672, loss_ce: 0.012877
2021-12-13 01:09:47,312 iteration 5083 : loss : 0.053071, loss_ce: 0.014943
 75%|█████████████████████▋       | 299/400 [2:19:44<44:26, 26.40s/it]2021-12-13 01:09:48,848 iteration 5084 : loss : 0.049118, loss_ce: 0.018391
2021-12-13 01:09:50,445 iteration 5085 : loss : 0.052680, loss_ce: 0.017791
2021-12-13 01:09:51,856 iteration 5086 : loss : 0.036844, loss_ce: 0.009121
2021-12-13 01:09:53,362 iteration 5087 : loss : 0.048468, loss_ce: 0.012236
2021-12-13 01:09:54,785 iteration 5088 : loss : 0.042514, loss_ce: 0.015418
2021-12-13 01:09:56,257 iteration 5089 : loss : 0.049388, loss_ce: 0.014917
2021-12-13 01:09:57,774 iteration 5090 : loss : 0.059814, loss_ce: 0.023342
2021-12-13 01:09:59,305 iteration 5091 : loss : 0.054338, loss_ce: 0.017310
2021-12-13 01:10:00,793 iteration 5092 : loss : 0.054821, loss_ce: 0.011991
2021-12-13 01:10:02,291 iteration 5093 : loss : 0.052539, loss_ce: 0.016031
2021-12-13 01:10:03,807 iteration 5094 : loss : 0.045332, loss_ce: 0.011741
2021-12-13 01:10:05,322 iteration 5095 : loss : 0.044384, loss_ce: 0.015538
2021-12-13 01:10:06,899 iteration 5096 : loss : 0.054067, loss_ce: 0.013879
2021-12-13 01:10:08,446 iteration 5097 : loss : 0.046894, loss_ce: 0.016863
2021-12-13 01:10:09,939 iteration 5098 : loss : 0.048144, loss_ce: 0.016441
2021-12-13 01:10:11,482 iteration 5099 : loss : 0.044899, loss_ce: 0.012314
2021-12-13 01:10:11,483 Training Data Eval:
2021-12-13 01:10:19,209   Average segmentation loss on training set: 0.0347
2021-12-13 01:10:19,209 Validation Data Eval:
2021-12-13 01:10:21,814   Average segmentation loss on validation set: 0.0878
2021-12-13 01:10:23,272 iteration 5100 : loss : 0.050801, loss_ce: 0.012174
 75%|█████████████████████▊       | 300/400 [2:20:20<48:46, 29.27s/it]2021-12-13 01:10:24,840 iteration 5101 : loss : 0.048217, loss_ce: 0.008865
2021-12-13 01:10:26,421 iteration 5102 : loss : 0.046290, loss_ce: 0.018112
2021-12-13 01:10:27,877 iteration 5103 : loss : 0.043550, loss_ce: 0.012255
2021-12-13 01:10:29,373 iteration 5104 : loss : 0.048316, loss_ce: 0.012404
2021-12-13 01:10:30,841 iteration 5105 : loss : 0.047041, loss_ce: 0.012254
2021-12-13 01:10:32,337 iteration 5106 : loss : 0.062307, loss_ce: 0.020428
2021-12-13 01:10:33,857 iteration 5107 : loss : 0.045801, loss_ce: 0.016460
2021-12-13 01:10:35,378 iteration 5108 : loss : 0.045668, loss_ce: 0.016554
2021-12-13 01:10:36,868 iteration 5109 : loss : 0.044647, loss_ce: 0.014442
2021-12-13 01:10:38,260 iteration 5110 : loss : 0.040732, loss_ce: 0.012421
2021-12-13 01:10:39,772 iteration 5111 : loss : 0.050608, loss_ce: 0.013041
2021-12-13 01:10:41,202 iteration 5112 : loss : 0.038571, loss_ce: 0.012269
2021-12-13 01:10:42,772 iteration 5113 : loss : 0.052359, loss_ce: 0.015311
2021-12-13 01:10:44,216 iteration 5114 : loss : 0.039917, loss_ce: 0.012365
2021-12-13 01:10:45,667 iteration 5115 : loss : 0.051901, loss_ce: 0.021149
2021-12-13 01:10:47,165 iteration 5116 : loss : 0.051590, loss_ce: 0.016704
2021-12-13 01:10:48,666 iteration 5117 : loss : 0.042540, loss_ce: 0.012347
 75%|█████████████████████▊       | 301/400 [2:20:45<46:22, 28.11s/it]2021-12-13 01:10:50,213 iteration 5118 : loss : 0.045849, loss_ce: 0.013872
2021-12-13 01:10:51,708 iteration 5119 : loss : 0.053074, loss_ce: 0.015398
2021-12-13 01:10:53,293 iteration 5120 : loss : 0.055867, loss_ce: 0.015286
2021-12-13 01:10:54,743 iteration 5121 : loss : 0.042721, loss_ce: 0.013246
2021-12-13 01:10:56,204 iteration 5122 : loss : 0.053825, loss_ce: 0.012533
2021-12-13 01:10:57,689 iteration 5123 : loss : 0.058076, loss_ce: 0.024742
2021-12-13 01:10:59,135 iteration 5124 : loss : 0.064860, loss_ce: 0.012726
2021-12-13 01:11:00,635 iteration 5125 : loss : 0.046613, loss_ce: 0.014755
2021-12-13 01:11:02,093 iteration 5126 : loss : 0.045903, loss_ce: 0.014284
2021-12-13 01:11:03,603 iteration 5127 : loss : 0.059060, loss_ce: 0.018604
2021-12-13 01:11:05,207 iteration 5128 : loss : 0.051530, loss_ce: 0.014602
2021-12-13 01:11:06,776 iteration 5129 : loss : 0.051799, loss_ce: 0.016811
2021-12-13 01:11:08,197 iteration 5130 : loss : 0.043291, loss_ce: 0.013230
2021-12-13 01:11:09,721 iteration 5131 : loss : 0.041211, loss_ce: 0.009323
2021-12-13 01:11:11,233 iteration 5132 : loss : 0.045678, loss_ce: 0.016396
2021-12-13 01:11:12,679 iteration 5133 : loss : 0.046547, loss_ce: 0.015297
2021-12-13 01:11:14,106 iteration 5134 : loss : 0.036637, loss_ce: 0.010950
 76%|█████████████████████▉       | 302/400 [2:21:11<44:36, 27.31s/it]2021-12-13 01:11:15,625 iteration 5135 : loss : 0.042784, loss_ce: 0.011131
2021-12-13 01:11:17,208 iteration 5136 : loss : 0.054228, loss_ce: 0.016217
2021-12-13 01:11:18,782 iteration 5137 : loss : 0.053552, loss_ce: 0.013453
2021-12-13 01:11:20,380 iteration 5138 : loss : 0.054963, loss_ce: 0.021057
2021-12-13 01:11:21,802 iteration 5139 : loss : 0.037439, loss_ce: 0.009101
2021-12-13 01:11:23,354 iteration 5140 : loss : 0.046119, loss_ce: 0.015141
2021-12-13 01:11:24,785 iteration 5141 : loss : 0.042380, loss_ce: 0.017386
2021-12-13 01:11:26,377 iteration 5142 : loss : 0.058321, loss_ce: 0.018315
2021-12-13 01:11:27,806 iteration 5143 : loss : 0.045958, loss_ce: 0.014588
2021-12-13 01:11:29,229 iteration 5144 : loss : 0.041129, loss_ce: 0.014378
2021-12-13 01:11:30,745 iteration 5145 : loss : 0.053736, loss_ce: 0.016928
2021-12-13 01:11:32,386 iteration 5146 : loss : 0.070452, loss_ce: 0.018904
2021-12-13 01:11:33,890 iteration 5147 : loss : 0.044715, loss_ce: 0.016868
2021-12-13 01:11:35,416 iteration 5148 : loss : 0.049968, loss_ce: 0.014364
2021-12-13 01:11:36,883 iteration 5149 : loss : 0.041267, loss_ce: 0.012116
2021-12-13 01:11:38,350 iteration 5150 : loss : 0.043943, loss_ce: 0.013266
2021-12-13 01:11:39,777 iteration 5151 : loss : 0.041088, loss_ce: 0.011391
 76%|█████████████████████▉       | 303/400 [2:21:37<43:21, 26.82s/it]2021-12-13 01:11:41,359 iteration 5152 : loss : 0.050818, loss_ce: 0.010936
2021-12-13 01:11:42,783 iteration 5153 : loss : 0.041227, loss_ce: 0.015796
2021-12-13 01:11:44,333 iteration 5154 : loss : 0.053942, loss_ce: 0.016830
2021-12-13 01:11:45,762 iteration 5155 : loss : 0.038746, loss_ce: 0.011045
2021-12-13 01:11:47,234 iteration 5156 : loss : 0.045638, loss_ce: 0.012500
2021-12-13 01:11:48,785 iteration 5157 : loss : 0.043716, loss_ce: 0.013403
2021-12-13 01:11:50,311 iteration 5158 : loss : 0.055839, loss_ce: 0.016868
2021-12-13 01:11:51,865 iteration 5159 : loss : 0.061817, loss_ce: 0.016772
2021-12-13 01:11:53,438 iteration 5160 : loss : 0.044163, loss_ce: 0.015007
2021-12-13 01:11:55,001 iteration 5161 : loss : 0.049773, loss_ce: 0.016063
2021-12-13 01:11:56,465 iteration 5162 : loss : 0.047038, loss_ce: 0.011575
2021-12-13 01:11:57,943 iteration 5163 : loss : 0.052566, loss_ce: 0.014044
2021-12-13 01:11:59,371 iteration 5164 : loss : 0.047265, loss_ce: 0.020333
2021-12-13 01:12:00,875 iteration 5165 : loss : 0.041592, loss_ce: 0.010694
2021-12-13 01:12:02,387 iteration 5166 : loss : 0.049215, loss_ce: 0.018129
2021-12-13 01:12:03,828 iteration 5167 : loss : 0.054307, loss_ce: 0.020277
2021-12-13 01:12:05,379 iteration 5168 : loss : 0.057987, loss_ce: 0.019192
 76%|██████████████████████       | 304/400 [2:22:02<42:19, 26.45s/it]2021-12-13 01:12:06,878 iteration 5169 : loss : 0.053075, loss_ce: 0.017582
2021-12-13 01:12:08,384 iteration 5170 : loss : 0.046970, loss_ce: 0.016962
2021-12-13 01:12:09,848 iteration 5171 : loss : 0.052128, loss_ce: 0.011606
2021-12-13 01:12:11,357 iteration 5172 : loss : 0.046693, loss_ce: 0.014236
2021-12-13 01:12:12,817 iteration 5173 : loss : 0.051426, loss_ce: 0.020258
2021-12-13 01:12:14,342 iteration 5174 : loss : 0.044500, loss_ce: 0.013533
2021-12-13 01:12:15,930 iteration 5175 : loss : 0.052200, loss_ce: 0.016111
2021-12-13 01:12:17,426 iteration 5176 : loss : 0.045692, loss_ce: 0.011407
2021-12-13 01:12:19,060 iteration 5177 : loss : 0.056613, loss_ce: 0.022358
2021-12-13 01:12:20,544 iteration 5178 : loss : 0.052317, loss_ce: 0.010980
2021-12-13 01:12:22,109 iteration 5179 : loss : 0.048165, loss_ce: 0.017032
2021-12-13 01:12:23,620 iteration 5180 : loss : 0.048001, loss_ce: 0.015275
2021-12-13 01:12:25,173 iteration 5181 : loss : 0.055336, loss_ce: 0.018346
2021-12-13 01:12:26,737 iteration 5182 : loss : 0.055059, loss_ce: 0.015081
2021-12-13 01:12:28,299 iteration 5183 : loss : 0.053078, loss_ce: 0.012607
2021-12-13 01:12:29,903 iteration 5184 : loss : 0.050439, loss_ce: 0.019928
2021-12-13 01:12:29,903 Training Data Eval:
2021-12-13 01:12:37,552   Average segmentation loss on training set: 0.0342
2021-12-13 01:12:37,552 Validation Data Eval:
2021-12-13 01:12:40,161   Average segmentation loss on validation set: 0.0897
2021-12-13 01:12:41,665 iteration 5185 : loss : 0.045590, loss_ce: 0.016841
 76%|██████████████████████       | 305/400 [2:22:38<46:33, 29.40s/it]2021-12-13 01:12:43,208 iteration 5186 : loss : 0.041421, loss_ce: 0.013230
2021-12-13 01:12:44,758 iteration 5187 : loss : 0.048245, loss_ce: 0.016185
2021-12-13 01:12:46,181 iteration 5188 : loss : 0.046955, loss_ce: 0.012565
2021-12-13 01:12:47,682 iteration 5189 : loss : 0.045357, loss_ce: 0.015402
2021-12-13 01:12:49,098 iteration 5190 : loss : 0.049187, loss_ce: 0.017211
2021-12-13 01:12:50,539 iteration 5191 : loss : 0.046620, loss_ce: 0.015755
2021-12-13 01:12:52,084 iteration 5192 : loss : 0.045492, loss_ce: 0.012787
2021-12-13 01:12:53,600 iteration 5193 : loss : 0.053187, loss_ce: 0.019532
2021-12-13 01:12:55,095 iteration 5194 : loss : 0.048932, loss_ce: 0.016059
2021-12-13 01:12:56,610 iteration 5195 : loss : 0.053643, loss_ce: 0.015834
2021-12-13 01:12:58,232 iteration 5196 : loss : 0.060520, loss_ce: 0.013211
2021-12-13 01:12:59,631 iteration 5197 : loss : 0.045686, loss_ce: 0.011718
2021-12-13 01:13:01,192 iteration 5198 : loss : 0.058966, loss_ce: 0.019505
2021-12-13 01:13:02,736 iteration 5199 : loss : 0.044483, loss_ce: 0.014082
2021-12-13 01:13:04,269 iteration 5200 : loss : 0.040509, loss_ce: 0.010101
2021-12-13 01:13:05,736 iteration 5201 : loss : 0.038458, loss_ce: 0.008178
2021-12-13 01:13:07,236 iteration 5202 : loss : 0.062992, loss_ce: 0.025279
 76%|██████████████████████▏      | 306/400 [2:23:04<44:15, 28.25s/it]2021-12-13 01:13:08,669 iteration 5203 : loss : 0.044157, loss_ce: 0.010974
2021-12-13 01:13:10,196 iteration 5204 : loss : 0.052537, loss_ce: 0.014530
2021-12-13 01:13:11,709 iteration 5205 : loss : 0.050670, loss_ce: 0.014355
2021-12-13 01:13:13,137 iteration 5206 : loss : 0.042046, loss_ce: 0.014096
2021-12-13 01:13:14,612 iteration 5207 : loss : 0.042003, loss_ce: 0.013689
2021-12-13 01:13:16,184 iteration 5208 : loss : 0.060053, loss_ce: 0.020861
2021-12-13 01:13:17,698 iteration 5209 : loss : 0.050491, loss_ce: 0.016626
2021-12-13 01:13:19,180 iteration 5210 : loss : 0.044214, loss_ce: 0.012926
2021-12-13 01:13:20,629 iteration 5211 : loss : 0.045440, loss_ce: 0.011802
2021-12-13 01:13:22,128 iteration 5212 : loss : 0.048470, loss_ce: 0.013561
2021-12-13 01:13:23,656 iteration 5213 : loss : 0.052080, loss_ce: 0.014041
2021-12-13 01:13:25,133 iteration 5214 : loss : 0.054599, loss_ce: 0.026891
2021-12-13 01:13:26,600 iteration 5215 : loss : 0.054123, loss_ce: 0.017258
2021-12-13 01:13:28,074 iteration 5216 : loss : 0.043245, loss_ce: 0.014132
2021-12-13 01:13:29,506 iteration 5217 : loss : 0.045694, loss_ce: 0.013302
2021-12-13 01:13:30,974 iteration 5218 : loss : 0.054826, loss_ce: 0.014191
2021-12-13 01:13:32,485 iteration 5219 : loss : 0.055282, loss_ce: 0.018335
 77%|██████████████████████▎      | 307/400 [2:23:29<42:23, 27.35s/it]2021-12-13 01:13:34,098 iteration 5220 : loss : 0.053636, loss_ce: 0.015725
2021-12-13 01:13:35,566 iteration 5221 : loss : 0.045042, loss_ce: 0.014706
2021-12-13 01:13:36,970 iteration 5222 : loss : 0.045141, loss_ce: 0.008649
2021-12-13 01:13:38,438 iteration 5223 : loss : 0.047224, loss_ce: 0.015255
2021-12-13 01:13:40,002 iteration 5224 : loss : 0.048963, loss_ce: 0.013794
2021-12-13 01:13:41,421 iteration 5225 : loss : 0.047738, loss_ce: 0.012170
2021-12-13 01:13:43,047 iteration 5226 : loss : 0.056530, loss_ce: 0.018224
2021-12-13 01:13:44,444 iteration 5227 : loss : 0.040035, loss_ce: 0.011890
2021-12-13 01:13:45,845 iteration 5228 : loss : 0.039660, loss_ce: 0.013248
2021-12-13 01:13:47,285 iteration 5229 : loss : 0.057033, loss_ce: 0.021276
2021-12-13 01:13:48,797 iteration 5230 : loss : 0.051762, loss_ce: 0.021561
2021-12-13 01:13:50,319 iteration 5231 : loss : 0.073939, loss_ce: 0.016695
2021-12-13 01:13:51,767 iteration 5232 : loss : 0.046340, loss_ce: 0.016025
2021-12-13 01:13:53,291 iteration 5233 : loss : 0.051320, loss_ce: 0.017858
2021-12-13 01:13:54,683 iteration 5234 : loss : 0.040084, loss_ce: 0.011784
2021-12-13 01:13:56,262 iteration 5235 : loss : 0.052218, loss_ce: 0.014584
2021-12-13 01:13:57,736 iteration 5236 : loss : 0.055836, loss_ce: 0.019023
 77%|██████████████████████▎      | 308/400 [2:23:54<40:58, 26.72s/it]2021-12-13 01:13:59,297 iteration 5237 : loss : 0.049602, loss_ce: 0.012757
2021-12-13 01:14:00,793 iteration 5238 : loss : 0.049294, loss_ce: 0.015640
2021-12-13 01:14:02,286 iteration 5239 : loss : 0.041572, loss_ce: 0.010312
2021-12-13 01:14:03,877 iteration 5240 : loss : 0.047128, loss_ce: 0.014077
2021-12-13 01:14:05,412 iteration 5241 : loss : 0.047224, loss_ce: 0.016025
2021-12-13 01:14:06,948 iteration 5242 : loss : 0.056158, loss_ce: 0.019863
2021-12-13 01:14:08,459 iteration 5243 : loss : 0.050893, loss_ce: 0.014535
2021-12-13 01:14:09,985 iteration 5244 : loss : 0.058125, loss_ce: 0.018312
2021-12-13 01:14:11,486 iteration 5245 : loss : 0.048819, loss_ce: 0.015342
2021-12-13 01:14:12,865 iteration 5246 : loss : 0.039067, loss_ce: 0.013729
2021-12-13 01:14:14,354 iteration 5247 : loss : 0.041370, loss_ce: 0.011962
2021-12-13 01:14:15,883 iteration 5248 : loss : 0.050004, loss_ce: 0.014847
2021-12-13 01:14:17,355 iteration 5249 : loss : 0.049497, loss_ce: 0.016818
2021-12-13 01:14:18,844 iteration 5250 : loss : 0.051150, loss_ce: 0.019018
2021-12-13 01:14:20,398 iteration 5251 : loss : 0.044629, loss_ce: 0.014906
2021-12-13 01:14:21,879 iteration 5252 : loss : 0.047011, loss_ce: 0.011549
2021-12-13 01:14:23,413 iteration 5253 : loss : 0.045206, loss_ce: 0.014670
 77%|██████████████████████▍      | 309/400 [2:24:20<40:03, 26.41s/it]2021-12-13 01:14:24,867 iteration 5254 : loss : 0.045753, loss_ce: 0.013479
2021-12-13 01:14:26,418 iteration 5255 : loss : 0.046614, loss_ce: 0.012620
2021-12-13 01:14:27,924 iteration 5256 : loss : 0.037876, loss_ce: 0.010625
2021-12-13 01:14:29,511 iteration 5257 : loss : 0.055062, loss_ce: 0.013877
2021-12-13 01:14:30,954 iteration 5258 : loss : 0.052906, loss_ce: 0.018847
2021-12-13 01:14:32,483 iteration 5259 : loss : 0.066022, loss_ce: 0.016712
2021-12-13 01:14:33,952 iteration 5260 : loss : 0.049472, loss_ce: 0.015287
2021-12-13 01:14:35,438 iteration 5261 : loss : 0.044849, loss_ce: 0.014832
2021-12-13 01:14:36,862 iteration 5262 : loss : 0.037919, loss_ce: 0.012441
2021-12-13 01:14:38,356 iteration 5263 : loss : 0.047055, loss_ce: 0.013951
2021-12-13 01:14:39,820 iteration 5264 : loss : 0.046279, loss_ce: 0.013526
2021-12-13 01:14:41,226 iteration 5265 : loss : 0.040773, loss_ce: 0.012231
2021-12-13 01:14:42,720 iteration 5266 : loss : 0.044997, loss_ce: 0.015792
2021-12-13 01:14:44,147 iteration 5267 : loss : 0.045425, loss_ce: 0.013249
2021-12-13 01:14:45,704 iteration 5268 : loss : 0.045628, loss_ce: 0.014634
2021-12-13 01:14:47,281 iteration 5269 : loss : 0.052370, loss_ce: 0.019715
2021-12-13 01:14:47,281 Training Data Eval:
2021-12-13 01:14:54,940   Average segmentation loss on training set: 0.0348
2021-12-13 01:14:54,940 Validation Data Eval:
2021-12-13 01:14:57,543   Average segmentation loss on validation set: 0.0909
2021-12-13 01:14:59,082 iteration 5270 : loss : 0.058512, loss_ce: 0.018411
 78%|██████████████████████▍      | 310/400 [2:24:56<43:46, 29.18s/it]2021-12-13 01:15:00,656 iteration 5271 : loss : 0.055525, loss_ce: 0.020463
2021-12-13 01:15:02,150 iteration 5272 : loss : 0.050326, loss_ce: 0.016839
2021-12-13 01:15:03,636 iteration 5273 : loss : 0.042880, loss_ce: 0.015383
2021-12-13 01:15:05,076 iteration 5274 : loss : 0.043393, loss_ce: 0.012435
2021-12-13 01:15:06,589 iteration 5275 : loss : 0.044580, loss_ce: 0.009431
2021-12-13 01:15:08,107 iteration 5276 : loss : 0.044612, loss_ce: 0.011727
2021-12-13 01:15:09,524 iteration 5277 : loss : 0.048754, loss_ce: 0.012365
2021-12-13 01:15:11,153 iteration 5278 : loss : 0.050206, loss_ce: 0.015683
2021-12-13 01:15:12,634 iteration 5279 : loss : 0.050837, loss_ce: 0.012049
2021-12-13 01:15:14,203 iteration 5280 : loss : 0.060941, loss_ce: 0.021538
2021-12-13 01:15:15,706 iteration 5281 : loss : 0.044374, loss_ce: 0.017451
2021-12-13 01:15:17,236 iteration 5282 : loss : 0.049993, loss_ce: 0.015151
2021-12-13 01:15:18,729 iteration 5283 : loss : 0.050758, loss_ce: 0.013809
2021-12-13 01:15:20,255 iteration 5284 : loss : 0.053733, loss_ce: 0.019366
2021-12-13 01:15:21,776 iteration 5285 : loss : 0.045937, loss_ce: 0.015601
2021-12-13 01:15:23,332 iteration 5286 : loss : 0.047431, loss_ce: 0.014874
2021-12-13 01:15:24,944 iteration 5287 : loss : 0.056457, loss_ce: 0.019412
 78%|██████████████████████▌      | 311/400 [2:25:22<41:48, 28.19s/it]2021-12-13 01:15:26,432 iteration 5288 : loss : 0.047308, loss_ce: 0.014102
2021-12-13 01:15:27,938 iteration 5289 : loss : 0.052524, loss_ce: 0.014498
2021-12-13 01:15:29,494 iteration 5290 : loss : 0.045325, loss_ce: 0.012898
2021-12-13 01:15:31,047 iteration 5291 : loss : 0.042118, loss_ce: 0.014072
2021-12-13 01:15:32,586 iteration 5292 : loss : 0.042938, loss_ce: 0.013885
2021-12-13 01:15:34,063 iteration 5293 : loss : 0.054212, loss_ce: 0.015221
2021-12-13 01:15:35,638 iteration 5294 : loss : 0.051633, loss_ce: 0.018012
2021-12-13 01:15:37,095 iteration 5295 : loss : 0.039490, loss_ce: 0.012018
2021-12-13 01:15:38,599 iteration 5296 : loss : 0.052639, loss_ce: 0.014810
2021-12-13 01:15:40,180 iteration 5297 : loss : 0.050284, loss_ce: 0.012033
2021-12-13 01:15:41,623 iteration 5298 : loss : 0.044665, loss_ce: 0.017122
2021-12-13 01:15:43,188 iteration 5299 : loss : 0.049945, loss_ce: 0.016234
2021-12-13 01:15:44,603 iteration 5300 : loss : 0.044057, loss_ce: 0.013240
2021-12-13 01:15:46,101 iteration 5301 : loss : 0.058383, loss_ce: 0.016720
2021-12-13 01:15:47,555 iteration 5302 : loss : 0.047445, loss_ce: 0.015376
2021-12-13 01:15:48,932 iteration 5303 : loss : 0.040059, loss_ce: 0.011130
2021-12-13 01:15:50,472 iteration 5304 : loss : 0.046480, loss_ce: 0.014737
 78%|██████████████████████▌      | 312/400 [2:25:47<40:10, 27.39s/it]2021-12-13 01:15:52,007 iteration 5305 : loss : 0.048878, loss_ce: 0.013844
2021-12-13 01:15:53,534 iteration 5306 : loss : 0.056227, loss_ce: 0.015405
2021-12-13 01:15:55,022 iteration 5307 : loss : 0.042854, loss_ce: 0.012842
2021-12-13 01:15:56,554 iteration 5308 : loss : 0.049459, loss_ce: 0.018558
2021-12-13 01:15:58,037 iteration 5309 : loss : 0.041742, loss_ce: 0.011640
2021-12-13 01:15:59,529 iteration 5310 : loss : 0.047829, loss_ce: 0.018840
2021-12-13 01:16:01,032 iteration 5311 : loss : 0.044106, loss_ce: 0.013429
2021-12-13 01:16:02,526 iteration 5312 : loss : 0.049325, loss_ce: 0.011546
2021-12-13 01:16:04,060 iteration 5313 : loss : 0.062325, loss_ce: 0.014771
2021-12-13 01:16:05,508 iteration 5314 : loss : 0.043683, loss_ce: 0.015609
2021-12-13 01:16:07,040 iteration 5315 : loss : 0.044273, loss_ce: 0.013606
2021-12-13 01:16:08,411 iteration 5316 : loss : 0.039426, loss_ce: 0.014055
2021-12-13 01:16:09,915 iteration 5317 : loss : 0.045100, loss_ce: 0.013417
2021-12-13 01:16:11,449 iteration 5318 : loss : 0.055016, loss_ce: 0.014724
2021-12-13 01:16:12,994 iteration 5319 : loss : 0.052777, loss_ce: 0.016202
2021-12-13 01:16:14,537 iteration 5320 : loss : 0.050477, loss_ce: 0.016307
2021-12-13 01:16:15,965 iteration 5321 : loss : 0.049424, loss_ce: 0.009866
 78%|██████████████████████▋      | 313/400 [2:26:13<38:53, 26.82s/it]2021-12-13 01:16:17,553 iteration 5322 : loss : 0.050128, loss_ce: 0.019242
2021-12-13 01:16:19,129 iteration 5323 : loss : 0.051455, loss_ce: 0.015514
2021-12-13 01:16:20,643 iteration 5324 : loss : 0.042468, loss_ce: 0.014246
2021-12-13 01:16:22,157 iteration 5325 : loss : 0.053987, loss_ce: 0.016068
2021-12-13 01:16:23,750 iteration 5326 : loss : 0.049157, loss_ce: 0.010447
2021-12-13 01:16:25,305 iteration 5327 : loss : 0.052117, loss_ce: 0.017849
2021-12-13 01:16:26,795 iteration 5328 : loss : 0.049413, loss_ce: 0.015974
2021-12-13 01:16:28,294 iteration 5329 : loss : 0.046488, loss_ce: 0.014715
2021-12-13 01:16:29,787 iteration 5330 : loss : 0.050860, loss_ce: 0.015562
2021-12-13 01:16:31,214 iteration 5331 : loss : 0.044735, loss_ce: 0.015572
2021-12-13 01:16:32,904 iteration 5332 : loss : 0.059577, loss_ce: 0.020887
2021-12-13 01:16:34,391 iteration 5333 : loss : 0.048934, loss_ce: 0.013314
2021-12-13 01:16:35,772 iteration 5334 : loss : 0.055270, loss_ce: 0.014208
2021-12-13 01:16:37,266 iteration 5335 : loss : 0.046360, loss_ce: 0.011739
2021-12-13 01:16:38,738 iteration 5336 : loss : 0.044518, loss_ce: 0.012147
2021-12-13 01:16:40,229 iteration 5337 : loss : 0.046057, loss_ce: 0.015300
2021-12-13 01:16:41,738 iteration 5338 : loss : 0.049484, loss_ce: 0.014303
 78%|██████████████████████▊      | 314/400 [2:26:39<37:59, 26.51s/it]2021-12-13 01:16:43,259 iteration 5339 : loss : 0.048958, loss_ce: 0.016320
2021-12-13 01:16:44,721 iteration 5340 : loss : 0.046072, loss_ce: 0.015288
2021-12-13 01:16:46,286 iteration 5341 : loss : 0.049682, loss_ce: 0.011098
2021-12-13 01:16:47,824 iteration 5342 : loss : 0.044729, loss_ce: 0.012209
2021-12-13 01:16:49,345 iteration 5343 : loss : 0.053379, loss_ce: 0.016563
2021-12-13 01:16:50,794 iteration 5344 : loss : 0.046643, loss_ce: 0.016318
2021-12-13 01:16:52,260 iteration 5345 : loss : 0.046204, loss_ce: 0.014681
2021-12-13 01:16:53,715 iteration 5346 : loss : 0.046840, loss_ce: 0.014585
2021-12-13 01:16:55,224 iteration 5347 : loss : 0.040826, loss_ce: 0.013142
2021-12-13 01:16:56,662 iteration 5348 : loss : 0.051042, loss_ce: 0.015405
2021-12-13 01:16:58,116 iteration 5349 : loss : 0.043793, loss_ce: 0.012837
2021-12-13 01:16:59,590 iteration 5350 : loss : 0.047570, loss_ce: 0.012278
2021-12-13 01:17:01,014 iteration 5351 : loss : 0.042459, loss_ce: 0.010764
2021-12-13 01:17:02,571 iteration 5352 : loss : 0.061050, loss_ce: 0.019839
2021-12-13 01:17:04,073 iteration 5353 : loss : 0.046208, loss_ce: 0.015647
2021-12-13 01:17:05,507 iteration 5354 : loss : 0.046271, loss_ce: 0.016727
2021-12-13 01:17:05,507 Training Data Eval:
2021-12-13 01:17:13,147   Average segmentation loss on training set: 0.0350
2021-12-13 01:17:13,147 Validation Data Eval:
2021-12-13 01:17:15,749   Average segmentation loss on validation set: 0.0881
2021-12-13 01:17:17,232 iteration 5355 : loss : 0.049072, loss_ce: 0.018912
 79%|██████████████████████▊      | 315/400 [2:27:14<41:22, 29.20s/it]2021-12-13 01:17:18,805 iteration 5356 : loss : 0.050091, loss_ce: 0.018086
2021-12-13 01:17:20,256 iteration 5357 : loss : 0.040373, loss_ce: 0.011876
2021-12-13 01:17:21,693 iteration 5358 : loss : 0.044406, loss_ce: 0.014093
2021-12-13 01:17:23,130 iteration 5359 : loss : 0.040647, loss_ce: 0.011649
2021-12-13 01:17:24,666 iteration 5360 : loss : 0.046625, loss_ce: 0.015521
2021-12-13 01:17:26,138 iteration 5361 : loss : 0.051066, loss_ce: 0.019709
2021-12-13 01:17:27,648 iteration 5362 : loss : 0.044180, loss_ce: 0.014389
2021-12-13 01:17:29,301 iteration 5363 : loss : 0.050459, loss_ce: 0.018281
2021-12-13 01:17:30,789 iteration 5364 : loss : 0.050545, loss_ce: 0.011318
2021-12-13 01:17:32,304 iteration 5365 : loss : 0.052696, loss_ce: 0.015670
2021-12-13 01:17:33,776 iteration 5366 : loss : 0.044292, loss_ce: 0.014094
2021-12-13 01:17:35,169 iteration 5367 : loss : 0.045065, loss_ce: 0.012648
2021-12-13 01:17:36,773 iteration 5368 : loss : 0.060602, loss_ce: 0.016446
2021-12-13 01:17:38,235 iteration 5369 : loss : 0.038297, loss_ce: 0.011706
2021-12-13 01:17:39,699 iteration 5370 : loss : 0.044654, loss_ce: 0.015579
2021-12-13 01:17:41,173 iteration 5371 : loss : 0.042573, loss_ce: 0.009414
2021-12-13 01:17:42,565 iteration 5372 : loss : 0.039843, loss_ce: 0.012293
 79%|██████████████████████▉      | 316/400 [2:27:39<39:15, 28.04s/it]2021-12-13 01:17:44,075 iteration 5373 : loss : 0.047637, loss_ce: 0.014754
2021-12-13 01:17:45,528 iteration 5374 : loss : 0.050642, loss_ce: 0.011121
2021-12-13 01:17:47,026 iteration 5375 : loss : 0.041787, loss_ce: 0.012634
2021-12-13 01:17:48,523 iteration 5376 : loss : 0.053442, loss_ce: 0.014112
2021-12-13 01:17:50,031 iteration 5377 : loss : 0.056873, loss_ce: 0.014320
2021-12-13 01:17:51,536 iteration 5378 : loss : 0.045336, loss_ce: 0.011644
2021-12-13 01:17:53,007 iteration 5379 : loss : 0.045002, loss_ce: 0.015003
2021-12-13 01:17:54,418 iteration 5380 : loss : 0.044480, loss_ce: 0.014490
2021-12-13 01:17:55,981 iteration 5381 : loss : 0.046668, loss_ce: 0.015498
2021-12-13 01:17:57,466 iteration 5382 : loss : 0.049136, loss_ce: 0.014568
2021-12-13 01:17:58,941 iteration 5383 : loss : 0.054150, loss_ce: 0.020781
2021-12-13 01:18:00,403 iteration 5384 : loss : 0.040815, loss_ce: 0.013367
2021-12-13 01:18:01,910 iteration 5385 : loss : 0.047824, loss_ce: 0.015621
2021-12-13 01:18:03,357 iteration 5386 : loss : 0.047482, loss_ce: 0.015161
2021-12-13 01:18:04,948 iteration 5387 : loss : 0.057764, loss_ce: 0.016964
2021-12-13 01:18:06,473 iteration 5388 : loss : 0.046224, loss_ce: 0.012540
2021-12-13 01:18:08,049 iteration 5389 : loss : 0.055294, loss_ce: 0.020147
 79%|██████████████████████▉      | 317/400 [2:28:05<37:43, 27.27s/it]2021-12-13 01:18:09,545 iteration 5390 : loss : 0.039800, loss_ce: 0.010987
2021-12-13 01:18:11,122 iteration 5391 : loss : 0.056986, loss_ce: 0.015271
2021-12-13 01:18:12,599 iteration 5392 : loss : 0.045781, loss_ce: 0.011908
2021-12-13 01:18:14,038 iteration 5393 : loss : 0.045779, loss_ce: 0.013536
2021-12-13 01:18:15,514 iteration 5394 : loss : 0.050341, loss_ce: 0.013513
2021-12-13 01:18:16,997 iteration 5395 : loss : 0.046867, loss_ce: 0.012724
2021-12-13 01:18:18,451 iteration 5396 : loss : 0.045576, loss_ce: 0.014377
2021-12-13 01:18:19,982 iteration 5397 : loss : 0.050820, loss_ce: 0.019584
2021-12-13 01:18:21,630 iteration 5398 : loss : 0.058752, loss_ce: 0.016442
2021-12-13 01:18:23,167 iteration 5399 : loss : 0.050343, loss_ce: 0.013375
2021-12-13 01:18:24,628 iteration 5400 : loss : 0.048478, loss_ce: 0.016812
2021-12-13 01:18:26,151 iteration 5401 : loss : 0.048064, loss_ce: 0.020870
2021-12-13 01:18:27,614 iteration 5402 : loss : 0.045364, loss_ce: 0.018694
2021-12-13 01:18:29,104 iteration 5403 : loss : 0.051671, loss_ce: 0.016899
2021-12-13 01:18:30,625 iteration 5404 : loss : 0.038676, loss_ce: 0.011546
2021-12-13 01:18:32,237 iteration 5405 : loss : 0.055378, loss_ce: 0.013768
2021-12-13 01:18:33,676 iteration 5406 : loss : 0.041663, loss_ce: 0.014573
 80%|███████████████████████      | 318/400 [2:28:30<36:35, 26.78s/it]2021-12-13 01:18:35,180 iteration 5407 : loss : 0.045340, loss_ce: 0.015632
2021-12-13 01:18:36,645 iteration 5408 : loss : 0.052792, loss_ce: 0.016953
2021-12-13 01:18:38,046 iteration 5409 : loss : 0.041666, loss_ce: 0.011985
2021-12-13 01:18:39,543 iteration 5410 : loss : 0.047130, loss_ce: 0.013832
2021-12-13 01:18:40,976 iteration 5411 : loss : 0.043090, loss_ce: 0.013760
2021-12-13 01:18:42,483 iteration 5412 : loss : 0.047222, loss_ce: 0.015621
2021-12-13 01:18:43,940 iteration 5413 : loss : 0.039634, loss_ce: 0.013069
2021-12-13 01:18:45,442 iteration 5414 : loss : 0.041160, loss_ce: 0.013676
2021-12-13 01:18:47,040 iteration 5415 : loss : 0.052850, loss_ce: 0.015238
2021-12-13 01:18:48,489 iteration 5416 : loss : 0.043711, loss_ce: 0.012309
2021-12-13 01:18:49,975 iteration 5417 : loss : 0.041772, loss_ce: 0.010841
2021-12-13 01:18:51,550 iteration 5418 : loss : 0.060148, loss_ce: 0.023576
2021-12-13 01:18:52,975 iteration 5419 : loss : 0.047902, loss_ce: 0.016041
2021-12-13 01:18:54,395 iteration 5420 : loss : 0.045009, loss_ce: 0.015317
2021-12-13 01:18:55,927 iteration 5421 : loss : 0.057673, loss_ce: 0.019114
2021-12-13 01:18:57,496 iteration 5422 : loss : 0.050722, loss_ce: 0.011128
2021-12-13 01:18:59,044 iteration 5423 : loss : 0.054242, loss_ce: 0.020903
 80%|███████████████████████▏     | 319/400 [2:28:56<35:34, 26.36s/it]2021-12-13 01:19:00,574 iteration 5424 : loss : 0.046132, loss_ce: 0.012401
2021-12-13 01:19:02,028 iteration 5425 : loss : 0.045473, loss_ce: 0.010237
2021-12-13 01:19:03,581 iteration 5426 : loss : 0.053967, loss_ce: 0.019179
2021-12-13 01:19:04,989 iteration 5427 : loss : 0.046625, loss_ce: 0.012262
2021-12-13 01:19:06,433 iteration 5428 : loss : 0.038366, loss_ce: 0.011164
2021-12-13 01:19:07,900 iteration 5429 : loss : 0.055690, loss_ce: 0.017254
2021-12-13 01:19:09,484 iteration 5430 : loss : 0.048417, loss_ce: 0.013595
2021-12-13 01:19:10,947 iteration 5431 : loss : 0.052211, loss_ce: 0.017582
2021-12-13 01:19:12,400 iteration 5432 : loss : 0.046381, loss_ce: 0.015967
2021-12-13 01:19:13,960 iteration 5433 : loss : 0.046028, loss_ce: 0.016022
2021-12-13 01:19:15,437 iteration 5434 : loss : 0.049499, loss_ce: 0.016841
2021-12-13 01:19:17,026 iteration 5435 : loss : 0.055964, loss_ce: 0.022883
2021-12-13 01:19:18,504 iteration 5436 : loss : 0.039712, loss_ce: 0.013363
2021-12-13 01:19:19,865 iteration 5437 : loss : 0.038867, loss_ce: 0.011611
2021-12-13 01:19:21,463 iteration 5438 : loss : 0.050626, loss_ce: 0.015230
2021-12-13 01:19:22,915 iteration 5439 : loss : 0.042474, loss_ce: 0.010249
2021-12-13 01:19:22,915 Training Data Eval:
2021-12-13 01:19:30,571   Average segmentation loss on training set: 0.0350
2021-12-13 01:19:30,572 Validation Data Eval:
2021-12-13 01:19:33,180   Average segmentation loss on validation set: 0.0901
2021-12-13 01:19:34,716 iteration 5440 : loss : 0.060190, loss_ce: 0.016926
 80%|███████████████████████▏     | 320/400 [2:29:31<38:52, 29.15s/it]2021-12-13 01:19:36,129 iteration 5441 : loss : 0.041069, loss_ce: 0.010290
2021-12-13 01:19:37,616 iteration 5442 : loss : 0.049856, loss_ce: 0.013411
2021-12-13 01:19:39,142 iteration 5443 : loss : 0.039888, loss_ce: 0.011406
2021-12-13 01:19:40,653 iteration 5444 : loss : 0.048807, loss_ce: 0.014353
2021-12-13 01:19:42,248 iteration 5445 : loss : 0.047832, loss_ce: 0.016975
2021-12-13 01:19:43,699 iteration 5446 : loss : 0.049733, loss_ce: 0.013653
2021-12-13 01:19:45,175 iteration 5447 : loss : 0.047269, loss_ce: 0.016994
2021-12-13 01:19:46,682 iteration 5448 : loss : 0.054653, loss_ce: 0.016189
2021-12-13 01:19:48,160 iteration 5449 : loss : 0.046291, loss_ce: 0.012412
2021-12-13 01:19:49,720 iteration 5450 : loss : 0.050104, loss_ce: 0.021708
2021-12-13 01:19:51,208 iteration 5451 : loss : 0.047577, loss_ce: 0.015973
2021-12-13 01:19:52,763 iteration 5452 : loss : 0.052422, loss_ce: 0.018007
2021-12-13 01:19:54,282 iteration 5453 : loss : 0.045149, loss_ce: 0.013419
2021-12-13 01:19:55,663 iteration 5454 : loss : 0.037933, loss_ce: 0.013517
2021-12-13 01:19:57,270 iteration 5455 : loss : 0.061752, loss_ce: 0.018159
2021-12-13 01:19:58,840 iteration 5456 : loss : 0.049803, loss_ce: 0.017364
2021-12-13 01:20:00,332 iteration 5457 : loss : 0.050211, loss_ce: 0.014629
 80%|███████████████████████▎     | 321/400 [2:29:57<36:59, 28.09s/it]2021-12-13 01:20:01,843 iteration 5458 : loss : 0.042279, loss_ce: 0.012575
2021-12-13 01:20:03,299 iteration 5459 : loss : 0.045863, loss_ce: 0.015594
2021-12-13 01:20:04,820 iteration 5460 : loss : 0.046894, loss_ce: 0.015003
2021-12-13 01:20:06,354 iteration 5461 : loss : 0.047648, loss_ce: 0.016289
2021-12-13 01:20:07,855 iteration 5462 : loss : 0.043091, loss_ce: 0.010738
2021-12-13 01:20:09,325 iteration 5463 : loss : 0.044152, loss_ce: 0.012730
2021-12-13 01:20:10,800 iteration 5464 : loss : 0.047030, loss_ce: 0.014957
2021-12-13 01:20:12,232 iteration 5465 : loss : 0.048052, loss_ce: 0.013139
2021-12-13 01:20:13,811 iteration 5466 : loss : 0.045190, loss_ce: 0.014370
2021-12-13 01:20:15,290 iteration 5467 : loss : 0.039461, loss_ce: 0.013383
2021-12-13 01:20:16,801 iteration 5468 : loss : 0.054737, loss_ce: 0.021230
2021-12-13 01:20:18,229 iteration 5469 : loss : 0.040433, loss_ce: 0.011454
2021-12-13 01:20:19,768 iteration 5470 : loss : 0.050896, loss_ce: 0.016564
2021-12-13 01:20:21,270 iteration 5471 : loss : 0.047536, loss_ce: 0.013603
2021-12-13 01:20:22,821 iteration 5472 : loss : 0.048898, loss_ce: 0.013357
2021-12-13 01:20:24,355 iteration 5473 : loss : 0.047322, loss_ce: 0.014224
2021-12-13 01:20:25,897 iteration 5474 : loss : 0.046572, loss_ce: 0.016467
 80%|███████████████████████▎     | 322/400 [2:30:23<35:32, 27.33s/it]2021-12-13 01:20:27,415 iteration 5475 : loss : 0.040826, loss_ce: 0.010183
2021-12-13 01:20:28,878 iteration 5476 : loss : 0.052938, loss_ce: 0.013660
2021-12-13 01:20:30,331 iteration 5477 : loss : 0.046057, loss_ce: 0.015328
2021-12-13 01:20:31,814 iteration 5478 : loss : 0.053496, loss_ce: 0.019880
2021-12-13 01:20:33,409 iteration 5479 : loss : 0.047638, loss_ce: 0.017484
2021-12-13 01:20:34,977 iteration 5480 : loss : 0.066934, loss_ce: 0.022751
2021-12-13 01:20:36,470 iteration 5481 : loss : 0.045509, loss_ce: 0.011358
2021-12-13 01:20:37,956 iteration 5482 : loss : 0.046382, loss_ce: 0.013848
2021-12-13 01:20:39,459 iteration 5483 : loss : 0.046906, loss_ce: 0.014913
2021-12-13 01:20:40,920 iteration 5484 : loss : 0.045125, loss_ce: 0.014886
2021-12-13 01:20:42,305 iteration 5485 : loss : 0.043110, loss_ce: 0.013176
2021-12-13 01:20:43,885 iteration 5486 : loss : 0.049994, loss_ce: 0.015406
2021-12-13 01:20:45,532 iteration 5487 : loss : 0.053232, loss_ce: 0.016920
2021-12-13 01:20:47,028 iteration 5488 : loss : 0.044481, loss_ce: 0.013670
2021-12-13 01:20:48,518 iteration 5489 : loss : 0.045343, loss_ce: 0.014656
2021-12-13 01:20:49,973 iteration 5490 : loss : 0.058583, loss_ce: 0.016216
2021-12-13 01:20:51,443 iteration 5491 : loss : 0.044132, loss_ce: 0.013287
 81%|███████████████████████▍     | 323/400 [2:30:48<34:23, 26.79s/it]2021-12-13 01:20:52,975 iteration 5492 : loss : 0.048870, loss_ce: 0.016565
2021-12-13 01:20:54,438 iteration 5493 : loss : 0.045927, loss_ce: 0.016541
2021-12-13 01:20:55,868 iteration 5494 : loss : 0.043400, loss_ce: 0.010391
2021-12-13 01:20:57,420 iteration 5495 : loss : 0.053286, loss_ce: 0.007225
2021-12-13 01:20:58,867 iteration 5496 : loss : 0.044593, loss_ce: 0.011277
2021-12-13 01:21:00,351 iteration 5497 : loss : 0.039480, loss_ce: 0.010569
2021-12-13 01:21:01,865 iteration 5498 : loss : 0.050682, loss_ce: 0.023400
2021-12-13 01:21:03,346 iteration 5499 : loss : 0.048201, loss_ce: 0.016314
2021-12-13 01:21:04,862 iteration 5500 : loss : 0.051081, loss_ce: 0.017030
2021-12-13 01:21:06,450 iteration 5501 : loss : 0.052040, loss_ce: 0.018392
2021-12-13 01:21:07,989 iteration 5502 : loss : 0.051074, loss_ce: 0.017410
2021-12-13 01:21:09,454 iteration 5503 : loss : 0.057231, loss_ce: 0.013436
2021-12-13 01:21:10,971 iteration 5504 : loss : 0.044469, loss_ce: 0.016260
2021-12-13 01:21:12,475 iteration 5505 : loss : 0.043054, loss_ce: 0.015387
2021-12-13 01:21:13,916 iteration 5506 : loss : 0.044786, loss_ce: 0.013541
2021-12-13 01:21:15,432 iteration 5507 : loss : 0.052398, loss_ce: 0.013654
2021-12-13 01:21:16,935 iteration 5508 : loss : 0.050600, loss_ce: 0.018383
 81%|███████████████████████▍     | 324/400 [2:31:14<33:26, 26.41s/it]2021-12-13 01:21:18,452 iteration 5509 : loss : 0.049793, loss_ce: 0.014335
2021-12-13 01:21:20,079 iteration 5510 : loss : 0.059573, loss_ce: 0.015781
2021-12-13 01:21:21,581 iteration 5511 : loss : 0.052134, loss_ce: 0.019482
2021-12-13 01:21:23,142 iteration 5512 : loss : 0.067552, loss_ce: 0.009368
2021-12-13 01:21:24,595 iteration 5513 : loss : 0.042558, loss_ce: 0.008144
2021-12-13 01:21:26,139 iteration 5514 : loss : 0.046645, loss_ce: 0.015466
2021-12-13 01:21:27,717 iteration 5515 : loss : 0.062538, loss_ce: 0.018239
2021-12-13 01:21:29,120 iteration 5516 : loss : 0.039325, loss_ce: 0.012939
2021-12-13 01:21:30,643 iteration 5517 : loss : 0.044110, loss_ce: 0.013173
2021-12-13 01:21:32,115 iteration 5518 : loss : 0.068309, loss_ce: 0.023956
2021-12-13 01:21:33,576 iteration 5519 : loss : 0.053588, loss_ce: 0.015657
2021-12-13 01:21:35,094 iteration 5520 : loss : 0.052156, loss_ce: 0.016026
2021-12-13 01:21:36,721 iteration 5521 : loss : 0.047838, loss_ce: 0.013878
2021-12-13 01:21:38,146 iteration 5522 : loss : 0.040687, loss_ce: 0.014423
2021-12-13 01:21:39,651 iteration 5523 : loss : 0.048372, loss_ce: 0.017249
2021-12-13 01:21:41,168 iteration 5524 : loss : 0.045553, loss_ce: 0.018573
2021-12-13 01:21:41,204 Training Data Eval:
2021-12-13 01:21:48,865   Average segmentation loss on training set: 0.0338
2021-12-13 01:21:48,866 Validation Data Eval:
2021-12-13 01:21:51,472   Average segmentation loss on validation set: 0.0920
2021-12-13 01:21:53,019 iteration 5525 : loss : 0.054169, loss_ce: 0.019357
 81%|███████████████████████▌     | 325/400 [2:31:50<36:38, 29.31s/it]2021-12-13 01:21:54,653 iteration 5526 : loss : 0.054583, loss_ce: 0.017570
2021-12-13 01:21:56,108 iteration 5527 : loss : 0.048418, loss_ce: 0.014611
2021-12-13 01:21:57,632 iteration 5528 : loss : 0.048905, loss_ce: 0.014665
2021-12-13 01:21:59,099 iteration 5529 : loss : 0.043960, loss_ce: 0.010811
2021-12-13 01:22:00,526 iteration 5530 : loss : 0.043976, loss_ce: 0.010397
2021-12-13 01:22:02,025 iteration 5531 : loss : 0.044150, loss_ce: 0.013669
2021-12-13 01:22:03,423 iteration 5532 : loss : 0.042427, loss_ce: 0.012723
2021-12-13 01:22:04,945 iteration 5533 : loss : 0.047656, loss_ce: 0.018348
2021-12-13 01:22:06,457 iteration 5534 : loss : 0.052485, loss_ce: 0.014445
2021-12-13 01:22:07,973 iteration 5535 : loss : 0.045447, loss_ce: 0.014739
2021-12-13 01:22:09,453 iteration 5536 : loss : 0.045086, loss_ce: 0.012449
2021-12-13 01:22:10,935 iteration 5537 : loss : 0.039885, loss_ce: 0.010063
2021-12-13 01:22:12,406 iteration 5538 : loss : 0.047929, loss_ce: 0.020286
2021-12-13 01:22:13,906 iteration 5539 : loss : 0.044772, loss_ce: 0.014670
2021-12-13 01:22:15,423 iteration 5540 : loss : 0.042004, loss_ce: 0.012137
2021-12-13 01:22:16,834 iteration 5541 : loss : 0.048140, loss_ce: 0.015270
2021-12-13 01:22:18,289 iteration 5542 : loss : 0.049864, loss_ce: 0.016086
 82%|███████████████████████▋     | 326/400 [2:32:15<34:39, 28.10s/it]2021-12-13 01:22:19,879 iteration 5543 : loss : 0.055065, loss_ce: 0.019302
2021-12-13 01:22:21,484 iteration 5544 : loss : 0.055449, loss_ce: 0.013480
2021-12-13 01:22:23,044 iteration 5545 : loss : 0.050606, loss_ce: 0.015056
2021-12-13 01:22:24,458 iteration 5546 : loss : 0.036943, loss_ce: 0.012726
2021-12-13 01:22:25,976 iteration 5547 : loss : 0.040745, loss_ce: 0.013827
2021-12-13 01:22:27,427 iteration 5548 : loss : 0.043008, loss_ce: 0.012597
2021-12-13 01:22:28,898 iteration 5549 : loss : 0.044322, loss_ce: 0.014228
2021-12-13 01:22:30,411 iteration 5550 : loss : 0.049980, loss_ce: 0.016381
2021-12-13 01:22:31,925 iteration 5551 : loss : 0.050273, loss_ce: 0.011792
2021-12-13 01:22:33,523 iteration 5552 : loss : 0.066367, loss_ce: 0.016511
2021-12-13 01:22:35,047 iteration 5553 : loss : 0.048480, loss_ce: 0.015956
2021-12-13 01:22:36,639 iteration 5554 : loss : 0.050504, loss_ce: 0.012518
2021-12-13 01:22:38,208 iteration 5555 : loss : 0.046148, loss_ce: 0.015292
2021-12-13 01:22:39,753 iteration 5556 : loss : 0.051374, loss_ce: 0.017602
2021-12-13 01:22:41,299 iteration 5557 : loss : 0.054914, loss_ce: 0.015509
2021-12-13 01:22:42,775 iteration 5558 : loss : 0.064458, loss_ce: 0.017081
2021-12-13 01:22:44,203 iteration 5559 : loss : 0.044174, loss_ce: 0.012348
 82%|███████████████████████▋     | 327/400 [2:32:41<33:23, 27.44s/it]2021-12-13 01:22:45,728 iteration 5560 : loss : 0.045328, loss_ce: 0.014311
2021-12-13 01:22:47,310 iteration 5561 : loss : 0.046050, loss_ce: 0.010908
2021-12-13 01:22:48,824 iteration 5562 : loss : 0.048463, loss_ce: 0.013230
2021-12-13 01:22:50,385 iteration 5563 : loss : 0.055384, loss_ce: 0.016618
2021-12-13 01:22:51,797 iteration 5564 : loss : 0.046768, loss_ce: 0.012399
2021-12-13 01:22:53,317 iteration 5565 : loss : 0.049473, loss_ce: 0.012125
2021-12-13 01:22:54,766 iteration 5566 : loss : 0.042074, loss_ce: 0.011954
2021-12-13 01:22:56,319 iteration 5567 : loss : 0.043891, loss_ce: 0.015547
2021-12-13 01:22:57,869 iteration 5568 : loss : 0.048069, loss_ce: 0.019017
2021-12-13 01:22:59,423 iteration 5569 : loss : 0.048372, loss_ce: 0.015582
2021-12-13 01:23:00,919 iteration 5570 : loss : 0.045710, loss_ce: 0.016751
2021-12-13 01:23:02,407 iteration 5571 : loss : 0.046239, loss_ce: 0.010809
2021-12-13 01:23:03,795 iteration 5572 : loss : 0.038865, loss_ce: 0.011219
2021-12-13 01:23:05,324 iteration 5573 : loss : 0.048019, loss_ce: 0.016971
2021-12-13 01:23:06,775 iteration 5574 : loss : 0.042168, loss_ce: 0.013102
2021-12-13 01:23:08,194 iteration 5575 : loss : 0.042203, loss_ce: 0.013254
2021-12-13 01:23:09,619 iteration 5576 : loss : 0.042030, loss_ce: 0.011475
 82%|███████████████████████▊     | 328/400 [2:33:06<32:12, 26.83s/it]2021-12-13 01:23:11,206 iteration 5577 : loss : 0.056999, loss_ce: 0.015811
2021-12-13 01:23:12,708 iteration 5578 : loss : 0.041699, loss_ce: 0.011299
2021-12-13 01:23:14,310 iteration 5579 : loss : 0.067146, loss_ce: 0.016577
2021-12-13 01:23:15,873 iteration 5580 : loss : 0.053655, loss_ce: 0.015255
2021-12-13 01:23:17,396 iteration 5581 : loss : 0.054791, loss_ce: 0.021770
2021-12-13 01:23:18,881 iteration 5582 : loss : 0.042585, loss_ce: 0.013571
2021-12-13 01:23:20,324 iteration 5583 : loss : 0.040313, loss_ce: 0.010158
2021-12-13 01:23:21,787 iteration 5584 : loss : 0.046805, loss_ce: 0.016969
2021-12-13 01:23:23,201 iteration 5585 : loss : 0.045043, loss_ce: 0.012383
2021-12-13 01:23:24,634 iteration 5586 : loss : 0.044363, loss_ce: 0.014928
2021-12-13 01:23:26,206 iteration 5587 : loss : 0.051999, loss_ce: 0.017993
2021-12-13 01:23:27,693 iteration 5588 : loss : 0.048720, loss_ce: 0.014244
2021-12-13 01:23:29,254 iteration 5589 : loss : 0.060116, loss_ce: 0.015019
2021-12-13 01:23:30,680 iteration 5590 : loss : 0.043159, loss_ce: 0.015024
2021-12-13 01:23:32,200 iteration 5591 : loss : 0.063149, loss_ce: 0.022253
2021-12-13 01:23:33,713 iteration 5592 : loss : 0.048436, loss_ce: 0.015385
2021-12-13 01:23:35,112 iteration 5593 : loss : 0.041879, loss_ce: 0.016897
 82%|███████████████████████▊     | 329/400 [2:33:32<31:16, 26.43s/it]2021-12-13 01:23:36,641 iteration 5594 : loss : 0.036788, loss_ce: 0.007715
2021-12-13 01:23:38,079 iteration 5595 : loss : 0.039967, loss_ce: 0.012503
2021-12-13 01:23:39,649 iteration 5596 : loss : 0.046045, loss_ce: 0.010998
2021-12-13 01:23:41,163 iteration 5597 : loss : 0.048138, loss_ce: 0.014936
2021-12-13 01:23:42,787 iteration 5598 : loss : 0.057282, loss_ce: 0.022404
2021-12-13 01:23:44,374 iteration 5599 : loss : 0.045464, loss_ce: 0.013966
2021-12-13 01:23:45,806 iteration 5600 : loss : 0.042439, loss_ce: 0.013205
2021-12-13 01:23:47,317 iteration 5601 : loss : 0.046445, loss_ce: 0.011993
2021-12-13 01:23:48,815 iteration 5602 : loss : 0.043550, loss_ce: 0.012431
2021-12-13 01:23:50,312 iteration 5603 : loss : 0.045866, loss_ce: 0.017670
2021-12-13 01:23:51,814 iteration 5604 : loss : 0.053138, loss_ce: 0.015360
2021-12-13 01:23:53,262 iteration 5605 : loss : 0.042390, loss_ce: 0.015547
2021-12-13 01:23:54,788 iteration 5606 : loss : 0.062143, loss_ce: 0.018668
2021-12-13 01:23:56,226 iteration 5607 : loss : 0.043567, loss_ce: 0.014726
2021-12-13 01:23:57,746 iteration 5608 : loss : 0.042501, loss_ce: 0.015116
2021-12-13 01:23:59,194 iteration 5609 : loss : 0.049521, loss_ce: 0.015662
2021-12-13 01:23:59,194 Training Data Eval:
2021-12-13 01:24:06,841   Average segmentation loss on training set: 0.0348
2021-12-13 01:24:06,841 Validation Data Eval:
2021-12-13 01:24:09,456   Average segmentation loss on validation set: 0.0850
2021-12-13 01:24:15,793 Found new lowest validation loss at iteration 5609! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/TU_3seeds/TU_SGD_best_val_loss_seed100.pth
2021-12-13 01:24:17,227 iteration 5610 : loss : 0.049285, loss_ce: 0.017334
 82%|███████████████████████▉     | 330/400 [2:34:14<36:19, 31.14s/it]2021-12-13 01:24:18,582 iteration 5611 : loss : 0.037668, loss_ce: 0.012123
2021-12-13 01:24:20,015 iteration 5612 : loss : 0.053105, loss_ce: 0.015397
2021-12-13 01:24:21,355 iteration 5613 : loss : 0.045424, loss_ce: 0.008093
2021-12-13 01:24:22,786 iteration 5614 : loss : 0.046365, loss_ce: 0.013453
2021-12-13 01:24:24,270 iteration 5615 : loss : 0.048487, loss_ce: 0.018756
2021-12-13 01:24:25,760 iteration 5616 : loss : 0.039630, loss_ce: 0.009737
2021-12-13 01:24:27,413 iteration 5617 : loss : 0.057525, loss_ce: 0.016622
2021-12-13 01:24:28,960 iteration 5618 : loss : 0.049061, loss_ce: 0.012817
2021-12-13 01:24:30,460 iteration 5619 : loss : 0.043645, loss_ce: 0.016123
2021-12-13 01:24:31,897 iteration 5620 : loss : 0.046010, loss_ce: 0.015454
2021-12-13 01:24:33,325 iteration 5621 : loss : 0.046073, loss_ce: 0.014238
2021-12-13 01:24:34,712 iteration 5622 : loss : 0.039789, loss_ce: 0.013510
2021-12-13 01:24:36,255 iteration 5623 : loss : 0.042090, loss_ce: 0.011519
2021-12-13 01:24:37,732 iteration 5624 : loss : 0.040934, loss_ce: 0.012980
2021-12-13 01:24:39,206 iteration 5625 : loss : 0.051934, loss_ce: 0.019217
2021-12-13 01:24:40,719 iteration 5626 : loss : 0.048700, loss_ce: 0.015456
2021-12-13 01:24:42,200 iteration 5627 : loss : 0.042051, loss_ce: 0.014456
 83%|███████████████████████▉     | 331/400 [2:34:39<33:40, 29.29s/it]2021-12-13 01:24:43,727 iteration 5628 : loss : 0.039910, loss_ce: 0.011113
2021-12-13 01:24:45,230 iteration 5629 : loss : 0.053638, loss_ce: 0.018216
2021-12-13 01:24:46,748 iteration 5630 : loss : 0.048980, loss_ce: 0.015735
2021-12-13 01:24:48,277 iteration 5631 : loss : 0.055548, loss_ce: 0.024419
2021-12-13 01:24:49,857 iteration 5632 : loss : 0.055439, loss_ce: 0.014840
2021-12-13 01:24:51,258 iteration 5633 : loss : 0.044113, loss_ce: 0.009218
2021-12-13 01:24:52,817 iteration 5634 : loss : 0.058786, loss_ce: 0.014288
2021-12-13 01:24:54,310 iteration 5635 : loss : 0.045972, loss_ce: 0.014491
2021-12-13 01:24:55,724 iteration 5636 : loss : 0.054242, loss_ce: 0.016270
2021-12-13 01:24:57,301 iteration 5637 : loss : 0.046286, loss_ce: 0.014340
2021-12-13 01:24:58,789 iteration 5638 : loss : 0.044411, loss_ce: 0.017356
2021-12-13 01:25:00,299 iteration 5639 : loss : 0.047470, loss_ce: 0.013248
2021-12-13 01:25:01,792 iteration 5640 : loss : 0.047348, loss_ce: 0.014771
2021-12-13 01:25:03,227 iteration 5641 : loss : 0.041912, loss_ce: 0.011925
2021-12-13 01:25:04,829 iteration 5642 : loss : 0.051437, loss_ce: 0.018278
2021-12-13 01:25:06,231 iteration 5643 : loss : 0.049433, loss_ce: 0.015472
2021-12-13 01:25:07,704 iteration 5644 : loss : 0.049341, loss_ce: 0.017939
 83%|████████████████████████     | 332/400 [2:35:04<31:54, 28.15s/it]2021-12-13 01:25:09,176 iteration 5645 : loss : 0.049301, loss_ce: 0.012545
2021-12-13 01:25:10,639 iteration 5646 : loss : 0.043281, loss_ce: 0.010423
2021-12-13 01:25:12,058 iteration 5647 : loss : 0.036490, loss_ce: 0.010078
2021-12-13 01:25:13,543 iteration 5648 : loss : 0.044541, loss_ce: 0.016264
2021-12-13 01:25:14,985 iteration 5649 : loss : 0.047082, loss_ce: 0.009020
2021-12-13 01:25:16,537 iteration 5650 : loss : 0.049863, loss_ce: 0.015187
2021-12-13 01:25:18,022 iteration 5651 : loss : 0.045808, loss_ce: 0.016258
2021-12-13 01:25:19,523 iteration 5652 : loss : 0.045445, loss_ce: 0.012213
2021-12-13 01:25:21,010 iteration 5653 : loss : 0.049202, loss_ce: 0.013215
2021-12-13 01:25:22,497 iteration 5654 : loss : 0.042100, loss_ce: 0.013500
2021-12-13 01:25:23,892 iteration 5655 : loss : 0.044828, loss_ce: 0.016894
2021-12-13 01:25:25,473 iteration 5656 : loss : 0.044870, loss_ce: 0.012484
2021-12-13 01:25:27,023 iteration 5657 : loss : 0.061111, loss_ce: 0.019508
2021-12-13 01:25:28,573 iteration 5658 : loss : 0.056137, loss_ce: 0.020648
2021-12-13 01:25:30,027 iteration 5659 : loss : 0.042527, loss_ce: 0.013723
2021-12-13 01:25:31,548 iteration 5660 : loss : 0.047468, loss_ce: 0.012508
2021-12-13 01:25:33,040 iteration 5661 : loss : 0.046434, loss_ce: 0.015098
 83%|████████████████████████▏    | 333/400 [2:35:30<30:29, 27.31s/it]2021-12-13 01:25:34,564 iteration 5662 : loss : 0.050132, loss_ce: 0.015028
2021-12-13 01:25:36,015 iteration 5663 : loss : 0.048884, loss_ce: 0.013175
2021-12-13 01:25:37,516 iteration 5664 : loss : 0.048109, loss_ce: 0.019457
2021-12-13 01:25:38,953 iteration 5665 : loss : 0.043293, loss_ce: 0.014114
2021-12-13 01:25:40,459 iteration 5666 : loss : 0.050846, loss_ce: 0.010940
2021-12-13 01:25:42,000 iteration 5667 : loss : 0.043806, loss_ce: 0.013151
2021-12-13 01:25:43,505 iteration 5668 : loss : 0.046973, loss_ce: 0.013910
2021-12-13 01:25:44,936 iteration 5669 : loss : 0.047266, loss_ce: 0.015019
2021-12-13 01:25:46,404 iteration 5670 : loss : 0.047212, loss_ce: 0.012780
2021-12-13 01:25:47,845 iteration 5671 : loss : 0.054143, loss_ce: 0.015670
2021-12-13 01:25:49,438 iteration 5672 : loss : 0.053756, loss_ce: 0.020613
2021-12-13 01:25:50,908 iteration 5673 : loss : 0.046248, loss_ce: 0.014170
2021-12-13 01:25:52,392 iteration 5674 : loss : 0.042410, loss_ce: 0.010815
2021-12-13 01:25:53,840 iteration 5675 : loss : 0.042995, loss_ce: 0.015471
2021-12-13 01:25:55,391 iteration 5676 : loss : 0.052981, loss_ce: 0.018194
2021-12-13 01:25:56,870 iteration 5677 : loss : 0.048320, loss_ce: 0.012488
2021-12-13 01:25:58,458 iteration 5678 : loss : 0.045292, loss_ce: 0.014241
 84%|████████████████████████▏    | 334/400 [2:35:55<29:24, 26.74s/it]2021-12-13 01:26:00,032 iteration 5679 : loss : 0.044601, loss_ce: 0.013060
2021-12-13 01:26:01,479 iteration 5680 : loss : 0.045705, loss_ce: 0.014200
2021-12-13 01:26:02,984 iteration 5681 : loss : 0.045919, loss_ce: 0.016514
2021-12-13 01:26:04,463 iteration 5682 : loss : 0.047819, loss_ce: 0.015221
2021-12-13 01:26:05,993 iteration 5683 : loss : 0.049099, loss_ce: 0.015861
2021-12-13 01:26:07,562 iteration 5684 : loss : 0.059523, loss_ce: 0.017226
2021-12-13 01:26:09,104 iteration 5685 : loss : 0.037854, loss_ce: 0.008598
2021-12-13 01:26:10,596 iteration 5686 : loss : 0.047173, loss_ce: 0.015164
2021-12-13 01:26:12,050 iteration 5687 : loss : 0.045354, loss_ce: 0.008799
2021-12-13 01:26:13,536 iteration 5688 : loss : 0.043968, loss_ce: 0.011858
2021-12-13 01:26:15,006 iteration 5689 : loss : 0.048872, loss_ce: 0.018429
2021-12-13 01:26:16,527 iteration 5690 : loss : 0.045070, loss_ce: 0.018155
2021-12-13 01:26:17,988 iteration 5691 : loss : 0.039904, loss_ce: 0.010247
2021-12-13 01:26:19,437 iteration 5692 : loss : 0.040107, loss_ce: 0.011747
2021-12-13 01:26:20,958 iteration 5693 : loss : 0.052612, loss_ce: 0.021583
2021-12-13 01:26:22,495 iteration 5694 : loss : 0.054523, loss_ce: 0.018814
2021-12-13 01:26:22,495 Training Data Eval:
2021-12-13 01:26:30,139   Average segmentation loss on training set: 0.0331
2021-12-13 01:26:30,140 Validation Data Eval:
2021-12-13 01:26:32,746   Average segmentation loss on validation set: 0.0872
2021-12-13 01:26:34,200 iteration 5695 : loss : 0.052639, loss_ce: 0.015842
 84%|████████████████████████▎    | 335/400 [2:36:31<31:53, 29.44s/it]2021-12-13 01:26:35,709 iteration 5696 : loss : 0.046857, loss_ce: 0.012918
2021-12-13 01:26:37,272 iteration 5697 : loss : 0.055572, loss_ce: 0.020205
2021-12-13 01:26:38,773 iteration 5698 : loss : 0.040637, loss_ce: 0.011783
2021-12-13 01:26:40,346 iteration 5699 : loss : 0.046358, loss_ce: 0.012949
2021-12-13 01:26:41,845 iteration 5700 : loss : 0.045244, loss_ce: 0.013829
2021-12-13 01:26:43,317 iteration 5701 : loss : 0.047610, loss_ce: 0.019562
2021-12-13 01:26:44,806 iteration 5702 : loss : 0.038796, loss_ce: 0.008977
2021-12-13 01:26:46,294 iteration 5703 : loss : 0.043775, loss_ce: 0.015802
2021-12-13 01:26:47,834 iteration 5704 : loss : 0.048381, loss_ce: 0.016693
2021-12-13 01:26:49,362 iteration 5705 : loss : 0.041751, loss_ce: 0.011411
2021-12-13 01:26:50,822 iteration 5706 : loss : 0.045580, loss_ce: 0.008686
2021-12-13 01:26:52,451 iteration 5707 : loss : 0.051258, loss_ce: 0.014743
2021-12-13 01:26:53,934 iteration 5708 : loss : 0.043315, loss_ce: 0.014540
2021-12-13 01:26:55,456 iteration 5709 : loss : 0.046285, loss_ce: 0.016964
2021-12-13 01:26:57,050 iteration 5710 : loss : 0.050651, loss_ce: 0.015916
2021-12-13 01:26:58,521 iteration 5711 : loss : 0.052316, loss_ce: 0.023266
2021-12-13 01:27:00,001 iteration 5712 : loss : 0.042228, loss_ce: 0.014656
 84%|████████████████████████▎    | 336/400 [2:36:57<30:14, 28.35s/it]2021-12-13 01:27:01,491 iteration 5713 : loss : 0.046317, loss_ce: 0.011202
2021-12-13 01:27:02,954 iteration 5714 : loss : 0.043583, loss_ce: 0.012699
2021-12-13 01:27:04,417 iteration 5715 : loss : 0.042341, loss_ce: 0.013572
2021-12-13 01:27:05,927 iteration 5716 : loss : 0.048694, loss_ce: 0.019043
2021-12-13 01:27:07,476 iteration 5717 : loss : 0.049462, loss_ce: 0.015269
2021-12-13 01:27:08,899 iteration 5718 : loss : 0.044991, loss_ce: 0.011032
2021-12-13 01:27:10,431 iteration 5719 : loss : 0.042115, loss_ce: 0.014285
2021-12-13 01:27:11,894 iteration 5720 : loss : 0.048311, loss_ce: 0.016476
2021-12-13 01:27:13,280 iteration 5721 : loss : 0.036532, loss_ce: 0.008792
2021-12-13 01:27:14,758 iteration 5722 : loss : 0.043766, loss_ce: 0.013400
2021-12-13 01:27:16,239 iteration 5723 : loss : 0.044240, loss_ce: 0.013438
2021-12-13 01:27:17,626 iteration 5724 : loss : 0.037666, loss_ce: 0.009456
2021-12-13 01:27:19,152 iteration 5725 : loss : 0.045056, loss_ce: 0.016179
2021-12-13 01:27:20,587 iteration 5726 : loss : 0.041219, loss_ce: 0.014375
2021-12-13 01:27:22,066 iteration 5727 : loss : 0.045046, loss_ce: 0.012166
2021-12-13 01:27:23,532 iteration 5728 : loss : 0.047155, loss_ce: 0.017646
2021-12-13 01:27:25,058 iteration 5729 : loss : 0.040345, loss_ce: 0.011602
 84%|████████████████████████▍    | 337/400 [2:37:22<28:43, 27.36s/it]2021-12-13 01:27:26,612 iteration 5730 : loss : 0.060182, loss_ce: 0.017737
2021-12-13 01:27:28,163 iteration 5731 : loss : 0.040584, loss_ce: 0.012589
2021-12-13 01:27:29,645 iteration 5732 : loss : 0.044835, loss_ce: 0.010619
2021-12-13 01:27:31,097 iteration 5733 : loss : 0.063146, loss_ce: 0.014734
2021-12-13 01:27:32,594 iteration 5734 : loss : 0.058331, loss_ce: 0.011918
2021-12-13 01:27:34,100 iteration 5735 : loss : 0.043303, loss_ce: 0.016089
2021-12-13 01:27:35,579 iteration 5736 : loss : 0.056765, loss_ce: 0.025283
2021-12-13 01:27:37,051 iteration 5737 : loss : 0.054212, loss_ce: 0.019182
2021-12-13 01:27:38,511 iteration 5738 : loss : 0.035380, loss_ce: 0.010695
2021-12-13 01:27:39,936 iteration 5739 : loss : 0.045806, loss_ce: 0.015594
2021-12-13 01:27:41,457 iteration 5740 : loss : 0.056417, loss_ce: 0.016881
2021-12-13 01:27:42,964 iteration 5741 : loss : 0.049971, loss_ce: 0.020163
2021-12-13 01:27:44,452 iteration 5742 : loss : 0.053776, loss_ce: 0.019053
2021-12-13 01:27:45,909 iteration 5743 : loss : 0.038844, loss_ce: 0.014529
2021-12-13 01:27:47,455 iteration 5744 : loss : 0.043360, loss_ce: 0.015340
2021-12-13 01:27:48,947 iteration 5745 : loss : 0.043373, loss_ce: 0.010347
2021-12-13 01:27:50,356 iteration 5746 : loss : 0.041361, loss_ce: 0.009171
 84%|████████████████████████▌    | 338/400 [2:37:47<27:38, 26.74s/it]2021-12-13 01:27:51,920 iteration 5747 : loss : 0.043979, loss_ce: 0.012307
2021-12-13 01:27:53,445 iteration 5748 : loss : 0.053839, loss_ce: 0.012677
2021-12-13 01:27:54,985 iteration 5749 : loss : 0.046195, loss_ce: 0.016375
2021-12-13 01:27:56,571 iteration 5750 : loss : 0.053945, loss_ce: 0.014117
2021-12-13 01:27:57,995 iteration 5751 : loss : 0.044739, loss_ce: 0.013898
2021-12-13 01:27:59,512 iteration 5752 : loss : 0.050317, loss_ce: 0.012023
2021-12-13 01:28:00,987 iteration 5753 : loss : 0.052133, loss_ce: 0.016359
2021-12-13 01:28:02,405 iteration 5754 : loss : 0.044971, loss_ce: 0.014037
2021-12-13 01:28:03,894 iteration 5755 : loss : 0.046987, loss_ce: 0.017186
2021-12-13 01:28:05,385 iteration 5756 : loss : 0.046121, loss_ce: 0.012501
2021-12-13 01:28:06,850 iteration 5757 : loss : 0.044531, loss_ce: 0.018453
2021-12-13 01:28:08,337 iteration 5758 : loss : 0.045797, loss_ce: 0.009189
2021-12-13 01:28:09,841 iteration 5759 : loss : 0.046221, loss_ce: 0.018992
2021-12-13 01:28:11,417 iteration 5760 : loss : 0.047411, loss_ce: 0.015700
2021-12-13 01:28:12,826 iteration 5761 : loss : 0.037255, loss_ce: 0.011156
2021-12-13 01:28:14,505 iteration 5762 : loss : 0.051226, loss_ce: 0.016562
2021-12-13 01:28:15,995 iteration 5763 : loss : 0.044712, loss_ce: 0.014784
 85%|████████████████████████▌    | 339/400 [2:38:13<26:51, 26.41s/it]2021-12-13 01:28:17,548 iteration 5764 : loss : 0.048207, loss_ce: 0.015043
2021-12-13 01:28:19,082 iteration 5765 : loss : 0.048601, loss_ce: 0.011609
2021-12-13 01:28:20,592 iteration 5766 : loss : 0.047920, loss_ce: 0.015614
2021-12-13 01:28:22,112 iteration 5767 : loss : 0.044154, loss_ce: 0.014042
2021-12-13 01:28:23,732 iteration 5768 : loss : 0.055885, loss_ce: 0.014569
2021-12-13 01:28:25,221 iteration 5769 : loss : 0.040854, loss_ce: 0.012552
2021-12-13 01:28:26,709 iteration 5770 : loss : 0.041985, loss_ce: 0.013162
2021-12-13 01:28:28,203 iteration 5771 : loss : 0.050322, loss_ce: 0.020130
2021-12-13 01:28:29,710 iteration 5772 : loss : 0.046603, loss_ce: 0.014095
2021-12-13 01:28:31,298 iteration 5773 : loss : 0.047107, loss_ce: 0.017955
2021-12-13 01:28:32,873 iteration 5774 : loss : 0.044353, loss_ce: 0.015783
2021-12-13 01:28:34,470 iteration 5775 : loss : 0.043705, loss_ce: 0.011345
2021-12-13 01:28:35,926 iteration 5776 : loss : 0.045252, loss_ce: 0.012310
2021-12-13 01:28:37,383 iteration 5777 : loss : 0.045787, loss_ce: 0.014855
2021-12-13 01:28:38,916 iteration 5778 : loss : 0.045087, loss_ce: 0.011845
2021-12-13 01:28:40,405 iteration 5779 : loss : 0.048947, loss_ce: 0.016803
2021-12-13 01:28:40,405 Training Data Eval:
2021-12-13 01:28:48,042   Average segmentation loss on training set: 0.0339
2021-12-13 01:28:48,043 Validation Data Eval:
2021-12-13 01:28:50,638   Average segmentation loss on validation set: 0.0884
2021-12-13 01:28:52,176 iteration 5780 : loss : 0.042288, loss_ce: 0.013231
 85%|████████████████████████▋    | 340/400 [2:38:49<29:20, 29.34s/it]2021-12-13 01:28:53,845 iteration 5781 : loss : 0.059285, loss_ce: 0.016219
2021-12-13 01:28:55,241 iteration 5782 : loss : 0.040635, loss_ce: 0.014400
2021-12-13 01:28:56,675 iteration 5783 : loss : 0.050150, loss_ce: 0.016387
2021-12-13 01:28:58,119 iteration 5784 : loss : 0.047442, loss_ce: 0.012436
2021-12-13 01:28:59,528 iteration 5785 : loss : 0.040688, loss_ce: 0.014388
2021-12-13 01:29:01,023 iteration 5786 : loss : 0.060370, loss_ce: 0.014218
2021-12-13 01:29:02,491 iteration 5787 : loss : 0.049367, loss_ce: 0.014923
2021-12-13 01:29:03,871 iteration 5788 : loss : 0.045466, loss_ce: 0.015268
2021-12-13 01:29:05,276 iteration 5789 : loss : 0.037562, loss_ce: 0.012566
2021-12-13 01:29:06,783 iteration 5790 : loss : 0.045498, loss_ce: 0.013639
2021-12-13 01:29:08,253 iteration 5791 : loss : 0.038298, loss_ce: 0.011694
2021-12-13 01:29:09,684 iteration 5792 : loss : 0.040622, loss_ce: 0.012495
2021-12-13 01:29:11,217 iteration 5793 : loss : 0.046206, loss_ce: 0.011187
2021-12-13 01:29:12,780 iteration 5794 : loss : 0.047260, loss_ce: 0.016876
2021-12-13 01:29:14,218 iteration 5795 : loss : 0.037791, loss_ce: 0.012340
2021-12-13 01:29:15,721 iteration 5796 : loss : 0.047947, loss_ce: 0.017221
2021-12-13 01:29:17,214 iteration 5797 : loss : 0.060062, loss_ce: 0.017406
 85%|████████████████████████▋    | 341/400 [2:39:14<27:35, 28.05s/it]2021-12-13 01:29:18,838 iteration 5798 : loss : 0.044498, loss_ce: 0.010220
2021-12-13 01:29:20,321 iteration 5799 : loss : 0.046256, loss_ce: 0.012507
2021-12-13 01:29:21,793 iteration 5800 : loss : 0.046882, loss_ce: 0.010535
2021-12-13 01:29:23,183 iteration 5801 : loss : 0.038446, loss_ce: 0.010353
2021-12-13 01:29:24,696 iteration 5802 : loss : 0.045002, loss_ce: 0.017501
2021-12-13 01:29:26,109 iteration 5803 : loss : 0.042759, loss_ce: 0.012722
2021-12-13 01:29:27,700 iteration 5804 : loss : 0.053999, loss_ce: 0.016067
2021-12-13 01:29:29,151 iteration 5805 : loss : 0.050520, loss_ce: 0.020721
2021-12-13 01:29:30,580 iteration 5806 : loss : 0.041243, loss_ce: 0.012477
2021-12-13 01:29:32,081 iteration 5807 : loss : 0.041937, loss_ce: 0.013369
2021-12-13 01:29:33,541 iteration 5808 : loss : 0.052748, loss_ce: 0.020132
2021-12-13 01:29:35,129 iteration 5809 : loss : 0.049605, loss_ce: 0.014070
2021-12-13 01:29:36,542 iteration 5810 : loss : 0.052090, loss_ce: 0.012186
2021-12-13 01:29:38,006 iteration 5811 : loss : 0.042130, loss_ce: 0.014597
2021-12-13 01:29:39,523 iteration 5812 : loss : 0.044738, loss_ce: 0.016021
2021-12-13 01:29:40,962 iteration 5813 : loss : 0.045720, loss_ce: 0.012039
2021-12-13 01:29:42,446 iteration 5814 : loss : 0.048375, loss_ce: 0.015726
 86%|████████████████████████▊    | 342/400 [2:39:39<26:17, 27.20s/it]2021-12-13 01:29:44,088 iteration 5815 : loss : 0.054584, loss_ce: 0.019664
2021-12-13 01:29:45,506 iteration 5816 : loss : 0.048103, loss_ce: 0.012591
2021-12-13 01:29:47,014 iteration 5817 : loss : 0.043198, loss_ce: 0.010526
2021-12-13 01:29:48,481 iteration 5818 : loss : 0.041696, loss_ce: 0.011658
2021-12-13 01:29:50,070 iteration 5819 : loss : 0.045880, loss_ce: 0.014992
2021-12-13 01:29:51,554 iteration 5820 : loss : 0.042660, loss_ce: 0.015050
2021-12-13 01:29:53,026 iteration 5821 : loss : 0.047717, loss_ce: 0.013720
2021-12-13 01:29:54,584 iteration 5822 : loss : 0.050839, loss_ce: 0.014639
2021-12-13 01:29:56,093 iteration 5823 : loss : 0.039706, loss_ce: 0.010386
2021-12-13 01:29:57,581 iteration 5824 : loss : 0.047316, loss_ce: 0.014024
2021-12-13 01:29:59,064 iteration 5825 : loss : 0.045086, loss_ce: 0.015255
2021-12-13 01:30:00,515 iteration 5826 : loss : 0.046657, loss_ce: 0.012524
2021-12-13 01:30:01,943 iteration 5827 : loss : 0.044715, loss_ce: 0.010593
2021-12-13 01:30:03,336 iteration 5828 : loss : 0.038069, loss_ce: 0.013748
2021-12-13 01:30:04,816 iteration 5829 : loss : 0.040814, loss_ce: 0.013961
2021-12-13 01:30:06,312 iteration 5830 : loss : 0.046467, loss_ce: 0.014771
2021-12-13 01:30:07,853 iteration 5831 : loss : 0.048864, loss_ce: 0.017466
 86%|████████████████████████▊    | 343/400 [2:40:05<25:19, 26.67s/it]2021-12-13 01:30:09,476 iteration 5832 : loss : 0.052226, loss_ce: 0.016363
2021-12-13 01:30:10,939 iteration 5833 : loss : 0.041255, loss_ce: 0.012942
2021-12-13 01:30:12,520 iteration 5834 : loss : 0.050392, loss_ce: 0.016535
2021-12-13 01:30:14,182 iteration 5835 : loss : 0.062344, loss_ce: 0.020531
2021-12-13 01:30:15,659 iteration 5836 : loss : 0.046051, loss_ce: 0.016140
2021-12-13 01:30:17,237 iteration 5837 : loss : 0.061389, loss_ce: 0.022852
2021-12-13 01:30:18,814 iteration 5838 : loss : 0.055791, loss_ce: 0.014377
2021-12-13 01:30:20,231 iteration 5839 : loss : 0.046145, loss_ce: 0.013025
2021-12-13 01:30:21,930 iteration 5840 : loss : 0.052536, loss_ce: 0.019350
2021-12-13 01:30:23,420 iteration 5841 : loss : 0.041867, loss_ce: 0.012971
2021-12-13 01:30:24,989 iteration 5842 : loss : 0.051583, loss_ce: 0.018127
2021-12-13 01:30:26,566 iteration 5843 : loss : 0.045860, loss_ce: 0.013220
2021-12-13 01:30:28,092 iteration 5844 : loss : 0.052748, loss_ce: 0.013110
2021-12-13 01:30:29,641 iteration 5845 : loss : 0.050240, loss_ce: 0.014239
2021-12-13 01:30:31,148 iteration 5846 : loss : 0.044291, loss_ce: 0.015251
2021-12-13 01:30:32,628 iteration 5847 : loss : 0.041134, loss_ce: 0.011610
2021-12-13 01:30:34,039 iteration 5848 : loss : 0.042975, loss_ce: 0.012076
 86%|████████████████████████▉    | 344/400 [2:40:31<24:45, 26.52s/it]2021-12-13 01:30:35,505 iteration 5849 : loss : 0.042190, loss_ce: 0.011643
2021-12-13 01:30:37,062 iteration 5850 : loss : 0.054577, loss_ce: 0.014666
2021-12-13 01:30:38,509 iteration 5851 : loss : 0.041159, loss_ce: 0.013074
2021-12-13 01:30:39,965 iteration 5852 : loss : 0.040531, loss_ce: 0.011900
2021-12-13 01:30:41,384 iteration 5853 : loss : 0.052052, loss_ce: 0.012054
2021-12-13 01:30:42,867 iteration 5854 : loss : 0.050276, loss_ce: 0.017030
2021-12-13 01:30:44,339 iteration 5855 : loss : 0.048344, loss_ce: 0.011020
2021-12-13 01:30:45,757 iteration 5856 : loss : 0.048030, loss_ce: 0.014137
2021-12-13 01:30:47,275 iteration 5857 : loss : 0.049817, loss_ce: 0.019875
2021-12-13 01:30:48,738 iteration 5858 : loss : 0.046537, loss_ce: 0.009922
2021-12-13 01:30:50,252 iteration 5859 : loss : 0.056940, loss_ce: 0.020865
2021-12-13 01:30:51,773 iteration 5860 : loss : 0.044296, loss_ce: 0.014611
2021-12-13 01:30:53,288 iteration 5861 : loss : 0.049277, loss_ce: 0.018122
2021-12-13 01:30:54,763 iteration 5862 : loss : 0.039741, loss_ce: 0.012136
2021-12-13 01:30:56,235 iteration 5863 : loss : 0.045628, loss_ce: 0.014140
2021-12-13 01:30:57,706 iteration 5864 : loss : 0.048630, loss_ce: 0.015730
2021-12-13 01:30:57,707 Training Data Eval:
2021-12-13 01:31:05,362   Average segmentation loss on training set: 0.0336
2021-12-13 01:31:05,362 Validation Data Eval:
2021-12-13 01:31:07,974   Average segmentation loss on validation set: 0.0855
2021-12-13 01:31:09,424 iteration 5865 : loss : 0.041797, loss_ce: 0.012100
 86%|█████████████████████████    | 345/400 [2:41:06<26:45, 29.18s/it]2021-12-13 01:31:10,963 iteration 5866 : loss : 0.044009, loss_ce: 0.011682
2021-12-13 01:31:12,466 iteration 5867 : loss : 0.048940, loss_ce: 0.017346
2021-12-13 01:31:13,869 iteration 5868 : loss : 0.038605, loss_ce: 0.010582
2021-12-13 01:31:15,319 iteration 5869 : loss : 0.039952, loss_ce: 0.012318
2021-12-13 01:31:16,715 iteration 5870 : loss : 0.043459, loss_ce: 0.013373
2021-12-13 01:31:18,177 iteration 5871 : loss : 0.040409, loss_ce: 0.013149
2021-12-13 01:31:19,642 iteration 5872 : loss : 0.039925, loss_ce: 0.013459
2021-12-13 01:31:21,049 iteration 5873 : loss : 0.042762, loss_ce: 0.015501
2021-12-13 01:31:22,459 iteration 5874 : loss : 0.053262, loss_ce: 0.012434
2021-12-13 01:31:24,014 iteration 5875 : loss : 0.048604, loss_ce: 0.016797
2021-12-13 01:31:25,587 iteration 5876 : loss : 0.055069, loss_ce: 0.020183
2021-12-13 01:31:27,101 iteration 5877 : loss : 0.047473, loss_ce: 0.014642
2021-12-13 01:31:28,640 iteration 5878 : loss : 0.047513, loss_ce: 0.013459
2021-12-13 01:31:30,092 iteration 5879 : loss : 0.048666, loss_ce: 0.012733
2021-12-13 01:31:31,563 iteration 5880 : loss : 0.045111, loss_ce: 0.014178
2021-12-13 01:31:33,093 iteration 5881 : loss : 0.049322, loss_ce: 0.011785
2021-12-13 01:31:34,554 iteration 5882 : loss : 0.040506, loss_ce: 0.012545
 86%|█████████████████████████    | 346/400 [2:41:31<25:10, 27.96s/it]2021-12-13 01:31:36,197 iteration 5883 : loss : 0.050260, loss_ce: 0.014610
2021-12-13 01:31:37,644 iteration 5884 : loss : 0.046615, loss_ce: 0.014428
2021-12-13 01:31:39,200 iteration 5885 : loss : 0.076548, loss_ce: 0.017686
2021-12-13 01:31:40,648 iteration 5886 : loss : 0.034015, loss_ce: 0.008052
2021-12-13 01:31:42,103 iteration 5887 : loss : 0.056767, loss_ce: 0.013577
2021-12-13 01:31:43,488 iteration 5888 : loss : 0.042186, loss_ce: 0.015366
2021-12-13 01:31:44,946 iteration 5889 : loss : 0.043167, loss_ce: 0.014981
2021-12-13 01:31:46,474 iteration 5890 : loss : 0.049421, loss_ce: 0.018730
2021-12-13 01:31:48,008 iteration 5891 : loss : 0.050607, loss_ce: 0.012680
2021-12-13 01:31:49,575 iteration 5892 : loss : 0.042502, loss_ce: 0.016794
2021-12-13 01:31:50,983 iteration 5893 : loss : 0.038927, loss_ce: 0.009128
2021-12-13 01:31:52,419 iteration 5894 : loss : 0.039064, loss_ce: 0.010181
2021-12-13 01:31:53,939 iteration 5895 : loss : 0.051872, loss_ce: 0.016989
2021-12-13 01:31:55,334 iteration 5896 : loss : 0.042827, loss_ce: 0.012714
2021-12-13 01:31:56,835 iteration 5897 : loss : 0.042730, loss_ce: 0.014580
2021-12-13 01:31:58,387 iteration 5898 : loss : 0.060173, loss_ce: 0.021638
2021-12-13 01:31:59,855 iteration 5899 : loss : 0.045686, loss_ce: 0.012200
 87%|█████████████████████████▏   | 347/400 [2:41:57<23:59, 27.17s/it]2021-12-13 01:32:01,271 iteration 5900 : loss : 0.040246, loss_ce: 0.012872
2021-12-13 01:32:02,709 iteration 5901 : loss : 0.039954, loss_ce: 0.013930
2021-12-13 01:32:04,101 iteration 5902 : loss : 0.036857, loss_ce: 0.012294
2021-12-13 01:32:05,543 iteration 5903 : loss : 0.046274, loss_ce: 0.014663
2021-12-13 01:32:07,036 iteration 5904 : loss : 0.047253, loss_ce: 0.011423
2021-12-13 01:32:08,568 iteration 5905 : loss : 0.051151, loss_ce: 0.020250
2021-12-13 01:32:10,115 iteration 5906 : loss : 0.045209, loss_ce: 0.010852
2021-12-13 01:32:11,524 iteration 5907 : loss : 0.039311, loss_ce: 0.013938
2021-12-13 01:32:13,004 iteration 5908 : loss : 0.052173, loss_ce: 0.014689
2021-12-13 01:32:14,536 iteration 5909 : loss : 0.043747, loss_ce: 0.015568
2021-12-13 01:32:16,014 iteration 5910 : loss : 0.041266, loss_ce: 0.012401
2021-12-13 01:32:17,539 iteration 5911 : loss : 0.049319, loss_ce: 0.011624
2021-12-13 01:32:19,038 iteration 5912 : loss : 0.050541, loss_ce: 0.010964
2021-12-13 01:32:20,520 iteration 5913 : loss : 0.042845, loss_ce: 0.015552
2021-12-13 01:32:21,986 iteration 5914 : loss : 0.044346, loss_ce: 0.016563
2021-12-13 01:32:23,498 iteration 5915 : loss : 0.048521, loss_ce: 0.019469
2021-12-13 01:32:24,925 iteration 5916 : loss : 0.039989, loss_ce: 0.012541
 87%|█████████████████████████▏   | 348/400 [2:42:22<22:59, 26.54s/it]2021-12-13 01:32:26,391 iteration 5917 : loss : 0.043047, loss_ce: 0.013169
2021-12-13 01:32:27,954 iteration 5918 : loss : 0.057352, loss_ce: 0.023510
2021-12-13 01:32:29,520 iteration 5919 : loss : 0.046128, loss_ce: 0.014245
2021-12-13 01:32:31,098 iteration 5920 : loss : 0.045736, loss_ce: 0.012774
2021-12-13 01:32:32,575 iteration 5921 : loss : 0.045180, loss_ce: 0.013411
2021-12-13 01:32:34,112 iteration 5922 : loss : 0.045833, loss_ce: 0.017801
2021-12-13 01:32:35,620 iteration 5923 : loss : 0.051905, loss_ce: 0.017160
2021-12-13 01:32:37,021 iteration 5924 : loss : 0.040345, loss_ce: 0.012872
2021-12-13 01:32:38,492 iteration 5925 : loss : 0.045130, loss_ce: 0.012845
2021-12-13 01:32:40,052 iteration 5926 : loss : 0.052392, loss_ce: 0.012855
2021-12-13 01:32:41,498 iteration 5927 : loss : 0.035205, loss_ce: 0.011450
2021-12-13 01:32:43,047 iteration 5928 : loss : 0.042538, loss_ce: 0.013404
2021-12-13 01:32:44,572 iteration 5929 : loss : 0.041943, loss_ce: 0.014644
2021-12-13 01:32:46,147 iteration 5930 : loss : 0.054981, loss_ce: 0.016962
2021-12-13 01:32:47,586 iteration 5931 : loss : 0.040227, loss_ce: 0.013660
2021-12-13 01:32:49,091 iteration 5932 : loss : 0.041596, loss_ce: 0.010191
2021-12-13 01:32:50,625 iteration 5933 : loss : 0.053651, loss_ce: 0.013108
 87%|█████████████████████████▎   | 349/400 [2:42:47<22:20, 26.29s/it]2021-12-13 01:32:52,180 iteration 5934 : loss : 0.040203, loss_ce: 0.011537
2021-12-13 01:32:53,690 iteration 5935 : loss : 0.053028, loss_ce: 0.011618
2021-12-13 01:32:55,304 iteration 5936 : loss : 0.054355, loss_ce: 0.015770
2021-12-13 01:32:56,884 iteration 5937 : loss : 0.050029, loss_ce: 0.018697
2021-12-13 01:32:58,404 iteration 5938 : loss : 0.048222, loss_ce: 0.014010
2021-12-13 01:32:59,871 iteration 5939 : loss : 0.045841, loss_ce: 0.015402
2021-12-13 01:33:01,395 iteration 5940 : loss : 0.047790, loss_ce: 0.018076
2021-12-13 01:33:02,786 iteration 5941 : loss : 0.045564, loss_ce: 0.016567
2021-12-13 01:33:04,317 iteration 5942 : loss : 0.054352, loss_ce: 0.014766
2021-12-13 01:33:05,732 iteration 5943 : loss : 0.038196, loss_ce: 0.011755
2021-12-13 01:33:07,218 iteration 5944 : loss : 0.047148, loss_ce: 0.013881
2021-12-13 01:33:08,686 iteration 5945 : loss : 0.039839, loss_ce: 0.010583
2021-12-13 01:33:10,118 iteration 5946 : loss : 0.046817, loss_ce: 0.014665
2021-12-13 01:33:11,538 iteration 5947 : loss : 0.042629, loss_ce: 0.013271
2021-12-13 01:33:13,089 iteration 5948 : loss : 0.052789, loss_ce: 0.019454
2021-12-13 01:33:14,530 iteration 5949 : loss : 0.038212, loss_ce: 0.012763
2021-12-13 01:33:14,530 Training Data Eval:
2021-12-13 01:33:22,187   Average segmentation loss on training set: 0.0330
2021-12-13 01:33:22,187 Validation Data Eval:
2021-12-13 01:33:24,791   Average segmentation loss on validation set: 0.0867
2021-12-13 01:33:26,354 iteration 5950 : loss : 0.051130, loss_ce: 0.011568
 88%|█████████████████████████▍   | 350/400 [2:43:23<24:15, 29.12s/it]2021-12-13 01:33:27,855 iteration 5951 : loss : 0.045157, loss_ce: 0.012795
2021-12-13 01:33:29,354 iteration 5952 : loss : 0.041631, loss_ce: 0.017024
2021-12-13 01:33:30,860 iteration 5953 : loss : 0.046914, loss_ce: 0.017927
2021-12-13 01:33:32,432 iteration 5954 : loss : 0.047750, loss_ce: 0.013995
2021-12-13 01:33:33,902 iteration 5955 : loss : 0.042770, loss_ce: 0.012386
2021-12-13 01:33:35,467 iteration 5956 : loss : 0.048009, loss_ce: 0.013175
2021-12-13 01:33:36,941 iteration 5957 : loss : 0.044817, loss_ce: 0.011969
2021-12-13 01:33:38,386 iteration 5958 : loss : 0.040093, loss_ce: 0.014517
2021-12-13 01:33:39,862 iteration 5959 : loss : 0.040486, loss_ce: 0.011239
2021-12-13 01:33:41,301 iteration 5960 : loss : 0.033695, loss_ce: 0.009226
2021-12-13 01:33:42,805 iteration 5961 : loss : 0.045518, loss_ce: 0.014841
2021-12-13 01:33:44,350 iteration 5962 : loss : 0.046967, loss_ce: 0.015307
2021-12-13 01:33:45,754 iteration 5963 : loss : 0.039802, loss_ce: 0.014226
2021-12-13 01:33:47,425 iteration 5964 : loss : 0.054277, loss_ce: 0.017019
2021-12-13 01:33:48,886 iteration 5965 : loss : 0.044838, loss_ce: 0.014161
2021-12-13 01:33:50,428 iteration 5966 : loss : 0.052531, loss_ce: 0.017485
2021-12-13 01:33:51,884 iteration 5967 : loss : 0.049176, loss_ce: 0.013471
 88%|█████████████████████████▍   | 351/400 [2:43:49<22:53, 28.04s/it]2021-12-13 01:33:53,392 iteration 5968 : loss : 0.046980, loss_ce: 0.015018
2021-12-13 01:33:54,925 iteration 5969 : loss : 0.050944, loss_ce: 0.010120
2021-12-13 01:33:56,427 iteration 5970 : loss : 0.043822, loss_ce: 0.013279
2021-12-13 01:33:57,822 iteration 5971 : loss : 0.041464, loss_ce: 0.014367
2021-12-13 01:33:59,266 iteration 5972 : loss : 0.041850, loss_ce: 0.013319
2021-12-13 01:34:00,766 iteration 5973 : loss : 0.039150, loss_ce: 0.012880
2021-12-13 01:34:02,198 iteration 5974 : loss : 0.059367, loss_ce: 0.016706
2021-12-13 01:34:03,727 iteration 5975 : loss : 0.048488, loss_ce: 0.012837
2021-12-13 01:34:05,230 iteration 5976 : loss : 0.061500, loss_ce: 0.013314
2021-12-13 01:34:06,749 iteration 5977 : loss : 0.043834, loss_ce: 0.012624
2021-12-13 01:34:08,251 iteration 5978 : loss : 0.042841, loss_ce: 0.015124
2021-12-13 01:34:09,699 iteration 5979 : loss : 0.041284, loss_ce: 0.012295
2021-12-13 01:34:11,235 iteration 5980 : loss : 0.036610, loss_ce: 0.009662
2021-12-13 01:34:12,758 iteration 5981 : loss : 0.048598, loss_ce: 0.015531
2021-12-13 01:34:14,313 iteration 5982 : loss : 0.051521, loss_ce: 0.017789
2021-12-13 01:34:15,732 iteration 5983 : loss : 0.045148, loss_ce: 0.017298
2021-12-13 01:34:17,159 iteration 5984 : loss : 0.046459, loss_ce: 0.012851
 88%|█████████████████████████▌   | 352/400 [2:44:14<21:46, 27.21s/it]2021-12-13 01:34:18,807 iteration 5985 : loss : 0.060622, loss_ce: 0.023876
2021-12-13 01:34:20,272 iteration 5986 : loss : 0.047483, loss_ce: 0.018080
2021-12-13 01:34:21,828 iteration 5987 : loss : 0.056715, loss_ce: 0.008483
2021-12-13 01:34:23,417 iteration 5988 : loss : 0.041746, loss_ce: 0.014937
2021-12-13 01:34:24,902 iteration 5989 : loss : 0.045704, loss_ce: 0.015059
2021-12-13 01:34:26,435 iteration 5990 : loss : 0.046683, loss_ce: 0.020945
2021-12-13 01:34:27,835 iteration 5991 : loss : 0.037718, loss_ce: 0.010896
2021-12-13 01:34:29,373 iteration 5992 : loss : 0.043936, loss_ce: 0.012026
2021-12-13 01:34:30,807 iteration 5993 : loss : 0.033863, loss_ce: 0.008873
2021-12-13 01:34:32,283 iteration 5994 : loss : 0.051309, loss_ce: 0.012914
2021-12-13 01:34:33,801 iteration 5995 : loss : 0.052416, loss_ce: 0.018132
2021-12-13 01:34:35,381 iteration 5996 : loss : 0.042235, loss_ce: 0.010492
2021-12-13 01:34:36,956 iteration 5997 : loss : 0.049600, loss_ce: 0.018656
2021-12-13 01:34:38,406 iteration 5998 : loss : 0.037463, loss_ce: 0.010746
2021-12-13 01:34:39,929 iteration 5999 : loss : 0.063293, loss_ce: 0.015921
2021-12-13 01:34:41,453 iteration 6000 : loss : 0.049880, loss_ce: 0.018307
2021-12-13 01:34:43,026 iteration 6001 : loss : 0.063208, loss_ce: 0.018735
 88%|█████████████████████████▌   | 353/400 [2:44:40<20:59, 26.81s/it]2021-12-13 01:34:44,584 iteration 6002 : loss : 0.040948, loss_ce: 0.013504
2021-12-13 01:34:46,142 iteration 6003 : loss : 0.045778, loss_ce: 0.012812
2021-12-13 01:34:47,644 iteration 6004 : loss : 0.046853, loss_ce: 0.018128
2021-12-13 01:34:49,180 iteration 6005 : loss : 0.042318, loss_ce: 0.015970
2021-12-13 01:34:50,616 iteration 6006 : loss : 0.049085, loss_ce: 0.019461
2021-12-13 01:34:52,109 iteration 6007 : loss : 0.042649, loss_ce: 0.012415
2021-12-13 01:34:53,601 iteration 6008 : loss : 0.041257, loss_ce: 0.011449
2021-12-13 01:34:55,186 iteration 6009 : loss : 0.055115, loss_ce: 0.016774
2021-12-13 01:34:56,585 iteration 6010 : loss : 0.038674, loss_ce: 0.012665
2021-12-13 01:34:58,099 iteration 6011 : loss : 0.053888, loss_ce: 0.016165
2021-12-13 01:34:59,590 iteration 6012 : loss : 0.054526, loss_ce: 0.014055
2021-12-13 01:35:01,205 iteration 6013 : loss : 0.054866, loss_ce: 0.018100
2021-12-13 01:35:02,702 iteration 6014 : loss : 0.044149, loss_ce: 0.013670
2021-12-13 01:35:04,159 iteration 6015 : loss : 0.054416, loss_ce: 0.015292
2021-12-13 01:35:05,661 iteration 6016 : loss : 0.044541, loss_ce: 0.012909
2021-12-13 01:35:07,091 iteration 6017 : loss : 0.053832, loss_ce: 0.011375
2021-12-13 01:35:08,684 iteration 6018 : loss : 0.042925, loss_ce: 0.012655
 88%|█████████████████████████▋   | 354/400 [2:45:05<20:17, 26.47s/it]2021-12-13 01:35:10,271 iteration 6019 : loss : 0.054525, loss_ce: 0.018502
2021-12-13 01:35:11,768 iteration 6020 : loss : 0.044448, loss_ce: 0.014680
2021-12-13 01:35:13,257 iteration 6021 : loss : 0.040759, loss_ce: 0.011732
2021-12-13 01:35:14,778 iteration 6022 : loss : 0.040339, loss_ce: 0.012600
2021-12-13 01:35:16,182 iteration 6023 : loss : 0.037797, loss_ce: 0.010369
2021-12-13 01:35:17,737 iteration 6024 : loss : 0.059914, loss_ce: 0.019754
2021-12-13 01:35:19,291 iteration 6025 : loss : 0.054415, loss_ce: 0.019167
2021-12-13 01:35:20,837 iteration 6026 : loss : 0.054545, loss_ce: 0.016877
2021-12-13 01:35:22,296 iteration 6027 : loss : 0.042895, loss_ce: 0.014613
2021-12-13 01:35:23,739 iteration 6028 : loss : 0.044705, loss_ce: 0.015053
2021-12-13 01:35:25,277 iteration 6029 : loss : 0.056086, loss_ce: 0.021164
2021-12-13 01:35:26,717 iteration 6030 : loss : 0.046476, loss_ce: 0.014049
2021-12-13 01:35:28,313 iteration 6031 : loss : 0.056998, loss_ce: 0.013350
2021-12-13 01:35:29,854 iteration 6032 : loss : 0.042313, loss_ce: 0.012153
2021-12-13 01:35:31,394 iteration 6033 : loss : 0.061926, loss_ce: 0.024446
2021-12-13 01:35:32,976 iteration 6034 : loss : 0.041848, loss_ce: 0.010931
2021-12-13 01:35:32,977 Training Data Eval:
2021-12-13 01:35:40,629   Average segmentation loss on training set: 0.0340
2021-12-13 01:35:40,629 Validation Data Eval:
2021-12-13 01:35:43,237   Average segmentation loss on validation set: 0.0844
2021-12-13 01:35:49,578 Found new lowest validation loss at iteration 6034! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/TU_3seeds/TU_SGD_best_val_loss_seed100.pth
2021-12-13 01:35:50,892 iteration 6035 : loss : 0.052572, loss_ce: 0.011984
 89%|█████████████████████████▋   | 355/400 [2:45:48<23:23, 31.19s/it]2021-12-13 01:35:52,355 iteration 6036 : loss : 0.047358, loss_ce: 0.014736
2021-12-13 01:35:53,818 iteration 6037 : loss : 0.059479, loss_ce: 0.017323
2021-12-13 01:35:55,162 iteration 6038 : loss : 0.045979, loss_ce: 0.014382
2021-12-13 01:35:56,645 iteration 6039 : loss : 0.044816, loss_ce: 0.015631
2021-12-13 01:35:58,128 iteration 6040 : loss : 0.051730, loss_ce: 0.018984
2021-12-13 01:35:59,740 iteration 6041 : loss : 0.066067, loss_ce: 0.021295
2021-12-13 01:36:01,304 iteration 6042 : loss : 0.043974, loss_ce: 0.014442
2021-12-13 01:36:02,797 iteration 6043 : loss : 0.043462, loss_ce: 0.017099
2021-12-13 01:36:04,274 iteration 6044 : loss : 0.049581, loss_ce: 0.012734
2021-12-13 01:36:05,821 iteration 6045 : loss : 0.054585, loss_ce: 0.016525
2021-12-13 01:36:07,316 iteration 6046 : loss : 0.062591, loss_ce: 0.019128
2021-12-13 01:36:08,774 iteration 6047 : loss : 0.048512, loss_ce: 0.011803
2021-12-13 01:36:10,382 iteration 6048 : loss : 0.052440, loss_ce: 0.008847
2021-12-13 01:36:11,929 iteration 6049 : loss : 0.044118, loss_ce: 0.015318
2021-12-13 01:36:13,389 iteration 6050 : loss : 0.042588, loss_ce: 0.015446
2021-12-13 01:36:14,943 iteration 6051 : loss : 0.046996, loss_ce: 0.015865
2021-12-13 01:36:16,339 iteration 6052 : loss : 0.040945, loss_ce: 0.009601
 89%|█████████████████████████▊   | 356/400 [2:46:13<21:36, 29.46s/it]2021-12-13 01:36:17,837 iteration 6053 : loss : 0.043969, loss_ce: 0.017412
2021-12-13 01:36:19,424 iteration 6054 : loss : 0.053014, loss_ce: 0.018259
2021-12-13 01:36:20,897 iteration 6055 : loss : 0.039216, loss_ce: 0.010647
2021-12-13 01:36:22,352 iteration 6056 : loss : 0.041687, loss_ce: 0.013123
2021-12-13 01:36:23,802 iteration 6057 : loss : 0.043325, loss_ce: 0.011767
2021-12-13 01:36:25,354 iteration 6058 : loss : 0.042279, loss_ce: 0.014217
2021-12-13 01:36:26,779 iteration 6059 : loss : 0.039703, loss_ce: 0.011965
2021-12-13 01:36:28,333 iteration 6060 : loss : 0.039970, loss_ce: 0.013292
2021-12-13 01:36:29,766 iteration 6061 : loss : 0.047729, loss_ce: 0.017631
2021-12-13 01:36:31,359 iteration 6062 : loss : 0.047032, loss_ce: 0.018969
2021-12-13 01:36:32,940 iteration 6063 : loss : 0.054731, loss_ce: 0.013167
2021-12-13 01:36:34,445 iteration 6064 : loss : 0.062905, loss_ce: 0.015443
2021-12-13 01:36:36,031 iteration 6065 : loss : 0.055364, loss_ce: 0.023360
2021-12-13 01:36:37,509 iteration 6066 : loss : 0.040877, loss_ce: 0.012764
2021-12-13 01:36:38,953 iteration 6067 : loss : 0.062655, loss_ce: 0.009773
2021-12-13 01:36:40,549 iteration 6068 : loss : 0.058672, loss_ce: 0.018958
2021-12-13 01:36:42,020 iteration 6069 : loss : 0.045755, loss_ce: 0.005819
 89%|█████████████████████████▉   | 357/400 [2:46:39<20:18, 28.33s/it]2021-12-13 01:36:43,603 iteration 6070 : loss : 0.043547, loss_ce: 0.015247
2021-12-13 01:36:45,145 iteration 6071 : loss : 0.042837, loss_ce: 0.011529
2021-12-13 01:36:46,646 iteration 6072 : loss : 0.044576, loss_ce: 0.010944
2021-12-13 01:36:48,177 iteration 6073 : loss : 0.051255, loss_ce: 0.017097
2021-12-13 01:36:49,685 iteration 6074 : loss : 0.047220, loss_ce: 0.012813
2021-12-13 01:36:51,252 iteration 6075 : loss : 0.048158, loss_ce: 0.017725
2021-12-13 01:36:52,722 iteration 6076 : loss : 0.043426, loss_ce: 0.015731
2021-12-13 01:36:54,181 iteration 6077 : loss : 0.044166, loss_ce: 0.017357
2021-12-13 01:36:55,653 iteration 6078 : loss : 0.048328, loss_ce: 0.014455
2021-12-13 01:36:57,156 iteration 6079 : loss : 0.051109, loss_ce: 0.010182
2021-12-13 01:36:58,649 iteration 6080 : loss : 0.047589, loss_ce: 0.014805
2021-12-13 01:37:00,049 iteration 6081 : loss : 0.043408, loss_ce: 0.012748
2021-12-13 01:37:01,601 iteration 6082 : loss : 0.048274, loss_ce: 0.015989
2021-12-13 01:37:03,180 iteration 6083 : loss : 0.056984, loss_ce: 0.020178
2021-12-13 01:37:04,618 iteration 6084 : loss : 0.054668, loss_ce: 0.011949
2021-12-13 01:37:06,053 iteration 6085 : loss : 0.043231, loss_ce: 0.017388
2021-12-13 01:37:07,681 iteration 6086 : loss : 0.064213, loss_ce: 0.019767
 90%|█████████████████████████▉   | 358/400 [2:47:04<19:16, 27.53s/it]2021-12-13 01:37:09,221 iteration 6087 : loss : 0.050085, loss_ce: 0.012623
2021-12-13 01:37:10,719 iteration 6088 : loss : 0.048498, loss_ce: 0.013430
2021-12-13 01:37:12,268 iteration 6089 : loss : 0.050883, loss_ce: 0.020125
2021-12-13 01:37:13,734 iteration 6090 : loss : 0.039731, loss_ce: 0.009252
2021-12-13 01:37:15,177 iteration 6091 : loss : 0.039532, loss_ce: 0.010029
2021-12-13 01:37:16,747 iteration 6092 : loss : 0.060316, loss_ce: 0.019129
2021-12-13 01:37:18,357 iteration 6093 : loss : 0.065600, loss_ce: 0.023544
2021-12-13 01:37:19,921 iteration 6094 : loss : 0.045108, loss_ce: 0.017747
2021-12-13 01:37:21,344 iteration 6095 : loss : 0.048765, loss_ce: 0.011066
2021-12-13 01:37:22,917 iteration 6096 : loss : 0.057294, loss_ce: 0.013691
2021-12-13 01:37:24,353 iteration 6097 : loss : 0.041104, loss_ce: 0.015476
2021-12-13 01:37:25,867 iteration 6098 : loss : 0.046520, loss_ce: 0.019355
2021-12-13 01:37:27,353 iteration 6099 : loss : 0.047653, loss_ce: 0.018317
2021-12-13 01:37:28,877 iteration 6100 : loss : 0.051399, loss_ce: 0.013079
2021-12-13 01:37:30,456 iteration 6101 : loss : 0.055827, loss_ce: 0.019625
2021-12-13 01:37:31,896 iteration 6102 : loss : 0.040943, loss_ce: 0.013754
2021-12-13 01:37:33,411 iteration 6103 : loss : 0.049867, loss_ce: 0.012040
 90%|██████████████████████████   | 359/400 [2:47:30<18:26, 26.99s/it]2021-12-13 01:37:35,002 iteration 6104 : loss : 0.042433, loss_ce: 0.014324
2021-12-13 01:37:36,469 iteration 6105 : loss : 0.052516, loss_ce: 0.013166
2021-12-13 01:37:37,903 iteration 6106 : loss : 0.037370, loss_ce: 0.012346
2021-12-13 01:37:39,334 iteration 6107 : loss : 0.039129, loss_ce: 0.013283
2021-12-13 01:37:40,815 iteration 6108 : loss : 0.052489, loss_ce: 0.015300
2021-12-13 01:37:42,345 iteration 6109 : loss : 0.041492, loss_ce: 0.013518
2021-12-13 01:37:43,773 iteration 6110 : loss : 0.044095, loss_ce: 0.017131
2021-12-13 01:37:45,269 iteration 6111 : loss : 0.055003, loss_ce: 0.017453
2021-12-13 01:37:46,810 iteration 6112 : loss : 0.047525, loss_ce: 0.011758
2021-12-13 01:37:48,293 iteration 6113 : loss : 0.046274, loss_ce: 0.013073
2021-12-13 01:37:49,791 iteration 6114 : loss : 0.048840, loss_ce: 0.014733
2021-12-13 01:37:51,351 iteration 6115 : loss : 0.042378, loss_ce: 0.010735
2021-12-13 01:37:52,837 iteration 6116 : loss : 0.053523, loss_ce: 0.015295
2021-12-13 01:37:54,292 iteration 6117 : loss : 0.042147, loss_ce: 0.012853
2021-12-13 01:37:55,808 iteration 6118 : loss : 0.042225, loss_ce: 0.016591
2021-12-13 01:37:57,285 iteration 6119 : loss : 0.043790, loss_ce: 0.015722
2021-12-13 01:37:57,286 Training Data Eval:
2021-12-13 01:38:04,924   Average segmentation loss on training set: 0.0326
2021-12-13 01:38:04,924 Validation Data Eval:
2021-12-13 01:38:07,527   Average segmentation loss on validation set: 0.0877
2021-12-13 01:38:09,067 iteration 6120 : loss : 0.052209, loss_ce: 0.018701
 90%|██████████████████████████   | 360/400 [2:48:06<19:43, 29.59s/it]2021-12-13 01:38:10,638 iteration 6121 : loss : 0.046740, loss_ce: 0.014089
2021-12-13 01:38:12,145 iteration 6122 : loss : 0.048219, loss_ce: 0.016974
2021-12-13 01:38:13,624 iteration 6123 : loss : 0.046061, loss_ce: 0.014379
2021-12-13 01:38:15,199 iteration 6124 : loss : 0.049568, loss_ce: 0.012738
2021-12-13 01:38:16,699 iteration 6125 : loss : 0.042274, loss_ce: 0.010447
2021-12-13 01:38:18,313 iteration 6126 : loss : 0.044417, loss_ce: 0.014365
2021-12-13 01:38:19,926 iteration 6127 : loss : 0.054180, loss_ce: 0.022771
2021-12-13 01:38:21,417 iteration 6128 : loss : 0.044600, loss_ce: 0.013120
2021-12-13 01:38:22,862 iteration 6129 : loss : 0.045257, loss_ce: 0.012311
2021-12-13 01:38:24,378 iteration 6130 : loss : 0.047801, loss_ce: 0.015842
2021-12-13 01:38:25,827 iteration 6131 : loss : 0.044823, loss_ce: 0.011031
2021-12-13 01:38:27,381 iteration 6132 : loss : 0.043825, loss_ce: 0.010731
2021-12-13 01:38:28,920 iteration 6133 : loss : 0.045606, loss_ce: 0.015726
2021-12-13 01:38:30,390 iteration 6134 : loss : 0.040728, loss_ce: 0.011146
2021-12-13 01:38:31,820 iteration 6135 : loss : 0.042787, loss_ce: 0.017286
2021-12-13 01:38:33,270 iteration 6136 : loss : 0.037039, loss_ce: 0.009433
2021-12-13 01:38:34,718 iteration 6137 : loss : 0.047207, loss_ce: 0.013243
 90%|██████████████████████████▏  | 361/400 [2:48:31<18:27, 28.41s/it]2021-12-13 01:38:36,195 iteration 6138 : loss : 0.043788, loss_ce: 0.012231
2021-12-13 01:38:37,651 iteration 6139 : loss : 0.043361, loss_ce: 0.007570
2021-12-13 01:38:39,096 iteration 6140 : loss : 0.046161, loss_ce: 0.013292
2021-12-13 01:38:40,511 iteration 6141 : loss : 0.042720, loss_ce: 0.016420
2021-12-13 01:38:42,034 iteration 6142 : loss : 0.046124, loss_ce: 0.014085
2021-12-13 01:38:43,471 iteration 6143 : loss : 0.044160, loss_ce: 0.016755
2021-12-13 01:38:45,049 iteration 6144 : loss : 0.049207, loss_ce: 0.019214
2021-12-13 01:38:46,493 iteration 6145 : loss : 0.042228, loss_ce: 0.014058
2021-12-13 01:38:47,896 iteration 6146 : loss : 0.039142, loss_ce: 0.010084
2021-12-13 01:38:49,361 iteration 6147 : loss : 0.047822, loss_ce: 0.016404
2021-12-13 01:38:50,822 iteration 6148 : loss : 0.049172, loss_ce: 0.017328
2021-12-13 01:38:52,266 iteration 6149 : loss : 0.046006, loss_ce: 0.014045
2021-12-13 01:38:53,786 iteration 6150 : loss : 0.049915, loss_ce: 0.015085
2021-12-13 01:38:55,426 iteration 6151 : loss : 0.049226, loss_ce: 0.010965
2021-12-13 01:38:56,905 iteration 6152 : loss : 0.045029, loss_ce: 0.016454
2021-12-13 01:38:58,420 iteration 6153 : loss : 0.043557, loss_ce: 0.013892
2021-12-13 01:39:00,000 iteration 6154 : loss : 0.054994, loss_ce: 0.017446
 90%|██████████████████████████▏  | 362/400 [2:48:57<17:23, 27.47s/it]2021-12-13 01:39:01,612 iteration 6155 : loss : 0.050751, loss_ce: 0.016661
2021-12-13 01:39:03,054 iteration 6156 : loss : 0.038287, loss_ce: 0.013015
2021-12-13 01:39:04,529 iteration 6157 : loss : 0.035299, loss_ce: 0.008997
2021-12-13 01:39:06,039 iteration 6158 : loss : 0.046290, loss_ce: 0.013200
2021-12-13 01:39:07,481 iteration 6159 : loss : 0.050244, loss_ce: 0.014619
2021-12-13 01:39:08,959 iteration 6160 : loss : 0.043498, loss_ce: 0.014652
2021-12-13 01:39:10,457 iteration 6161 : loss : 0.046127, loss_ce: 0.011395
2021-12-13 01:39:11,994 iteration 6162 : loss : 0.047183, loss_ce: 0.009479
2021-12-13 01:39:13,499 iteration 6163 : loss : 0.046797, loss_ce: 0.016320
2021-12-13 01:39:15,047 iteration 6164 : loss : 0.054187, loss_ce: 0.021004
2021-12-13 01:39:16,535 iteration 6165 : loss : 0.044359, loss_ce: 0.012250
2021-12-13 01:39:18,014 iteration 6166 : loss : 0.039622, loss_ce: 0.013933
2021-12-13 01:39:19,468 iteration 6167 : loss : 0.045775, loss_ce: 0.016001
2021-12-13 01:39:21,012 iteration 6168 : loss : 0.047361, loss_ce: 0.014874
2021-12-13 01:39:22,530 iteration 6169 : loss : 0.049411, loss_ce: 0.011380
2021-12-13 01:39:23,999 iteration 6170 : loss : 0.046951, loss_ce: 0.014796
2021-12-13 01:39:25,447 iteration 6171 : loss : 0.041150, loss_ce: 0.012704
 91%|██████████████████████████▎  | 363/400 [2:49:22<16:33, 26.86s/it]2021-12-13 01:39:26,945 iteration 6172 : loss : 0.042914, loss_ce: 0.012509
2021-12-13 01:39:28,522 iteration 6173 : loss : 0.049249, loss_ce: 0.015484
2021-12-13 01:39:30,007 iteration 6174 : loss : 0.041269, loss_ce: 0.011469
2021-12-13 01:39:31,553 iteration 6175 : loss : 0.054125, loss_ce: 0.019204
2021-12-13 01:39:33,062 iteration 6176 : loss : 0.043807, loss_ce: 0.013150
2021-12-13 01:39:34,655 iteration 6177 : loss : 0.056374, loss_ce: 0.019816
2021-12-13 01:39:36,154 iteration 6178 : loss : 0.041587, loss_ce: 0.013776
2021-12-13 01:39:37,650 iteration 6179 : loss : 0.042862, loss_ce: 0.016857
2021-12-13 01:39:39,147 iteration 6180 : loss : 0.041897, loss_ce: 0.012879
2021-12-13 01:39:40,534 iteration 6181 : loss : 0.039329, loss_ce: 0.011013
2021-12-13 01:39:42,035 iteration 6182 : loss : 0.046218, loss_ce: 0.010022
2021-12-13 01:39:43,610 iteration 6183 : loss : 0.045860, loss_ce: 0.017170
2021-12-13 01:39:45,130 iteration 6184 : loss : 0.047154, loss_ce: 0.013345
2021-12-13 01:39:46,608 iteration 6185 : loss : 0.057556, loss_ce: 0.017647
2021-12-13 01:39:48,114 iteration 6186 : loss : 0.045428, loss_ce: 0.013003
2021-12-13 01:39:49,674 iteration 6187 : loss : 0.043644, loss_ce: 0.013061
2021-12-13 01:39:51,071 iteration 6188 : loss : 0.039968, loss_ce: 0.010957
 91%|██████████████████████████▍  | 364/400 [2:49:48<15:53, 26.49s/it]2021-12-13 01:39:52,525 iteration 6189 : loss : 0.038881, loss_ce: 0.009699
2021-12-13 01:39:53,912 iteration 6190 : loss : 0.043526, loss_ce: 0.012212
2021-12-13 01:39:55,541 iteration 6191 : loss : 0.052527, loss_ce: 0.016981
2021-12-13 01:39:57,082 iteration 6192 : loss : 0.045047, loss_ce: 0.014399
2021-12-13 01:39:58,449 iteration 6193 : loss : 0.040220, loss_ce: 0.012767
2021-12-13 01:39:59,928 iteration 6194 : loss : 0.048550, loss_ce: 0.015134
2021-12-13 01:40:01,456 iteration 6195 : loss : 0.040574, loss_ce: 0.013968
2021-12-13 01:40:02,991 iteration 6196 : loss : 0.041790, loss_ce: 0.013788
2021-12-13 01:40:04,500 iteration 6197 : loss : 0.045183, loss_ce: 0.017780
2021-12-13 01:40:05,992 iteration 6198 : loss : 0.049415, loss_ce: 0.014590
2021-12-13 01:40:07,481 iteration 6199 : loss : 0.045549, loss_ce: 0.010237
2021-12-13 01:40:08,883 iteration 6200 : loss : 0.038947, loss_ce: 0.014737
2021-12-13 01:40:10,378 iteration 6201 : loss : 0.057557, loss_ce: 0.016406
2021-12-13 01:40:11,819 iteration 6202 : loss : 0.041285, loss_ce: 0.014716
2021-12-13 01:40:13,268 iteration 6203 : loss : 0.041525, loss_ce: 0.008729
2021-12-13 01:40:14,784 iteration 6204 : loss : 0.041882, loss_ce: 0.014356
2021-12-13 01:40:14,784 Training Data Eval:
2021-12-13 01:40:22,442   Average segmentation loss on training set: 0.0333
2021-12-13 01:40:22,443 Validation Data Eval:
2021-12-13 01:40:25,052   Average segmentation loss on validation set: 0.0898
2021-12-13 01:40:26,553 iteration 6205 : loss : 0.044758, loss_ce: 0.011297
 91%|██████████████████████████▍  | 365/400 [2:50:23<17:01, 29.19s/it]2021-12-13 01:40:28,104 iteration 6206 : loss : 0.047825, loss_ce: 0.018616
2021-12-13 01:40:29,562 iteration 6207 : loss : 0.046501, loss_ce: 0.012159
2021-12-13 01:40:31,138 iteration 6208 : loss : 0.053282, loss_ce: 0.014190
2021-12-13 01:40:32,606 iteration 6209 : loss : 0.052098, loss_ce: 0.013033
2021-12-13 01:40:34,042 iteration 6210 : loss : 0.045462, loss_ce: 0.015075
2021-12-13 01:40:35,568 iteration 6211 : loss : 0.046229, loss_ce: 0.016329
2021-12-13 01:40:37,024 iteration 6212 : loss : 0.043922, loss_ce: 0.012227
2021-12-13 01:40:38,478 iteration 6213 : loss : 0.045674, loss_ce: 0.009863
2021-12-13 01:40:39,936 iteration 6214 : loss : 0.049511, loss_ce: 0.009657
2021-12-13 01:40:41,455 iteration 6215 : loss : 0.046921, loss_ce: 0.019740
2021-12-13 01:40:42,949 iteration 6216 : loss : 0.043622, loss_ce: 0.012180
2021-12-13 01:40:44,347 iteration 6217 : loss : 0.041208, loss_ce: 0.011731
2021-12-13 01:40:45,835 iteration 6218 : loss : 0.038051, loss_ce: 0.012932
2021-12-13 01:40:47,289 iteration 6219 : loss : 0.042084, loss_ce: 0.013567
2021-12-13 01:40:48,764 iteration 6220 : loss : 0.044809, loss_ce: 0.011809
2021-12-13 01:40:50,342 iteration 6221 : loss : 0.045255, loss_ce: 0.016555
2021-12-13 01:40:51,847 iteration 6222 : loss : 0.048997, loss_ce: 0.019042
 92%|██████████████████████████▌  | 366/400 [2:50:49<15:52, 28.02s/it]2021-12-13 01:40:53,404 iteration 6223 : loss : 0.059380, loss_ce: 0.012125
2021-12-13 01:40:54,869 iteration 6224 : loss : 0.044669, loss_ce: 0.011160
2021-12-13 01:40:56,344 iteration 6225 : loss : 0.061348, loss_ce: 0.021255
2021-12-13 01:40:57,912 iteration 6226 : loss : 0.050331, loss_ce: 0.017146
2021-12-13 01:40:59,399 iteration 6227 : loss : 0.041013, loss_ce: 0.009248
2021-12-13 01:41:00,912 iteration 6228 : loss : 0.050840, loss_ce: 0.016250
2021-12-13 01:41:02,328 iteration 6229 : loss : 0.038705, loss_ce: 0.013497
2021-12-13 01:41:03,770 iteration 6230 : loss : 0.046251, loss_ce: 0.015934
2021-12-13 01:41:05,268 iteration 6231 : loss : 0.042804, loss_ce: 0.015090
2021-12-13 01:41:06,770 iteration 6232 : loss : 0.041175, loss_ce: 0.014083
2021-12-13 01:41:08,240 iteration 6233 : loss : 0.046571, loss_ce: 0.017531
2021-12-13 01:41:09,749 iteration 6234 : loss : 0.042369, loss_ce: 0.012639
2021-12-13 01:41:11,328 iteration 6235 : loss : 0.058662, loss_ce: 0.012861
2021-12-13 01:41:12,727 iteration 6236 : loss : 0.038149, loss_ce: 0.012077
2021-12-13 01:41:14,198 iteration 6237 : loss : 0.044452, loss_ce: 0.015630
2021-12-13 01:41:15,615 iteration 6238 : loss : 0.035818, loss_ce: 0.009551
2021-12-13 01:41:17,057 iteration 6239 : loss : 0.043712, loss_ce: 0.015686
 92%|██████████████████████████▌  | 367/400 [2:51:14<14:56, 27.18s/it]2021-12-13 01:41:18,634 iteration 6240 : loss : 0.041810, loss_ce: 0.013076
2021-12-13 01:41:20,170 iteration 6241 : loss : 0.054059, loss_ce: 0.019965
2021-12-13 01:41:21,713 iteration 6242 : loss : 0.043395, loss_ce: 0.014475
2021-12-13 01:41:23,121 iteration 6243 : loss : 0.039561, loss_ce: 0.012034
2021-12-13 01:41:24,562 iteration 6244 : loss : 0.042536, loss_ce: 0.010956
2021-12-13 01:41:26,024 iteration 6245 : loss : 0.043951, loss_ce: 0.012855
2021-12-13 01:41:27,469 iteration 6246 : loss : 0.041823, loss_ce: 0.013747
2021-12-13 01:41:28,912 iteration 6247 : loss : 0.048385, loss_ce: 0.009792
2021-12-13 01:41:30,455 iteration 6248 : loss : 0.045377, loss_ce: 0.016602
2021-12-13 01:41:31,916 iteration 6249 : loss : 0.049148, loss_ce: 0.014543
2021-12-13 01:41:33,430 iteration 6250 : loss : 0.048874, loss_ce: 0.016550
2021-12-13 01:41:35,019 iteration 6251 : loss : 0.046455, loss_ce: 0.013367
2021-12-13 01:41:36,554 iteration 6252 : loss : 0.065453, loss_ce: 0.012150
2021-12-13 01:41:38,007 iteration 6253 : loss : 0.050432, loss_ce: 0.011154
2021-12-13 01:41:39,416 iteration 6254 : loss : 0.038836, loss_ce: 0.013960
2021-12-13 01:41:40,966 iteration 6255 : loss : 0.046062, loss_ce: 0.013775
2021-12-13 01:41:42,454 iteration 6256 : loss : 0.044481, loss_ce: 0.013609
 92%|██████████████████████████▋  | 368/400 [2:51:39<14:12, 26.64s/it]2021-12-13 01:41:44,009 iteration 6257 : loss : 0.038899, loss_ce: 0.010742
2021-12-13 01:41:45,439 iteration 6258 : loss : 0.040785, loss_ce: 0.011897
2021-12-13 01:41:46,990 iteration 6259 : loss : 0.045237, loss_ce: 0.012565
2021-12-13 01:41:48,619 iteration 6260 : loss : 0.071700, loss_ce: 0.018360
2021-12-13 01:41:50,108 iteration 6261 : loss : 0.048132, loss_ce: 0.017470
2021-12-13 01:41:51,664 iteration 6262 : loss : 0.045480, loss_ce: 0.013347
2021-12-13 01:41:53,121 iteration 6263 : loss : 0.051639, loss_ce: 0.015632
2021-12-13 01:41:54,550 iteration 6264 : loss : 0.049674, loss_ce: 0.017898
2021-12-13 01:41:56,044 iteration 6265 : loss : 0.049876, loss_ce: 0.016394
2021-12-13 01:41:57,506 iteration 6266 : loss : 0.038703, loss_ce: 0.010688
2021-12-13 01:41:59,186 iteration 6267 : loss : 0.057615, loss_ce: 0.019702
2021-12-13 01:42:00,673 iteration 6268 : loss : 0.044168, loss_ce: 0.012926
2021-12-13 01:42:02,176 iteration 6269 : loss : 0.042492, loss_ce: 0.013035
2021-12-13 01:42:03,644 iteration 6270 : loss : 0.046520, loss_ce: 0.016301
2021-12-13 01:42:05,220 iteration 6271 : loss : 0.044887, loss_ce: 0.015765
2021-12-13 01:42:06,667 iteration 6272 : loss : 0.045543, loss_ce: 0.012390
2021-12-13 01:42:08,257 iteration 6273 : loss : 0.059576, loss_ce: 0.019808
 92%|██████████████████████████▊  | 369/400 [2:52:05<13:38, 26.39s/it]2021-12-13 01:42:09,905 iteration 6274 : loss : 0.048800, loss_ce: 0.013815
2021-12-13 01:42:11,372 iteration 6275 : loss : 0.043689, loss_ce: 0.010712
2021-12-13 01:42:12,792 iteration 6276 : loss : 0.044775, loss_ce: 0.016487
2021-12-13 01:42:14,288 iteration 6277 : loss : 0.040909, loss_ce: 0.012649
2021-12-13 01:42:15,854 iteration 6278 : loss : 0.048950, loss_ce: 0.015603
2021-12-13 01:42:17,417 iteration 6279 : loss : 0.050284, loss_ce: 0.015428
2021-12-13 01:42:18,851 iteration 6280 : loss : 0.046409, loss_ce: 0.013545
2021-12-13 01:42:20,318 iteration 6281 : loss : 0.049681, loss_ce: 0.012762
2021-12-13 01:42:21,806 iteration 6282 : loss : 0.051692, loss_ce: 0.020594
2021-12-13 01:42:23,394 iteration 6283 : loss : 0.042682, loss_ce: 0.013838
2021-12-13 01:42:24,922 iteration 6284 : loss : 0.040451, loss_ce: 0.011426
2021-12-13 01:42:26,442 iteration 6285 : loss : 0.050228, loss_ce: 0.017723
2021-12-13 01:42:28,004 iteration 6286 : loss : 0.054155, loss_ce: 0.023889
2021-12-13 01:42:29,500 iteration 6287 : loss : 0.048228, loss_ce: 0.014457
2021-12-13 01:42:30,909 iteration 6288 : loss : 0.043972, loss_ce: 0.012471
2021-12-13 01:42:32,374 iteration 6289 : loss : 0.044571, loss_ce: 0.015029
2021-12-13 01:42:32,375 Training Data Eval:
2021-12-13 01:42:40,038   Average segmentation loss on training set: 0.0332
2021-12-13 01:42:40,038 Validation Data Eval:
2021-12-13 01:42:42,650   Average segmentation loss on validation set: 0.0868
2021-12-13 01:42:44,129 iteration 6290 : loss : 0.041402, loss_ce: 0.012884
 92%|██████████████████████████▊  | 370/400 [2:52:41<14:37, 29.23s/it]2021-12-13 01:42:45,632 iteration 6291 : loss : 0.051072, loss_ce: 0.013436
2021-12-13 01:42:47,020 iteration 6292 : loss : 0.042271, loss_ce: 0.010863
2021-12-13 01:42:48,458 iteration 6293 : loss : 0.042445, loss_ce: 0.008961
2021-12-13 01:42:50,038 iteration 6294 : loss : 0.058494, loss_ce: 0.015039
2021-12-13 01:42:51,525 iteration 6295 : loss : 0.038756, loss_ce: 0.011354
2021-12-13 01:42:52,930 iteration 6296 : loss : 0.042199, loss_ce: 0.014766
2021-12-13 01:42:54,480 iteration 6297 : loss : 0.049862, loss_ce: 0.017088
2021-12-13 01:42:55,961 iteration 6298 : loss : 0.048664, loss_ce: 0.021222
2021-12-13 01:42:57,438 iteration 6299 : loss : 0.049123, loss_ce: 0.015103
2021-12-13 01:42:58,966 iteration 6300 : loss : 0.045968, loss_ce: 0.010983
2021-12-13 01:43:00,471 iteration 6301 : loss : 0.038842, loss_ce: 0.014534
2021-12-13 01:43:02,038 iteration 6302 : loss : 0.047357, loss_ce: 0.014789
2021-12-13 01:43:03,491 iteration 6303 : loss : 0.040920, loss_ce: 0.013186
2021-12-13 01:43:04,998 iteration 6304 : loss : 0.051427, loss_ce: 0.016864
2021-12-13 01:43:06,454 iteration 6305 : loss : 0.043955, loss_ce: 0.011983
2021-12-13 01:43:07,943 iteration 6306 : loss : 0.045683, loss_ce: 0.015826
2021-12-13 01:43:09,403 iteration 6307 : loss : 0.041205, loss_ce: 0.011753
 93%|██████████████████████████▉  | 371/400 [2:53:06<13:33, 28.05s/it]2021-12-13 01:43:10,969 iteration 6308 : loss : 0.044795, loss_ce: 0.015084
2021-12-13 01:43:12,429 iteration 6309 : loss : 0.044590, loss_ce: 0.011339
2021-12-13 01:43:13,915 iteration 6310 : loss : 0.041672, loss_ce: 0.012583
2021-12-13 01:43:15,468 iteration 6311 : loss : 0.052899, loss_ce: 0.013418
2021-12-13 01:43:16,932 iteration 6312 : loss : 0.043775, loss_ce: 0.016356
2021-12-13 01:43:18,438 iteration 6313 : loss : 0.048378, loss_ce: 0.010976
2021-12-13 01:43:19,968 iteration 6314 : loss : 0.042375, loss_ce: 0.013351
2021-12-13 01:43:21,434 iteration 6315 : loss : 0.054917, loss_ce: 0.020275
2021-12-13 01:43:22,848 iteration 6316 : loss : 0.041294, loss_ce: 0.010002
2021-12-13 01:43:24,245 iteration 6317 : loss : 0.043755, loss_ce: 0.012769
2021-12-13 01:43:25,774 iteration 6318 : loss : 0.053717, loss_ce: 0.017146
2021-12-13 01:43:27,234 iteration 6319 : loss : 0.048140, loss_ce: 0.014979
2021-12-13 01:43:28,718 iteration 6320 : loss : 0.040685, loss_ce: 0.015103
2021-12-13 01:43:30,193 iteration 6321 : loss : 0.045544, loss_ce: 0.015384
2021-12-13 01:43:31,766 iteration 6322 : loss : 0.050737, loss_ce: 0.015101
2021-12-13 01:43:33,361 iteration 6323 : loss : 0.058748, loss_ce: 0.013208
2021-12-13 01:43:34,830 iteration 6324 : loss : 0.043065, loss_ce: 0.014216
 93%|██████████████████████████▉  | 372/400 [2:53:32<12:43, 27.26s/it]2021-12-13 01:43:36,305 iteration 6325 : loss : 0.043105, loss_ce: 0.015299
2021-12-13 01:43:37,718 iteration 6326 : loss : 0.037760, loss_ce: 0.012277
2021-12-13 01:43:39,260 iteration 6327 : loss : 0.042805, loss_ce: 0.014946
2021-12-13 01:43:40,729 iteration 6328 : loss : 0.038307, loss_ce: 0.008911
2021-12-13 01:43:42,154 iteration 6329 : loss : 0.044365, loss_ce: 0.011394
2021-12-13 01:43:43,636 iteration 6330 : loss : 0.038321, loss_ce: 0.010385
2021-12-13 01:43:45,328 iteration 6331 : loss : 0.049331, loss_ce: 0.013662
2021-12-13 01:43:46,780 iteration 6332 : loss : 0.044114, loss_ce: 0.015429
2021-12-13 01:43:48,296 iteration 6333 : loss : 0.042024, loss_ce: 0.010848
2021-12-13 01:43:49,780 iteration 6334 : loss : 0.044443, loss_ce: 0.015346
2021-12-13 01:43:51,376 iteration 6335 : loss : 0.048823, loss_ce: 0.015922
2021-12-13 01:43:52,790 iteration 6336 : loss : 0.037327, loss_ce: 0.012389
2021-12-13 01:43:54,300 iteration 6337 : loss : 0.048279, loss_ce: 0.017055
2021-12-13 01:43:55,885 iteration 6338 : loss : 0.060747, loss_ce: 0.008370
2021-12-13 01:43:57,350 iteration 6339 : loss : 0.052377, loss_ce: 0.020921
2021-12-13 01:43:58,799 iteration 6340 : loss : 0.038825, loss_ce: 0.014412
2021-12-13 01:44:00,308 iteration 6341 : loss : 0.044354, loss_ce: 0.013083
 93%|███████████████████████████  | 373/400 [2:53:57<12:01, 26.73s/it]2021-12-13 01:44:01,845 iteration 6342 : loss : 0.051427, loss_ce: 0.016491
2021-12-13 01:44:03,311 iteration 6343 : loss : 0.049967, loss_ce: 0.012757
2021-12-13 01:44:04,881 iteration 6344 : loss : 0.062290, loss_ce: 0.023143
2021-12-13 01:44:06,367 iteration 6345 : loss : 0.035549, loss_ce: 0.008465
2021-12-13 01:44:07,814 iteration 6346 : loss : 0.040918, loss_ce: 0.013902
2021-12-13 01:44:09,342 iteration 6347 : loss : 0.049831, loss_ce: 0.019597
2021-12-13 01:44:10,790 iteration 6348 : loss : 0.043253, loss_ce: 0.014744
2021-12-13 01:44:12,266 iteration 6349 : loss : 0.047989, loss_ce: 0.012421
2021-12-13 01:44:13,750 iteration 6350 : loss : 0.043227, loss_ce: 0.013714
2021-12-13 01:44:15,259 iteration 6351 : loss : 0.041854, loss_ce: 0.015819
2021-12-13 01:44:16,799 iteration 6352 : loss : 0.042381, loss_ce: 0.012263
2021-12-13 01:44:18,415 iteration 6353 : loss : 0.064008, loss_ce: 0.022202
2021-12-13 01:44:19,880 iteration 6354 : loss : 0.047108, loss_ce: 0.010829
2021-12-13 01:44:21,453 iteration 6355 : loss : 0.044916, loss_ce: 0.014469
2021-12-13 01:44:23,050 iteration 6356 : loss : 0.054586, loss_ce: 0.018581
2021-12-13 01:44:24,576 iteration 6357 : loss : 0.047029, loss_ce: 0.018648
2021-12-13 01:44:26,139 iteration 6358 : loss : 0.043845, loss_ce: 0.012000
 94%|███████████████████████████  | 374/400 [2:54:23<11:27, 26.46s/it]2021-12-13 01:44:27,675 iteration 6359 : loss : 0.048169, loss_ce: 0.014259
2021-12-13 01:44:29,048 iteration 6360 : loss : 0.036054, loss_ce: 0.010154
2021-12-13 01:44:30,614 iteration 6361 : loss : 0.054839, loss_ce: 0.016533
2021-12-13 01:44:32,115 iteration 6362 : loss : 0.044712, loss_ce: 0.015478
2021-12-13 01:44:33,710 iteration 6363 : loss : 0.058228, loss_ce: 0.014950
2021-12-13 01:44:35,256 iteration 6364 : loss : 0.041119, loss_ce: 0.009671
2021-12-13 01:44:36,750 iteration 6365 : loss : 0.047146, loss_ce: 0.016644
2021-12-13 01:44:38,235 iteration 6366 : loss : 0.048935, loss_ce: 0.015571
2021-12-13 01:44:39,796 iteration 6367 : loss : 0.052398, loss_ce: 0.019384
2021-12-13 01:44:41,327 iteration 6368 : loss : 0.049817, loss_ce: 0.012302
2021-12-13 01:44:42,819 iteration 6369 : loss : 0.049583, loss_ce: 0.011048
2021-12-13 01:44:44,341 iteration 6370 : loss : 0.049211, loss_ce: 0.018120
2021-12-13 01:44:45,778 iteration 6371 : loss : 0.039596, loss_ce: 0.013514
2021-12-13 01:44:47,291 iteration 6372 : loss : 0.047871, loss_ce: 0.016322
2021-12-13 01:44:48,730 iteration 6373 : loss : 0.040286, loss_ce: 0.014498
2021-12-13 01:44:50,120 iteration 6374 : loss : 0.036363, loss_ce: 0.011080
2021-12-13 01:44:50,120 Training Data Eval:
2021-12-13 01:44:57,776   Average segmentation loss on training set: 0.0332
2021-12-13 01:44:57,776 Validation Data Eval:
2021-12-13 01:45:00,383   Average segmentation loss on validation set: 0.0870
2021-12-13 01:45:01,905 iteration 6375 : loss : 0.046928, loss_ce: 0.020707
 94%|███████████████████████████▏ | 375/400 [2:54:59<12:11, 29.25s/it]2021-12-13 01:45:03,513 iteration 6376 : loss : 0.050875, loss_ce: 0.016026
2021-12-13 01:45:05,028 iteration 6377 : loss : 0.049958, loss_ce: 0.011821
2021-12-13 01:45:06,497 iteration 6378 : loss : 0.048082, loss_ce: 0.018431
2021-12-13 01:45:07,993 iteration 6379 : loss : 0.042989, loss_ce: 0.015591
2021-12-13 01:45:09,545 iteration 6380 : loss : 0.047484, loss_ce: 0.013237
2021-12-13 01:45:11,110 iteration 6381 : loss : 0.056168, loss_ce: 0.021361
2021-12-13 01:45:12,608 iteration 6382 : loss : 0.049213, loss_ce: 0.021235
2021-12-13 01:45:14,151 iteration 6383 : loss : 0.050917, loss_ce: 0.015787
2021-12-13 01:45:15,708 iteration 6384 : loss : 0.053383, loss_ce: 0.017473
2021-12-13 01:45:17,059 iteration 6385 : loss : 0.034517, loss_ce: 0.009511
2021-12-13 01:45:18,598 iteration 6386 : loss : 0.054190, loss_ce: 0.015569
2021-12-13 01:45:20,128 iteration 6387 : loss : 0.050845, loss_ce: 0.019679
2021-12-13 01:45:21,543 iteration 6388 : loss : 0.043540, loss_ce: 0.016800
2021-12-13 01:45:23,085 iteration 6389 : loss : 0.040414, loss_ce: 0.012952
2021-12-13 01:45:24,604 iteration 6390 : loss : 0.044857, loss_ce: 0.011029
2021-12-13 01:45:26,145 iteration 6391 : loss : 0.044413, loss_ce: 0.014197
2021-12-13 01:45:27,705 iteration 6392 : loss : 0.073715, loss_ce: 0.017082
 94%|███████████████████████████▎ | 376/400 [2:55:24<11:17, 28.21s/it]2021-12-13 01:45:29,196 iteration 6393 : loss : 0.040244, loss_ce: 0.014653
2021-12-13 01:45:30,666 iteration 6394 : loss : 0.041156, loss_ce: 0.014535
2021-12-13 01:45:32,110 iteration 6395 : loss : 0.037400, loss_ce: 0.009801
2021-12-13 01:45:33,636 iteration 6396 : loss : 0.046824, loss_ce: 0.014500
2021-12-13 01:45:35,176 iteration 6397 : loss : 0.061384, loss_ce: 0.016629
2021-12-13 01:45:36,731 iteration 6398 : loss : 0.042221, loss_ce: 0.012123
2021-12-13 01:45:38,150 iteration 6399 : loss : 0.044854, loss_ce: 0.011751
2021-12-13 01:45:39,611 iteration 6400 : loss : 0.041534, loss_ce: 0.013536
2021-12-13 01:45:41,017 iteration 6401 : loss : 0.039058, loss_ce: 0.012317
2021-12-13 01:45:42,547 iteration 6402 : loss : 0.048597, loss_ce: 0.017116
2021-12-13 01:45:43,994 iteration 6403 : loss : 0.040186, loss_ce: 0.012828
2021-12-13 01:45:45,411 iteration 6404 : loss : 0.041430, loss_ce: 0.015875
2021-12-13 01:45:47,045 iteration 6405 : loss : 0.059087, loss_ce: 0.020329
2021-12-13 01:45:48,472 iteration 6406 : loss : 0.039808, loss_ce: 0.013176
2021-12-13 01:45:49,988 iteration 6407 : loss : 0.050555, loss_ce: 0.016138
2021-12-13 01:45:51,521 iteration 6408 : loss : 0.049790, loss_ce: 0.016061
2021-12-13 01:45:52,954 iteration 6409 : loss : 0.037721, loss_ce: 0.010770
 94%|███████████████████████████▎ | 377/400 [2:55:50<10:28, 27.33s/it]2021-12-13 01:45:54,414 iteration 6410 : loss : 0.042468, loss_ce: 0.012369
2021-12-13 01:45:55,878 iteration 6411 : loss : 0.039986, loss_ce: 0.011730
2021-12-13 01:45:57,438 iteration 6412 : loss : 0.048212, loss_ce: 0.011767
2021-12-13 01:45:58,934 iteration 6413 : loss : 0.051242, loss_ce: 0.018604
2021-12-13 01:46:00,476 iteration 6414 : loss : 0.045857, loss_ce: 0.013210
2021-12-13 01:46:02,016 iteration 6415 : loss : 0.039182, loss_ce: 0.009381
2021-12-13 01:46:03,452 iteration 6416 : loss : 0.043433, loss_ce: 0.012745
2021-12-13 01:46:04,989 iteration 6417 : loss : 0.049051, loss_ce: 0.013427
2021-12-13 01:46:06,483 iteration 6418 : loss : 0.043888, loss_ce: 0.015769
2021-12-13 01:46:07,972 iteration 6419 : loss : 0.044827, loss_ce: 0.015006
2021-12-13 01:46:09,495 iteration 6420 : loss : 0.051634, loss_ce: 0.015410
2021-12-13 01:46:11,039 iteration 6421 : loss : 0.041306, loss_ce: 0.012587
2021-12-13 01:46:12,519 iteration 6422 : loss : 0.043308, loss_ce: 0.014696
2021-12-13 01:46:14,024 iteration 6423 : loss : 0.039387, loss_ce: 0.012565
2021-12-13 01:46:15,462 iteration 6424 : loss : 0.040198, loss_ce: 0.010784
2021-12-13 01:46:17,102 iteration 6425 : loss : 0.045769, loss_ce: 0.013309
2021-12-13 01:46:18,533 iteration 6426 : loss : 0.046715, loss_ce: 0.012910
 94%|███████████████████████████▍ | 378/400 [2:56:15<09:49, 26.80s/it]2021-12-13 01:46:20,094 iteration 6427 : loss : 0.047004, loss_ce: 0.016417
2021-12-13 01:46:21,639 iteration 6428 : loss : 0.047490, loss_ce: 0.012229
2021-12-13 01:46:23,123 iteration 6429 : loss : 0.046219, loss_ce: 0.016878
2021-12-13 01:46:24,690 iteration 6430 : loss : 0.042478, loss_ce: 0.014416
2021-12-13 01:46:26,179 iteration 6431 : loss : 0.045280, loss_ce: 0.010424
2021-12-13 01:46:27,669 iteration 6432 : loss : 0.050582, loss_ce: 0.014792
2021-12-13 01:46:29,165 iteration 6433 : loss : 0.041202, loss_ce: 0.009773
2021-12-13 01:46:30,662 iteration 6434 : loss : 0.057327, loss_ce: 0.019767
2021-12-13 01:46:32,044 iteration 6435 : loss : 0.039736, loss_ce: 0.011541
2021-12-13 01:46:33,471 iteration 6436 : loss : 0.038955, loss_ce: 0.015194
2021-12-13 01:46:34,876 iteration 6437 : loss : 0.040266, loss_ce: 0.011790
2021-12-13 01:46:36,270 iteration 6438 : loss : 0.040255, loss_ce: 0.014647
2021-12-13 01:46:37,815 iteration 6439 : loss : 0.054499, loss_ce: 0.015112
2021-12-13 01:46:39,311 iteration 6440 : loss : 0.046343, loss_ce: 0.014168
2021-12-13 01:46:40,813 iteration 6441 : loss : 0.038914, loss_ce: 0.012493
2021-12-13 01:46:42,297 iteration 6442 : loss : 0.037970, loss_ce: 0.012517
2021-12-13 01:46:43,752 iteration 6443 : loss : 0.045025, loss_ce: 0.012890
 95%|███████████████████████████▍ | 379/400 [2:56:41<09:12, 26.33s/it]2021-12-13 01:46:45,246 iteration 6444 : loss : 0.047267, loss_ce: 0.018376
2021-12-13 01:46:46,688 iteration 6445 : loss : 0.039031, loss_ce: 0.011990
2021-12-13 01:46:48,168 iteration 6446 : loss : 0.042788, loss_ce: 0.011505
2021-12-13 01:46:49,648 iteration 6447 : loss : 0.042052, loss_ce: 0.015316
2021-12-13 01:46:51,127 iteration 6448 : loss : 0.047518, loss_ce: 0.009611
2021-12-13 01:46:52,637 iteration 6449 : loss : 0.050489, loss_ce: 0.014841
2021-12-13 01:46:54,190 iteration 6450 : loss : 0.055131, loss_ce: 0.013279
2021-12-13 01:46:55,660 iteration 6451 : loss : 0.048846, loss_ce: 0.014394
2021-12-13 01:46:57,198 iteration 6452 : loss : 0.063562, loss_ce: 0.013029
2021-12-13 01:46:58,662 iteration 6453 : loss : 0.042521, loss_ce: 0.014079
2021-12-13 01:47:00,153 iteration 6454 : loss : 0.040414, loss_ce: 0.011657
2021-12-13 01:47:01,584 iteration 6455 : loss : 0.040264, loss_ce: 0.012653
2021-12-13 01:47:03,087 iteration 6456 : loss : 0.040742, loss_ce: 0.015096
2021-12-13 01:47:04,549 iteration 6457 : loss : 0.043232, loss_ce: 0.014817
2021-12-13 01:47:05,983 iteration 6458 : loss : 0.039272, loss_ce: 0.016154
2021-12-13 01:47:07,530 iteration 6459 : loss : 0.052895, loss_ce: 0.017565
2021-12-13 01:47:07,530 Training Data Eval:
2021-12-13 01:47:15,182   Average segmentation loss on training set: 0.0329
2021-12-13 01:47:15,183 Validation Data Eval:
2021-12-13 01:47:17,794   Average segmentation loss on validation set: 0.0851
2021-12-13 01:47:19,364 iteration 6460 : loss : 0.051076, loss_ce: 0.013435
 95%|███████████████████████████▌ | 380/400 [2:57:16<09:42, 29.11s/it]2021-12-13 01:47:20,871 iteration 6461 : loss : 0.044116, loss_ce: 0.012599
2021-12-13 01:47:22,363 iteration 6462 : loss : 0.044870, loss_ce: 0.014915
2021-12-13 01:47:23,791 iteration 6463 : loss : 0.040775, loss_ce: 0.012978
2021-12-13 01:47:25,252 iteration 6464 : loss : 0.044697, loss_ce: 0.013696
2021-12-13 01:47:26,729 iteration 6465 : loss : 0.039393, loss_ce: 0.010583
2021-12-13 01:47:28,204 iteration 6466 : loss : 0.048483, loss_ce: 0.017461
2021-12-13 01:47:29,691 iteration 6467 : loss : 0.046592, loss_ce: 0.018453
2021-12-13 01:47:31,116 iteration 6468 : loss : 0.037454, loss_ce: 0.009246
2021-12-13 01:47:32,652 iteration 6469 : loss : 0.052354, loss_ce: 0.013631
2021-12-13 01:47:34,169 iteration 6470 : loss : 0.042092, loss_ce: 0.014642
2021-12-13 01:47:35,673 iteration 6471 : loss : 0.048540, loss_ce: 0.017181
2021-12-13 01:47:37,101 iteration 6472 : loss : 0.047083, loss_ce: 0.014108
2021-12-13 01:47:38,624 iteration 6473 : loss : 0.046594, loss_ce: 0.014136
2021-12-13 01:47:40,135 iteration 6474 : loss : 0.045020, loss_ce: 0.014111
2021-12-13 01:47:41,674 iteration 6475 : loss : 0.046884, loss_ce: 0.013269
2021-12-13 01:47:43,194 iteration 6476 : loss : 0.050058, loss_ce: 0.015484
2021-12-13 01:47:44,697 iteration 6477 : loss : 0.042725, loss_ce: 0.013458
 95%|███████████████████████████▌ | 381/400 [2:57:41<08:51, 27.98s/it]2021-12-13 01:47:46,227 iteration 6478 : loss : 0.042908, loss_ce: 0.014333
2021-12-13 01:47:47,722 iteration 6479 : loss : 0.044911, loss_ce: 0.012870
2021-12-13 01:47:49,180 iteration 6480 : loss : 0.042325, loss_ce: 0.014127
2021-12-13 01:47:50,816 iteration 6481 : loss : 0.055800, loss_ce: 0.018089
2021-12-13 01:47:52,280 iteration 6482 : loss : 0.043189, loss_ce: 0.014479
2021-12-13 01:47:53,834 iteration 6483 : loss : 0.044118, loss_ce: 0.014569
2021-12-13 01:47:55,228 iteration 6484 : loss : 0.042329, loss_ce: 0.012299
2021-12-13 01:47:56,719 iteration 6485 : loss : 0.042251, loss_ce: 0.011100
2021-12-13 01:47:58,233 iteration 6486 : loss : 0.051102, loss_ce: 0.015770
2021-12-13 01:47:59,763 iteration 6487 : loss : 0.050956, loss_ce: 0.016190
2021-12-13 01:48:01,306 iteration 6488 : loss : 0.039039, loss_ce: 0.010504
2021-12-13 01:48:02,802 iteration 6489 : loss : 0.046158, loss_ce: 0.011361
2021-12-13 01:48:04,289 iteration 6490 : loss : 0.038607, loss_ce: 0.011814
2021-12-13 01:48:05,730 iteration 6491 : loss : 0.045271, loss_ce: 0.017790
2021-12-13 01:48:07,167 iteration 6492 : loss : 0.039082, loss_ce: 0.014876
2021-12-13 01:48:08,635 iteration 6493 : loss : 0.050245, loss_ce: 0.016337
2021-12-13 01:48:10,145 iteration 6494 : loss : 0.042494, loss_ce: 0.014444
 96%|███████████████████████████▋ | 382/400 [2:58:07<08:09, 27.22s/it]2021-12-13 01:48:11,604 iteration 6495 : loss : 0.044287, loss_ce: 0.014917
2021-12-13 01:48:13,088 iteration 6496 : loss : 0.047530, loss_ce: 0.014306
2021-12-13 01:48:14,616 iteration 6497 : loss : 0.035572, loss_ce: 0.012120
2021-12-13 01:48:16,065 iteration 6498 : loss : 0.053337, loss_ce: 0.011520
2021-12-13 01:48:17,543 iteration 6499 : loss : 0.046747, loss_ce: 0.009448
2021-12-13 01:48:19,140 iteration 6500 : loss : 0.065786, loss_ce: 0.015397
2021-12-13 01:48:20,715 iteration 6501 : loss : 0.049196, loss_ce: 0.020739
2021-12-13 01:48:22,230 iteration 6502 : loss : 0.042614, loss_ce: 0.011726
2021-12-13 01:48:23,736 iteration 6503 : loss : 0.044857, loss_ce: 0.013956
2021-12-13 01:48:25,259 iteration 6504 : loss : 0.044885, loss_ce: 0.015616
2021-12-13 01:48:26,678 iteration 6505 : loss : 0.037950, loss_ce: 0.011802
2021-12-13 01:48:28,185 iteration 6506 : loss : 0.052285, loss_ce: 0.012008
2021-12-13 01:48:29,642 iteration 6507 : loss : 0.040483, loss_ce: 0.014676
2021-12-13 01:48:31,033 iteration 6508 : loss : 0.044346, loss_ce: 0.012677
2021-12-13 01:48:32,499 iteration 6509 : loss : 0.043359, loss_ce: 0.013481
2021-12-13 01:48:33,866 iteration 6510 : loss : 0.036332, loss_ce: 0.012288
2021-12-13 01:48:35,370 iteration 6511 : loss : 0.046961, loss_ce: 0.015031
 96%|███████████████████████████▊ | 383/400 [2:58:32<07:32, 26.62s/it]2021-12-13 01:48:36,932 iteration 6512 : loss : 0.050313, loss_ce: 0.015364
2021-12-13 01:48:38,487 iteration 6513 : loss : 0.043895, loss_ce: 0.014309
2021-12-13 01:48:39,992 iteration 6514 : loss : 0.048792, loss_ce: 0.013104
2021-12-13 01:48:41,550 iteration 6515 : loss : 0.064350, loss_ce: 0.019233
2021-12-13 01:48:43,007 iteration 6516 : loss : 0.039505, loss_ce: 0.013678
2021-12-13 01:48:44,439 iteration 6517 : loss : 0.040720, loss_ce: 0.012883
2021-12-13 01:48:45,945 iteration 6518 : loss : 0.044416, loss_ce: 0.013914
2021-12-13 01:48:47,504 iteration 6519 : loss : 0.050045, loss_ce: 0.017671
2021-12-13 01:48:48,924 iteration 6520 : loss : 0.043160, loss_ce: 0.013762
2021-12-13 01:48:50,449 iteration 6521 : loss : 0.046062, loss_ce: 0.016450
2021-12-13 01:48:51,951 iteration 6522 : loss : 0.042906, loss_ce: 0.012902
2021-12-13 01:48:53,475 iteration 6523 : loss : 0.047058, loss_ce: 0.016664
2021-12-13 01:48:54,993 iteration 6524 : loss : 0.046219, loss_ce: 0.013968
2021-12-13 01:48:56,541 iteration 6525 : loss : 0.044591, loss_ce: 0.013162
2021-12-13 01:48:58,038 iteration 6526 : loss : 0.044738, loss_ce: 0.015374
2021-12-13 01:48:59,486 iteration 6527 : loss : 0.046453, loss_ce: 0.014189
2021-12-13 01:49:00,932 iteration 6528 : loss : 0.039807, loss_ce: 0.012967
 96%|███████████████████████████▊ | 384/400 [2:58:58<07:00, 26.30s/it]2021-12-13 01:49:02,642 iteration 6529 : loss : 0.055103, loss_ce: 0.018881
2021-12-13 01:49:04,132 iteration 6530 : loss : 0.047157, loss_ce: 0.017445
2021-12-13 01:49:05,731 iteration 6531 : loss : 0.049639, loss_ce: 0.014803
2021-12-13 01:49:07,245 iteration 6532 : loss : 0.054527, loss_ce: 0.019289
2021-12-13 01:49:08,799 iteration 6533 : loss : 0.054456, loss_ce: 0.011246
2021-12-13 01:49:10,465 iteration 6534 : loss : 0.053434, loss_ce: 0.011986
2021-12-13 01:49:11,892 iteration 6535 : loss : 0.039896, loss_ce: 0.007454
2021-12-13 01:49:13,388 iteration 6536 : loss : 0.050038, loss_ce: 0.015433
2021-12-13 01:49:14,890 iteration 6537 : loss : 0.042361, loss_ce: 0.013680
2021-12-13 01:49:16,412 iteration 6538 : loss : 0.049558, loss_ce: 0.020790
2021-12-13 01:49:18,013 iteration 6539 : loss : 0.049528, loss_ce: 0.013786
2021-12-13 01:49:19,512 iteration 6540 : loss : 0.045196, loss_ce: 0.017729
2021-12-13 01:49:20,984 iteration 6541 : loss : 0.039690, loss_ce: 0.013697
2021-12-13 01:49:22,548 iteration 6542 : loss : 0.047663, loss_ce: 0.016200
2021-12-13 01:49:24,079 iteration 6543 : loss : 0.050527, loss_ce: 0.016132
2021-12-13 01:49:25,492 iteration 6544 : loss : 0.038087, loss_ce: 0.010513
2021-12-13 01:49:25,493 Training Data Eval:
2021-12-13 01:49:33,143   Average segmentation loss on training set: 0.0324
2021-12-13 01:49:33,143 Validation Data Eval:
2021-12-13 01:49:35,740   Average segmentation loss on validation set: 0.0861
2021-12-13 01:49:37,103 iteration 6545 : loss : 0.035061, loss_ce: 0.007383
 96%|███████████████████████████▉ | 385/400 [2:59:34<07:18, 29.26s/it]2021-12-13 01:49:38,665 iteration 6546 : loss : 0.059098, loss_ce: 0.016216
2021-12-13 01:49:40,363 iteration 6547 : loss : 0.048830, loss_ce: 0.015798
2021-12-13 01:49:41,769 iteration 6548 : loss : 0.044087, loss_ce: 0.020592
2021-12-13 01:49:43,159 iteration 6549 : loss : 0.038775, loss_ce: 0.011256
2021-12-13 01:49:44,616 iteration 6550 : loss : 0.041296, loss_ce: 0.011604
2021-12-13 01:49:46,155 iteration 6551 : loss : 0.051367, loss_ce: 0.020619
2021-12-13 01:49:47,547 iteration 6552 : loss : 0.039551, loss_ce: 0.015218
2021-12-13 01:49:49,120 iteration 6553 : loss : 0.055481, loss_ce: 0.017620
2021-12-13 01:49:50,615 iteration 6554 : loss : 0.048038, loss_ce: 0.012640
2021-12-13 01:49:52,190 iteration 6555 : loss : 0.045429, loss_ce: 0.014694
2021-12-13 01:49:53,735 iteration 6556 : loss : 0.045728, loss_ce: 0.012141
2021-12-13 01:49:55,172 iteration 6557 : loss : 0.047438, loss_ce: 0.014372
2021-12-13 01:49:56,618 iteration 6558 : loss : 0.043236, loss_ce: 0.010218
2021-12-13 01:49:58,146 iteration 6559 : loss : 0.052347, loss_ce: 0.018033
2021-12-13 01:49:59,739 iteration 6560 : loss : 0.073524, loss_ce: 0.014228
2021-12-13 01:50:01,192 iteration 6561 : loss : 0.043019, loss_ce: 0.015413
2021-12-13 01:50:02,585 iteration 6562 : loss : 0.036025, loss_ce: 0.010010
 96%|███████████████████████████▉ | 386/400 [2:59:59<06:33, 28.13s/it]2021-12-13 01:50:04,102 iteration 6563 : loss : 0.040007, loss_ce: 0.014524
2021-12-13 01:50:05,549 iteration 6564 : loss : 0.045919, loss_ce: 0.009158
2021-12-13 01:50:06,961 iteration 6565 : loss : 0.038503, loss_ce: 0.009793
2021-12-13 01:50:08,415 iteration 6566 : loss : 0.036563, loss_ce: 0.011939
2021-12-13 01:50:09,935 iteration 6567 : loss : 0.042466, loss_ce: 0.013092
2021-12-13 01:50:11,449 iteration 6568 : loss : 0.055543, loss_ce: 0.018274
2021-12-13 01:50:13,009 iteration 6569 : loss : 0.049824, loss_ce: 0.016679
2021-12-13 01:50:14,500 iteration 6570 : loss : 0.053530, loss_ce: 0.012331
2021-12-13 01:50:16,138 iteration 6571 : loss : 0.048590, loss_ce: 0.018785
2021-12-13 01:50:17,566 iteration 6572 : loss : 0.039321, loss_ce: 0.013806
2021-12-13 01:50:19,127 iteration 6573 : loss : 0.054043, loss_ce: 0.015792
2021-12-13 01:50:20,596 iteration 6574 : loss : 0.036364, loss_ce: 0.009340
2021-12-13 01:50:22,167 iteration 6575 : loss : 0.060079, loss_ce: 0.017243
2021-12-13 01:50:23,671 iteration 6576 : loss : 0.048692, loss_ce: 0.015741
2021-12-13 01:50:25,124 iteration 6577 : loss : 0.041806, loss_ce: 0.011772
2021-12-13 01:50:26,647 iteration 6578 : loss : 0.036631, loss_ce: 0.010752
2021-12-13 01:50:28,109 iteration 6579 : loss : 0.049146, loss_ce: 0.018294
 97%|████████████████████████████ | 387/400 [3:00:25<05:55, 27.34s/it]2021-12-13 01:50:29,704 iteration 6580 : loss : 0.050524, loss_ce: 0.018619
2021-12-13 01:50:31,139 iteration 6581 : loss : 0.039222, loss_ce: 0.010227
2021-12-13 01:50:32,724 iteration 6582 : loss : 0.050484, loss_ce: 0.015231
2021-12-13 01:50:34,223 iteration 6583 : loss : 0.054421, loss_ce: 0.014759
2021-12-13 01:50:35,665 iteration 6584 : loss : 0.045782, loss_ce: 0.014382
2021-12-13 01:50:37,062 iteration 6585 : loss : 0.039204, loss_ce: 0.010917
2021-12-13 01:50:38,519 iteration 6586 : loss : 0.050433, loss_ce: 0.010945
2021-12-13 01:50:39,925 iteration 6587 : loss : 0.040879, loss_ce: 0.015774
2021-12-13 01:50:41,420 iteration 6588 : loss : 0.043238, loss_ce: 0.015713
2021-12-13 01:50:42,926 iteration 6589 : loss : 0.043791, loss_ce: 0.015371
2021-12-13 01:50:44,383 iteration 6590 : loss : 0.042615, loss_ce: 0.014649
2021-12-13 01:50:45,854 iteration 6591 : loss : 0.038314, loss_ce: 0.011808
2021-12-13 01:50:47,427 iteration 6592 : loss : 0.050735, loss_ce: 0.019811
2021-12-13 01:50:48,818 iteration 6593 : loss : 0.041401, loss_ce: 0.018388
2021-12-13 01:50:50,269 iteration 6594 : loss : 0.041177, loss_ce: 0.011366
2021-12-13 01:50:51,795 iteration 6595 : loss : 0.051003, loss_ce: 0.014536
2021-12-13 01:50:53,227 iteration 6596 : loss : 0.039522, loss_ce: 0.009455
 97%|████████████████████████████▏| 388/400 [3:00:50<05:20, 26.68s/it]2021-12-13 01:50:54,704 iteration 6597 : loss : 0.036829, loss_ce: 0.012329
2021-12-13 01:50:56,242 iteration 6598 : loss : 0.046924, loss_ce: 0.007445
2021-12-13 01:50:57,798 iteration 6599 : loss : 0.049639, loss_ce: 0.015504
2021-12-13 01:50:59,231 iteration 6600 : loss : 0.037454, loss_ce: 0.012059
2021-12-13 01:51:00,824 iteration 6601 : loss : 0.051528, loss_ce: 0.019168
2021-12-13 01:51:02,365 iteration 6602 : loss : 0.045765, loss_ce: 0.013682
2021-12-13 01:51:03,818 iteration 6603 : loss : 0.046354, loss_ce: 0.016942
2021-12-13 01:51:05,355 iteration 6604 : loss : 0.042056, loss_ce: 0.016065
2021-12-13 01:51:06,825 iteration 6605 : loss : 0.045725, loss_ce: 0.017276
2021-12-13 01:51:08,390 iteration 6606 : loss : 0.047587, loss_ce: 0.014283
2021-12-13 01:51:09,958 iteration 6607 : loss : 0.048739, loss_ce: 0.016326
2021-12-13 01:51:11,387 iteration 6608 : loss : 0.045752, loss_ce: 0.016069
2021-12-13 01:51:12,933 iteration 6609 : loss : 0.048881, loss_ce: 0.017329
2021-12-13 01:51:14,403 iteration 6610 : loss : 0.044997, loss_ce: 0.014229
2021-12-13 01:51:15,865 iteration 6611 : loss : 0.040502, loss_ce: 0.009690
2021-12-13 01:51:17,344 iteration 6612 : loss : 0.045161, loss_ce: 0.011237
2021-12-13 01:51:18,881 iteration 6613 : loss : 0.042135, loss_ce: 0.012358
 97%|████████████████████████████▏| 389/400 [3:01:16<04:50, 26.37s/it]2021-12-13 01:51:20,371 iteration 6614 : loss : 0.036529, loss_ce: 0.011460
2021-12-13 01:51:21,776 iteration 6615 : loss : 0.035177, loss_ce: 0.011331
2021-12-13 01:51:23,298 iteration 6616 : loss : 0.046730, loss_ce: 0.013036
2021-12-13 01:51:24,761 iteration 6617 : loss : 0.067068, loss_ce: 0.021443
2021-12-13 01:51:26,341 iteration 6618 : loss : 0.054182, loss_ce: 0.016184
2021-12-13 01:51:27,835 iteration 6619 : loss : 0.045501, loss_ce: 0.014522
2021-12-13 01:51:29,336 iteration 6620 : loss : 0.038991, loss_ce: 0.011640
2021-12-13 01:51:30,889 iteration 6621 : loss : 0.053211, loss_ce: 0.017404
2021-12-13 01:51:32,355 iteration 6622 : loss : 0.041819, loss_ce: 0.016097
2021-12-13 01:51:33,782 iteration 6623 : loss : 0.041203, loss_ce: 0.011980
2021-12-13 01:51:35,260 iteration 6624 : loss : 0.041077, loss_ce: 0.013625
2021-12-13 01:51:36,774 iteration 6625 : loss : 0.063847, loss_ce: 0.020400
2021-12-13 01:51:38,259 iteration 6626 : loss : 0.044915, loss_ce: 0.015601
2021-12-13 01:51:39,795 iteration 6627 : loss : 0.055144, loss_ce: 0.016561
2021-12-13 01:51:41,321 iteration 6628 : loss : 0.049880, loss_ce: 0.017775
2021-12-13 01:51:42,945 iteration 6629 : loss : 0.057758, loss_ce: 0.014267
2021-12-13 01:51:42,945 Training Data Eval:
2021-12-13 01:51:50,599   Average segmentation loss on training set: 0.0327
2021-12-13 01:51:50,599 Validation Data Eval:
2021-12-13 01:51:53,203   Average segmentation loss on validation set: 0.0927
2021-12-13 01:51:54,658 iteration 6630 : loss : 0.044475, loss_ce: 0.015207
 98%|████████████████████████████▎| 390/400 [3:01:51<04:51, 29.19s/it]2021-12-13 01:51:56,178 iteration 6631 : loss : 0.043835, loss_ce: 0.012734
2021-12-13 01:51:57,632 iteration 6632 : loss : 0.043516, loss_ce: 0.018094
2021-12-13 01:51:59,160 iteration 6633 : loss : 0.047970, loss_ce: 0.010637
2021-12-13 01:52:00,624 iteration 6634 : loss : 0.041605, loss_ce: 0.013886
2021-12-13 01:52:02,115 iteration 6635 : loss : 0.040731, loss_ce: 0.012876
2021-12-13 01:52:03,614 iteration 6636 : loss : 0.038283, loss_ce: 0.013487
2021-12-13 01:52:05,058 iteration 6637 : loss : 0.047124, loss_ce: 0.015369
2021-12-13 01:52:06,609 iteration 6638 : loss : 0.054049, loss_ce: 0.010237
2021-12-13 01:52:08,164 iteration 6639 : loss : 0.044372, loss_ce: 0.015676
2021-12-13 01:52:09,641 iteration 6640 : loss : 0.044877, loss_ce: 0.012166
2021-12-13 01:52:11,151 iteration 6641 : loss : 0.047403, loss_ce: 0.013154
2021-12-13 01:52:12,655 iteration 6642 : loss : 0.050162, loss_ce: 0.018943
2021-12-13 01:52:14,160 iteration 6643 : loss : 0.042170, loss_ce: 0.012866
2021-12-13 01:52:15,781 iteration 6644 : loss : 0.073665, loss_ce: 0.026938
2021-12-13 01:52:17,243 iteration 6645 : loss : 0.040827, loss_ce: 0.013286
2021-12-13 01:52:18,852 iteration 6646 : loss : 0.056602, loss_ce: 0.015032
2021-12-13 01:52:20,333 iteration 6647 : loss : 0.048886, loss_ce: 0.013833
 98%|████████████████████████████▎| 391/400 [3:02:17<04:13, 28.14s/it]2021-12-13 01:52:21,840 iteration 6648 : loss : 0.051891, loss_ce: 0.009500
2021-12-13 01:52:23,372 iteration 6649 : loss : 0.044393, loss_ce: 0.010385
2021-12-13 01:52:24,953 iteration 6650 : loss : 0.051451, loss_ce: 0.017329
2021-12-13 01:52:26,569 iteration 6651 : loss : 0.054985, loss_ce: 0.019111
2021-12-13 01:52:28,051 iteration 6652 : loss : 0.044824, loss_ce: 0.016499
2021-12-13 01:52:29,466 iteration 6653 : loss : 0.037009, loss_ce: 0.012465
2021-12-13 01:52:30,941 iteration 6654 : loss : 0.047524, loss_ce: 0.014301
2021-12-13 01:52:32,470 iteration 6655 : loss : 0.046325, loss_ce: 0.015352
2021-12-13 01:52:33,993 iteration 6656 : loss : 0.050998, loss_ce: 0.017464
2021-12-13 01:52:35,504 iteration 6657 : loss : 0.048001, loss_ce: 0.010513
2021-12-13 01:52:37,042 iteration 6658 : loss : 0.042931, loss_ce: 0.013193
2021-12-13 01:52:38,546 iteration 6659 : loss : 0.050896, loss_ce: 0.018645
2021-12-13 01:52:40,107 iteration 6660 : loss : 0.048286, loss_ce: 0.016777
2021-12-13 01:52:41,633 iteration 6661 : loss : 0.050245, loss_ce: 0.016365
2021-12-13 01:52:43,169 iteration 6662 : loss : 0.046386, loss_ce: 0.010399
2021-12-13 01:52:44,679 iteration 6663 : loss : 0.043528, loss_ce: 0.012503
2021-12-13 01:52:46,230 iteration 6664 : loss : 0.049676, loss_ce: 0.022782
 98%|████████████████████████████▍| 392/400 [3:02:43<03:39, 27.47s/it]2021-12-13 01:52:47,796 iteration 6665 : loss : 0.043271, loss_ce: 0.011564
2021-12-13 01:52:49,424 iteration 6666 : loss : 0.058669, loss_ce: 0.013699
2021-12-13 01:52:50,961 iteration 6667 : loss : 0.044927, loss_ce: 0.007709
2021-12-13 01:52:52,377 iteration 6668 : loss : 0.044555, loss_ce: 0.015073
2021-12-13 01:52:53,859 iteration 6669 : loss : 0.044271, loss_ce: 0.014229
2021-12-13 01:52:55,342 iteration 6670 : loss : 0.042055, loss_ce: 0.014023
2021-12-13 01:52:56,864 iteration 6671 : loss : 0.041277, loss_ce: 0.012741
2021-12-13 01:52:58,409 iteration 6672 : loss : 0.053808, loss_ce: 0.013120
2021-12-13 01:52:59,903 iteration 6673 : loss : 0.062699, loss_ce: 0.014624
2021-12-13 01:53:01,396 iteration 6674 : loss : 0.056044, loss_ce: 0.019468
2021-12-13 01:53:02,979 iteration 6675 : loss : 0.047660, loss_ce: 0.019471
2021-12-13 01:53:04,449 iteration 6676 : loss : 0.044654, loss_ce: 0.015701
2021-12-13 01:53:05,896 iteration 6677 : loss : 0.041297, loss_ce: 0.011979
2021-12-13 01:53:07,384 iteration 6678 : loss : 0.048222, loss_ce: 0.020858
2021-12-13 01:53:08,937 iteration 6679 : loss : 0.048315, loss_ce: 0.016793
2021-12-13 01:53:10,370 iteration 6680 : loss : 0.042113, loss_ce: 0.011341
2021-12-13 01:53:11,785 iteration 6681 : loss : 0.040050, loss_ce: 0.011808
 98%|████████████████████████████▍| 393/400 [3:03:09<03:08, 26.89s/it]2021-12-13 01:53:13,417 iteration 6682 : loss : 0.055466, loss_ce: 0.020181
2021-12-13 01:53:14,862 iteration 6683 : loss : 0.043873, loss_ce: 0.015686
2021-12-13 01:53:16,351 iteration 6684 : loss : 0.044716, loss_ce: 0.016247
2021-12-13 01:53:17,790 iteration 6685 : loss : 0.048349, loss_ce: 0.015958
2021-12-13 01:53:19,362 iteration 6686 : loss : 0.047951, loss_ce: 0.016224
2021-12-13 01:53:20,843 iteration 6687 : loss : 0.046115, loss_ce: 0.014104
2021-12-13 01:53:22,334 iteration 6688 : loss : 0.043788, loss_ce: 0.014051
2021-12-13 01:53:23,743 iteration 6689 : loss : 0.038029, loss_ce: 0.008316
2021-12-13 01:53:25,227 iteration 6690 : loss : 0.035964, loss_ce: 0.009280
2021-12-13 01:53:26,755 iteration 6691 : loss : 0.048471, loss_ce: 0.013916
2021-12-13 01:53:28,209 iteration 6692 : loss : 0.050554, loss_ce: 0.015248
2021-12-13 01:53:29,685 iteration 6693 : loss : 0.052616, loss_ce: 0.016062
2021-12-13 01:53:31,199 iteration 6694 : loss : 0.043237, loss_ce: 0.012157
2021-12-13 01:53:32,654 iteration 6695 : loss : 0.045106, loss_ce: 0.013117
2021-12-13 01:53:34,142 iteration 6696 : loss : 0.040433, loss_ce: 0.012206
2021-12-13 01:53:35,623 iteration 6697 : loss : 0.042793, loss_ce: 0.012723
2021-12-13 01:53:37,195 iteration 6698 : loss : 0.039025, loss_ce: 0.010322
 98%|████████████████████████████▌| 394/400 [3:03:34<02:38, 26.45s/it]2021-12-13 01:53:38,698 iteration 6699 : loss : 0.054017, loss_ce: 0.014602
2021-12-13 01:53:40,252 iteration 6700 : loss : 0.050207, loss_ce: 0.011990
2021-12-13 01:53:41,734 iteration 6701 : loss : 0.037356, loss_ce: 0.009702
2021-12-13 01:53:43,146 iteration 6702 : loss : 0.038553, loss_ce: 0.015212
2021-12-13 01:53:44,554 iteration 6703 : loss : 0.040396, loss_ce: 0.011746
2021-12-13 01:53:46,082 iteration 6704 : loss : 0.053561, loss_ce: 0.021617
2021-12-13 01:53:47,512 iteration 6705 : loss : 0.047283, loss_ce: 0.019234
2021-12-13 01:53:49,072 iteration 6706 : loss : 0.045512, loss_ce: 0.012953
2021-12-13 01:53:50,497 iteration 6707 : loss : 0.036043, loss_ce: 0.009649
2021-12-13 01:53:51,990 iteration 6708 : loss : 0.046026, loss_ce: 0.014193
2021-12-13 01:53:53,496 iteration 6709 : loss : 0.052249, loss_ce: 0.019385
2021-12-13 01:53:55,029 iteration 6710 : loss : 0.060487, loss_ce: 0.011890
2021-12-13 01:53:56,559 iteration 6711 : loss : 0.043055, loss_ce: 0.010668
2021-12-13 01:53:58,152 iteration 6712 : loss : 0.045635, loss_ce: 0.016098
2021-12-13 01:53:59,581 iteration 6713 : loss : 0.039775, loss_ce: 0.011609
2021-12-13 01:54:01,083 iteration 6714 : loss : 0.040074, loss_ce: 0.011770
2021-12-13 01:54:01,083 Training Data Eval:
2021-12-13 01:54:08,733   Average segmentation loss on training set: 0.0335
2021-12-13 01:54:08,734 Validation Data Eval:
2021-12-13 01:54:11,345   Average segmentation loss on validation set: 0.0866
2021-12-13 01:54:12,781 iteration 6715 : loss : 0.040093, loss_ce: 0.013211
 99%|████████████████████████████▋| 395/400 [3:04:10<02:25, 29.19s/it]2021-12-13 01:54:14,296 iteration 6716 : loss : 0.046659, loss_ce: 0.018287
2021-12-13 01:54:15,753 iteration 6717 : loss : 0.041747, loss_ce: 0.014237
2021-12-13 01:54:17,234 iteration 6718 : loss : 0.042013, loss_ce: 0.013640
2021-12-13 01:54:18,679 iteration 6719 : loss : 0.046272, loss_ce: 0.014083
2021-12-13 01:54:20,134 iteration 6720 : loss : 0.043327, loss_ce: 0.011516
2021-12-13 01:54:21,569 iteration 6721 : loss : 0.039189, loss_ce: 0.010835
2021-12-13 01:54:23,140 iteration 6722 : loss : 0.044547, loss_ce: 0.014643
2021-12-13 01:54:24,527 iteration 6723 : loss : 0.039596, loss_ce: 0.010007
2021-12-13 01:54:26,010 iteration 6724 : loss : 0.052210, loss_ce: 0.013596
2021-12-13 01:54:27,520 iteration 6725 : loss : 0.038218, loss_ce: 0.010679
2021-12-13 01:54:29,057 iteration 6726 : loss : 0.050551, loss_ce: 0.018518
2021-12-13 01:54:30,664 iteration 6727 : loss : 0.056391, loss_ce: 0.015330
2021-12-13 01:54:32,148 iteration 6728 : loss : 0.048498, loss_ce: 0.017267
2021-12-13 01:54:33,626 iteration 6729 : loss : 0.041359, loss_ce: 0.011245
2021-12-13 01:54:35,062 iteration 6730 : loss : 0.039022, loss_ce: 0.011458
2021-12-13 01:54:36,543 iteration 6731 : loss : 0.042796, loss_ce: 0.014345
2021-12-13 01:54:38,071 iteration 6732 : loss : 0.044680, loss_ce: 0.015627
 99%|████████████████████████████▋| 396/400 [3:04:35<01:52, 28.02s/it]2021-12-13 01:54:39,587 iteration 6733 : loss : 0.039204, loss_ce: 0.011977
2021-12-13 01:54:41,188 iteration 6734 : loss : 0.051053, loss_ce: 0.017286
2021-12-13 01:54:42,631 iteration 6735 : loss : 0.040502, loss_ce: 0.012681
2021-12-13 01:54:44,169 iteration 6736 : loss : 0.047020, loss_ce: 0.015307
2021-12-13 01:54:45,712 iteration 6737 : loss : 0.061860, loss_ce: 0.018784
2021-12-13 01:54:47,106 iteration 6738 : loss : 0.043618, loss_ce: 0.011262
2021-12-13 01:54:48,561 iteration 6739 : loss : 0.040969, loss_ce: 0.012636
2021-12-13 01:54:50,085 iteration 6740 : loss : 0.037631, loss_ce: 0.012775
2021-12-13 01:54:51,559 iteration 6741 : loss : 0.034703, loss_ce: 0.010197
2021-12-13 01:54:53,006 iteration 6742 : loss : 0.045483, loss_ce: 0.012454
2021-12-13 01:54:54,524 iteration 6743 : loss : 0.045684, loss_ce: 0.016253
2021-12-13 01:54:56,045 iteration 6744 : loss : 0.055931, loss_ce: 0.020999
2021-12-13 01:54:57,485 iteration 6745 : loss : 0.043655, loss_ce: 0.015266
2021-12-13 01:54:58,990 iteration 6746 : loss : 0.039411, loss_ce: 0.012415
2021-12-13 01:55:00,545 iteration 6747 : loss : 0.054111, loss_ce: 0.011595
2021-12-13 01:55:02,085 iteration 6748 : loss : 0.047851, loss_ce: 0.012737
2021-12-13 01:55:03,592 iteration 6749 : loss : 0.044331, loss_ce: 0.015602
 99%|████████████████████████████▊| 397/400 [3:05:00<01:21, 27.27s/it]2021-12-13 01:55:05,162 iteration 6750 : loss : 0.037991, loss_ce: 0.010262
2021-12-13 01:55:06,776 iteration 6751 : loss : 0.049734, loss_ce: 0.014401
2021-12-13 01:55:08,346 iteration 6752 : loss : 0.047049, loss_ce: 0.015995
2021-12-13 01:55:09,831 iteration 6753 : loss : 0.049531, loss_ce: 0.016999
2021-12-13 01:55:11,408 iteration 6754 : loss : 0.042714, loss_ce: 0.013696
2021-12-13 01:55:12,859 iteration 6755 : loss : 0.040638, loss_ce: 0.008463
2021-12-13 01:55:14,232 iteration 6756 : loss : 0.038348, loss_ce: 0.013032
2021-12-13 01:55:15,647 iteration 6757 : loss : 0.040320, loss_ce: 0.014394
2021-12-13 01:55:17,108 iteration 6758 : loss : 0.040857, loss_ce: 0.015003
2021-12-13 01:55:18,637 iteration 6759 : loss : 0.054784, loss_ce: 0.013828
2021-12-13 01:55:20,092 iteration 6760 : loss : 0.040590, loss_ce: 0.014276
2021-12-13 01:55:21,551 iteration 6761 : loss : 0.044119, loss_ce: 0.014871
2021-12-13 01:55:23,050 iteration 6762 : loss : 0.042599, loss_ce: 0.011004
2021-12-13 01:55:24,534 iteration 6763 : loss : 0.042887, loss_ce: 0.015493
2021-12-13 01:55:26,095 iteration 6764 : loss : 0.048389, loss_ce: 0.014902
2021-12-13 01:55:27,609 iteration 6765 : loss : 0.050714, loss_ce: 0.012147
2021-12-13 01:55:29,085 iteration 6766 : loss : 0.038937, loss_ce: 0.014071
100%|████████████████████████████▊| 398/400 [3:05:26<00:53, 26.74s/it]2021-12-13 01:55:30,679 iteration 6767 : loss : 0.054691, loss_ce: 0.018553
2021-12-13 01:55:32,140 iteration 6768 : loss : 0.046387, loss_ce: 0.012110
2021-12-13 01:55:33,600 iteration 6769 : loss : 0.040013, loss_ce: 0.011450
2021-12-13 01:55:35,096 iteration 6770 : loss : 0.051945, loss_ce: 0.013301
2021-12-13 01:55:36,556 iteration 6771 : loss : 0.044392, loss_ce: 0.015665
2021-12-13 01:55:38,056 iteration 6772 : loss : 0.047259, loss_ce: 0.015714
2021-12-13 01:55:39,466 iteration 6773 : loss : 0.039226, loss_ce: 0.012896
2021-12-13 01:55:40,937 iteration 6774 : loss : 0.048316, loss_ce: 0.013510
2021-12-13 01:55:42,371 iteration 6775 : loss : 0.049420, loss_ce: 0.008057
2021-12-13 01:55:43,842 iteration 6776 : loss : 0.047355, loss_ce: 0.016088
2021-12-13 01:55:45,336 iteration 6777 : loss : 0.053107, loss_ce: 0.017333
2021-12-13 01:55:46,761 iteration 6778 : loss : 0.041460, loss_ce: 0.013228
2021-12-13 01:55:48,223 iteration 6779 : loss : 0.039078, loss_ce: 0.012261
2021-12-13 01:55:49,751 iteration 6780 : loss : 0.045392, loss_ce: 0.013541
2021-12-13 01:55:51,253 iteration 6781 : loss : 0.042844, loss_ce: 0.014291
2021-12-13 01:55:52,707 iteration 6782 : loss : 0.047966, loss_ce: 0.017173
2021-12-13 01:55:54,256 iteration 6783 : loss : 0.050765, loss_ce: 0.012309
100%|████████████████████████████▉| 399/400 [3:05:51<00:26, 26.27s/it]2021-12-13 01:55:55,710 iteration 6784 : loss : 0.045535, loss_ce: 0.018652
2021-12-13 01:55:57,178 iteration 6785 : loss : 0.042267, loss_ce: 0.011183
2021-12-13 01:55:58,706 iteration 6786 : loss : 0.044850, loss_ce: 0.014120
2021-12-13 01:56:00,181 iteration 6787 : loss : 0.049924, loss_ce: 0.011546
2021-12-13 01:56:01,666 iteration 6788 : loss : 0.041217, loss_ce: 0.016162
2021-12-13 01:56:03,230 iteration 6789 : loss : 0.048312, loss_ce: 0.014029
2021-12-13 01:56:04,721 iteration 6790 : loss : 0.048071, loss_ce: 0.016399
2021-12-13 01:56:06,283 iteration 6791 : loss : 0.051163, loss_ce: 0.014404
2021-12-13 01:56:07,755 iteration 6792 : loss : 0.043155, loss_ce: 0.014484
2021-12-13 01:56:09,154 iteration 6793 : loss : 0.037802, loss_ce: 0.011607
2021-12-13 01:56:10,675 iteration 6794 : loss : 0.059437, loss_ce: 0.018529
2021-12-13 01:56:12,106 iteration 6795 : loss : 0.042407, loss_ce: 0.013989
2021-12-13 01:56:13,583 iteration 6796 : loss : 0.044021, loss_ce: 0.010118
2021-12-13 01:56:15,034 iteration 6797 : loss : 0.049627, loss_ce: 0.015136
2021-12-13 01:56:16,570 iteration 6798 : loss : 0.051961, loss_ce: 0.015502
2021-12-13 01:56:18,048 iteration 6799 : loss : 0.042737, loss_ce: 0.011160
2021-12-13 01:56:18,048 Training Data Eval:
2021-12-13 01:56:25,693   Average segmentation loss on training set: 0.0328
2021-12-13 01:56:25,694 Validation Data Eval:
2021-12-13 01:56:28,301   Average segmentation loss on validation set: 0.0869
2021-12-13 01:56:29,771 iteration 6800 : loss : 0.046594, loss_ce: 0.015023
100%|█████████████████████████████| 400/400 [3:06:27<00:00, 29.04s/it]100%|█████████████████████████████| 400/400 [3:06:27<00:00, 27.97s/it]
