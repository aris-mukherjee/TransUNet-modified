2021-12-11 19:23:01,830 Logging directory: /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/TransUNet/log_dir/trRUNMC_cv1_r3/i2i2l/
2021-12-11 19:23:01,831 Tensorboard directory: /scratch_net/biwidl217_second/arismu/Tensorboard/trRUNMC_cv1_r3/i2i2l/
2021-12-11 19:23:01,831 ============================================================
2021-12-11 19:23:01,831 EXPERIMENT NAME: trRUNMC_cv1_r3/i2i2l/
2021-12-11 19:23:01,831 ============================================================
2021-12-11 19:23:01,831 Loading data...
2021-12-11 19:23:01,831 Reading NCI - RUNMC images...
2021-12-11 19:23:01,831 Data root directory: /itet-stor/arismu/bmicdatasets-originals/Originals/Challenge_Datasets/NCI_Prostate/
2021-12-11 19:23:01,832 Already preprocessed this configuration. Loading now!
2021-12-11 19:23:01,849 Training Images: (256, 256, 286)
2021-12-11 19:23:01,849 Training Labels: (256, 256, 286)
2021-12-11 19:23:01,849 Validation Images: (256, 256, 98)
2021-12-11 19:23:01,849 Validation Labels: (256, 256, 98)
2021-12-11 19:23:01,849 ============================================================
2021-12-11 19:23:01,884 17 iterations per epoch. 6800 max iterations 
  0%|                                         | 0/400 [00:00<?, ?it/s]2021-12-11 19:23:04,612 iteration 1 : loss : 0.925517, loss_ce: 1.122663
2021-12-11 19:23:06,010 iteration 2 : loss : 0.920106, loss_ce: 1.121381
2021-12-11 19:23:07,510 iteration 3 : loss : 0.915587, loss_ce: 1.114921
2021-12-11 19:23:08,945 iteration 4 : loss : 0.918800, loss_ce: 1.096240
2021-12-11 19:23:10,300 iteration 5 : loss : 0.914687, loss_ce: 1.078900
2021-12-11 19:23:11,720 iteration 6 : loss : 0.902007, loss_ce: 1.067257
2021-12-11 19:23:13,184 iteration 7 : loss : 0.889682, loss_ce: 1.048106
2021-12-11 19:23:14,736 iteration 8 : loss : 0.879263, loss_ce: 1.028971
2021-12-11 19:23:16,118 iteration 9 : loss : 0.873398, loss_ce: 1.004114
2021-12-11 19:23:17,503 iteration 10 : loss : 0.853625, loss_ce: 0.982322
2021-12-11 19:23:18,874 iteration 11 : loss : 0.843191, loss_ce: 0.956596
2021-12-11 19:23:20,313 iteration 12 : loss : 0.824321, loss_ce: 0.937803
2021-12-11 19:23:21,887 iteration 13 : loss : 0.802203, loss_ce: 0.918237
2021-12-11 19:23:23,370 iteration 14 : loss : 0.791077, loss_ce: 0.889699
2021-12-11 19:23:24,948 iteration 15 : loss : 0.781859, loss_ce: 0.862915
2021-12-11 19:23:26,473 iteration 16 : loss : 0.767079, loss_ce: 0.839316
2021-12-11 19:23:27,988 iteration 17 : loss : 0.747702, loss_ce: 0.826442
  0%|                               | 1/400 [00:26<2:54:10, 26.19s/it]2021-12-11 19:23:29,612 iteration 18 : loss : 0.741203, loss_ce: 0.791200
2021-12-11 19:23:31,174 iteration 19 : loss : 0.725156, loss_ce: 0.769733
2021-12-11 19:23:32,720 iteration 20 : loss : 0.707563, loss_ce: 0.746352
2021-12-11 19:23:34,353 iteration 21 : loss : 0.693871, loss_ce: 0.723862
2021-12-11 19:23:35,880 iteration 22 : loss : 0.683735, loss_ce: 0.707426
2021-12-11 19:23:37,469 iteration 23 : loss : 0.669976, loss_ce: 0.679755
2021-12-11 19:23:38,957 iteration 24 : loss : 0.653810, loss_ce: 0.669864
2021-12-11 19:23:40,399 iteration 25 : loss : 0.648858, loss_ce: 0.644516
2021-12-11 19:23:41,882 iteration 26 : loss : 0.635747, loss_ce: 0.619116
2021-12-11 19:23:43,442 iteration 27 : loss : 0.623205, loss_ce: 0.616448
2021-12-11 19:23:44,968 iteration 28 : loss : 0.610101, loss_ce: 0.585074
2021-12-11 19:23:46,665 iteration 29 : loss : 0.600555, loss_ce: 0.576819
2021-12-11 19:23:48,260 iteration 30 : loss : 0.592443, loss_ce: 0.553846
2021-12-11 19:23:49,757 iteration 31 : loss : 0.580457, loss_ce: 0.550053
2021-12-11 19:23:51,277 iteration 32 : loss : 0.573090, loss_ce: 0.531729
2021-12-11 19:23:52,907 iteration 33 : loss : 0.567588, loss_ce: 0.527308
2021-12-11 19:23:54,418 iteration 34 : loss : 0.559644, loss_ce: 0.492213
  0%|▏                              | 2/400 [00:52<2:54:32, 26.31s/it]2021-12-11 19:23:56,069 iteration 35 : loss : 0.548087, loss_ce: 0.505164
2021-12-11 19:23:57,642 iteration 36 : loss : 0.538831, loss_ce: 0.481626
2021-12-11 19:23:59,253 iteration 37 : loss : 0.535859, loss_ce: 0.484527
2021-12-11 19:24:00,929 iteration 38 : loss : 0.526110, loss_ce: 0.447365
2021-12-11 19:24:02,525 iteration 39 : loss : 0.528022, loss_ce: 0.432936
2021-12-11 19:24:04,160 iteration 40 : loss : 0.517030, loss_ce: 0.456552
2021-12-11 19:24:05,718 iteration 41 : loss : 0.512925, loss_ce: 0.419140
2021-12-11 19:24:07,471 iteration 42 : loss : 0.507655, loss_ce: 0.420859
2021-12-11 19:24:09,078 iteration 43 : loss : 0.507147, loss_ce: 0.393735
2021-12-11 19:24:10,770 iteration 44 : loss : 0.496882, loss_ce: 0.396908
2021-12-11 19:24:12,511 iteration 45 : loss : 0.492012, loss_ce: 0.380196
2021-12-11 19:24:14,241 iteration 46 : loss : 0.482366, loss_ce: 0.377117
2021-12-11 19:24:16,120 iteration 47 : loss : 0.487677, loss_ce: 0.357533
2021-12-11 19:24:18,068 iteration 48 : loss : 0.472112, loss_ce: 0.376423
2021-12-11 19:24:19,952 iteration 49 : loss : 0.478959, loss_ce: 0.352107
2021-12-11 19:24:21,871 iteration 50 : loss : 0.469434, loss_ce: 0.344476
2021-12-11 19:24:23,822 iteration 51 : loss : 0.468874, loss_ce: 0.364536
  1%|▏                              | 3/400 [01:21<3:03:26, 27.72s/it]2021-12-11 19:24:25,846 iteration 52 : loss : 0.463589, loss_ce: 0.350464
2021-12-11 19:24:27,953 iteration 53 : loss : 0.459482, loss_ce: 0.347132
2021-12-11 19:24:30,159 iteration 54 : loss : 0.451072, loss_ce: 0.336981
2021-12-11 19:24:32,290 iteration 55 : loss : 0.455171, loss_ce: 0.320295
2021-12-11 19:24:34,543 iteration 56 : loss : 0.446127, loss_ce: 0.313558
2021-12-11 19:24:36,826 iteration 57 : loss : 0.443229, loss_ce: 0.298601
2021-12-11 19:24:39,109 iteration 58 : loss : 0.447020, loss_ce: 0.319532
2021-12-11 19:24:41,442 iteration 59 : loss : 0.440124, loss_ce: 0.300849
2021-12-11 19:24:43,820 iteration 60 : loss : 0.442254, loss_ce: 0.297780
2021-12-11 19:24:46,372 iteration 61 : loss : 0.437851, loss_ce: 0.309483
2021-12-11 19:24:48,678 iteration 62 : loss : 0.436029, loss_ce: 0.295873
2021-12-11 19:24:51,015 iteration 63 : loss : 0.433740, loss_ce: 0.313964
2021-12-11 19:24:53,417 iteration 64 : loss : 0.442319, loss_ce: 0.317427
2021-12-11 19:24:55,764 iteration 65 : loss : 0.433208, loss_ce: 0.270520
2021-12-11 19:24:58,315 iteration 66 : loss : 0.429380, loss_ce: 0.276834
2021-12-11 19:25:00,632 iteration 67 : loss : 0.426303, loss_ce: 0.288044
2021-12-11 19:25:03,129 iteration 68 : loss : 0.427763, loss_ce: 0.256864
  1%|▎                              | 4/400 [02:01<3:33:09, 32.30s/it]2021-12-11 19:25:05,535 iteration 69 : loss : 0.422094, loss_ce: 0.250828
2021-12-11 19:25:07,982 iteration 70 : loss : 0.430637, loss_ce: 0.284205
2021-12-11 19:25:10,477 iteration 71 : loss : 0.420352, loss_ce: 0.270342
2021-12-11 19:25:12,888 iteration 72 : loss : 0.417736, loss_ce: 0.259363
2021-12-11 19:25:15,360 iteration 73 : loss : 0.415015, loss_ce: 0.272794
2021-12-11 19:25:17,856 iteration 74 : loss : 0.419663, loss_ce: 0.269627
2021-12-11 19:25:20,271 iteration 75 : loss : 0.426804, loss_ce: 0.291828
2021-12-11 19:25:22,736 iteration 76 : loss : 0.423487, loss_ce: 0.266250
2021-12-11 19:25:25,233 iteration 77 : loss : 0.411240, loss_ce: 0.242359
2021-12-11 19:25:27,849 iteration 78 : loss : 0.408761, loss_ce: 0.275951
2021-12-11 19:25:30,280 iteration 79 : loss : 0.406273, loss_ce: 0.241858
2021-12-11 19:25:32,947 iteration 80 : loss : 0.415928, loss_ce: 0.225390
2021-12-11 19:25:35,386 iteration 81 : loss : 0.404683, loss_ce: 0.236749
2021-12-11 19:25:37,991 iteration 82 : loss : 0.418400, loss_ce: 0.268997
2021-12-11 19:25:40,476 iteration 83 : loss : 0.406177, loss_ce: 0.233094
2021-12-11 19:25:43,046 iteration 84 : loss : 0.408318, loss_ce: 0.247549
2021-12-11 19:25:43,046 Training Data Eval:
2021-12-11 19:25:56,831   Average segmentation loss on training set: 0.4020
2021-12-11 19:25:56,831 Validation Data Eval:
2021-12-11 19:26:01,878   Average segmentation loss on validation set: 0.4268
2021-12-11 19:26:03,936 Found new lowest validation loss at iteration 84! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_best_val_loss_seed2.pth
2021-12-11 19:26:06,059 iteration 85 : loss : 0.406183, loss_ce: 0.258538
  1%|▍                              | 5/400 [03:04<4:45:20, 43.34s/it]2021-12-11 19:26:08,606 iteration 86 : loss : 0.396654, loss_ce: 0.221981
2021-12-11 19:26:11,169 iteration 87 : loss : 0.400715, loss_ce: 0.238051
2021-12-11 19:26:13,747 iteration 88 : loss : 0.395814, loss_ce: 0.220522
2021-12-11 19:26:16,359 iteration 89 : loss : 0.395098, loss_ce: 0.241240
2021-12-11 19:26:18,941 iteration 90 : loss : 0.404743, loss_ce: 0.249695
2021-12-11 19:26:21,598 iteration 91 : loss : 0.407124, loss_ce: 0.232137
2021-12-11 19:26:24,184 iteration 92 : loss : 0.395851, loss_ce: 0.234289
2021-12-11 19:26:27,031 iteration 93 : loss : 0.393297, loss_ce: 0.226730
2021-12-11 19:26:29,718 iteration 94 : loss : 0.406408, loss_ce: 0.254392
2021-12-11 19:26:32,436 iteration 95 : loss : 0.392098, loss_ce: 0.213984
2021-12-11 19:26:35,058 iteration 96 : loss : 0.390984, loss_ce: 0.211577
2021-12-11 19:26:37,716 iteration 97 : loss : 0.393267, loss_ce: 0.221902
2021-12-11 19:26:40,397 iteration 98 : loss : 0.391329, loss_ce: 0.222347
2021-12-11 19:26:43,230 iteration 99 : loss : 0.391405, loss_ce: 0.226847
2021-12-11 19:26:46,000 iteration 100 : loss : 0.399636, loss_ce: 0.233191
2021-12-11 19:26:48,743 iteration 101 : loss : 0.407680, loss_ce: 0.234083
2021-12-11 19:26:51,462 iteration 102 : loss : 0.395543, loss_ce: 0.216613
  2%|▍                              | 6/400 [03:49<4:49:14, 44.05s/it]2021-12-11 19:26:54,309 iteration 103 : loss : 0.394118, loss_ce: 0.219746
2021-12-11 19:26:57,178 iteration 104 : loss : 0.393413, loss_ce: 0.210239
2021-12-11 19:26:59,963 iteration 105 : loss : 0.387360, loss_ce: 0.223132
2021-12-11 19:27:02,756 iteration 106 : loss : 0.376764, loss_ce: 0.204840
2021-12-11 19:27:05,400 iteration 107 : loss : 0.391721, loss_ce: 0.220787
2021-12-11 19:27:08,190 iteration 108 : loss : 0.402475, loss_ce: 0.235840
2021-12-11 19:27:11,008 iteration 109 : loss : 0.382922, loss_ce: 0.208742
2021-12-11 19:27:13,548 iteration 110 : loss : 0.389181, loss_ce: 0.221895
2021-12-11 19:27:16,417 iteration 111 : loss : 0.382877, loss_ce: 0.204829
2021-12-11 19:27:19,004 iteration 112 : loss : 0.380968, loss_ce: 0.186843
2021-12-11 19:27:21,808 iteration 113 : loss : 0.383764, loss_ce: 0.199932
2021-12-11 19:27:24,513 iteration 114 : loss : 0.377935, loss_ce: 0.197023
2021-12-11 19:27:27,182 iteration 115 : loss : 0.375687, loss_ce: 0.200488
2021-12-11 19:27:29,998 iteration 116 : loss : 0.390297, loss_ce: 0.220675
2021-12-11 19:27:32,751 iteration 117 : loss : 0.378568, loss_ce: 0.210364
2021-12-11 19:27:35,441 iteration 118 : loss : 0.378195, loss_ce: 0.199623
2021-12-11 19:27:38,093 iteration 119 : loss : 0.389382, loss_ce: 0.217761
  2%|▌                              | 7/400 [04:36<4:54:01, 44.89s/it]2021-12-11 19:27:40,999 iteration 120 : loss : 0.398024, loss_ce: 0.221551
2021-12-11 19:27:43,809 iteration 121 : loss : 0.378778, loss_ce: 0.194029
2021-12-11 19:27:46,480 iteration 122 : loss : 0.368942, loss_ce: 0.191543
2021-12-11 19:27:49,182 iteration 123 : loss : 0.387143, loss_ce: 0.210930
2021-12-11 19:27:51,992 iteration 124 : loss : 0.377895, loss_ce: 0.195626
2021-12-11 19:27:54,795 iteration 125 : loss : 0.378835, loss_ce: 0.191057
2021-12-11 19:27:57,445 iteration 126 : loss : 0.375375, loss_ce: 0.202194
2021-12-11 19:28:00,223 iteration 127 : loss : 0.396596, loss_ce: 0.225636
2021-12-11 19:28:02,844 iteration 128 : loss : 0.373763, loss_ce: 0.192919
2021-12-11 19:28:05,708 iteration 129 : loss : 0.391467, loss_ce: 0.198180
2021-12-11 19:28:08,343 iteration 130 : loss : 0.377595, loss_ce: 0.203145
2021-12-11 19:28:11,109 iteration 131 : loss : 0.393677, loss_ce: 0.223851
2021-12-11 19:28:13,889 iteration 132 : loss : 0.382280, loss_ce: 0.191590
2021-12-11 19:28:16,510 iteration 133 : loss : 0.404325, loss_ce: 0.241719
2021-12-11 19:28:19,179 iteration 134 : loss : 0.374770, loss_ce: 0.195581
2021-12-11 19:28:21,942 iteration 135 : loss : 0.359840, loss_ce: 0.172779
2021-12-11 19:28:24,740 iteration 136 : loss : 0.377853, loss_ce: 0.200831
  2%|▌                              | 8/400 [05:22<4:56:55, 45.45s/it]2021-12-11 19:28:27,378 iteration 137 : loss : 0.364716, loss_ce: 0.175338
2021-12-11 19:28:30,122 iteration 138 : loss : 0.379330, loss_ce: 0.198236
2021-12-11 19:28:32,951 iteration 139 : loss : 0.373167, loss_ce: 0.164298
2021-12-11 19:28:35,669 iteration 140 : loss : 0.375772, loss_ce: 0.191601
2021-12-11 19:28:38,358 iteration 141 : loss : 0.382343, loss_ce: 0.197057
2021-12-11 19:28:40,974 iteration 142 : loss : 0.368529, loss_ce: 0.194394
2021-12-11 19:28:43,776 iteration 143 : loss : 0.388088, loss_ce: 0.190151
2021-12-11 19:28:46,687 iteration 144 : loss : 0.355080, loss_ce: 0.160369
2021-12-11 19:28:49,399 iteration 145 : loss : 0.397531, loss_ce: 0.232395
2021-12-11 19:28:52,223 iteration 146 : loss : 0.365943, loss_ce: 0.194365
2021-12-11 19:28:55,061 iteration 147 : loss : 0.374986, loss_ce: 0.206783
2021-12-11 19:28:57,835 iteration 148 : loss : 0.378834, loss_ce: 0.195731
2021-12-11 19:29:00,456 iteration 149 : loss : 0.373994, loss_ce: 0.185966
2021-12-11 19:29:03,333 iteration 150 : loss : 0.367304, loss_ce: 0.190021
2021-12-11 19:29:06,179 iteration 151 : loss : 0.394197, loss_ce: 0.231392
2021-12-11 19:29:08,742 iteration 152 : loss : 0.365433, loss_ce: 0.197145
2021-12-11 19:29:11,514 iteration 153 : loss : 0.366064, loss_ce: 0.185325
  2%|▋                              | 9/400 [06:09<4:58:51, 45.86s/it]2021-12-11 19:29:14,512 iteration 154 : loss : 0.385596, loss_ce: 0.203100
2021-12-11 19:29:17,147 iteration 155 : loss : 0.376007, loss_ce: 0.198838
2021-12-11 19:29:19,855 iteration 156 : loss : 0.381744, loss_ce: 0.200060
2021-12-11 19:29:22,528 iteration 157 : loss : 0.377792, loss_ce: 0.191057
2021-12-11 19:29:25,357 iteration 158 : loss : 0.372822, loss_ce: 0.182953
2021-12-11 19:29:28,246 iteration 159 : loss : 0.376778, loss_ce: 0.182880
2021-12-11 19:29:30,930 iteration 160 : loss : 0.357286, loss_ce: 0.173489
2021-12-11 19:29:33,786 iteration 161 : loss : 0.375261, loss_ce: 0.192014
2021-12-11 19:29:36,642 iteration 162 : loss : 0.372871, loss_ce: 0.171903
2021-12-11 19:29:39,374 iteration 163 : loss : 0.381393, loss_ce: 0.207263
2021-12-11 19:29:42,131 iteration 164 : loss : 0.357395, loss_ce: 0.162708
2021-12-11 19:29:44,873 iteration 165 : loss : 0.361891, loss_ce: 0.185925
2021-12-11 19:29:47,739 iteration 166 : loss : 0.399619, loss_ce: 0.238282
2021-12-11 19:29:50,662 iteration 167 : loss : 0.387338, loss_ce: 0.194953
2021-12-11 19:29:53,268 iteration 168 : loss : 0.366483, loss_ce: 0.167116
2021-12-11 19:29:56,078 iteration 169 : loss : 0.356144, loss_ce: 0.171514
2021-12-11 19:29:56,079 Training Data Eval:
2021-12-11 19:30:10,946   Average segmentation loss on training set: 0.3614
2021-12-11 19:30:10,946 Validation Data Eval:
2021-12-11 19:30:16,098   Average segmentation loss on validation set: 0.3872
2021-12-11 19:30:18,066 Found new lowest validation loss at iteration 169! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_best_val_loss_seed2.pth
2021-12-11 19:30:20,463 iteration 170 : loss : 0.378545, loss_ce: 0.203958
  2%|▊                             | 10/400 [07:18<5:44:25, 52.99s/it]2021-12-11 19:30:23,190 iteration 171 : loss : 0.354626, loss_ce: 0.183558
2021-12-11 19:30:25,887 iteration 172 : loss : 0.371330, loss_ce: 0.198834
2021-12-11 19:30:28,611 iteration 173 : loss : 0.356847, loss_ce: 0.169931
2021-12-11 19:30:31,149 iteration 174 : loss : 0.382301, loss_ce: 0.211985
2021-12-11 19:30:33,972 iteration 175 : loss : 0.359089, loss_ce: 0.159545
2021-12-11 19:30:36,719 iteration 176 : loss : 0.372696, loss_ce: 0.186870
2021-12-11 19:30:39,503 iteration 177 : loss : 0.366421, loss_ce: 0.183545
2021-12-11 19:30:42,198 iteration 178 : loss : 0.356932, loss_ce: 0.180119
2021-12-11 19:30:44,811 iteration 179 : loss : 0.346149, loss_ce: 0.164995
2021-12-11 19:30:47,661 iteration 180 : loss : 0.373491, loss_ce: 0.191005
2021-12-11 19:30:50,607 iteration 181 : loss : 0.368785, loss_ce: 0.202938
2021-12-11 19:30:53,385 iteration 182 : loss : 0.368281, loss_ce: 0.189077
2021-12-11 19:30:56,144 iteration 183 : loss : 0.354475, loss_ce: 0.165789
2021-12-11 19:30:59,156 iteration 184 : loss : 0.361749, loss_ce: 0.175011
2021-12-11 19:31:01,863 iteration 185 : loss : 0.368222, loss_ce: 0.191376
2021-12-11 19:31:04,646 iteration 186 : loss : 0.371912, loss_ce: 0.161836
2021-12-11 19:31:07,385 iteration 187 : loss : 0.364737, loss_ce: 0.174555
  3%|▊                             | 11/400 [08:05<5:31:30, 51.13s/it]2021-12-11 19:31:10,226 iteration 188 : loss : 0.363149, loss_ce: 0.188052
2021-12-11 19:31:13,027 iteration 189 : loss : 0.370842, loss_ce: 0.195248
2021-12-11 19:31:16,006 iteration 190 : loss : 0.370711, loss_ce: 0.187853
2021-12-11 19:31:18,749 iteration 191 : loss : 0.356623, loss_ce: 0.168632
2021-12-11 19:31:21,491 iteration 192 : loss : 0.369248, loss_ce: 0.162887
2021-12-11 19:31:24,186 iteration 193 : loss : 0.373296, loss_ce: 0.189007
2021-12-11 19:31:27,041 iteration 194 : loss : 0.359819, loss_ce: 0.179056
2021-12-11 19:31:29,893 iteration 195 : loss : 0.354698, loss_ce: 0.167204
2021-12-11 19:31:32,589 iteration 196 : loss : 0.361838, loss_ce: 0.199431
2021-12-11 19:31:35,364 iteration 197 : loss : 0.347940, loss_ce: 0.162919
2021-12-11 19:31:38,302 iteration 198 : loss : 0.346169, loss_ce: 0.140991
2021-12-11 19:31:40,917 iteration 199 : loss : 0.346938, loss_ce: 0.157300
2021-12-11 19:31:43,843 iteration 200 : loss : 0.386993, loss_ce: 0.207490
2021-12-11 19:31:46,688 iteration 201 : loss : 0.358076, loss_ce: 0.180060
2021-12-11 19:31:49,377 iteration 202 : loss : 0.359964, loss_ce: 0.173154
2021-12-11 19:31:52,146 iteration 203 : loss : 0.381111, loss_ce: 0.224679
2021-12-11 19:31:54,877 iteration 204 : loss : 0.366919, loss_ce: 0.178421
  3%|▉                             | 12/400 [08:53<5:23:30, 50.03s/it]2021-12-11 19:31:57,761 iteration 205 : loss : 0.371425, loss_ce: 0.173092
2021-12-11 19:32:00,577 iteration 206 : loss : 0.373499, loss_ce: 0.193066
2021-12-11 19:32:03,544 iteration 207 : loss : 0.372568, loss_ce: 0.194013
2021-12-11 19:32:06,511 iteration 208 : loss : 0.349314, loss_ce: 0.167302
2021-12-11 19:32:09,209 iteration 209 : loss : 0.348525, loss_ce: 0.166934
2021-12-11 19:32:11,906 iteration 210 : loss : 0.337234, loss_ce: 0.156269
2021-12-11 19:32:14,549 iteration 211 : loss : 0.350151, loss_ce: 0.164436
2021-12-11 19:32:17,295 iteration 212 : loss : 0.345673, loss_ce: 0.153137
2021-12-11 19:32:20,260 iteration 213 : loss : 0.375273, loss_ce: 0.192775
2021-12-11 19:32:23,080 iteration 214 : loss : 0.340359, loss_ce: 0.148221
2021-12-11 19:32:25,775 iteration 215 : loss : 0.359093, loss_ce: 0.174332
2021-12-11 19:32:28,401 iteration 216 : loss : 0.380505, loss_ce: 0.185132
2021-12-11 19:32:31,121 iteration 217 : loss : 0.359723, loss_ce: 0.187675
2021-12-11 19:32:33,919 iteration 218 : loss : 0.347473, loss_ce: 0.167577
2021-12-11 19:32:36,597 iteration 219 : loss : 0.343817, loss_ce: 0.148951
2021-12-11 19:32:39,402 iteration 220 : loss : 0.373552, loss_ce: 0.211692
2021-12-11 19:32:42,168 iteration 221 : loss : 0.365039, loss_ce: 0.199893
  3%|▉                             | 13/400 [09:40<5:17:19, 49.20s/it]2021-12-11 19:32:45,274 iteration 222 : loss : 0.368374, loss_ce: 0.173797
2021-12-11 19:32:48,155 iteration 223 : loss : 0.344734, loss_ce: 0.151771
2021-12-11 19:32:50,865 iteration 224 : loss : 0.362164, loss_ce: 0.170321
2021-12-11 19:32:53,643 iteration 225 : loss : 0.349195, loss_ce: 0.154209
2021-12-11 19:32:56,532 iteration 226 : loss : 0.363820, loss_ce: 0.162245
2021-12-11 19:32:59,228 iteration 227 : loss : 0.343766, loss_ce: 0.171486
2021-12-11 19:33:02,068 iteration 228 : loss : 0.348430, loss_ce: 0.165099
2021-12-11 19:33:04,805 iteration 229 : loss : 0.358775, loss_ce: 0.192817
2021-12-11 19:33:07,598 iteration 230 : loss : 0.376451, loss_ce: 0.182604
2021-12-11 19:33:10,231 iteration 231 : loss : 0.364411, loss_ce: 0.167192
2021-12-11 19:33:13,245 iteration 232 : loss : 0.368640, loss_ce: 0.208694
2021-12-11 19:33:16,031 iteration 233 : loss : 0.377377, loss_ce: 0.197190
2021-12-11 19:33:18,896 iteration 234 : loss : 0.334977, loss_ce: 0.140902
2021-12-11 19:33:21,697 iteration 235 : loss : 0.350104, loss_ce: 0.162570
2021-12-11 19:33:24,400 iteration 236 : loss : 0.355873, loss_ce: 0.169243
2021-12-11 19:33:27,182 iteration 237 : loss : 0.342340, loss_ce: 0.159235
2021-12-11 19:33:29,729 iteration 238 : loss : 0.357876, loss_ce: 0.184943
  4%|█                             | 14/400 [10:27<5:13:18, 48.70s/it]2021-12-11 19:33:32,557 iteration 239 : loss : 0.339917, loss_ce: 0.152163
2021-12-11 19:33:35,427 iteration 240 : loss : 0.377003, loss_ce: 0.218275
2021-12-11 19:33:38,123 iteration 241 : loss : 0.352949, loss_ce: 0.144471
2021-12-11 19:33:40,944 iteration 242 : loss : 0.366315, loss_ce: 0.173803
2021-12-11 19:33:43,886 iteration 243 : loss : 0.360772, loss_ce: 0.190161
2021-12-11 19:33:46,761 iteration 244 : loss : 0.375993, loss_ce: 0.207106
2021-12-11 19:33:49,502 iteration 245 : loss : 0.330161, loss_ce: 0.141576
2021-12-11 19:33:52,286 iteration 246 : loss : 0.344212, loss_ce: 0.162100
2021-12-11 19:33:55,143 iteration 247 : loss : 0.349413, loss_ce: 0.175037
2021-12-11 19:33:57,951 iteration 248 : loss : 0.342355, loss_ce: 0.146865
2021-12-11 19:34:00,828 iteration 249 : loss : 0.348981, loss_ce: 0.156042
2021-12-11 19:34:03,719 iteration 250 : loss : 0.369439, loss_ce: 0.190774
2021-12-11 19:34:06,601 iteration 251 : loss : 0.349480, loss_ce: 0.163532
2021-12-11 19:34:09,396 iteration 252 : loss : 0.367483, loss_ce: 0.205102
2021-12-11 19:34:12,094 iteration 253 : loss : 0.360727, loss_ce: 0.167465
2021-12-11 19:34:14,876 iteration 254 : loss : 0.354154, loss_ce: 0.165316
2021-12-11 19:34:14,877 Training Data Eval:
2021-12-11 19:34:29,504   Average segmentation loss on training set: 0.3402
2021-12-11 19:34:29,505 Validation Data Eval:
2021-12-11 19:34:34,669   Average segmentation loss on validation set: 0.3649
2021-12-11 19:34:36,760 Found new lowest validation loss at iteration 254! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_best_val_loss_seed2.pth
2021-12-11 19:34:39,065 iteration 255 : loss : 0.380134, loss_ce: 0.165756
  4%|█▏                            | 15/400 [11:37<5:52:24, 54.92s/it]2021-12-11 19:34:42,006 iteration 256 : loss : 0.340231, loss_ce: 0.157853
2021-12-11 19:34:44,687 iteration 257 : loss : 0.356212, loss_ce: 0.192798
2021-12-11 19:34:47,640 iteration 258 : loss : 0.354572, loss_ce: 0.151172
2021-12-11 19:34:50,396 iteration 259 : loss : 0.358194, loss_ce: 0.152773
2021-12-11 19:34:53,064 iteration 260 : loss : 0.353027, loss_ce: 0.181107
2021-12-11 19:34:55,848 iteration 261 : loss : 0.372178, loss_ce: 0.190187
2021-12-11 19:34:58,646 iteration 262 : loss : 0.365393, loss_ce: 0.190025
2021-12-11 19:35:01,254 iteration 263 : loss : 0.380395, loss_ce: 0.207583
2021-12-11 19:35:04,017 iteration 264 : loss : 0.363443, loss_ce: 0.160099
2021-12-11 19:35:06,877 iteration 265 : loss : 0.355927, loss_ce: 0.175346
2021-12-11 19:35:09,836 iteration 266 : loss : 0.367093, loss_ce: 0.152797
2021-12-11 19:35:12,670 iteration 267 : loss : 0.360803, loss_ce: 0.191079
2021-12-11 19:35:15,446 iteration 268 : loss : 0.350627, loss_ce: 0.177900
2021-12-11 19:35:18,277 iteration 269 : loss : 0.358282, loss_ce: 0.184586
2021-12-11 19:35:20,941 iteration 270 : loss : 0.337675, loss_ce: 0.139069
2021-12-11 19:35:23,679 iteration 271 : loss : 0.362666, loss_ce: 0.159492
2021-12-11 19:35:26,564 iteration 272 : loss : 0.349183, loss_ce: 0.163707
  4%|█▏                            | 16/400 [12:24<5:37:13, 52.69s/it]2021-12-11 19:35:29,147 iteration 273 : loss : 0.337080, loss_ce: 0.156383
2021-12-11 19:35:31,943 iteration 274 : loss : 0.362520, loss_ce: 0.178298
2021-12-11 19:35:34,877 iteration 275 : loss : 0.346984, loss_ce: 0.179569
2021-12-11 19:35:37,556 iteration 276 : loss : 0.376226, loss_ce: 0.192510
2021-12-11 19:35:40,566 iteration 277 : loss : 0.343215, loss_ce: 0.158659
2021-12-11 19:35:43,222 iteration 278 : loss : 0.358348, loss_ce: 0.157066
2021-12-11 19:35:46,004 iteration 279 : loss : 0.356886, loss_ce: 0.166425
2021-12-11 19:35:48,673 iteration 280 : loss : 0.363868, loss_ce: 0.176402
2021-12-11 19:35:51,631 iteration 281 : loss : 0.332947, loss_ce: 0.140636
2021-12-11 19:35:54,324 iteration 282 : loss : 0.355976, loss_ce: 0.174075
2021-12-11 19:35:57,170 iteration 283 : loss : 0.356871, loss_ce: 0.185744
2021-12-11 19:35:59,917 iteration 284 : loss : 0.326409, loss_ce: 0.154176
2021-12-11 19:36:02,832 iteration 285 : loss : 0.348136, loss_ce: 0.159907
2021-12-11 19:36:05,622 iteration 286 : loss : 0.344697, loss_ce: 0.152457
2021-12-11 19:36:08,284 iteration 287 : loss : 0.345473, loss_ce: 0.151744
2021-12-11 19:36:11,035 iteration 288 : loss : 0.357858, loss_ce: 0.174770
2021-12-11 19:36:13,763 iteration 289 : loss : 0.371636, loss_ce: 0.186253
  4%|█▎                            | 17/400 [13:11<5:25:48, 51.04s/it]2021-12-11 19:36:16,660 iteration 290 : loss : 0.354875, loss_ce: 0.163377
2021-12-11 19:36:19,486 iteration 291 : loss : 0.345047, loss_ce: 0.166676
2021-12-11 19:36:22,299 iteration 292 : loss : 0.329453, loss_ce: 0.157920
2021-12-11 19:36:25,208 iteration 293 : loss : 0.365586, loss_ce: 0.185303
2021-12-11 19:36:27,833 iteration 294 : loss : 0.346255, loss_ce: 0.155645
2021-12-11 19:36:30,536 iteration 295 : loss : 0.356506, loss_ce: 0.153759
2021-12-11 19:36:33,323 iteration 296 : loss : 0.336453, loss_ce: 0.150778
2021-12-11 19:36:35,994 iteration 297 : loss : 0.363851, loss_ce: 0.187766
2021-12-11 19:36:38,677 iteration 298 : loss : 0.345349, loss_ce: 0.153519
2021-12-11 19:36:41,426 iteration 299 : loss : 0.344676, loss_ce: 0.139548
2021-12-11 19:36:44,208 iteration 300 : loss : 0.345040, loss_ce: 0.155078
2021-12-11 19:36:46,875 iteration 301 : loss : 0.345271, loss_ce: 0.156265
2021-12-11 19:36:49,785 iteration 302 : loss : 0.389643, loss_ce: 0.232389
2021-12-11 19:36:52,657 iteration 303 : loss : 0.354662, loss_ce: 0.190453
2021-12-11 19:36:55,349 iteration 304 : loss : 0.343709, loss_ce: 0.158064
2021-12-11 19:36:57,995 iteration 305 : loss : 0.319150, loss_ce: 0.145029
2021-12-11 19:37:00,862 iteration 306 : loss : 0.361515, loss_ce: 0.164180
  4%|█▎                            | 18/400 [13:59<5:17:23, 49.85s/it]2021-12-11 19:37:03,692 iteration 307 : loss : 0.356646, loss_ce: 0.158902
2021-12-11 19:37:06,638 iteration 308 : loss : 0.335968, loss_ce: 0.147696
2021-12-11 19:37:09,519 iteration 309 : loss : 0.370274, loss_ce: 0.190988
2021-12-11 19:37:12,165 iteration 310 : loss : 0.337596, loss_ce: 0.134552
2021-12-11 19:37:14,820 iteration 311 : loss : 0.339241, loss_ce: 0.161984
2021-12-11 19:37:17,576 iteration 312 : loss : 0.332116, loss_ce: 0.110744
2021-12-11 19:37:20,364 iteration 313 : loss : 0.334271, loss_ce: 0.143439
2021-12-11 19:37:23,019 iteration 314 : loss : 0.352326, loss_ce: 0.170375
2021-12-11 19:37:25,788 iteration 315 : loss : 0.342463, loss_ce: 0.161443
2021-12-11 19:37:28,749 iteration 316 : loss : 0.348859, loss_ce: 0.163699
2021-12-11 19:37:31,567 iteration 317 : loss : 0.368103, loss_ce: 0.191103
2021-12-11 19:37:34,451 iteration 318 : loss : 0.372897, loss_ce: 0.191405
2021-12-11 19:37:37,196 iteration 319 : loss : 0.331862, loss_ce: 0.157811
2021-12-11 19:37:39,992 iteration 320 : loss : 0.317769, loss_ce: 0.134934
2021-12-11 19:37:42,826 iteration 321 : loss : 0.323710, loss_ce: 0.141802
2021-12-11 19:37:45,576 iteration 322 : loss : 0.338732, loss_ce: 0.159061
2021-12-11 19:37:48,297 iteration 323 : loss : 0.350305, loss_ce: 0.161950
  5%|█▍                            | 19/400 [14:46<5:11:58, 49.13s/it]2021-12-11 19:37:51,238 iteration 324 : loss : 0.324407, loss_ce: 0.139732
2021-12-11 19:37:53,938 iteration 325 : loss : 0.313748, loss_ce: 0.112904
2021-12-11 19:37:56,662 iteration 326 : loss : 0.383819, loss_ce: 0.163089
2021-12-11 19:37:59,279 iteration 327 : loss : 0.379689, loss_ce: 0.196440
2021-12-11 19:38:02,046 iteration 328 : loss : 0.320149, loss_ce: 0.129619
2021-12-11 19:38:05,004 iteration 329 : loss : 0.335323, loss_ce: 0.156058
2021-12-11 19:38:07,764 iteration 330 : loss : 0.368610, loss_ce: 0.194917
2021-12-11 19:38:10,450 iteration 331 : loss : 0.343995, loss_ce: 0.160527
2021-12-11 19:38:12,977 iteration 332 : loss : 0.328876, loss_ce: 0.153572
2021-12-11 19:38:15,798 iteration 333 : loss : 0.341210, loss_ce: 0.172926
2021-12-11 19:38:18,718 iteration 334 : loss : 0.338411, loss_ce: 0.156231
2021-12-11 19:38:21,534 iteration 335 : loss : 0.366584, loss_ce: 0.200660
2021-12-11 19:38:24,289 iteration 336 : loss : 0.332866, loss_ce: 0.153845
2021-12-11 19:38:27,061 iteration 337 : loss : 0.322692, loss_ce: 0.134442
2021-12-11 19:38:29,636 iteration 338 : loss : 0.337367, loss_ce: 0.166374
2021-12-11 19:38:32,298 iteration 339 : loss : 0.364413, loss_ce: 0.170516
2021-12-11 19:38:32,298 Training Data Eval:
2021-12-11 19:38:46,837   Average segmentation loss on training set: 0.3236
2021-12-11 19:38:46,838 Validation Data Eval:
2021-12-11 19:38:52,108   Average segmentation loss on validation set: 0.3423
2021-12-11 19:38:54,108 Found new lowest validation loss at iteration 339! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_best_val_loss_seed2.pth
2021-12-11 19:38:56,496 iteration 340 : loss : 0.348950, loss_ce: 0.178235
  5%|█▌                            | 20/400 [15:54<5:47:24, 54.85s/it]2021-12-11 19:38:59,255 iteration 341 : loss : 0.336376, loss_ce: 0.173266
2021-12-11 19:39:02,115 iteration 342 : loss : 0.337033, loss_ce: 0.176041
2021-12-11 19:39:04,952 iteration 343 : loss : 0.347090, loss_ce: 0.163185
2021-12-11 19:39:07,697 iteration 344 : loss : 0.332902, loss_ce: 0.168184
2021-12-11 19:39:10,466 iteration 345 : loss : 0.321702, loss_ce: 0.151128
2021-12-11 19:39:13,225 iteration 346 : loss : 0.339266, loss_ce: 0.148457
2021-12-11 19:39:15,994 iteration 347 : loss : 0.337902, loss_ce: 0.162047
2021-12-11 19:39:18,890 iteration 348 : loss : 0.332568, loss_ce: 0.144862
2021-12-11 19:39:21,617 iteration 349 : loss : 0.330407, loss_ce: 0.144416
2021-12-11 19:39:24,289 iteration 350 : loss : 0.371791, loss_ce: 0.191731
2021-12-11 19:39:26,903 iteration 351 : loss : 0.343075, loss_ce: 0.159706
2021-12-11 19:39:29,883 iteration 352 : loss : 0.320592, loss_ce: 0.159637
2021-12-11 19:39:32,710 iteration 353 : loss : 0.336726, loss_ce: 0.158599
2021-12-11 19:39:35,488 iteration 354 : loss : 0.336192, loss_ce: 0.156962
2021-12-11 19:39:38,300 iteration 355 : loss : 0.338919, loss_ce: 0.136463
2021-12-11 19:39:41,028 iteration 356 : loss : 0.332573, loss_ce: 0.143207
2021-12-11 19:39:43,686 iteration 357 : loss : 0.321430, loss_ce: 0.144622
  5%|█▌                            | 21/400 [16:41<5:31:57, 52.55s/it]2021-12-11 19:39:46,507 iteration 358 : loss : 0.358655, loss_ce: 0.194840
2021-12-11 19:39:49,390 iteration 359 : loss : 0.335060, loss_ce: 0.157538
2021-12-11 19:39:52,075 iteration 360 : loss : 0.331289, loss_ce: 0.138567
2021-12-11 19:39:54,794 iteration 361 : loss : 0.344463, loss_ce: 0.161472
2021-12-11 19:39:57,655 iteration 362 : loss : 0.365206, loss_ce: 0.209357
2021-12-11 19:40:00,399 iteration 363 : loss : 0.346207, loss_ce: 0.174799
2021-12-11 19:40:03,136 iteration 364 : loss : 0.333915, loss_ce: 0.129227
2021-12-11 19:40:05,994 iteration 365 : loss : 0.323518, loss_ce: 0.150564
2021-12-11 19:40:08,682 iteration 366 : loss : 0.322926, loss_ce: 0.122552
2021-12-11 19:40:11,528 iteration 367 : loss : 0.323580, loss_ce: 0.147421
2021-12-11 19:40:14,281 iteration 368 : loss : 0.357560, loss_ce: 0.162522
2021-12-11 19:40:17,063 iteration 369 : loss : 0.323170, loss_ce: 0.144587
2021-12-11 19:40:19,719 iteration 370 : loss : 0.367918, loss_ce: 0.201257
2021-12-11 19:40:22,412 iteration 371 : loss : 0.342202, loss_ce: 0.159804
2021-12-11 19:40:25,253 iteration 372 : loss : 0.328990, loss_ce: 0.157005
2021-12-11 19:40:28,153 iteration 373 : loss : 0.345170, loss_ce: 0.152381
2021-12-11 19:40:30,927 iteration 374 : loss : 0.333261, loss_ce: 0.158846
  6%|█▋                            | 22/400 [17:29<5:21:02, 50.96s/it]2021-12-11 19:40:33,783 iteration 375 : loss : 0.331032, loss_ce: 0.150184
2021-12-11 19:40:36,545 iteration 376 : loss : 0.315982, loss_ce: 0.113101
2021-12-11 19:40:39,247 iteration 377 : loss : 0.312580, loss_ce: 0.130161
2021-12-11 19:40:42,299 iteration 378 : loss : 0.312137, loss_ce: 0.126494
2021-12-11 19:40:45,129 iteration 379 : loss : 0.331670, loss_ce: 0.143372
2021-12-11 19:40:47,973 iteration 380 : loss : 0.363919, loss_ce: 0.187523
2021-12-11 19:40:50,646 iteration 381 : loss : 0.319092, loss_ce: 0.153093
2021-12-11 19:40:53,381 iteration 382 : loss : 0.338077, loss_ce: 0.168459
2021-12-11 19:40:56,208 iteration 383 : loss : 0.325746, loss_ce: 0.118978
2021-12-11 19:40:59,029 iteration 384 : loss : 0.318599, loss_ce: 0.152533
2021-12-11 19:41:01,721 iteration 385 : loss : 0.345228, loss_ce: 0.175348
2021-12-11 19:41:04,431 iteration 386 : loss : 0.325153, loss_ce: 0.136036
2021-12-11 19:41:07,172 iteration 387 : loss : 0.327500, loss_ce: 0.145237
2021-12-11 19:41:10,163 iteration 388 : loss : 0.344866, loss_ce: 0.158715
2021-12-11 19:41:12,950 iteration 389 : loss : 0.331466, loss_ce: 0.152694
2021-12-11 19:41:15,772 iteration 390 : loss : 0.345930, loss_ce: 0.171648
2021-12-11 19:41:18,511 iteration 391 : loss : 0.303387, loss_ce: 0.140160
  6%|█▋                            | 23/400 [18:16<5:13:50, 49.95s/it]2021-12-11 19:41:21,514 iteration 392 : loss : 0.322335, loss_ce: 0.150836
2021-12-11 19:41:24,170 iteration 393 : loss : 0.371292, loss_ce: 0.190234
2021-12-11 19:41:27,106 iteration 394 : loss : 0.330801, loss_ce: 0.128675
2021-12-11 19:41:29,724 iteration 395 : loss : 0.322370, loss_ce: 0.138581
2021-12-11 19:41:32,616 iteration 396 : loss : 0.312775, loss_ce: 0.138913
2021-12-11 19:41:35,336 iteration 397 : loss : 0.331283, loss_ce: 0.158736
2021-12-11 19:41:37,970 iteration 398 : loss : 0.303167, loss_ce: 0.135707
2021-12-11 19:41:40,729 iteration 399 : loss : 0.340745, loss_ce: 0.160273
2021-12-11 19:41:43,528 iteration 400 : loss : 0.312583, loss_ce: 0.133276
2021-12-11 19:41:46,362 iteration 401 : loss : 0.307382, loss_ce: 0.154567
2021-12-11 19:41:49,234 iteration 402 : loss : 0.331748, loss_ce: 0.144751
2021-12-11 19:41:51,934 iteration 403 : loss : 0.326130, loss_ce: 0.151188
2021-12-11 19:41:54,667 iteration 404 : loss : 0.293081, loss_ce: 0.127953
2021-12-11 19:41:57,441 iteration 405 : loss : 0.340776, loss_ce: 0.174752
2021-12-11 19:42:00,233 iteration 406 : loss : 0.318194, loss_ce: 0.146880
2021-12-11 19:42:03,169 iteration 407 : loss : 0.298063, loss_ce: 0.133516
2021-12-11 19:42:05,754 iteration 408 : loss : 0.309990, loss_ce: 0.151299
  6%|█▊                            | 24/400 [19:03<5:07:55, 49.14s/it]2021-12-11 19:42:08,533 iteration 409 : loss : 0.308712, loss_ce: 0.126270
2021-12-11 19:42:11,187 iteration 410 : loss : 0.310396, loss_ce: 0.137507
2021-12-11 19:42:14,015 iteration 411 : loss : 0.317964, loss_ce: 0.143739
2021-12-11 19:42:16,687 iteration 412 : loss : 0.303369, loss_ce: 0.153550
2021-12-11 19:42:19,515 iteration 413 : loss : 0.304365, loss_ce: 0.126810
2021-12-11 19:42:22,374 iteration 414 : loss : 0.308815, loss_ce: 0.138927
2021-12-11 19:42:25,260 iteration 415 : loss : 0.333934, loss_ce: 0.149353
2021-12-11 19:42:27,913 iteration 416 : loss : 0.337778, loss_ce: 0.169151
2021-12-11 19:42:30,740 iteration 417 : loss : 0.303552, loss_ce: 0.125292
2021-12-11 19:42:33,402 iteration 418 : loss : 0.339629, loss_ce: 0.198030
2021-12-11 19:42:36,183 iteration 419 : loss : 0.305963, loss_ce: 0.131495
2021-12-11 19:42:38,817 iteration 420 : loss : 0.296118, loss_ce: 0.136659
2021-12-11 19:42:41,824 iteration 421 : loss : 0.357233, loss_ce: 0.179041
2021-12-11 19:42:44,578 iteration 422 : loss : 0.284459, loss_ce: 0.122310
2021-12-11 19:42:47,321 iteration 423 : loss : 0.310128, loss_ce: 0.146775
2021-12-11 19:42:50,085 iteration 424 : loss : 0.311094, loss_ce: 0.132463
2021-12-11 19:42:50,085 Training Data Eval:
2021-12-11 19:43:04,861   Average segmentation loss on training set: 0.2950
2021-12-11 19:43:04,862 Validation Data Eval:
2021-12-11 19:43:10,010   Average segmentation loss on validation set: 0.3049
2021-12-11 19:43:11,953 Found new lowest validation loss at iteration 424! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_best_val_loss_seed2.pth
2021-12-11 19:43:14,350 iteration 425 : loss : 0.317925, loss_ce: 0.141537
  6%|█▉                            | 25/400 [20:12<5:43:34, 54.97s/it]2021-12-11 19:43:17,261 iteration 426 : loss : 0.315230, loss_ce: 0.142720
2021-12-11 19:43:20,110 iteration 427 : loss : 0.311373, loss_ce: 0.128917
2021-12-11 19:43:22,857 iteration 428 : loss : 0.316645, loss_ce: 0.150271
2021-12-11 19:43:25,630 iteration 429 : loss : 0.287593, loss_ce: 0.122995
2021-12-11 19:43:28,436 iteration 430 : loss : 0.333091, loss_ce: 0.147042
2021-12-11 19:43:31,207 iteration 431 : loss : 0.325502, loss_ce: 0.129769
2021-12-11 19:43:33,981 iteration 432 : loss : 0.303412, loss_ce: 0.126627
2021-12-11 19:43:36,783 iteration 433 : loss : 0.288923, loss_ce: 0.119246
2021-12-11 19:43:39,640 iteration 434 : loss : 0.297073, loss_ce: 0.129216
2021-12-11 19:43:42,390 iteration 435 : loss : 0.340254, loss_ce: 0.189718
2021-12-11 19:43:45,155 iteration 436 : loss : 0.323495, loss_ce: 0.137004
2021-12-11 19:43:48,051 iteration 437 : loss : 0.312705, loss_ce: 0.142503
2021-12-11 19:43:50,806 iteration 438 : loss : 0.352185, loss_ce: 0.194512
2021-12-11 19:43:53,531 iteration 439 : loss : 0.313797, loss_ce: 0.160927
2021-12-11 19:43:56,405 iteration 440 : loss : 0.289945, loss_ce: 0.121833
2021-12-11 19:43:59,262 iteration 441 : loss : 0.328906, loss_ce: 0.166625
2021-12-11 19:44:02,036 iteration 442 : loss : 0.337964, loss_ce: 0.134564
  6%|█▉                            | 26/400 [21:00<5:29:01, 52.79s/it]2021-12-11 19:44:04,956 iteration 443 : loss : 0.305594, loss_ce: 0.160708
2021-12-11 19:44:07,906 iteration 444 : loss : 0.316824, loss_ce: 0.142458
2021-12-11 19:44:10,759 iteration 445 : loss : 0.336614, loss_ce: 0.171672
2021-12-11 19:44:13,452 iteration 446 : loss : 0.327190, loss_ce: 0.122683
2021-12-11 19:44:16,396 iteration 447 : loss : 0.320907, loss_ce: 0.162497
2021-12-11 19:44:19,261 iteration 448 : loss : 0.325516, loss_ce: 0.157935
2021-12-11 19:44:22,011 iteration 449 : loss : 0.307949, loss_ce: 0.131906
2021-12-11 19:44:24,620 iteration 450 : loss : 0.322035, loss_ce: 0.149915
2021-12-11 19:44:27,375 iteration 451 : loss : 0.298052, loss_ce: 0.128049
2021-12-11 19:44:30,167 iteration 452 : loss : 0.284294, loss_ce: 0.127215
2021-12-11 19:44:33,103 iteration 453 : loss : 0.293360, loss_ce: 0.118950
2021-12-11 19:44:35,898 iteration 454 : loss : 0.322924, loss_ce: 0.134160
2021-12-11 19:44:38,793 iteration 455 : loss : 0.259880, loss_ce: 0.100228
2021-12-11 19:44:41,479 iteration 456 : loss : 0.318835, loss_ce: 0.142625
2021-12-11 19:44:44,150 iteration 457 : loss : 0.318103, loss_ce: 0.156033
2021-12-11 19:44:46,924 iteration 458 : loss : 0.328639, loss_ce: 0.163974
2021-12-11 19:44:49,864 iteration 459 : loss : 0.303953, loss_ce: 0.138726
  7%|██                            | 27/400 [21:48<5:18:55, 51.30s/it]2021-12-11 19:44:52,728 iteration 460 : loss : 0.316271, loss_ce: 0.134285
2021-12-11 19:44:55,435 iteration 461 : loss : 0.310298, loss_ce: 0.136435
2021-12-11 19:44:58,285 iteration 462 : loss : 0.281795, loss_ce: 0.107662
2021-12-11 19:45:01,007 iteration 463 : loss : 0.305371, loss_ce: 0.151939
2021-12-11 19:45:03,972 iteration 464 : loss : 0.339586, loss_ce: 0.161182
2021-12-11 19:45:06,600 iteration 465 : loss : 0.294927, loss_ce: 0.154626
2021-12-11 19:45:09,528 iteration 466 : loss : 0.313814, loss_ce: 0.150663
2021-12-11 19:45:12,273 iteration 467 : loss : 0.315367, loss_ce: 0.139624
2021-12-11 19:45:15,060 iteration 468 : loss : 0.303899, loss_ce: 0.132902
2021-12-11 19:45:17,676 iteration 469 : loss : 0.298373, loss_ce: 0.133888
2021-12-11 19:45:20,528 iteration 470 : loss : 0.294367, loss_ce: 0.140612
2021-12-11 19:45:23,190 iteration 471 : loss : 0.302282, loss_ce: 0.142966
2021-12-11 19:45:26,142 iteration 472 : loss : 0.314872, loss_ce: 0.140751
2021-12-11 19:45:28,979 iteration 473 : loss : 0.270605, loss_ce: 0.126981
2021-12-11 19:45:31,853 iteration 474 : loss : 0.293952, loss_ce: 0.133032
2021-12-11 19:45:34,514 iteration 475 : loss : 0.281122, loss_ce: 0.109444
2021-12-11 19:45:37,228 iteration 476 : loss : 0.294665, loss_ce: 0.155761
  7%|██                            | 28/400 [22:35<5:10:43, 50.12s/it]2021-12-11 19:45:40,165 iteration 477 : loss : 0.279182, loss_ce: 0.130096
2021-12-11 19:45:43,057 iteration 478 : loss : 0.290399, loss_ce: 0.123791
2021-12-11 19:45:45,820 iteration 479 : loss : 0.303427, loss_ce: 0.138922
2021-12-11 19:45:48,356 iteration 480 : loss : 0.311542, loss_ce: 0.150941
2021-12-11 19:45:51,149 iteration 481 : loss : 0.280529, loss_ce: 0.136487
2021-12-11 19:45:53,981 iteration 482 : loss : 0.275866, loss_ce: 0.114395
2021-12-11 19:45:56,718 iteration 483 : loss : 0.289731, loss_ce: 0.111839
2021-12-11 19:45:59,442 iteration 484 : loss : 0.308145, loss_ce: 0.143514
2021-12-11 19:46:02,115 iteration 485 : loss : 0.280915, loss_ce: 0.118207
2021-12-11 19:46:04,889 iteration 486 : loss : 0.312576, loss_ce: 0.116094
2021-12-11 19:46:07,693 iteration 487 : loss : 0.323471, loss_ce: 0.171350
2021-12-11 19:46:10,601 iteration 488 : loss : 0.332844, loss_ce: 0.163938
2021-12-11 19:46:13,305 iteration 489 : loss : 0.277325, loss_ce: 0.119269
2021-12-11 19:46:15,908 iteration 490 : loss : 0.299194, loss_ce: 0.133347
2021-12-11 19:46:18,720 iteration 491 : loss : 0.299731, loss_ce: 0.132832
2021-12-11 19:46:21,360 iteration 492 : loss : 0.296293, loss_ce: 0.148528
2021-12-11 19:46:24,259 iteration 493 : loss : 0.338663, loss_ce: 0.162645
  7%|██▏                           | 29/400 [23:22<5:04:10, 49.19s/it]2021-12-11 19:46:27,090 iteration 494 : loss : 0.302193, loss_ce: 0.126287
2021-12-11 19:46:29,715 iteration 495 : loss : 0.278106, loss_ce: 0.121487
2021-12-11 19:46:32,263 iteration 496 : loss : 0.273531, loss_ce: 0.131731
2021-12-11 19:46:34,936 iteration 497 : loss : 0.256222, loss_ce: 0.108448
2021-12-11 19:46:37,831 iteration 498 : loss : 0.256638, loss_ce: 0.118662
2021-12-11 19:46:40,580 iteration 499 : loss : 0.310630, loss_ce: 0.148581
2021-12-11 19:46:43,378 iteration 500 : loss : 0.286959, loss_ce: 0.126342
2021-12-11 19:46:46,007 iteration 501 : loss : 0.247497, loss_ce: 0.096330
2021-12-11 19:46:48,639 iteration 502 : loss : 0.307179, loss_ce: 0.118662
2021-12-11 19:46:51,351 iteration 503 : loss : 0.291551, loss_ce: 0.144728
2021-12-11 19:46:54,201 iteration 504 : loss : 0.264372, loss_ce: 0.117307
2021-12-11 19:46:57,039 iteration 505 : loss : 0.261985, loss_ce: 0.121960
2021-12-11 19:46:59,822 iteration 506 : loss : 0.285849, loss_ce: 0.147249
2021-12-11 19:47:02,552 iteration 507 : loss : 0.266693, loss_ce: 0.110144
2021-12-11 19:47:05,374 iteration 508 : loss : 0.285392, loss_ce: 0.111371
2021-12-11 19:47:08,336 iteration 509 : loss : 0.268756, loss_ce: 0.132778
2021-12-11 19:47:08,336 Training Data Eval:
2021-12-11 19:47:23,084   Average segmentation loss on training set: 0.2567
2021-12-11 19:47:23,085 Validation Data Eval:
2021-12-11 19:47:28,215   Average segmentation loss on validation set: 0.2547
2021-12-11 19:47:30,128 Found new lowest validation loss at iteration 509! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_best_val_loss_seed2.pth
2021-12-11 19:47:32,442 iteration 510 : loss : 0.270704, loss_ce: 0.123293
  8%|██▎                           | 30/400 [24:30<5:38:29, 54.89s/it]2021-12-11 19:47:35,167 iteration 511 : loss : 0.255887, loss_ce: 0.106130
2021-12-11 19:47:37,912 iteration 512 : loss : 0.289418, loss_ce: 0.142977
2021-12-11 19:47:40,845 iteration 513 : loss : 0.269752, loss_ce: 0.122013
2021-12-11 19:47:43,609 iteration 514 : loss : 0.287759, loss_ce: 0.132666
2021-12-11 19:47:46,333 iteration 515 : loss : 0.296965, loss_ce: 0.144050
2021-12-11 19:47:49,045 iteration 516 : loss : 0.260314, loss_ce: 0.104079
2021-12-11 19:47:51,766 iteration 517 : loss : 0.252104, loss_ce: 0.114643
2021-12-11 19:47:54,782 iteration 518 : loss : 0.375577, loss_ce: 0.174167
2021-12-11 19:47:57,689 iteration 519 : loss : 0.288134, loss_ce: 0.141710
2021-12-11 19:48:00,456 iteration 520 : loss : 0.293542, loss_ce: 0.140236
2021-12-11 19:48:03,157 iteration 521 : loss : 0.273129, loss_ce: 0.111907
2021-12-11 19:48:06,041 iteration 522 : loss : 0.361850, loss_ce: 0.176514
2021-12-11 19:48:08,949 iteration 523 : loss : 0.260895, loss_ce: 0.096253
2021-12-11 19:48:11,814 iteration 524 : loss : 0.318549, loss_ce: 0.164779
2021-12-11 19:48:14,695 iteration 525 : loss : 0.258133, loss_ce: 0.116505
2021-12-11 19:48:17,544 iteration 526 : loss : 0.258509, loss_ce: 0.113653
2021-12-11 19:48:20,250 iteration 527 : loss : 0.292452, loss_ce: 0.127779
  8%|██▎                           | 31/400 [25:18<5:24:30, 52.77s/it]2021-12-11 19:48:23,188 iteration 528 : loss : 0.272406, loss_ce: 0.123363
2021-12-11 19:48:26,320 iteration 529 : loss : 0.296848, loss_ce: 0.108397
2021-12-11 19:48:29,075 iteration 530 : loss : 0.274802, loss_ce: 0.117197
2021-12-11 19:48:31,666 iteration 531 : loss : 0.259672, loss_ce: 0.110914
2021-12-11 19:48:34,441 iteration 532 : loss : 0.281084, loss_ce: 0.139237
2021-12-11 19:48:37,323 iteration 533 : loss : 0.266777, loss_ce: 0.120630
2021-12-11 19:48:40,145 iteration 534 : loss : 0.267010, loss_ce: 0.122615
2021-12-11 19:48:42,919 iteration 535 : loss : 0.259782, loss_ce: 0.115179
2021-12-11 19:48:45,694 iteration 536 : loss : 0.284666, loss_ce: 0.148079
2021-12-11 19:48:48,472 iteration 537 : loss : 0.314344, loss_ce: 0.153139
2021-12-11 19:48:51,321 iteration 538 : loss : 0.282769, loss_ce: 0.141491
2021-12-11 19:48:54,040 iteration 539 : loss : 0.245894, loss_ce: 0.108532
2021-12-11 19:48:57,065 iteration 540 : loss : 0.304080, loss_ce: 0.166024
2021-12-11 19:48:59,884 iteration 541 : loss : 0.305742, loss_ce: 0.150828
2021-12-11 19:49:02,716 iteration 542 : loss : 0.294705, loss_ce: 0.121055
2021-12-11 19:49:05,452 iteration 543 : loss : 0.261384, loss_ce: 0.117546
2021-12-11 19:49:08,134 iteration 544 : loss : 0.289529, loss_ce: 0.101425
  8%|██▍                           | 32/400 [26:06<5:14:39, 51.30s/it]2021-12-11 19:49:10,996 iteration 545 : loss : 0.300875, loss_ce: 0.139029
2021-12-11 19:49:13,911 iteration 546 : loss : 0.240473, loss_ce: 0.108908
2021-12-11 19:49:16,765 iteration 547 : loss : 0.306244, loss_ce: 0.132064
2021-12-11 19:49:19,480 iteration 548 : loss : 0.331562, loss_ce: 0.139637
2021-12-11 19:49:22,281 iteration 549 : loss : 0.243251, loss_ce: 0.107807
2021-12-11 19:49:24,932 iteration 550 : loss : 0.271533, loss_ce: 0.119952
2021-12-11 19:49:27,764 iteration 551 : loss : 0.244995, loss_ce: 0.091170
2021-12-11 19:49:30,467 iteration 552 : loss : 0.279611, loss_ce: 0.129981
2021-12-11 19:49:33,269 iteration 553 : loss : 0.235151, loss_ce: 0.101076
2021-12-11 19:49:35,993 iteration 554 : loss : 0.304540, loss_ce: 0.139967
2021-12-11 19:49:38,662 iteration 555 : loss : 0.252670, loss_ce: 0.102767
2021-12-11 19:49:41,555 iteration 556 : loss : 0.302821, loss_ce: 0.149569
2021-12-11 19:49:44,347 iteration 557 : loss : 0.289856, loss_ce: 0.149409
2021-12-11 19:49:46,977 iteration 558 : loss : 0.262049, loss_ce: 0.115291
2021-12-11 19:49:49,887 iteration 559 : loss : 0.293931, loss_ce: 0.130261
2021-12-11 19:49:52,711 iteration 560 : loss : 0.255074, loss_ce: 0.117277
2021-12-11 19:49:55,464 iteration 561 : loss : 0.250066, loss_ce: 0.120034
  8%|██▍                           | 33/400 [26:53<5:06:29, 50.11s/it]2021-12-11 19:49:58,336 iteration 562 : loss : 0.285432, loss_ce: 0.121970
2021-12-11 19:50:00,937 iteration 563 : loss : 0.244238, loss_ce: 0.118978
2021-12-11 19:50:03,863 iteration 564 : loss : 0.274037, loss_ce: 0.138403
2021-12-11 19:50:06,691 iteration 565 : loss : 0.247950, loss_ce: 0.119338
2021-12-11 19:50:09,523 iteration 566 : loss : 0.237212, loss_ce: 0.090947
2021-12-11 19:50:12,360 iteration 567 : loss : 0.323193, loss_ce: 0.143391
2021-12-11 19:50:15,172 iteration 568 : loss : 0.276267, loss_ce: 0.115495
2021-12-11 19:50:18,002 iteration 569 : loss : 0.302418, loss_ce: 0.109408
2021-12-11 19:50:20,770 iteration 570 : loss : 0.318676, loss_ce: 0.149240
2021-12-11 19:50:23,723 iteration 571 : loss : 0.233035, loss_ce: 0.108059
2021-12-11 19:50:26,613 iteration 572 : loss : 0.328105, loss_ce: 0.168328
2021-12-11 19:50:29,224 iteration 573 : loss : 0.260645, loss_ce: 0.123562
2021-12-11 19:50:32,053 iteration 574 : loss : 0.250262, loss_ce: 0.103778
2021-12-11 19:50:34,748 iteration 575 : loss : 0.279522, loss_ce: 0.114838
2021-12-11 19:50:37,415 iteration 576 : loss : 0.255397, loss_ce: 0.133476
2021-12-11 19:50:40,200 iteration 577 : loss : 0.261678, loss_ce: 0.123537
2021-12-11 19:50:43,055 iteration 578 : loss : 0.270308, loss_ce: 0.122457
  8%|██▌                           | 34/400 [27:41<5:01:03, 49.35s/it]2021-12-11 19:50:45,840 iteration 579 : loss : 0.256711, loss_ce: 0.119729
2021-12-11 19:50:48,717 iteration 580 : loss : 0.252181, loss_ce: 0.104298
2021-12-11 19:50:51,751 iteration 581 : loss : 0.241096, loss_ce: 0.104565
2021-12-11 19:50:54,500 iteration 582 : loss : 0.246633, loss_ce: 0.112243
2021-12-11 19:50:57,251 iteration 583 : loss : 0.229683, loss_ce: 0.110282
2021-12-11 19:51:00,184 iteration 584 : loss : 0.251197, loss_ce: 0.115022
2021-12-11 19:51:03,051 iteration 585 : loss : 0.298023, loss_ce: 0.126284
2021-12-11 19:51:05,897 iteration 586 : loss : 0.254725, loss_ce: 0.105577
2021-12-11 19:51:08,587 iteration 587 : loss : 0.233243, loss_ce: 0.110220
2021-12-11 19:51:11,259 iteration 588 : loss : 0.245927, loss_ce: 0.102701
2021-12-11 19:51:14,150 iteration 589 : loss : 0.271716, loss_ce: 0.131766
2021-12-11 19:51:16,960 iteration 590 : loss : 0.228729, loss_ce: 0.094665
2021-12-11 19:51:19,787 iteration 591 : loss : 0.283827, loss_ce: 0.129647
2021-12-11 19:51:22,519 iteration 592 : loss : 0.257394, loss_ce: 0.122368
2021-12-11 19:51:25,157 iteration 593 : loss : 0.265626, loss_ce: 0.117965
2021-12-11 19:51:27,855 iteration 594 : loss : 0.243809, loss_ce: 0.100830
2021-12-11 19:51:27,855 Training Data Eval:
2021-12-11 19:51:43,023   Average segmentation loss on training set: 0.2142
2021-12-11 19:51:43,023 Validation Data Eval:
2021-12-11 19:51:48,174   Average segmentation loss on validation set: 0.2198
2021-12-11 19:51:50,117 Found new lowest validation loss at iteration 594! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_best_val_loss_seed2.pth
2021-12-11 19:51:52,460 iteration 595 : loss : 0.320984, loss_ce: 0.153388
  9%|██▋                           | 35/400 [28:50<5:36:49, 55.37s/it]2021-12-11 19:51:55,445 iteration 596 : loss : 0.243718, loss_ce: 0.115736
2021-12-11 19:51:58,223 iteration 597 : loss : 0.231197, loss_ce: 0.099435
2021-12-11 19:52:00,949 iteration 598 : loss : 0.259165, loss_ce: 0.102850
2021-12-11 19:52:03,674 iteration 599 : loss : 0.266124, loss_ce: 0.103333
2021-12-11 19:52:06,461 iteration 600 : loss : 0.250258, loss_ce: 0.108162
2021-12-11 19:52:09,348 iteration 601 : loss : 0.259114, loss_ce: 0.107897
2021-12-11 19:52:12,078 iteration 602 : loss : 0.273380, loss_ce: 0.126383
2021-12-11 19:52:14,703 iteration 603 : loss : 0.232305, loss_ce: 0.096376
2021-12-11 19:52:17,364 iteration 604 : loss : 0.299482, loss_ce: 0.112476
2021-12-11 19:52:20,399 iteration 605 : loss : 0.230638, loss_ce: 0.099004
2021-12-11 19:52:23,036 iteration 606 : loss : 0.300085, loss_ce: 0.141704
2021-12-11 19:52:25,703 iteration 607 : loss : 0.222262, loss_ce: 0.093765
2021-12-11 19:52:28,357 iteration 608 : loss : 0.277424, loss_ce: 0.129843
2021-12-11 19:52:31,047 iteration 609 : loss : 0.211190, loss_ce: 0.082029
2021-12-11 19:52:33,982 iteration 610 : loss : 0.274981, loss_ce: 0.132424
2021-12-11 19:52:36,655 iteration 611 : loss : 0.235189, loss_ce: 0.110845
2021-12-11 19:52:39,283 iteration 612 : loss : 0.213298, loss_ce: 0.094459
  9%|██▋                           | 36/400 [29:37<5:20:20, 52.80s/it]2021-12-11 19:52:42,097 iteration 613 : loss : 0.266170, loss_ce: 0.115070
2021-12-11 19:52:45,036 iteration 614 : loss : 0.247371, loss_ce: 0.120801
2021-12-11 19:52:47,806 iteration 615 : loss : 0.262863, loss_ce: 0.127682
2021-12-11 19:52:50,551 iteration 616 : loss : 0.279770, loss_ce: 0.106719
2021-12-11 19:52:53,314 iteration 617 : loss : 0.300756, loss_ce: 0.132810
2021-12-11 19:52:56,214 iteration 618 : loss : 0.221564, loss_ce: 0.104054
2021-12-11 19:52:59,162 iteration 619 : loss : 0.228260, loss_ce: 0.102214
2021-12-11 19:53:02,033 iteration 620 : loss : 0.237731, loss_ce: 0.104599
2021-12-11 19:53:04,736 iteration 621 : loss : 0.302450, loss_ce: 0.137565
2021-12-11 19:53:07,729 iteration 622 : loss : 0.239543, loss_ce: 0.107391
2021-12-11 19:53:10,435 iteration 623 : loss : 0.323466, loss_ce: 0.156303
2021-12-11 19:53:13,149 iteration 624 : loss : 0.274249, loss_ce: 0.128816
2021-12-11 19:53:15,840 iteration 625 : loss : 0.219132, loss_ce: 0.098563
2021-12-11 19:53:18,657 iteration 626 : loss : 0.259918, loss_ce: 0.115685
2021-12-11 19:53:21,395 iteration 627 : loss : 0.233445, loss_ce: 0.094219
2021-12-11 19:53:24,095 iteration 628 : loss : 0.292767, loss_ce: 0.124458
2021-12-11 19:53:26,932 iteration 629 : loss : 0.207674, loss_ce: 0.084782
  9%|██▊                           | 37/400 [30:25<5:10:06, 51.26s/it]2021-12-11 19:53:29,695 iteration 630 : loss : 0.289141, loss_ce: 0.143290
2021-12-11 19:53:32,322 iteration 631 : loss : 0.278690, loss_ce: 0.117866
2021-12-11 19:53:34,934 iteration 632 : loss : 0.208976, loss_ce: 0.088048
2021-12-11 19:53:37,793 iteration 633 : loss : 0.321103, loss_ce: 0.170319
2021-12-11 19:53:40,738 iteration 634 : loss : 0.229144, loss_ce: 0.096151
2021-12-11 19:53:43,526 iteration 635 : loss : 0.283012, loss_ce: 0.131501
2021-12-11 19:53:46,231 iteration 636 : loss : 0.298411, loss_ce: 0.116778
2021-12-11 19:53:48,857 iteration 637 : loss : 0.244721, loss_ce: 0.115003
2021-12-11 19:53:51,710 iteration 638 : loss : 0.199856, loss_ce: 0.089710
2021-12-11 19:53:54,661 iteration 639 : loss : 0.217152, loss_ce: 0.098582
2021-12-11 19:53:57,489 iteration 640 : loss : 0.209281, loss_ce: 0.083788
2021-12-11 19:54:00,163 iteration 641 : loss : 0.229260, loss_ce: 0.103925
2021-12-11 19:54:03,010 iteration 642 : loss : 0.236117, loss_ce: 0.098992
2021-12-11 19:54:05,708 iteration 643 : loss : 0.244377, loss_ce: 0.103049
2021-12-11 19:54:08,600 iteration 644 : loss : 0.269226, loss_ce: 0.106905
2021-12-11 19:54:11,418 iteration 645 : loss : 0.305563, loss_ce: 0.139082
2021-12-11 19:54:14,284 iteration 646 : loss : 0.286273, loss_ce: 0.133053
 10%|██▊                           | 38/400 [31:12<5:02:12, 50.09s/it]2021-12-11 19:54:17,199 iteration 647 : loss : 0.207949, loss_ce: 0.086779
2021-12-11 19:54:20,009 iteration 648 : loss : 0.297417, loss_ce: 0.138053
2021-12-11 19:54:22,740 iteration 649 : loss : 0.243622, loss_ce: 0.109438
2021-12-11 19:54:25,476 iteration 650 : loss : 0.234158, loss_ce: 0.113025
2021-12-11 19:54:28,207 iteration 651 : loss : 0.333677, loss_ce: 0.155671
2021-12-11 19:54:30,893 iteration 652 : loss : 0.229403, loss_ce: 0.090901
2021-12-11 19:54:33,740 iteration 653 : loss : 0.292674, loss_ce: 0.119095
2021-12-11 19:54:36,444 iteration 654 : loss : 0.270170, loss_ce: 0.120089
2021-12-11 19:54:39,229 iteration 655 : loss : 0.204951, loss_ce: 0.089629
2021-12-11 19:54:41,847 iteration 656 : loss : 0.240846, loss_ce: 0.106987
2021-12-11 19:54:44,599 iteration 657 : loss : 0.210075, loss_ce: 0.094854
2021-12-11 19:54:47,436 iteration 658 : loss : 0.306059, loss_ce: 0.108374
2021-12-11 19:54:50,385 iteration 659 : loss : 0.252927, loss_ce: 0.118714
2021-12-11 19:54:53,154 iteration 660 : loss : 0.266631, loss_ce: 0.111142
2021-12-11 19:54:55,915 iteration 661 : loss : 0.211995, loss_ce: 0.091017
2021-12-11 19:54:58,622 iteration 662 : loss : 0.214213, loss_ce: 0.104870
2021-12-11 19:55:01,381 iteration 663 : loss : 0.270948, loss_ce: 0.106733
 10%|██▉                           | 39/400 [31:59<4:55:58, 49.19s/it]2021-12-11 19:55:04,179 iteration 664 : loss : 0.248648, loss_ce: 0.123115
2021-12-11 19:55:06,974 iteration 665 : loss : 0.243722, loss_ce: 0.110385
2021-12-11 19:55:09,821 iteration 666 : loss : 0.226164, loss_ce: 0.096638
2021-12-11 19:55:12,687 iteration 667 : loss : 0.279861, loss_ce: 0.129368
2021-12-11 19:55:15,575 iteration 668 : loss : 0.223153, loss_ce: 0.100782
2021-12-11 19:55:18,250 iteration 669 : loss : 0.230705, loss_ce: 0.113261
2021-12-11 19:55:21,100 iteration 670 : loss : 0.206991, loss_ce: 0.090360
2021-12-11 19:55:23,999 iteration 671 : loss : 0.207603, loss_ce: 0.076061
2021-12-11 19:55:26,742 iteration 672 : loss : 0.210383, loss_ce: 0.087665
2021-12-11 19:55:29,497 iteration 673 : loss : 0.238259, loss_ce: 0.118995
2021-12-11 19:55:32,440 iteration 674 : loss : 0.243372, loss_ce: 0.099528
2021-12-11 19:55:35,425 iteration 675 : loss : 0.210942, loss_ce: 0.086756
2021-12-11 19:55:38,104 iteration 676 : loss : 0.296893, loss_ce: 0.119550
2021-12-11 19:55:40,872 iteration 677 : loss : 0.225005, loss_ce: 0.106190
2021-12-11 19:55:43,799 iteration 678 : loss : 0.214856, loss_ce: 0.088109
2021-12-11 19:55:46,473 iteration 679 : loss : 0.241639, loss_ce: 0.121715
2021-12-11 19:55:46,473 Training Data Eval:
2021-12-11 19:56:01,443   Average segmentation loss on training set: 0.1996
2021-12-11 19:56:01,443 Validation Data Eval:
2021-12-11 19:56:06,624   Average segmentation loss on validation set: 0.2217
2021-12-11 19:56:09,573 iteration 680 : loss : 0.279079, loss_ce: 0.089392
 10%|███                           | 40/400 [33:07<5:29:20, 54.89s/it]2021-12-11 19:56:12,420 iteration 681 : loss : 0.235306, loss_ce: 0.112765
2021-12-11 19:56:15,183 iteration 682 : loss : 0.193663, loss_ce: 0.075701
2021-12-11 19:56:17,909 iteration 683 : loss : 0.231778, loss_ce: 0.097645
2021-12-11 19:56:20,850 iteration 684 : loss : 0.247739, loss_ce: 0.118014
2021-12-11 19:56:23,570 iteration 685 : loss : 0.200684, loss_ce: 0.082714
2021-12-11 19:56:26,241 iteration 686 : loss : 0.218325, loss_ce: 0.085992
2021-12-11 19:56:28,963 iteration 687 : loss : 0.199218, loss_ce: 0.088114
2021-12-11 19:56:31,925 iteration 688 : loss : 0.238746, loss_ce: 0.102740
2021-12-11 19:56:34,819 iteration 689 : loss : 0.214137, loss_ce: 0.097671
2021-12-11 19:56:37,540 iteration 690 : loss : 0.265306, loss_ce: 0.135759
2021-12-11 19:56:40,365 iteration 691 : loss : 0.218708, loss_ce: 0.102056
2021-12-11 19:56:43,212 iteration 692 : loss : 0.233086, loss_ce: 0.098816
2021-12-11 19:56:46,084 iteration 693 : loss : 0.221354, loss_ce: 0.099457
2021-12-11 19:56:48,879 iteration 694 : loss : 0.219902, loss_ce: 0.089074
2021-12-11 19:56:51,661 iteration 695 : loss : 0.241163, loss_ce: 0.092176
2021-12-11 19:56:54,317 iteration 696 : loss : 0.247433, loss_ce: 0.112475
2021-12-11 19:56:57,154 iteration 697 : loss : 0.265679, loss_ce: 0.116761
 10%|███                           | 41/400 [33:55<5:15:18, 52.70s/it]2021-12-11 19:57:00,040 iteration 698 : loss : 0.242880, loss_ce: 0.099880
2021-12-11 19:57:02,815 iteration 699 : loss : 0.179964, loss_ce: 0.071915
2021-12-11 19:57:05,721 iteration 700 : loss : 0.207235, loss_ce: 0.085942
2021-12-11 19:57:08,507 iteration 701 : loss : 0.272776, loss_ce: 0.130929
2021-12-11 19:57:11,348 iteration 702 : loss : 0.237711, loss_ce: 0.093101
2021-12-11 19:57:14,091 iteration 703 : loss : 0.213611, loss_ce: 0.093304
2021-12-11 19:57:17,009 iteration 704 : loss : 0.358397, loss_ce: 0.136839
2021-12-11 19:57:19,832 iteration 705 : loss : 0.284898, loss_ce: 0.140055
2021-12-11 19:57:22,674 iteration 706 : loss : 0.215873, loss_ce: 0.090579
2021-12-11 19:57:25,549 iteration 707 : loss : 0.267715, loss_ce: 0.120668
2021-12-11 19:57:28,289 iteration 708 : loss : 0.199208, loss_ce: 0.083213
2021-12-11 19:57:31,208 iteration 709 : loss : 0.203886, loss_ce: 0.089025
2021-12-11 19:57:34,010 iteration 710 : loss : 0.247370, loss_ce: 0.106666
2021-12-11 19:57:36,804 iteration 711 : loss : 0.265158, loss_ce: 0.113543
2021-12-11 19:57:39,630 iteration 712 : loss : 0.233802, loss_ce: 0.111046
2021-12-11 19:57:42,412 iteration 713 : loss : 0.224089, loss_ce: 0.096916
2021-12-11 19:57:45,226 iteration 714 : loss : 0.244076, loss_ce: 0.121050
 10%|███▏                          | 42/400 [34:43<5:06:09, 51.31s/it]2021-12-11 19:57:48,108 iteration 715 : loss : 0.232782, loss_ce: 0.112739
2021-12-11 19:57:50,978 iteration 716 : loss : 0.243886, loss_ce: 0.090215
2021-12-11 19:57:53,666 iteration 717 : loss : 0.199427, loss_ce: 0.083288
2021-12-11 19:57:56,362 iteration 718 : loss : 0.202775, loss_ce: 0.080072
2021-12-11 19:57:59,125 iteration 719 : loss : 0.261750, loss_ce: 0.120880
2021-12-11 19:58:01,862 iteration 720 : loss : 0.219217, loss_ce: 0.096441
2021-12-11 19:58:04,903 iteration 721 : loss : 0.275534, loss_ce: 0.120305
2021-12-11 19:58:07,788 iteration 722 : loss : 0.188851, loss_ce: 0.085413
2021-12-11 19:58:10,691 iteration 723 : loss : 0.224773, loss_ce: 0.094382
2021-12-11 19:58:13,654 iteration 724 : loss : 0.206249, loss_ce: 0.085645
2021-12-11 19:58:16,286 iteration 725 : loss : 0.181606, loss_ce: 0.077153
2021-12-11 19:58:19,102 iteration 726 : loss : 0.253541, loss_ce: 0.130590
2021-12-11 19:58:21,883 iteration 727 : loss : 0.187718, loss_ce: 0.083009
2021-12-11 19:58:24,576 iteration 728 : loss : 0.200341, loss_ce: 0.075533
2021-12-11 19:58:27,283 iteration 729 : loss : 0.292122, loss_ce: 0.150672
2021-12-11 19:58:30,145 iteration 730 : loss : 0.230215, loss_ce: 0.089902
2021-12-11 19:58:32,895 iteration 731 : loss : 0.246136, loss_ce: 0.095683
 11%|███▏                          | 43/400 [35:31<4:58:47, 50.22s/it]2021-12-11 19:58:35,876 iteration 732 : loss : 0.178786, loss_ce: 0.079173
2021-12-11 19:58:38,704 iteration 733 : loss : 0.241398, loss_ce: 0.098485
2021-12-11 19:58:41,366 iteration 734 : loss : 0.250839, loss_ce: 0.126453
2021-12-11 19:58:44,195 iteration 735 : loss : 0.210584, loss_ce: 0.084253
2021-12-11 19:58:47,133 iteration 736 : loss : 0.192513, loss_ce: 0.083122
2021-12-11 19:58:49,839 iteration 737 : loss : 0.209874, loss_ce: 0.089356
2021-12-11 19:58:52,803 iteration 738 : loss : 0.210494, loss_ce: 0.090083
2021-12-11 19:58:55,525 iteration 739 : loss : 0.242732, loss_ce: 0.134224
2021-12-11 19:58:58,339 iteration 740 : loss : 0.161928, loss_ce: 0.068283
2021-12-11 19:59:00,983 iteration 741 : loss : 0.221016, loss_ce: 0.101294
2021-12-11 19:59:03,913 iteration 742 : loss : 0.249953, loss_ce: 0.104095
2021-12-11 19:59:06,543 iteration 743 : loss : 0.253355, loss_ce: 0.100914
2021-12-11 19:59:09,196 iteration 744 : loss : 0.206192, loss_ce: 0.084158
2021-12-11 19:59:12,099 iteration 745 : loss : 0.231106, loss_ce: 0.100493
2021-12-11 19:59:14,862 iteration 746 : loss : 0.245135, loss_ce: 0.083652
2021-12-11 19:59:17,572 iteration 747 : loss : 0.213775, loss_ce: 0.104609
2021-12-11 19:59:20,309 iteration 748 : loss : 0.234218, loss_ce: 0.091633
 11%|███▎                          | 44/400 [36:18<4:52:56, 49.37s/it]2021-12-11 19:59:23,082 iteration 749 : loss : 0.245057, loss_ce: 0.098243
2021-12-11 19:59:25,807 iteration 750 : loss : 0.192548, loss_ce: 0.072669
2021-12-11 19:59:28,493 iteration 751 : loss : 0.200019, loss_ce: 0.082922
2021-12-11 19:59:31,338 iteration 752 : loss : 0.228055, loss_ce: 0.105900
2021-12-11 19:59:34,155 iteration 753 : loss : 0.191681, loss_ce: 0.079544
2021-12-11 19:59:37,012 iteration 754 : loss : 0.179644, loss_ce: 0.065149
2021-12-11 19:59:39,773 iteration 755 : loss : 0.201377, loss_ce: 0.081379
2021-12-11 19:59:42,480 iteration 756 : loss : 0.183249, loss_ce: 0.078433
2021-12-11 19:59:45,227 iteration 757 : loss : 0.212475, loss_ce: 0.086912
2021-12-11 19:59:47,943 iteration 758 : loss : 0.262699, loss_ce: 0.104405
2021-12-11 19:59:50,764 iteration 759 : loss : 0.220239, loss_ce: 0.088419
2021-12-11 19:59:53,580 iteration 760 : loss : 0.191981, loss_ce: 0.075500
2021-12-11 19:59:56,554 iteration 761 : loss : 0.191471, loss_ce: 0.079238
2021-12-11 19:59:59,225 iteration 762 : loss : 0.181946, loss_ce: 0.083252
2021-12-11 20:00:02,088 iteration 763 : loss : 0.216331, loss_ce: 0.099215
2021-12-11 20:00:04,754 iteration 764 : loss : 0.213879, loss_ce: 0.089617
2021-12-11 20:00:04,755 Training Data Eval:
2021-12-11 20:00:19,894   Average segmentation loss on training set: 0.1664
2021-12-11 20:00:19,894 Validation Data Eval:
2021-12-11 20:00:25,040   Average segmentation loss on validation set: 0.1796
2021-12-11 20:00:27,068 Found new lowest validation loss at iteration 764! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_best_val_loss_seed2.pth
2021-12-11 20:00:29,288 iteration 765 : loss : 0.193665, loss_ce: 0.081455
 11%|███▍                          | 45/400 [37:27<5:26:55, 55.25s/it]2021-12-11 20:00:32,090 iteration 766 : loss : 0.193075, loss_ce: 0.078733
2021-12-11 20:00:34,808 iteration 767 : loss : 0.204568, loss_ce: 0.089559
2021-12-11 20:00:37,553 iteration 768 : loss : 0.222156, loss_ce: 0.091820
2021-12-11 20:00:40,355 iteration 769 : loss : 0.229951, loss_ce: 0.104947
2021-12-11 20:00:43,230 iteration 770 : loss : 0.228684, loss_ce: 0.117471
2021-12-11 20:00:45,969 iteration 771 : loss : 0.219502, loss_ce: 0.108094
2021-12-11 20:00:48,586 iteration 772 : loss : 0.204838, loss_ce: 0.091506
2021-12-11 20:00:51,247 iteration 773 : loss : 0.279539, loss_ce: 0.118000
2021-12-11 20:00:54,190 iteration 774 : loss : 0.246670, loss_ce: 0.114057
2021-12-11 20:00:56,977 iteration 775 : loss : 0.197040, loss_ce: 0.090246
2021-12-11 20:00:59,782 iteration 776 : loss : 0.249563, loss_ce: 0.081711
2021-12-11 20:01:02,661 iteration 777 : loss : 0.179405, loss_ce: 0.079277
2021-12-11 20:01:05,364 iteration 778 : loss : 0.199977, loss_ce: 0.076352
2021-12-11 20:01:08,139 iteration 779 : loss : 0.253216, loss_ce: 0.110976
2021-12-11 20:01:11,024 iteration 780 : loss : 0.209605, loss_ce: 0.090027
2021-12-11 20:01:13,951 iteration 781 : loss : 0.177847, loss_ce: 0.063893
2021-12-11 20:01:16,682 iteration 782 : loss : 0.202172, loss_ce: 0.085505
 12%|███▍                          | 46/400 [38:14<5:12:06, 52.90s/it]2021-12-11 20:01:19,570 iteration 783 : loss : 0.179195, loss_ce: 0.072111
2021-12-11 20:01:22,306 iteration 784 : loss : 0.173812, loss_ce: 0.069353
2021-12-11 20:01:25,111 iteration 785 : loss : 0.204869, loss_ce: 0.083168
2021-12-11 20:01:27,890 iteration 786 : loss : 0.169791, loss_ce: 0.072627
2021-12-11 20:01:30,714 iteration 787 : loss : 0.213058, loss_ce: 0.093293
2021-12-11 20:01:33,396 iteration 788 : loss : 0.231996, loss_ce: 0.114103
2021-12-11 20:01:36,229 iteration 789 : loss : 0.192138, loss_ce: 0.081421
2021-12-11 20:01:39,182 iteration 790 : loss : 0.173212, loss_ce: 0.071617
2021-12-11 20:01:41,976 iteration 791 : loss : 0.214540, loss_ce: 0.112405
2021-12-11 20:01:44,691 iteration 792 : loss : 0.178393, loss_ce: 0.075430
2021-12-11 20:01:47,530 iteration 793 : loss : 0.220440, loss_ce: 0.087793
2021-12-11 20:01:50,369 iteration 794 : loss : 0.168471, loss_ce: 0.072260
2021-12-11 20:01:53,154 iteration 795 : loss : 0.179246, loss_ce: 0.072572
2021-12-11 20:01:56,040 iteration 796 : loss : 0.171727, loss_ce: 0.072435
2021-12-11 20:01:58,960 iteration 797 : loss : 0.188648, loss_ce: 0.087886
2021-12-11 20:02:01,770 iteration 798 : loss : 0.237031, loss_ce: 0.114200
2021-12-11 20:02:04,646 iteration 799 : loss : 0.249869, loss_ce: 0.090895
 12%|███▌                          | 47/400 [39:02<5:02:30, 51.42s/it]2021-12-11 20:02:07,356 iteration 800 : loss : 0.258542, loss_ce: 0.130862
2021-12-11 20:02:10,060 iteration 801 : loss : 0.194872, loss_ce: 0.073341
2021-12-11 20:02:12,884 iteration 802 : loss : 0.188263, loss_ce: 0.084031
2021-12-11 20:02:15,495 iteration 803 : loss : 0.165232, loss_ce: 0.069736
2021-12-11 20:02:18,425 iteration 804 : loss : 0.222628, loss_ce: 0.078473
2021-12-11 20:02:21,167 iteration 805 : loss : 0.214206, loss_ce: 0.097678
2021-12-11 20:02:23,860 iteration 806 : loss : 0.240353, loss_ce: 0.091232
2021-12-11 20:02:26,618 iteration 807 : loss : 0.206903, loss_ce: 0.103086
2021-12-11 20:02:29,313 iteration 808 : loss : 0.272265, loss_ce: 0.095411
2021-12-11 20:02:31,977 iteration 809 : loss : 0.193544, loss_ce: 0.086395
2021-12-11 20:02:34,638 iteration 810 : loss : 0.183781, loss_ce: 0.069454
2021-12-11 20:02:37,534 iteration 811 : loss : 0.195858, loss_ce: 0.081598
2021-12-11 20:02:40,396 iteration 812 : loss : 0.262593, loss_ce: 0.139724
2021-12-11 20:02:43,276 iteration 813 : loss : 0.218268, loss_ce: 0.098510
2021-12-11 20:02:46,154 iteration 814 : loss : 0.316390, loss_ce: 0.109201
2021-12-11 20:02:48,962 iteration 815 : loss : 0.156229, loss_ce: 0.075129
2021-12-11 20:02:51,679 iteration 816 : loss : 0.221765, loss_ce: 0.107240
 12%|███▌                          | 48/400 [39:49<4:53:56, 50.10s/it]2021-12-11 20:02:54,483 iteration 817 : loss : 0.229616, loss_ce: 0.100595
2021-12-11 20:02:57,337 iteration 818 : loss : 0.196775, loss_ce: 0.092271
2021-12-11 20:03:00,178 iteration 819 : loss : 0.209384, loss_ce: 0.103533
2021-12-11 20:03:03,142 iteration 820 : loss : 0.220009, loss_ce: 0.095548
2021-12-11 20:03:05,996 iteration 821 : loss : 0.238330, loss_ce: 0.103454
2021-12-11 20:03:08,815 iteration 822 : loss : 0.175251, loss_ce: 0.071526
2021-12-11 20:03:11,491 iteration 823 : loss : 0.193192, loss_ce: 0.082449
2021-12-11 20:03:14,101 iteration 824 : loss : 0.197701, loss_ce: 0.073417
2021-12-11 20:03:17,021 iteration 825 : loss : 0.208348, loss_ce: 0.071696
2021-12-11 20:03:19,631 iteration 826 : loss : 0.206153, loss_ce: 0.091600
2021-12-11 20:03:22,548 iteration 827 : loss : 0.213867, loss_ce: 0.087029
2021-12-11 20:03:25,268 iteration 828 : loss : 0.236404, loss_ce: 0.095654
2021-12-11 20:03:28,029 iteration 829 : loss : 0.163911, loss_ce: 0.070558
2021-12-11 20:03:30,723 iteration 830 : loss : 0.220521, loss_ce: 0.101654
2021-12-11 20:03:33,489 iteration 831 : loss : 0.187256, loss_ce: 0.084243
2021-12-11 20:03:36,287 iteration 832 : loss : 0.203384, loss_ce: 0.082881
2021-12-11 20:03:39,115 iteration 833 : loss : 0.179059, loss_ce: 0.073689
 12%|███▋                          | 49/400 [40:37<4:48:25, 49.30s/it]2021-12-11 20:03:41,882 iteration 834 : loss : 0.192251, loss_ce: 0.073247
2021-12-11 20:03:44,529 iteration 835 : loss : 0.161613, loss_ce: 0.065042
2021-12-11 20:03:47,507 iteration 836 : loss : 0.159127, loss_ce: 0.060589
2021-12-11 20:03:50,246 iteration 837 : loss : 0.136191, loss_ce: 0.056334
2021-12-11 20:03:53,000 iteration 838 : loss : 0.202870, loss_ce: 0.091006
2021-12-11 20:03:55,597 iteration 839 : loss : 0.170750, loss_ce: 0.069712
2021-12-11 20:03:58,234 iteration 840 : loss : 0.165060, loss_ce: 0.075776
2021-12-11 20:04:00,943 iteration 841 : loss : 0.175231, loss_ce: 0.078368
2021-12-11 20:04:03,936 iteration 842 : loss : 0.276022, loss_ce: 0.117567
2021-12-11 20:04:06,613 iteration 843 : loss : 0.218831, loss_ce: 0.090888
2021-12-11 20:04:09,352 iteration 844 : loss : 0.166769, loss_ce: 0.068068
2021-12-11 20:04:12,090 iteration 845 : loss : 0.207657, loss_ce: 0.092931
2021-12-11 20:04:14,980 iteration 846 : loss : 0.207405, loss_ce: 0.090035
2021-12-11 20:04:17,791 iteration 847 : loss : 0.189551, loss_ce: 0.076644
2021-12-11 20:04:20,495 iteration 848 : loss : 0.169006, loss_ce: 0.068222
2021-12-11 20:04:23,182 iteration 849 : loss : 0.171116, loss_ce: 0.067765
2021-12-11 20:04:23,182 Training Data Eval:
2021-12-11 20:04:37,772   Average segmentation loss on training set: 0.1688
2021-12-11 20:04:37,772 Validation Data Eval:
2021-12-11 20:04:42,884   Average segmentation loss on validation set: 0.1724
2021-12-11 20:04:44,814 Found new lowest validation loss at iteration 849! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_best_val_loss_seed2.pth
2021-12-11 20:04:47,105 iteration 850 : loss : 0.207258, loss_ce: 0.108160
 12%|███▊                          | 50/400 [41:45<5:20:17, 54.91s/it]2021-12-11 20:04:49,910 iteration 851 : loss : 0.153145, loss_ce: 0.069624
2021-12-11 20:04:52,627 iteration 852 : loss : 0.190578, loss_ce: 0.075068
2021-12-11 20:04:55,444 iteration 853 : loss : 0.206574, loss_ce: 0.071645
2021-12-11 20:04:58,179 iteration 854 : loss : 0.219538, loss_ce: 0.106376
2021-12-11 20:05:00,904 iteration 855 : loss : 0.167692, loss_ce: 0.087372
2021-12-11 20:05:03,674 iteration 856 : loss : 0.212832, loss_ce: 0.097967
2021-12-11 20:05:06,471 iteration 857 : loss : 0.214406, loss_ce: 0.096508
2021-12-11 20:05:09,354 iteration 858 : loss : 0.134462, loss_ce: 0.054793
2021-12-11 20:05:11,927 iteration 859 : loss : 0.170442, loss_ce: 0.067933
2021-12-11 20:05:14,549 iteration 860 : loss : 0.152172, loss_ce: 0.063053
2021-12-11 20:05:17,285 iteration 861 : loss : 0.197225, loss_ce: 0.070042
2021-12-11 20:05:20,032 iteration 862 : loss : 0.206678, loss_ce: 0.095713
2021-12-11 20:05:22,945 iteration 863 : loss : 0.251525, loss_ce: 0.096781
2021-12-11 20:05:25,650 iteration 864 : loss : 0.178858, loss_ce: 0.078737
2021-12-11 20:05:28,422 iteration 865 : loss : 0.223474, loss_ce: 0.092947
2021-12-11 20:05:31,142 iteration 866 : loss : 0.141933, loss_ce: 0.062955
2021-12-11 20:05:33,888 iteration 867 : loss : 0.175149, loss_ce: 0.074272
 13%|███▊                          | 51/400 [42:32<5:05:12, 52.47s/it]2021-12-11 20:05:36,674 iteration 868 : loss : 0.160526, loss_ce: 0.070947
2021-12-11 20:05:39,541 iteration 869 : loss : 0.197463, loss_ce: 0.085197
2021-12-11 20:05:42,184 iteration 870 : loss : 0.181761, loss_ce: 0.080235
2021-12-11 20:05:44,881 iteration 871 : loss : 0.167119, loss_ce: 0.075871
2021-12-11 20:05:47,650 iteration 872 : loss : 0.156444, loss_ce: 0.068366
2021-12-11 20:05:50,453 iteration 873 : loss : 0.170515, loss_ce: 0.077144
2021-12-11 20:05:53,331 iteration 874 : loss : 0.157488, loss_ce: 0.068771
2021-12-11 20:05:56,095 iteration 875 : loss : 0.190464, loss_ce: 0.075875
2021-12-11 20:05:58,859 iteration 876 : loss : 0.127839, loss_ce: 0.051627
2021-12-11 20:06:01,675 iteration 877 : loss : 0.155993, loss_ce: 0.068361
2021-12-11 20:06:04,395 iteration 878 : loss : 0.275865, loss_ce: 0.102262
2021-12-11 20:06:07,151 iteration 879 : loss : 0.225284, loss_ce: 0.067125
2021-12-11 20:06:10,054 iteration 880 : loss : 0.178808, loss_ce: 0.086464
2021-12-11 20:06:12,978 iteration 881 : loss : 0.200649, loss_ce: 0.073853
2021-12-11 20:06:15,751 iteration 882 : loss : 0.175835, loss_ce: 0.074619
2021-12-11 20:06:18,549 iteration 883 : loss : 0.162813, loss_ce: 0.061413
2021-12-11 20:06:21,189 iteration 884 : loss : 0.154192, loss_ce: 0.066204
 13%|███▉                          | 52/400 [43:19<4:55:19, 50.92s/it]2021-12-11 20:06:24,066 iteration 885 : loss : 0.193371, loss_ce: 0.087091
2021-12-11 20:06:26,853 iteration 886 : loss : 0.178952, loss_ce: 0.075567
2021-12-11 20:06:29,563 iteration 887 : loss : 0.174418, loss_ce: 0.067976
2021-12-11 20:06:32,362 iteration 888 : loss : 0.187725, loss_ce: 0.081533
2021-12-11 20:06:35,077 iteration 889 : loss : 0.261015, loss_ce: 0.083644
2021-12-11 20:06:37,713 iteration 890 : loss : 0.159319, loss_ce: 0.063753
2021-12-11 20:06:40,511 iteration 891 : loss : 0.179862, loss_ce: 0.089690
2021-12-11 20:06:43,408 iteration 892 : loss : 0.165338, loss_ce: 0.074356
2021-12-11 20:06:46,101 iteration 893 : loss : 0.150471, loss_ce: 0.070790
2021-12-11 20:06:48,949 iteration 894 : loss : 0.174184, loss_ce: 0.063071
2021-12-11 20:06:51,631 iteration 895 : loss : 0.197793, loss_ce: 0.079862
2021-12-11 20:06:54,324 iteration 896 : loss : 0.206075, loss_ce: 0.100086
2021-12-11 20:06:57,173 iteration 897 : loss : 0.166887, loss_ce: 0.071022
2021-12-11 20:06:59,855 iteration 898 : loss : 0.166759, loss_ce: 0.057567
2021-12-11 20:07:02,698 iteration 899 : loss : 0.204549, loss_ce: 0.083688
2021-12-11 20:07:05,418 iteration 900 : loss : 0.190038, loss_ce: 0.074016
2021-12-11 20:07:08,231 iteration 901 : loss : 0.213693, loss_ce: 0.090571
 13%|███▉                          | 53/400 [44:06<4:47:46, 49.76s/it]2021-12-11 20:07:11,049 iteration 902 : loss : 0.170942, loss_ce: 0.074512
2021-12-11 20:07:13,931 iteration 903 : loss : 0.164677, loss_ce: 0.060663
2021-12-11 20:07:16,653 iteration 904 : loss : 0.194731, loss_ce: 0.100105
2021-12-11 20:07:19,340 iteration 905 : loss : 0.136439, loss_ce: 0.062462
2021-12-11 20:07:22,156 iteration 906 : loss : 0.174663, loss_ce: 0.071703
2021-12-11 20:07:24,886 iteration 907 : loss : 0.170718, loss_ce: 0.073017
2021-12-11 20:07:27,541 iteration 908 : loss : 0.149325, loss_ce: 0.073878
2021-12-11 20:07:30,371 iteration 909 : loss : 0.165839, loss_ce: 0.068193
2021-12-11 20:07:33,249 iteration 910 : loss : 0.196175, loss_ce: 0.090011
2021-12-11 20:07:35,941 iteration 911 : loss : 0.132410, loss_ce: 0.053213
2021-12-11 20:07:38,774 iteration 912 : loss : 0.159501, loss_ce: 0.059775
2021-12-11 20:07:41,521 iteration 913 : loss : 0.180814, loss_ce: 0.063020
2021-12-11 20:07:44,275 iteration 914 : loss : 0.158486, loss_ce: 0.068196
2021-12-11 20:07:47,085 iteration 915 : loss : 0.174335, loss_ce: 0.072255
2021-12-11 20:07:49,860 iteration 916 : loss : 0.191653, loss_ce: 0.066476
2021-12-11 20:07:52,539 iteration 917 : loss : 0.171992, loss_ce: 0.074727
2021-12-11 20:07:55,522 iteration 918 : loss : 0.209347, loss_ce: 0.070948
 14%|████                          | 54/400 [44:53<4:42:40, 49.02s/it]2021-12-11 20:07:58,273 iteration 919 : loss : 0.158031, loss_ce: 0.058741
2021-12-11 20:08:01,077 iteration 920 : loss : 0.141004, loss_ce: 0.063939
2021-12-11 20:08:03,819 iteration 921 : loss : 0.148771, loss_ce: 0.063828
2021-12-11 20:08:06,549 iteration 922 : loss : 0.201051, loss_ce: 0.081699
2021-12-11 20:08:09,262 iteration 923 : loss : 0.143046, loss_ce: 0.056656
2021-12-11 20:08:12,169 iteration 924 : loss : 0.207275, loss_ce: 0.083087
2021-12-11 20:08:14,832 iteration 925 : loss : 0.196633, loss_ce: 0.070086
2021-12-11 20:08:17,719 iteration 926 : loss : 0.151698, loss_ce: 0.055298
2021-12-11 20:08:20,505 iteration 927 : loss : 0.174211, loss_ce: 0.061688
2021-12-11 20:08:23,379 iteration 928 : loss : 0.184277, loss_ce: 0.079466
2021-12-11 20:08:26,144 iteration 929 : loss : 0.128696, loss_ce: 0.052329
2021-12-11 20:08:28,944 iteration 930 : loss : 0.163697, loss_ce: 0.064256
2021-12-11 20:08:31,643 iteration 931 : loss : 0.245837, loss_ce: 0.129531
2021-12-11 20:08:34,460 iteration 932 : loss : 0.198484, loss_ce: 0.088473
2021-12-11 20:08:37,362 iteration 933 : loss : 0.150728, loss_ce: 0.064152
2021-12-11 20:08:40,171 iteration 934 : loss : 0.166650, loss_ce: 0.064351
2021-12-11 20:08:40,172 Training Data Eval:
2021-12-11 20:08:55,387   Average segmentation loss on training set: 0.2176
2021-12-11 20:08:55,388 Validation Data Eval:
2021-12-11 20:09:00,548   Average segmentation loss on validation set: 0.2045
2021-12-11 20:09:03,367 iteration 935 : loss : 0.196012, loss_ce: 0.080381
 14%|████▏                         | 55/400 [46:01<5:14:19, 54.66s/it]2021-12-11 20:09:06,154 iteration 936 : loss : 0.135365, loss_ce: 0.057700
2021-12-11 20:09:08,997 iteration 937 : loss : 0.184806, loss_ce: 0.073625
2021-12-11 20:09:11,785 iteration 938 : loss : 0.190672, loss_ce: 0.081808
2021-12-11 20:09:14,583 iteration 939 : loss : 0.208098, loss_ce: 0.074324
2021-12-11 20:09:17,355 iteration 940 : loss : 0.207930, loss_ce: 0.080429
2021-12-11 20:09:20,132 iteration 941 : loss : 0.152465, loss_ce: 0.060557
2021-12-11 20:09:22,928 iteration 942 : loss : 0.170070, loss_ce: 0.077856
2021-12-11 20:09:25,637 iteration 943 : loss : 0.239006, loss_ce: 0.091979
2021-12-11 20:09:28,562 iteration 944 : loss : 0.275426, loss_ce: 0.119468
2021-12-11 20:09:31,341 iteration 945 : loss : 0.217263, loss_ce: 0.107158
2021-12-11 20:09:34,259 iteration 946 : loss : 0.150331, loss_ce: 0.065570
2021-12-11 20:09:36,956 iteration 947 : loss : 0.203228, loss_ce: 0.078352
2021-12-11 20:09:39,822 iteration 948 : loss : 0.178280, loss_ce: 0.079785
2021-12-11 20:09:42,617 iteration 949 : loss : 0.182595, loss_ce: 0.073111
2021-12-11 20:09:45,569 iteration 950 : loss : 0.190771, loss_ce: 0.075939
2021-12-11 20:09:48,307 iteration 951 : loss : 0.136090, loss_ce: 0.059285
2021-12-11 20:09:51,178 iteration 952 : loss : 0.160769, loss_ce: 0.065111
 14%|████▏                         | 56/400 [46:49<5:01:38, 52.61s/it]2021-12-11 20:09:54,121 iteration 953 : loss : 0.156223, loss_ce: 0.066202
2021-12-11 20:09:56,901 iteration 954 : loss : 0.155698, loss_ce: 0.057502
2021-12-11 20:09:59,705 iteration 955 : loss : 0.193349, loss_ce: 0.076953
2021-12-11 20:10:02,542 iteration 956 : loss : 0.114511, loss_ce: 0.043894
2021-12-11 20:10:05,337 iteration 957 : loss : 0.178566, loss_ce: 0.079764
2021-12-11 20:10:08,071 iteration 958 : loss : 0.141942, loss_ce: 0.062495
2021-12-11 20:10:10,891 iteration 959 : loss : 0.166946, loss_ce: 0.069440
2021-12-11 20:10:13,727 iteration 960 : loss : 0.189124, loss_ce: 0.078322
2021-12-11 20:10:16,666 iteration 961 : loss : 0.177349, loss_ce: 0.070614
2021-12-11 20:10:19,523 iteration 962 : loss : 0.164456, loss_ce: 0.056573
2021-12-11 20:10:22,189 iteration 963 : loss : 0.173337, loss_ce: 0.081380
2021-12-11 20:10:25,051 iteration 964 : loss : 0.164090, loss_ce: 0.062768
2021-12-11 20:10:27,730 iteration 965 : loss : 0.132702, loss_ce: 0.050559
2021-12-11 20:10:30,531 iteration 966 : loss : 0.161870, loss_ce: 0.061470
2021-12-11 20:10:33,487 iteration 967 : loss : 0.178421, loss_ce: 0.084691
2021-12-11 20:10:36,132 iteration 968 : loss : 0.166148, loss_ce: 0.066252
2021-12-11 20:10:38,959 iteration 969 : loss : 0.132023, loss_ce: 0.060529
 14%|████▎                         | 57/400 [47:37<4:52:26, 51.16s/it]2021-12-11 20:10:41,693 iteration 970 : loss : 0.183551, loss_ce: 0.093261
2021-12-11 20:10:44,527 iteration 971 : loss : 0.193911, loss_ce: 0.092234
2021-12-11 20:10:47,408 iteration 972 : loss : 0.176329, loss_ce: 0.081615
2021-12-11 20:10:50,074 iteration 973 : loss : 0.179030, loss_ce: 0.077524
2021-12-11 20:10:52,942 iteration 974 : loss : 0.244204, loss_ce: 0.112080
2021-12-11 20:10:55,698 iteration 975 : loss : 0.176085, loss_ce: 0.064107
2021-12-11 20:10:58,680 iteration 976 : loss : 0.171850, loss_ce: 0.077964
2021-12-11 20:11:01,351 iteration 977 : loss : 0.133906, loss_ce: 0.053000
2021-12-11 20:11:04,010 iteration 978 : loss : 0.147063, loss_ce: 0.060139
2021-12-11 20:11:06,877 iteration 979 : loss : 0.143545, loss_ce: 0.056316
2021-12-11 20:11:09,726 iteration 980 : loss : 0.166315, loss_ce: 0.069318
2021-12-11 20:11:12,560 iteration 981 : loss : 0.101271, loss_ce: 0.042404
2021-12-11 20:11:15,274 iteration 982 : loss : 0.184173, loss_ce: 0.063051
2021-12-11 20:11:18,094 iteration 983 : loss : 0.151457, loss_ce: 0.060708
2021-12-11 20:11:21,004 iteration 984 : loss : 0.150962, loss_ce: 0.067935
2021-12-11 20:11:23,876 iteration 985 : loss : 0.178809, loss_ce: 0.072901
2021-12-11 20:11:26,525 iteration 986 : loss : 0.131927, loss_ce: 0.046025
 14%|████▎                         | 58/400 [48:24<4:45:27, 50.08s/it]2021-12-11 20:11:29,260 iteration 987 : loss : 0.166606, loss_ce: 0.066799
2021-12-11 20:11:31,944 iteration 988 : loss : 0.144075, loss_ce: 0.058481
2021-12-11 20:11:34,682 iteration 989 : loss : 0.123493, loss_ce: 0.048129
2021-12-11 20:11:37,414 iteration 990 : loss : 0.185643, loss_ce: 0.069854
2021-12-11 20:11:40,179 iteration 991 : loss : 0.188101, loss_ce: 0.078263
2021-12-11 20:11:42,898 iteration 992 : loss : 0.145481, loss_ce: 0.055901
2021-12-11 20:11:45,624 iteration 993 : loss : 0.177054, loss_ce: 0.070209
2021-12-11 20:11:48,442 iteration 994 : loss : 0.144842, loss_ce: 0.051227
2021-12-11 20:11:51,325 iteration 995 : loss : 0.189207, loss_ce: 0.082047
2021-12-11 20:11:53,971 iteration 996 : loss : 0.181908, loss_ce: 0.078041
2021-12-11 20:11:56,651 iteration 997 : loss : 0.199921, loss_ce: 0.082021
2021-12-11 20:11:59,443 iteration 998 : loss : 0.148214, loss_ce: 0.061308
2021-12-11 20:12:02,078 iteration 999 : loss : 0.137112, loss_ce: 0.058662
2021-12-11 20:12:05,037 iteration 1000 : loss : 0.150697, loss_ce: 0.074385
2021-12-11 20:12:07,712 iteration 1001 : loss : 0.129342, loss_ce: 0.053714
2021-12-11 20:12:10,576 iteration 1002 : loss : 0.202604, loss_ce: 0.086206
2021-12-11 20:12:13,381 iteration 1003 : loss : 0.182832, loss_ce: 0.055222
 15%|████▍                         | 59/400 [49:11<4:39:07, 49.11s/it]2021-12-11 20:12:16,062 iteration 1004 : loss : 0.146356, loss_ce: 0.051410
2021-12-11 20:12:18,907 iteration 1005 : loss : 0.189081, loss_ce: 0.095379
2021-12-11 20:12:21,798 iteration 1006 : loss : 0.168132, loss_ce: 0.080445
2021-12-11 20:12:24,602 iteration 1007 : loss : 0.201230, loss_ce: 0.089063
2021-12-11 20:12:27,560 iteration 1008 : loss : 0.170579, loss_ce: 0.079298
2021-12-11 20:12:30,190 iteration 1009 : loss : 0.174960, loss_ce: 0.077968
2021-12-11 20:12:32,914 iteration 1010 : loss : 0.171147, loss_ce: 0.065161
2021-12-11 20:12:35,718 iteration 1011 : loss : 0.177641, loss_ce: 0.072184
2021-12-11 20:12:38,531 iteration 1012 : loss : 0.164008, loss_ce: 0.073590
2021-12-11 20:12:41,387 iteration 1013 : loss : 0.137023, loss_ce: 0.056899
2021-12-11 20:12:44,076 iteration 1014 : loss : 0.115315, loss_ce: 0.048489
2021-12-11 20:12:46,837 iteration 1015 : loss : 0.194034, loss_ce: 0.077218
2021-12-11 20:12:49,643 iteration 1016 : loss : 0.137578, loss_ce: 0.056072
2021-12-11 20:12:52,258 iteration 1017 : loss : 0.147070, loss_ce: 0.060889
2021-12-11 20:12:55,019 iteration 1018 : loss : 0.135583, loss_ce: 0.054160
2021-12-11 20:12:57,855 iteration 1019 : loss : 0.138681, loss_ce: 0.057941
2021-12-11 20:12:57,855 Training Data Eval:
2021-12-11 20:13:12,648   Average segmentation loss on training set: 0.1330
2021-12-11 20:13:12,648 Validation Data Eval:
2021-12-11 20:13:17,757   Average segmentation loss on validation set: 0.1568
2021-12-11 20:13:19,698 Found new lowest validation loss at iteration 1019! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_best_val_loss_seed2.pth
2021-12-11 20:13:22,180 iteration 1020 : loss : 0.173822, loss_ce: 0.084845
 15%|████▌                         | 60/400 [50:20<5:11:46, 55.02s/it]2021-12-11 20:13:24,907 iteration 1021 : loss : 0.159146, loss_ce: 0.062737
2021-12-11 20:13:27,807 iteration 1022 : loss : 0.173652, loss_ce: 0.075115
2021-12-11 20:13:30,568 iteration 1023 : loss : 0.183613, loss_ce: 0.079204
2021-12-11 20:13:33,319 iteration 1024 : loss : 0.118859, loss_ce: 0.053790
2021-12-11 20:13:36,132 iteration 1025 : loss : 0.178011, loss_ce: 0.084604
2021-12-11 20:13:38,834 iteration 1026 : loss : 0.194333, loss_ce: 0.061168
2021-12-11 20:13:41,705 iteration 1027 : loss : 0.139156, loss_ce: 0.053118
2021-12-11 20:13:44,479 iteration 1028 : loss : 0.157905, loss_ce: 0.055550
2021-12-11 20:13:47,456 iteration 1029 : loss : 0.139556, loss_ce: 0.051463
2021-12-11 20:13:50,234 iteration 1030 : loss : 0.168717, loss_ce: 0.075874
2021-12-11 20:13:53,182 iteration 1031 : loss : 0.152779, loss_ce: 0.071752
2021-12-11 20:13:55,945 iteration 1032 : loss : 0.168675, loss_ce: 0.073402
2021-12-11 20:13:58,677 iteration 1033 : loss : 0.184829, loss_ce: 0.082321
2021-12-11 20:14:01,351 iteration 1034 : loss : 0.154886, loss_ce: 0.064812
2021-12-11 20:14:04,213 iteration 1035 : loss : 0.197993, loss_ce: 0.089732
2021-12-11 20:14:07,102 iteration 1036 : loss : 0.142323, loss_ce: 0.061523
2021-12-11 20:14:09,924 iteration 1037 : loss : 0.115015, loss_ce: 0.046098
 15%|████▌                         | 61/400 [51:08<4:58:31, 52.84s/it]2021-12-11 20:14:12,775 iteration 1038 : loss : 0.112955, loss_ce: 0.048854
2021-12-11 20:14:15,517 iteration 1039 : loss : 0.139452, loss_ce: 0.067018
2021-12-11 20:14:18,130 iteration 1040 : loss : 0.125672, loss_ce: 0.048005
2021-12-11 20:14:20,840 iteration 1041 : loss : 0.175371, loss_ce: 0.086871
2021-12-11 20:14:23,626 iteration 1042 : loss : 0.164215, loss_ce: 0.086053
2021-12-11 20:14:26,337 iteration 1043 : loss : 0.133579, loss_ce: 0.054203
2021-12-11 20:14:29,142 iteration 1044 : loss : 0.136949, loss_ce: 0.062168
2021-12-11 20:14:32,056 iteration 1045 : loss : 0.182064, loss_ce: 0.078026
2021-12-11 20:14:34,827 iteration 1046 : loss : 0.113993, loss_ce: 0.051348
2021-12-11 20:14:37,576 iteration 1047 : loss : 0.102117, loss_ce: 0.042688
2021-12-11 20:14:40,191 iteration 1048 : loss : 0.149911, loss_ce: 0.067166
2021-12-11 20:14:43,138 iteration 1049 : loss : 0.120505, loss_ce: 0.051113
2021-12-11 20:14:45,758 iteration 1050 : loss : 0.196668, loss_ce: 0.056865
2021-12-11 20:14:48,470 iteration 1051 : loss : 0.127346, loss_ce: 0.053606
2021-12-11 20:14:51,309 iteration 1052 : loss : 0.167185, loss_ce: 0.063903
2021-12-11 20:14:54,235 iteration 1053 : loss : 0.154746, loss_ce: 0.055646
2021-12-11 20:14:56,994 iteration 1054 : loss : 0.166530, loss_ce: 0.059563
 16%|████▋                         | 62/400 [51:55<4:47:54, 51.11s/it]2021-12-11 20:14:59,796 iteration 1055 : loss : 0.116666, loss_ce: 0.038637
2021-12-11 20:15:02,550 iteration 1056 : loss : 0.107797, loss_ce: 0.049313
2021-12-11 20:15:05,324 iteration 1057 : loss : 0.225423, loss_ce: 0.100523
2021-12-11 20:15:07,930 iteration 1058 : loss : 0.191219, loss_ce: 0.073756
2021-12-11 20:15:10,790 iteration 1059 : loss : 0.158638, loss_ce: 0.065989
2021-12-11 20:15:13,445 iteration 1060 : loss : 0.117691, loss_ce: 0.043524
2021-12-11 20:15:16,281 iteration 1061 : loss : 0.131966, loss_ce: 0.062144
2021-12-11 20:15:19,007 iteration 1062 : loss : 0.185683, loss_ce: 0.061295
2021-12-11 20:15:22,040 iteration 1063 : loss : 0.132725, loss_ce: 0.054893
2021-12-11 20:15:24,924 iteration 1064 : loss : 0.170887, loss_ce: 0.066786
2021-12-11 20:15:27,788 iteration 1065 : loss : 0.155019, loss_ce: 0.078443
2021-12-11 20:15:30,623 iteration 1066 : loss : 0.103989, loss_ce: 0.045666
2021-12-11 20:15:33,367 iteration 1067 : loss : 0.137551, loss_ce: 0.058739
2021-12-11 20:15:36,338 iteration 1068 : loss : 0.134863, loss_ce: 0.059156
2021-12-11 20:15:39,015 iteration 1069 : loss : 0.115152, loss_ce: 0.048614
2021-12-11 20:15:42,063 iteration 1070 : loss : 0.175096, loss_ce: 0.061355
2021-12-11 20:15:44,960 iteration 1071 : loss : 0.147828, loss_ce: 0.062754
 16%|████▋                         | 63/400 [52:43<4:41:45, 50.17s/it]2021-12-11 20:15:47,826 iteration 1072 : loss : 0.143882, loss_ce: 0.052237
2021-12-11 20:15:50,494 iteration 1073 : loss : 0.174703, loss_ce: 0.080098
2021-12-11 20:15:53,399 iteration 1074 : loss : 0.168052, loss_ce: 0.061985
2021-12-11 20:15:56,144 iteration 1075 : loss : 0.177926, loss_ce: 0.054972
2021-12-11 20:15:58,872 iteration 1076 : loss : 0.135584, loss_ce: 0.058784
2021-12-11 20:16:01,724 iteration 1077 : loss : 0.139917, loss_ce: 0.047276
2021-12-11 20:16:04,555 iteration 1078 : loss : 0.133493, loss_ce: 0.061719
2021-12-11 20:16:07,524 iteration 1079 : loss : 0.193144, loss_ce: 0.089923
2021-12-11 20:16:10,117 iteration 1080 : loss : 0.119905, loss_ce: 0.048476
2021-12-11 20:16:13,021 iteration 1081 : loss : 0.123279, loss_ce: 0.057014
2021-12-11 20:16:15,857 iteration 1082 : loss : 0.154447, loss_ce: 0.070926
2021-12-11 20:16:18,714 iteration 1083 : loss : 0.183944, loss_ce: 0.083241
2021-12-11 20:16:21,465 iteration 1084 : loss : 0.144519, loss_ce: 0.048280
2021-12-11 20:16:24,205 iteration 1085 : loss : 0.141436, loss_ce: 0.067453
2021-12-11 20:16:26,942 iteration 1086 : loss : 0.130336, loss_ce: 0.059944
2021-12-11 20:16:29,767 iteration 1087 : loss : 0.109591, loss_ce: 0.045982
2021-12-11 20:16:32,672 iteration 1088 : loss : 0.117212, loss_ce: 0.049143
 16%|████▊                         | 64/400 [53:30<4:36:47, 49.43s/it]2021-12-11 20:16:35,572 iteration 1089 : loss : 0.158653, loss_ce: 0.063327
2021-12-11 20:16:38,338 iteration 1090 : loss : 0.158556, loss_ce: 0.052039
2021-12-11 20:16:41,099 iteration 1091 : loss : 0.124299, loss_ce: 0.056617
2021-12-11 20:16:43,773 iteration 1092 : loss : 0.114255, loss_ce: 0.054330
2021-12-11 20:16:46,633 iteration 1093 : loss : 0.130105, loss_ce: 0.053273
2021-12-11 20:16:49,296 iteration 1094 : loss : 0.143122, loss_ce: 0.070484
2021-12-11 20:16:52,077 iteration 1095 : loss : 0.190820, loss_ce: 0.059292
2021-12-11 20:16:54,852 iteration 1096 : loss : 0.105513, loss_ce: 0.048773
2021-12-11 20:16:57,595 iteration 1097 : loss : 0.133973, loss_ce: 0.050619
2021-12-11 20:17:00,273 iteration 1098 : loss : 0.101187, loss_ce: 0.049237
2021-12-11 20:17:03,203 iteration 1099 : loss : 0.134542, loss_ce: 0.054769
2021-12-11 20:17:05,989 iteration 1100 : loss : 0.115692, loss_ce: 0.051949
2021-12-11 20:17:08,703 iteration 1101 : loss : 0.149381, loss_ce: 0.065388
2021-12-11 20:17:11,497 iteration 1102 : loss : 0.140168, loss_ce: 0.051575
2021-12-11 20:17:14,187 iteration 1103 : loss : 0.118696, loss_ce: 0.046583
2021-12-11 20:17:16,768 iteration 1104 : loss : 0.137688, loss_ce: 0.048357
2021-12-11 20:17:16,768 Training Data Eval:
2021-12-11 20:17:31,545   Average segmentation loss on training set: 0.1102
2021-12-11 20:17:31,546 Validation Data Eval:
2021-12-11 20:17:36,802   Average segmentation loss on validation set: 0.1249
2021-12-11 20:17:38,826 Found new lowest validation loss at iteration 1104! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_best_val_loss_seed2.pth
2021-12-11 20:17:41,130 iteration 1105 : loss : 0.130140, loss_ce: 0.055871
 16%|████▉                         | 65/400 [54:39<5:07:50, 55.14s/it]2021-12-11 20:17:44,014 iteration 1106 : loss : 0.124817, loss_ce: 0.051583
2021-12-11 20:17:46,781 iteration 1107 : loss : 0.137798, loss_ce: 0.065905
2021-12-11 20:17:49,541 iteration 1108 : loss : 0.102126, loss_ce: 0.039123
2021-12-11 20:17:52,269 iteration 1109 : loss : 0.175973, loss_ce: 0.067693
2021-12-11 20:17:54,962 iteration 1110 : loss : 0.107602, loss_ce: 0.054542
2021-12-11 20:17:57,620 iteration 1111 : loss : 0.127192, loss_ce: 0.056589
2021-12-11 20:18:00,513 iteration 1112 : loss : 0.129979, loss_ce: 0.064558
2021-12-11 20:18:03,151 iteration 1113 : loss : 0.128966, loss_ce: 0.054034
2021-12-11 20:18:05,964 iteration 1114 : loss : 0.112493, loss_ce: 0.044508
2021-12-11 20:18:08,737 iteration 1115 : loss : 0.124095, loss_ce: 0.044634
2021-12-11 20:18:11,545 iteration 1116 : loss : 0.159820, loss_ce: 0.067318
2021-12-11 20:18:14,307 iteration 1117 : loss : 0.132836, loss_ce: 0.051801
2021-12-11 20:18:16,949 iteration 1118 : loss : 0.081967, loss_ce: 0.037872
2021-12-11 20:18:19,744 iteration 1119 : loss : 0.092935, loss_ce: 0.038407
2021-12-11 20:18:22,611 iteration 1120 : loss : 0.119398, loss_ce: 0.045451
2021-12-11 20:18:25,245 iteration 1121 : loss : 0.126706, loss_ce: 0.043090
2021-12-11 20:18:27,945 iteration 1122 : loss : 0.118447, loss_ce: 0.054505
 16%|████▉                         | 66/400 [55:26<4:53:03, 52.64s/it]2021-12-11 20:18:30,697 iteration 1123 : loss : 0.126854, loss_ce: 0.054705
2021-12-11 20:18:33,501 iteration 1124 : loss : 0.132339, loss_ce: 0.058139
2021-12-11 20:18:36,199 iteration 1125 : loss : 0.161361, loss_ce: 0.069030
2021-12-11 20:18:39,135 iteration 1126 : loss : 0.151559, loss_ce: 0.074150
2021-12-11 20:18:41,934 iteration 1127 : loss : 0.178735, loss_ce: 0.074199
2021-12-11 20:18:44,697 iteration 1128 : loss : 0.135132, loss_ce: 0.049129
2021-12-11 20:18:47,434 iteration 1129 : loss : 0.098139, loss_ce: 0.043656
2021-12-11 20:18:50,290 iteration 1130 : loss : 0.133419, loss_ce: 0.065425
2021-12-11 20:18:53,146 iteration 1131 : loss : 0.168158, loss_ce: 0.057675
2021-12-11 20:18:55,826 iteration 1132 : loss : 0.121008, loss_ce: 0.053749
2021-12-11 20:18:58,637 iteration 1133 : loss : 0.141596, loss_ce: 0.062206
2021-12-11 20:19:01,412 iteration 1134 : loss : 0.155220, loss_ce: 0.073762
2021-12-11 20:19:04,154 iteration 1135 : loss : 0.093952, loss_ce: 0.037478
2021-12-11 20:19:06,952 iteration 1136 : loss : 0.113589, loss_ce: 0.048509
2021-12-11 20:19:09,739 iteration 1137 : loss : 0.140051, loss_ce: 0.054083
2021-12-11 20:19:12,549 iteration 1138 : loss : 0.118096, loss_ce: 0.056463
2021-12-11 20:19:15,246 iteration 1139 : loss : 0.114544, loss_ce: 0.058700
 17%|█████                         | 67/400 [56:13<4:43:16, 51.04s/it]2021-12-11 20:19:18,183 iteration 1140 : loss : 0.162164, loss_ce: 0.074470
2021-12-11 20:19:20,880 iteration 1141 : loss : 0.093283, loss_ce: 0.043024
2021-12-11 20:19:23,600 iteration 1142 : loss : 0.094775, loss_ce: 0.042947
2021-12-11 20:19:26,440 iteration 1143 : loss : 0.140931, loss_ce: 0.047870
2021-12-11 20:19:29,264 iteration 1144 : loss : 0.128465, loss_ce: 0.056373
2021-12-11 20:19:32,013 iteration 1145 : loss : 0.103895, loss_ce: 0.037182
2021-12-11 20:19:34,691 iteration 1146 : loss : 0.132159, loss_ce: 0.056812
2021-12-11 20:19:37,300 iteration 1147 : loss : 0.095275, loss_ce: 0.038230
2021-12-11 20:19:40,224 iteration 1148 : loss : 0.109794, loss_ce: 0.049389
2021-12-11 20:19:42,983 iteration 1149 : loss : 0.152322, loss_ce: 0.070793
2021-12-11 20:19:45,855 iteration 1150 : loss : 0.094794, loss_ce: 0.047033
2021-12-11 20:19:48,620 iteration 1151 : loss : 0.091780, loss_ce: 0.034867
2021-12-11 20:19:51,491 iteration 1152 : loss : 0.210548, loss_ce: 0.068404
2021-12-11 20:19:54,420 iteration 1153 : loss : 0.114835, loss_ce: 0.041049
2021-12-11 20:19:57,025 iteration 1154 : loss : 0.107238, loss_ce: 0.053595
2021-12-11 20:19:59,924 iteration 1155 : loss : 0.110510, loss_ce: 0.049701
2021-12-11 20:20:02,654 iteration 1156 : loss : 0.171282, loss_ce: 0.064448
 17%|█████                         | 68/400 [57:00<4:36:22, 49.95s/it]2021-12-11 20:20:05,417 iteration 1157 : loss : 0.114912, loss_ce: 0.053413
2021-12-11 20:20:08,380 iteration 1158 : loss : 0.129493, loss_ce: 0.050124
2021-12-11 20:20:11,165 iteration 1159 : loss : 0.123186, loss_ce: 0.052341
2021-12-11 20:20:13,713 iteration 1160 : loss : 0.110414, loss_ce: 0.054885
2021-12-11 20:20:16,490 iteration 1161 : loss : 0.122299, loss_ce: 0.042421
2021-12-11 20:20:19,391 iteration 1162 : loss : 0.116528, loss_ce: 0.033571
2021-12-11 20:20:22,200 iteration 1163 : loss : 0.120690, loss_ce: 0.043340
2021-12-11 20:20:25,121 iteration 1164 : loss : 0.114794, loss_ce: 0.050595
2021-12-11 20:20:27,775 iteration 1165 : loss : 0.148918, loss_ce: 0.066488
2021-12-11 20:20:30,523 iteration 1166 : loss : 0.113721, loss_ce: 0.061070
2021-12-11 20:20:33,286 iteration 1167 : loss : 0.151131, loss_ce: 0.066407
2021-12-11 20:20:35,963 iteration 1168 : loss : 0.080241, loss_ce: 0.037413
2021-12-11 20:20:38,631 iteration 1169 : loss : 0.126321, loss_ce: 0.045412
2021-12-11 20:20:41,391 iteration 1170 : loss : 0.121255, loss_ce: 0.048000
2021-12-11 20:20:44,201 iteration 1171 : loss : 0.123392, loss_ce: 0.048968
2021-12-11 20:20:47,007 iteration 1172 : loss : 0.144196, loss_ce: 0.064108
2021-12-11 20:20:49,821 iteration 1173 : loss : 0.134718, loss_ce: 0.077837
 17%|█████▏                        | 69/400 [57:48<4:30:57, 49.12s/it]2021-12-11 20:20:52,665 iteration 1174 : loss : 0.115272, loss_ce: 0.047209
2021-12-11 20:20:55,426 iteration 1175 : loss : 0.136224, loss_ce: 0.055709
2021-12-11 20:20:58,358 iteration 1176 : loss : 0.133205, loss_ce: 0.063365
2021-12-11 20:21:01,171 iteration 1177 : loss : 0.163691, loss_ce: 0.073194
2021-12-11 20:21:04,079 iteration 1178 : loss : 0.112019, loss_ce: 0.058845
2021-12-11 20:21:06,795 iteration 1179 : loss : 0.120571, loss_ce: 0.055358
2021-12-11 20:21:09,587 iteration 1180 : loss : 0.136757, loss_ce: 0.059364
2021-12-11 20:21:12,334 iteration 1181 : loss : 0.115126, loss_ce: 0.050175
2021-12-11 20:21:15,330 iteration 1182 : loss : 0.159094, loss_ce: 0.068045
2021-12-11 20:21:17,968 iteration 1183 : loss : 0.099503, loss_ce: 0.045594
2021-12-11 20:21:20,685 iteration 1184 : loss : 0.095346, loss_ce: 0.044370
2021-12-11 20:21:23,454 iteration 1185 : loss : 0.131814, loss_ce: 0.054373
2021-12-11 20:21:26,157 iteration 1186 : loss : 0.117304, loss_ce: 0.047351
2021-12-11 20:21:29,004 iteration 1187 : loss : 0.100800, loss_ce: 0.043464
2021-12-11 20:21:31,707 iteration 1188 : loss : 0.091341, loss_ce: 0.037833
2021-12-11 20:21:34,499 iteration 1189 : loss : 0.124240, loss_ce: 0.052434
2021-12-11 20:21:34,500 Training Data Eval:
2021-12-11 20:21:49,187   Average segmentation loss on training set: 0.0910
2021-12-11 20:21:49,187 Validation Data Eval:
2021-12-11 20:21:54,323   Average segmentation loss on validation set: 0.1580
2021-12-11 20:21:57,279 iteration 1190 : loss : 0.110530, loss_ce: 0.050629
 18%|█████▎                        | 70/400 [58:55<5:00:23, 54.62s/it]2021-12-11 20:22:00,049 iteration 1191 : loss : 0.094987, loss_ce: 0.047530
2021-12-11 20:22:02,811 iteration 1192 : loss : 0.103529, loss_ce: 0.045783
2021-12-11 20:22:05,614 iteration 1193 : loss : 0.139854, loss_ce: 0.048381
2021-12-11 20:22:08,394 iteration 1194 : loss : 0.137657, loss_ce: 0.067534
2021-12-11 20:22:11,217 iteration 1195 : loss : 0.129220, loss_ce: 0.059455
2021-12-11 20:22:13,882 iteration 1196 : loss : 0.123869, loss_ce: 0.049542
2021-12-11 20:22:16,732 iteration 1197 : loss : 0.112303, loss_ce: 0.054441
2021-12-11 20:22:19,529 iteration 1198 : loss : 0.097256, loss_ce: 0.056851
2021-12-11 20:22:22,483 iteration 1199 : loss : 0.173625, loss_ce: 0.074478
2021-12-11 20:22:25,260 iteration 1200 : loss : 0.135177, loss_ce: 0.052741
2021-12-11 20:22:28,188 iteration 1201 : loss : 0.090905, loss_ce: 0.043012
2021-12-11 20:22:30,952 iteration 1202 : loss : 0.086980, loss_ce: 0.041652
2021-12-11 20:22:33,732 iteration 1203 : loss : 0.112093, loss_ce: 0.048211
2021-12-11 20:22:36,436 iteration 1204 : loss : 0.109768, loss_ce: 0.047795
2021-12-11 20:22:39,278 iteration 1205 : loss : 0.147458, loss_ce: 0.081194
2021-12-11 20:22:42,058 iteration 1206 : loss : 0.148294, loss_ce: 0.045783
2021-12-11 20:22:44,760 iteration 1207 : loss : 0.121103, loss_ce: 0.044486
 18%|█████▎                        | 71/400 [59:42<4:47:45, 52.48s/it]2021-12-11 20:22:47,494 iteration 1208 : loss : 0.110282, loss_ce: 0.052305
2021-12-11 20:22:50,136 iteration 1209 : loss : 0.095597, loss_ce: 0.036754
2021-12-11 20:22:52,839 iteration 1210 : loss : 0.107709, loss_ce: 0.038775
2021-12-11 20:22:55,702 iteration 1211 : loss : 0.159011, loss_ce: 0.057137
2021-12-11 20:22:58,450 iteration 1212 : loss : 0.070786, loss_ce: 0.034489
2021-12-11 20:23:01,130 iteration 1213 : loss : 0.121860, loss_ce: 0.042321
2021-12-11 20:23:04,083 iteration 1214 : loss : 0.111285, loss_ce: 0.048000
2021-12-11 20:23:06,767 iteration 1215 : loss : 0.102050, loss_ce: 0.046281
2021-12-11 20:23:09,453 iteration 1216 : loss : 0.069192, loss_ce: 0.035368
2021-12-11 20:23:12,281 iteration 1217 : loss : 0.116041, loss_ce: 0.052307
2021-12-11 20:23:14,848 iteration 1218 : loss : 0.077537, loss_ce: 0.041429
2021-12-11 20:23:17,602 iteration 1219 : loss : 0.131260, loss_ce: 0.049903
2021-12-11 20:23:20,269 iteration 1220 : loss : 0.138513, loss_ce: 0.055489
2021-12-11 20:23:23,018 iteration 1221 : loss : 0.104498, loss_ce: 0.046311
2021-12-11 20:23:25,946 iteration 1222 : loss : 0.115041, loss_ce: 0.042471
2021-12-11 20:23:28,584 iteration 1223 : loss : 0.106741, loss_ce: 0.045716
2021-12-11 20:23:31,538 iteration 1224 : loss : 0.124990, loss_ce: 0.057596
 18%|█████                       | 72/400 [1:00:29<4:37:30, 50.77s/it]2021-12-11 20:23:34,550 iteration 1225 : loss : 0.099645, loss_ce: 0.041881
2021-12-11 20:23:37,202 iteration 1226 : loss : 0.104560, loss_ce: 0.049799
2021-12-11 20:23:40,079 iteration 1227 : loss : 0.118938, loss_ce: 0.054513
2021-12-11 20:23:42,947 iteration 1228 : loss : 0.148538, loss_ce: 0.050626
2021-12-11 20:23:45,754 iteration 1229 : loss : 0.134104, loss_ce: 0.062622
2021-12-11 20:23:48,638 iteration 1230 : loss : 0.092628, loss_ce: 0.053019
2021-12-11 20:23:51,360 iteration 1231 : loss : 0.130041, loss_ce: 0.051617
2021-12-11 20:23:54,220 iteration 1232 : loss : 0.102342, loss_ce: 0.044577
2021-12-11 20:23:57,109 iteration 1233 : loss : 0.118997, loss_ce: 0.054377
2021-12-11 20:23:59,808 iteration 1234 : loss : 0.148915, loss_ce: 0.067154
2021-12-11 20:24:02,678 iteration 1235 : loss : 0.137019, loss_ce: 0.064581
2021-12-11 20:24:05,359 iteration 1236 : loss : 0.089943, loss_ce: 0.033748
2021-12-11 20:24:08,240 iteration 1237 : loss : 0.177479, loss_ce: 0.075628
2021-12-11 20:24:11,003 iteration 1238 : loss : 0.122812, loss_ce: 0.053954
2021-12-11 20:24:13,934 iteration 1239 : loss : 0.107806, loss_ce: 0.044097
2021-12-11 20:24:16,644 iteration 1240 : loss : 0.103118, loss_ce: 0.046867
2021-12-11 20:24:19,537 iteration 1241 : loss : 0.085960, loss_ce: 0.047043
 18%|█████                       | 73/400 [1:01:17<4:32:10, 49.94s/it]2021-12-11 20:24:22,329 iteration 1242 : loss : 0.087510, loss_ce: 0.041081
2021-12-11 20:24:25,137 iteration 1243 : loss : 0.097377, loss_ce: 0.048526
2021-12-11 20:24:27,784 iteration 1244 : loss : 0.118293, loss_ce: 0.051260
2021-12-11 20:24:30,555 iteration 1245 : loss : 0.096120, loss_ce: 0.043417
2021-12-11 20:24:33,299 iteration 1246 : loss : 0.098015, loss_ce: 0.046061
2021-12-11 20:24:36,165 iteration 1247 : loss : 0.096092, loss_ce: 0.043603
2021-12-11 20:24:38,870 iteration 1248 : loss : 0.071115, loss_ce: 0.034250
2021-12-11 20:24:41,532 iteration 1249 : loss : 0.118253, loss_ce: 0.047633
2021-12-11 20:24:44,323 iteration 1250 : loss : 0.122620, loss_ce: 0.049897
2021-12-11 20:24:47,124 iteration 1251 : loss : 0.076167, loss_ce: 0.036017
2021-12-11 20:24:49,871 iteration 1252 : loss : 0.178058, loss_ce: 0.060964
2021-12-11 20:24:52,660 iteration 1253 : loss : 0.099007, loss_ce: 0.044453
2021-12-11 20:24:55,454 iteration 1254 : loss : 0.130322, loss_ce: 0.058725
2021-12-11 20:24:58,145 iteration 1255 : loss : 0.116516, loss_ce: 0.052012
2021-12-11 20:25:01,103 iteration 1256 : loss : 0.100870, loss_ce: 0.037674
2021-12-11 20:25:03,813 iteration 1257 : loss : 0.096837, loss_ce: 0.052431
2021-12-11 20:25:06,764 iteration 1258 : loss : 0.108742, loss_ce: 0.048663
 18%|█████▏                      | 74/400 [1:02:04<4:26:54, 49.12s/it]2021-12-11 20:25:09,545 iteration 1259 : loss : 0.096197, loss_ce: 0.042491
2021-12-11 20:25:12,404 iteration 1260 : loss : 0.090546, loss_ce: 0.038111
2021-12-11 20:25:15,236 iteration 1261 : loss : 0.100059, loss_ce: 0.041040
2021-12-11 20:25:17,953 iteration 1262 : loss : 0.082147, loss_ce: 0.038851
2021-12-11 20:25:20,653 iteration 1263 : loss : 0.086492, loss_ce: 0.036975
2021-12-11 20:25:23,533 iteration 1264 : loss : 0.099813, loss_ce: 0.050073
2021-12-11 20:25:26,305 iteration 1265 : loss : 0.115084, loss_ce: 0.048715
2021-12-11 20:25:28,972 iteration 1266 : loss : 0.077136, loss_ce: 0.041402
2021-12-11 20:25:31,794 iteration 1267 : loss : 0.096970, loss_ce: 0.040593
2021-12-11 20:25:34,533 iteration 1268 : loss : 0.184623, loss_ce: 0.048596
2021-12-11 20:25:37,483 iteration 1269 : loss : 0.124368, loss_ce: 0.049892
2021-12-11 20:25:40,126 iteration 1270 : loss : 0.103330, loss_ce: 0.038236
2021-12-11 20:25:42,812 iteration 1271 : loss : 0.091234, loss_ce: 0.047494
2021-12-11 20:25:45,551 iteration 1272 : loss : 0.109919, loss_ce: 0.051556
2021-12-11 20:25:48,351 iteration 1273 : loss : 0.104626, loss_ce: 0.051598
2021-12-11 20:25:51,132 iteration 1274 : loss : 0.095399, loss_ce: 0.041411
2021-12-11 20:25:51,132 Training Data Eval:
2021-12-11 20:26:06,135   Average segmentation loss on training set: 0.0828
2021-12-11 20:26:06,136 Validation Data Eval:
2021-12-11 20:26:11,309   Average segmentation loss on validation set: 0.1116
2021-12-11 20:26:13,261 Found new lowest validation loss at iteration 1274! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_best_val_loss_seed2.pth
2021-12-11 20:26:15,640 iteration 1275 : loss : 0.109343, loss_ce: 0.037551
 19%|█████▎                      | 75/400 [1:03:13<4:58:11, 55.05s/it]2021-12-11 20:26:18,598 iteration 1276 : loss : 0.142918, loss_ce: 0.045694
2021-12-11 20:26:21,343 iteration 1277 : loss : 0.076860, loss_ce: 0.037162
2021-12-11 20:26:24,181 iteration 1278 : loss : 0.079179, loss_ce: 0.039040
2021-12-11 20:26:26,865 iteration 1279 : loss : 0.086329, loss_ce: 0.045229
2021-12-11 20:26:29,563 iteration 1280 : loss : 0.103595, loss_ce: 0.047746
2021-12-11 20:26:32,396 iteration 1281 : loss : 0.111343, loss_ce: 0.050806
2021-12-11 20:26:34,966 iteration 1282 : loss : 0.129761, loss_ce: 0.078079
2021-12-11 20:26:37,667 iteration 1283 : loss : 0.097570, loss_ce: 0.041299
2021-12-11 20:26:40,518 iteration 1284 : loss : 0.100055, loss_ce: 0.047586
2021-12-11 20:26:43,391 iteration 1285 : loss : 0.144162, loss_ce: 0.044942
2021-12-11 20:26:46,184 iteration 1286 : loss : 0.107623, loss_ce: 0.044581
2021-12-11 20:26:48,994 iteration 1287 : loss : 0.089894, loss_ce: 0.038298
2021-12-11 20:26:51,842 iteration 1288 : loss : 0.111964, loss_ce: 0.049439
2021-12-11 20:26:54,841 iteration 1289 : loss : 0.118165, loss_ce: 0.049221
2021-12-11 20:26:57,483 iteration 1290 : loss : 0.137192, loss_ce: 0.044369
2021-12-11 20:27:00,418 iteration 1291 : loss : 0.092342, loss_ce: 0.036729
2021-12-11 20:27:03,134 iteration 1292 : loss : 0.091661, loss_ce: 0.041080
 19%|█████▎                      | 76/400 [1:04:01<4:45:00, 52.78s/it]2021-12-11 20:27:05,912 iteration 1293 : loss : 0.101557, loss_ce: 0.051114
2021-12-11 20:27:08,597 iteration 1294 : loss : 0.114709, loss_ce: 0.042936
2021-12-11 20:27:11,384 iteration 1295 : loss : 0.071346, loss_ce: 0.034150
2021-12-11 20:27:14,096 iteration 1296 : loss : 0.077838, loss_ce: 0.036407
2021-12-11 20:27:16,788 iteration 1297 : loss : 0.082495, loss_ce: 0.035747
2021-12-11 20:27:19,489 iteration 1298 : loss : 0.079757, loss_ce: 0.040724
2021-12-11 20:27:22,123 iteration 1299 : loss : 0.110731, loss_ce: 0.049825
2021-12-11 20:27:24,980 iteration 1300 : loss : 0.126980, loss_ce: 0.044511
2021-12-11 20:27:27,703 iteration 1301 : loss : 0.109912, loss_ce: 0.053238
2021-12-11 20:27:30,588 iteration 1302 : loss : 0.106577, loss_ce: 0.039869
2021-12-11 20:27:33,345 iteration 1303 : loss : 0.093807, loss_ce: 0.042515
2021-12-11 20:27:36,121 iteration 1304 : loss : 0.147329, loss_ce: 0.051543
2021-12-11 20:27:38,843 iteration 1305 : loss : 0.077870, loss_ce: 0.040075
2021-12-11 20:27:41,549 iteration 1306 : loss : 0.096226, loss_ce: 0.038357
2021-12-11 20:27:44,253 iteration 1307 : loss : 0.103914, loss_ce: 0.042119
2021-12-11 20:27:46,942 iteration 1308 : loss : 0.107460, loss_ce: 0.063057
2021-12-11 20:27:49,712 iteration 1309 : loss : 0.125853, loss_ce: 0.042625
 19%|█████▍                      | 77/400 [1:04:47<4:34:07, 50.92s/it]2021-12-11 20:27:52,677 iteration 1310 : loss : 0.146227, loss_ce: 0.058463
2021-12-11 20:27:55,526 iteration 1311 : loss : 0.101133, loss_ce: 0.040108
2021-12-11 20:27:58,340 iteration 1312 : loss : 0.113913, loss_ce: 0.055261
2021-12-11 20:28:01,012 iteration 1313 : loss : 0.099455, loss_ce: 0.044090
2021-12-11 20:28:03,728 iteration 1314 : loss : 0.078974, loss_ce: 0.038338
2021-12-11 20:28:06,586 iteration 1315 : loss : 0.116252, loss_ce: 0.049491
2021-12-11 20:28:09,424 iteration 1316 : loss : 0.093708, loss_ce: 0.048657
2021-12-11 20:28:12,323 iteration 1317 : loss : 0.094470, loss_ce: 0.047016
2021-12-11 20:28:15,193 iteration 1318 : loss : 0.113542, loss_ce: 0.053942
2021-12-11 20:28:17,976 iteration 1319 : loss : 0.111228, loss_ce: 0.042604
2021-12-11 20:28:20,695 iteration 1320 : loss : 0.099014, loss_ce: 0.040545
2021-12-11 20:28:23,543 iteration 1321 : loss : 0.116453, loss_ce: 0.047724
2021-12-11 20:28:26,350 iteration 1322 : loss : 0.086152, loss_ce: 0.040193
2021-12-11 20:28:29,308 iteration 1323 : loss : 0.079951, loss_ce: 0.039363
2021-12-11 20:28:32,096 iteration 1324 : loss : 0.082892, loss_ce: 0.040041
2021-12-11 20:28:35,055 iteration 1325 : loss : 0.086044, loss_ce: 0.040838
2021-12-11 20:28:37,795 iteration 1326 : loss : 0.119361, loss_ce: 0.050468
 20%|█████▍                      | 78/400 [1:05:35<4:28:43, 50.07s/it]2021-12-11 20:28:40,769 iteration 1327 : loss : 0.105314, loss_ce: 0.055491
2021-12-11 20:28:43,798 iteration 1328 : loss : 0.098958, loss_ce: 0.038821
2021-12-11 20:28:46,420 iteration 1329 : loss : 0.087645, loss_ce: 0.044333
2021-12-11 20:28:49,319 iteration 1330 : loss : 0.115417, loss_ce: 0.048894
2021-12-11 20:28:51,930 iteration 1331 : loss : 0.098824, loss_ce: 0.034264
2021-12-11 20:28:54,685 iteration 1332 : loss : 0.071417, loss_ce: 0.035551
2021-12-11 20:28:57,512 iteration 1333 : loss : 0.141182, loss_ce: 0.043152
2021-12-11 20:29:00,405 iteration 1334 : loss : 0.103076, loss_ce: 0.043784
2021-12-11 20:29:03,291 iteration 1335 : loss : 0.095057, loss_ce: 0.038900
2021-12-11 20:29:06,159 iteration 1336 : loss : 0.105939, loss_ce: 0.051079
2021-12-11 20:29:08,993 iteration 1337 : loss : 0.116620, loss_ce: 0.051561
2021-12-11 20:29:11,875 iteration 1338 : loss : 0.123464, loss_ce: 0.055907
2021-12-11 20:29:14,570 iteration 1339 : loss : 0.102319, loss_ce: 0.047483
2021-12-11 20:29:17,407 iteration 1340 : loss : 0.099505, loss_ce: 0.039784
2021-12-11 20:29:20,167 iteration 1341 : loss : 0.113944, loss_ce: 0.052453
2021-12-11 20:29:22,915 iteration 1342 : loss : 0.147625, loss_ce: 0.061049
2021-12-11 20:29:25,555 iteration 1343 : loss : 0.086713, loss_ce: 0.041644
 20%|█████▌                      | 79/400 [1:06:23<4:24:09, 49.38s/it]2021-12-11 20:29:28,510 iteration 1344 : loss : 0.070128, loss_ce: 0.033041
2021-12-11 20:29:31,161 iteration 1345 : loss : 0.066155, loss_ce: 0.035348
2021-12-11 20:29:33,823 iteration 1346 : loss : 0.100378, loss_ce: 0.042658
2021-12-11 20:29:36,811 iteration 1347 : loss : 0.170295, loss_ce: 0.046885
2021-12-11 20:29:39,651 iteration 1348 : loss : 0.081561, loss_ce: 0.040950
2021-12-11 20:29:42,443 iteration 1349 : loss : 0.128805, loss_ce: 0.057734
2021-12-11 20:29:45,235 iteration 1350 : loss : 0.148205, loss_ce: 0.072379
2021-12-11 20:29:47,919 iteration 1351 : loss : 0.095261, loss_ce: 0.039143
2021-12-11 20:29:50,822 iteration 1352 : loss : 0.094300, loss_ce: 0.039636
2021-12-11 20:29:53,659 iteration 1353 : loss : 0.081130, loss_ce: 0.038559
2021-12-11 20:29:56,338 iteration 1354 : loss : 0.064190, loss_ce: 0.032859
2021-12-11 20:29:58,979 iteration 1355 : loss : 0.097559, loss_ce: 0.037315
2021-12-11 20:30:01,667 iteration 1356 : loss : 0.097071, loss_ce: 0.039764
2021-12-11 20:30:04,511 iteration 1357 : loss : 0.118751, loss_ce: 0.064888
2021-12-11 20:30:07,416 iteration 1358 : loss : 0.083487, loss_ce: 0.032866
2021-12-11 20:30:10,249 iteration 1359 : loss : 0.099800, loss_ce: 0.060917
2021-12-11 20:30:10,249 Training Data Eval:
2021-12-11 20:30:24,853   Average segmentation loss on training set: 0.0716
2021-12-11 20:30:24,853 Validation Data Eval:
2021-12-11 20:30:30,010   Average segmentation loss on validation set: 0.1213
2021-12-11 20:30:32,602 iteration 1360 : loss : 0.105126, loss_ce: 0.041985
 20%|█████▌                      | 80/400 [1:07:30<4:51:37, 54.68s/it]2021-12-11 20:30:35,417 iteration 1361 : loss : 0.135408, loss_ce: 0.052267
2021-12-11 20:30:38,315 iteration 1362 : loss : 0.109801, loss_ce: 0.055607
2021-12-11 20:30:41,087 iteration 1363 : loss : 0.073807, loss_ce: 0.033628
2021-12-11 20:30:43,810 iteration 1364 : loss : 0.119867, loss_ce: 0.042636
2021-12-11 20:30:46,610 iteration 1365 : loss : 0.090628, loss_ce: 0.043276
2021-12-11 20:30:49,352 iteration 1366 : loss : 0.110396, loss_ce: 0.055212
2021-12-11 20:30:52,072 iteration 1367 : loss : 0.072106, loss_ce: 0.035466
2021-12-11 20:30:54,866 iteration 1368 : loss : 0.090878, loss_ce: 0.040765
2021-12-11 20:30:57,768 iteration 1369 : loss : 0.120683, loss_ce: 0.045822
2021-12-11 20:31:00,657 iteration 1370 : loss : 0.095659, loss_ce: 0.040924
2021-12-11 20:31:03,284 iteration 1371 : loss : 0.072662, loss_ce: 0.035037
2021-12-11 20:31:05,997 iteration 1372 : loss : 0.139429, loss_ce: 0.052042
2021-12-11 20:31:08,993 iteration 1373 : loss : 0.084282, loss_ce: 0.047567
2021-12-11 20:31:11,888 iteration 1374 : loss : 0.114865, loss_ce: 0.037778
2021-12-11 20:31:14,584 iteration 1375 : loss : 0.119507, loss_ce: 0.058656
2021-12-11 20:31:17,205 iteration 1376 : loss : 0.086990, loss_ce: 0.041184
2021-12-11 20:31:19,926 iteration 1377 : loss : 0.130768, loss_ce: 0.051310
 20%|█████▋                      | 81/400 [1:08:18<4:38:58, 52.47s/it]2021-12-11 20:31:22,757 iteration 1378 : loss : 0.097380, loss_ce: 0.046874
2021-12-11 20:31:25,715 iteration 1379 : loss : 0.124622, loss_ce: 0.037429
2021-12-11 20:31:28,528 iteration 1380 : loss : 0.070234, loss_ce: 0.035860
2021-12-11 20:31:31,326 iteration 1381 : loss : 0.069609, loss_ce: 0.034389
2021-12-11 20:31:34,225 iteration 1382 : loss : 0.107260, loss_ce: 0.046674
2021-12-11 20:31:36,909 iteration 1383 : loss : 0.081510, loss_ce: 0.030455
2021-12-11 20:31:39,944 iteration 1384 : loss : 0.101138, loss_ce: 0.048796
2021-12-11 20:31:42,672 iteration 1385 : loss : 0.068089, loss_ce: 0.033606
2021-12-11 20:31:45,662 iteration 1386 : loss : 0.094505, loss_ce: 0.047960
2021-12-11 20:31:48,438 iteration 1387 : loss : 0.093217, loss_ce: 0.036066
2021-12-11 20:31:51,400 iteration 1388 : loss : 0.118517, loss_ce: 0.039790
2021-12-11 20:31:54,129 iteration 1389 : loss : 0.109232, loss_ce: 0.058852
2021-12-11 20:31:56,804 iteration 1390 : loss : 0.089450, loss_ce: 0.044477
2021-12-11 20:31:59,661 iteration 1391 : loss : 0.109790, loss_ce: 0.037465
2021-12-11 20:32:02,328 iteration 1392 : loss : 0.121243, loss_ce: 0.066579
2021-12-11 20:32:05,172 iteration 1393 : loss : 0.119502, loss_ce: 0.042526
2021-12-11 20:32:08,050 iteration 1394 : loss : 0.101410, loss_ce: 0.040076
 20%|█████▋                      | 82/400 [1:09:06<4:31:10, 51.16s/it]2021-12-11 20:32:11,058 iteration 1395 : loss : 0.090537, loss_ce: 0.047430
2021-12-11 20:32:13,797 iteration 1396 : loss : 0.088873, loss_ce: 0.043984
2021-12-11 20:32:16,620 iteration 1397 : loss : 0.082025, loss_ce: 0.035391
2021-12-11 20:32:19,178 iteration 1398 : loss : 0.056229, loss_ce: 0.028290
2021-12-11 20:32:22,116 iteration 1399 : loss : 0.069839, loss_ce: 0.035647
2021-12-11 20:32:24,944 iteration 1400 : loss : 0.089840, loss_ce: 0.038458
2021-12-11 20:32:27,607 iteration 1401 : loss : 0.062041, loss_ce: 0.026981
2021-12-11 20:32:30,369 iteration 1402 : loss : 0.082757, loss_ce: 0.035528
2021-12-11 20:32:33,162 iteration 1403 : loss : 0.072260, loss_ce: 0.029694
2021-12-11 20:32:35,970 iteration 1404 : loss : 0.095403, loss_ce: 0.047258
2021-12-11 20:32:38,516 iteration 1405 : loss : 0.079187, loss_ce: 0.040509
2021-12-11 20:32:41,432 iteration 1406 : loss : 0.095257, loss_ce: 0.037873
2021-12-11 20:32:44,162 iteration 1407 : loss : 0.073809, loss_ce: 0.034424
2021-12-11 20:32:46,940 iteration 1408 : loss : 0.088319, loss_ce: 0.043725
2021-12-11 20:32:49,844 iteration 1409 : loss : 0.108113, loss_ce: 0.037759
2021-12-11 20:32:52,691 iteration 1410 : loss : 0.106980, loss_ce: 0.043352
2021-12-11 20:32:55,366 iteration 1411 : loss : 0.056781, loss_ce: 0.028812
 21%|█████▊                      | 83/400 [1:09:53<4:24:13, 50.01s/it]2021-12-11 20:32:58,362 iteration 1412 : loss : 0.085102, loss_ce: 0.044219
2021-12-11 20:33:01,045 iteration 1413 : loss : 0.061341, loss_ce: 0.028656
2021-12-11 20:33:03,962 iteration 1414 : loss : 0.075840, loss_ce: 0.040124
2021-12-11 20:33:06,707 iteration 1415 : loss : 0.071219, loss_ce: 0.035172
2021-12-11 20:33:09,493 iteration 1416 : loss : 0.097345, loss_ce: 0.038085
2021-12-11 20:33:12,325 iteration 1417 : loss : 0.120800, loss_ce: 0.047666
2021-12-11 20:33:15,048 iteration 1418 : loss : 0.066538, loss_ce: 0.029059
2021-12-11 20:33:17,852 iteration 1419 : loss : 0.068057, loss_ce: 0.033844
2021-12-11 20:33:20,746 iteration 1420 : loss : 0.067161, loss_ce: 0.027345
2021-12-11 20:33:23,420 iteration 1421 : loss : 0.110485, loss_ce: 0.062038
2021-12-11 20:33:26,104 iteration 1422 : loss : 0.097466, loss_ce: 0.041288
2021-12-11 20:33:28,862 iteration 1423 : loss : 0.084380, loss_ce: 0.036084
2021-12-11 20:33:31,564 iteration 1424 : loss : 0.049864, loss_ce: 0.026126
2021-12-11 20:33:34,474 iteration 1425 : loss : 0.116411, loss_ce: 0.049221
2021-12-11 20:33:37,310 iteration 1426 : loss : 0.099362, loss_ce: 0.057161
2021-12-11 20:33:40,092 iteration 1427 : loss : 0.115734, loss_ce: 0.034058
2021-12-11 20:33:43,065 iteration 1428 : loss : 0.117337, loss_ce: 0.048206
 21%|█████▉                      | 84/400 [1:10:41<4:19:45, 49.32s/it]2021-12-11 20:33:45,863 iteration 1429 : loss : 0.087349, loss_ce: 0.051233
2021-12-11 20:33:48,724 iteration 1430 : loss : 0.069734, loss_ce: 0.033788
2021-12-11 20:33:51,377 iteration 1431 : loss : 0.116741, loss_ce: 0.043403
2021-12-11 20:33:54,311 iteration 1432 : loss : 0.072824, loss_ce: 0.037399
2021-12-11 20:33:57,135 iteration 1433 : loss : 0.075300, loss_ce: 0.040501
2021-12-11 20:33:59,926 iteration 1434 : loss : 0.068253, loss_ce: 0.034736
2021-12-11 20:34:02,844 iteration 1435 : loss : 0.067957, loss_ce: 0.029109
2021-12-11 20:34:05,661 iteration 1436 : loss : 0.088800, loss_ce: 0.046662
2021-12-11 20:34:08,484 iteration 1437 : loss : 0.061516, loss_ce: 0.031640
2021-12-11 20:34:11,294 iteration 1438 : loss : 0.073965, loss_ce: 0.035307
2021-12-11 20:34:13,997 iteration 1439 : loss : 0.129504, loss_ce: 0.046461
2021-12-11 20:34:16,647 iteration 1440 : loss : 0.067068, loss_ce: 0.028483
2021-12-11 20:34:19,428 iteration 1441 : loss : 0.077488, loss_ce: 0.035905
2021-12-11 20:34:22,337 iteration 1442 : loss : 0.095243, loss_ce: 0.032822
2021-12-11 20:34:25,159 iteration 1443 : loss : 0.069566, loss_ce: 0.031467
2021-12-11 20:34:27,887 iteration 1444 : loss : 0.092270, loss_ce: 0.031704
2021-12-11 20:34:27,887 Training Data Eval:
2021-12-11 20:34:42,671   Average segmentation loss on training set: 0.0622
2021-12-11 20:34:42,671 Validation Data Eval:
2021-12-11 20:34:47,858   Average segmentation loss on validation set: 0.1417
2021-12-11 20:34:50,680 iteration 1445 : loss : 0.102965, loss_ce: 0.057366
 21%|█████▉                      | 85/400 [1:11:48<4:47:44, 54.81s/it]2021-12-11 20:34:53,302 iteration 1446 : loss : 0.070538, loss_ce: 0.033882
2021-12-11 20:34:56,147 iteration 1447 : loss : 0.076882, loss_ce: 0.038194
2021-12-11 20:34:58,918 iteration 1448 : loss : 0.067094, loss_ce: 0.027051
2021-12-11 20:35:01,636 iteration 1449 : loss : 0.091001, loss_ce: 0.034567
2021-12-11 20:35:04,537 iteration 1450 : loss : 0.077091, loss_ce: 0.032569
2021-12-11 20:35:07,214 iteration 1451 : loss : 0.081510, loss_ce: 0.030879
2021-12-11 20:35:09,929 iteration 1452 : loss : 0.098439, loss_ce: 0.058408
2021-12-11 20:35:12,739 iteration 1453 : loss : 0.067037, loss_ce: 0.036417
2021-12-11 20:35:15,586 iteration 1454 : loss : 0.082356, loss_ce: 0.032632
2021-12-11 20:35:18,259 iteration 1455 : loss : 0.094983, loss_ce: 0.039621
2021-12-11 20:35:20,925 iteration 1456 : loss : 0.062304, loss_ce: 0.025848
2021-12-11 20:35:23,758 iteration 1457 : loss : 0.084256, loss_ce: 0.038850
2021-12-11 20:35:26,704 iteration 1458 : loss : 0.080102, loss_ce: 0.040515
2021-12-11 20:35:29,360 iteration 1459 : loss : 0.090780, loss_ce: 0.035467
2021-12-11 20:35:32,184 iteration 1460 : loss : 0.073189, loss_ce: 0.033307
2021-12-11 20:35:34,810 iteration 1461 : loss : 0.101759, loss_ce: 0.044656
2021-12-11 20:35:37,765 iteration 1462 : loss : 0.086353, loss_ce: 0.043937
 22%|██████                      | 86/400 [1:12:35<4:34:42, 52.49s/it]2021-12-11 20:35:40,562 iteration 1463 : loss : 0.135527, loss_ce: 0.047811
2021-12-11 20:35:43,439 iteration 1464 : loss : 0.081319, loss_ce: 0.047145
2021-12-11 20:35:46,295 iteration 1465 : loss : 0.070669, loss_ce: 0.030206
2021-12-11 20:35:49,136 iteration 1466 : loss : 0.064978, loss_ce: 0.032713
2021-12-11 20:35:51,951 iteration 1467 : loss : 0.076036, loss_ce: 0.035329
2021-12-11 20:35:54,772 iteration 1468 : loss : 0.063769, loss_ce: 0.032044
2021-12-11 20:35:57,399 iteration 1469 : loss : 0.097279, loss_ce: 0.048196
2021-12-11 20:36:00,096 iteration 1470 : loss : 0.099313, loss_ce: 0.029458
2021-12-11 20:36:02,890 iteration 1471 : loss : 0.061847, loss_ce: 0.031952
2021-12-11 20:36:05,759 iteration 1472 : loss : 0.086887, loss_ce: 0.036042
2021-12-11 20:36:08,391 iteration 1473 : loss : 0.078016, loss_ce: 0.043798
2021-12-11 20:36:11,345 iteration 1474 : loss : 0.116433, loss_ce: 0.041331
2021-12-11 20:36:14,149 iteration 1475 : loss : 0.085684, loss_ce: 0.044073
2021-12-11 20:36:16,858 iteration 1476 : loss : 0.069138, loss_ce: 0.032813
2021-12-11 20:36:19,892 iteration 1477 : loss : 0.075071, loss_ce: 0.036139
2021-12-11 20:36:22,569 iteration 1478 : loss : 0.114470, loss_ce: 0.035886
2021-12-11 20:36:25,474 iteration 1479 : loss : 0.102355, loss_ce: 0.033640
 22%|██████                      | 87/400 [1:13:23<4:26:21, 51.06s/it]2021-12-11 20:36:28,107 iteration 1480 : loss : 0.049802, loss_ce: 0.024802
2021-12-11 20:36:30,945 iteration 1481 : loss : 0.076643, loss_ce: 0.039050
2021-12-11 20:36:33,836 iteration 1482 : loss : 0.078390, loss_ce: 0.032148
2021-12-11 20:36:36,640 iteration 1483 : loss : 0.097253, loss_ce: 0.037659
2021-12-11 20:36:39,461 iteration 1484 : loss : 0.072692, loss_ce: 0.028905
2021-12-11 20:36:42,294 iteration 1485 : loss : 0.080309, loss_ce: 0.040643
2021-12-11 20:36:45,084 iteration 1486 : loss : 0.078545, loss_ce: 0.029287
2021-12-11 20:36:47,726 iteration 1487 : loss : 0.076014, loss_ce: 0.033751
2021-12-11 20:36:50,509 iteration 1488 : loss : 0.074820, loss_ce: 0.036118
2021-12-11 20:36:53,141 iteration 1489 : loss : 0.079674, loss_ce: 0.038916
2021-12-11 20:36:55,789 iteration 1490 : loss : 0.081091, loss_ce: 0.035283
2021-12-11 20:36:58,512 iteration 1491 : loss : 0.082552, loss_ce: 0.032995
2021-12-11 20:37:01,149 iteration 1492 : loss : 0.080385, loss_ce: 0.044804
2021-12-11 20:37:03,923 iteration 1493 : loss : 0.078193, loss_ce: 0.034880
2021-12-11 20:37:06,657 iteration 1494 : loss : 0.090916, loss_ce: 0.038477
2021-12-11 20:37:09,279 iteration 1495 : loss : 0.068465, loss_ce: 0.025906
2021-12-11 20:37:11,991 iteration 1496 : loss : 0.092456, loss_ce: 0.044304
 22%|██████▏                     | 88/400 [1:14:10<4:18:24, 49.69s/it]2021-12-11 20:37:14,746 iteration 1497 : loss : 0.051302, loss_ce: 0.026901
2021-12-11 20:37:17,522 iteration 1498 : loss : 0.105381, loss_ce: 0.044272
2021-12-11 20:37:20,366 iteration 1499 : loss : 0.087562, loss_ce: 0.036923
2021-12-11 20:37:23,061 iteration 1500 : loss : 0.075635, loss_ce: 0.039853
2021-12-11 20:37:25,777 iteration 1501 : loss : 0.080485, loss_ce: 0.038025
2021-12-11 20:37:28,749 iteration 1502 : loss : 0.117278, loss_ce: 0.035855
2021-12-11 20:37:31,673 iteration 1503 : loss : 0.129395, loss_ce: 0.044107
2021-12-11 20:37:34,375 iteration 1504 : loss : 0.059193, loss_ce: 0.030213
2021-12-11 20:37:37,066 iteration 1505 : loss : 0.077830, loss_ce: 0.039464
2021-12-11 20:37:39,994 iteration 1506 : loss : 0.060440, loss_ce: 0.029530
2021-12-11 20:37:42,789 iteration 1507 : loss : 0.097007, loss_ce: 0.035066
2021-12-11 20:37:45,584 iteration 1508 : loss : 0.089773, loss_ce: 0.044909
2021-12-11 20:37:48,363 iteration 1509 : loss : 0.064974, loss_ce: 0.031197
2021-12-11 20:37:51,158 iteration 1510 : loss : 0.067064, loss_ce: 0.027998
2021-12-11 20:37:53,948 iteration 1511 : loss : 0.072526, loss_ce: 0.035676
2021-12-11 20:37:56,642 iteration 1512 : loss : 0.059109, loss_ce: 0.025725
2021-12-11 20:37:59,403 iteration 1513 : loss : 0.057638, loss_ce: 0.026291
 22%|██████▏                     | 89/400 [1:14:57<4:14:01, 49.01s/it]2021-12-11 20:38:02,267 iteration 1514 : loss : 0.069923, loss_ce: 0.029111
2021-12-11 20:38:04,911 iteration 1515 : loss : 0.069890, loss_ce: 0.031405
2021-12-11 20:38:07,744 iteration 1516 : loss : 0.067138, loss_ce: 0.032408
2021-12-11 20:38:10,407 iteration 1517 : loss : 0.057772, loss_ce: 0.028258
2021-12-11 20:38:13,350 iteration 1518 : loss : 0.055194, loss_ce: 0.026003
2021-12-11 20:38:16,166 iteration 1519 : loss : 0.085099, loss_ce: 0.032795
2021-12-11 20:38:18,931 iteration 1520 : loss : 0.064191, loss_ce: 0.030096
2021-12-11 20:38:21,748 iteration 1521 : loss : 0.087163, loss_ce: 0.029541
2021-12-11 20:38:24,532 iteration 1522 : loss : 0.065234, loss_ce: 0.032768
2021-12-11 20:38:27,122 iteration 1523 : loss : 0.068349, loss_ce: 0.026265
2021-12-11 20:38:29,864 iteration 1524 : loss : 0.065770, loss_ce: 0.030853
2021-12-11 20:38:32,450 iteration 1525 : loss : 0.094949, loss_ce: 0.043420
2021-12-11 20:38:35,337 iteration 1526 : loss : 0.096784, loss_ce: 0.047793
2021-12-11 20:38:38,116 iteration 1527 : loss : 0.068664, loss_ce: 0.037965
2021-12-11 20:38:40,903 iteration 1528 : loss : 0.073698, loss_ce: 0.035136
2021-12-11 20:38:43,653 iteration 1529 : loss : 0.078207, loss_ce: 0.034197
2021-12-11 20:38:43,653 Training Data Eval:
2021-12-11 20:38:58,318   Average segmentation loss on training set: 0.0475
2021-12-11 20:38:58,319 Validation Data Eval:
2021-12-11 20:39:03,441   Average segmentation loss on validation set: 0.1115
2021-12-11 20:39:05,355 Found new lowest validation loss at iteration 1529! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_best_val_loss_seed2.pth
2021-12-11 20:39:07,661 iteration 1530 : loss : 0.088971, loss_ce: 0.044178
 22%|██████▎                     | 90/400 [1:16:05<4:43:03, 54.79s/it]2021-12-11 20:39:10,572 iteration 1531 : loss : 0.079303, loss_ce: 0.036867
2021-12-11 20:39:13,282 iteration 1532 : loss : 0.091072, loss_ce: 0.041524
2021-12-11 20:39:15,959 iteration 1533 : loss : 0.055119, loss_ce: 0.025530
2021-12-11 20:39:18,724 iteration 1534 : loss : 0.074025, loss_ce: 0.037870
2021-12-11 20:39:21,528 iteration 1535 : loss : 0.058407, loss_ce: 0.029407
2021-12-11 20:39:24,303 iteration 1536 : loss : 0.076614, loss_ce: 0.041553
2021-12-11 20:39:26,920 iteration 1537 : loss : 0.060343, loss_ce: 0.026521
2021-12-11 20:39:29,857 iteration 1538 : loss : 0.063009, loss_ce: 0.030624
2021-12-11 20:39:32,601 iteration 1539 : loss : 0.052475, loss_ce: 0.027786
2021-12-11 20:39:35,451 iteration 1540 : loss : 0.103231, loss_ce: 0.047929
2021-12-11 20:39:38,283 iteration 1541 : loss : 0.053938, loss_ce: 0.028839
2021-12-11 20:39:41,001 iteration 1542 : loss : 0.058631, loss_ce: 0.029080
2021-12-11 20:39:43,878 iteration 1543 : loss : 0.077870, loss_ce: 0.028748
2021-12-11 20:39:46,547 iteration 1544 : loss : 0.057899, loss_ce: 0.026668
2021-12-11 20:39:49,468 iteration 1545 : loss : 0.073193, loss_ce: 0.032342
2021-12-11 20:39:52,115 iteration 1546 : loss : 0.091900, loss_ce: 0.028840
2021-12-11 20:39:54,912 iteration 1547 : loss : 0.065478, loss_ce: 0.028378
 23%|██████▎                     | 91/400 [1:16:53<4:30:30, 52.52s/it]2021-12-11 20:39:57,873 iteration 1548 : loss : 0.068609, loss_ce: 0.033745
2021-12-11 20:40:00,447 iteration 1549 : loss : 0.048636, loss_ce: 0.025096
2021-12-11 20:40:03,192 iteration 1550 : loss : 0.067178, loss_ce: 0.030721
2021-12-11 20:40:05,784 iteration 1551 : loss : 0.051826, loss_ce: 0.025469
2021-12-11 20:40:08,718 iteration 1552 : loss : 0.074921, loss_ce: 0.033144
2021-12-11 20:40:11,470 iteration 1553 : loss : 0.083798, loss_ce: 0.036943
2021-12-11 20:40:14,367 iteration 1554 : loss : 0.056705, loss_ce: 0.027757
2021-12-11 20:40:17,069 iteration 1555 : loss : 0.098208, loss_ce: 0.044747
2021-12-11 20:40:19,912 iteration 1556 : loss : 0.071102, loss_ce: 0.030712
2021-12-11 20:40:22,542 iteration 1557 : loss : 0.077970, loss_ce: 0.039987
2021-12-11 20:40:25,310 iteration 1558 : loss : 0.068249, loss_ce: 0.037694
2021-12-11 20:40:28,084 iteration 1559 : loss : 0.077123, loss_ce: 0.031783
2021-12-11 20:40:30,797 iteration 1560 : loss : 0.057099, loss_ce: 0.031393
2021-12-11 20:40:33,504 iteration 1561 : loss : 0.065589, loss_ce: 0.028099
2021-12-11 20:40:36,155 iteration 1562 : loss : 0.094201, loss_ce: 0.030112
2021-12-11 20:40:38,948 iteration 1563 : loss : 0.064996, loss_ce: 0.031736
2021-12-11 20:40:41,896 iteration 1564 : loss : 0.067091, loss_ce: 0.036332
 23%|██████▍                     | 92/400 [1:17:40<4:21:04, 50.86s/it]2021-12-11 20:40:44,627 iteration 1565 : loss : 0.082850, loss_ce: 0.044586
2021-12-11 20:40:47,447 iteration 1566 : loss : 0.064728, loss_ce: 0.030133
2021-12-11 20:40:50,228 iteration 1567 : loss : 0.066567, loss_ce: 0.026227
2021-12-11 20:40:52,917 iteration 1568 : loss : 0.070594, loss_ce: 0.030363
2021-12-11 20:40:55,581 iteration 1569 : loss : 0.072545, loss_ce: 0.038073
2021-12-11 20:40:58,226 iteration 1570 : loss : 0.054066, loss_ce: 0.028007
2021-12-11 20:41:00,956 iteration 1571 : loss : 0.082243, loss_ce: 0.026521
2021-12-11 20:41:03,758 iteration 1572 : loss : 0.098545, loss_ce: 0.043053
2021-12-11 20:41:06,467 iteration 1573 : loss : 0.071997, loss_ce: 0.041085
2021-12-11 20:41:09,155 iteration 1574 : loss : 0.046843, loss_ce: 0.025684
2021-12-11 20:41:11,753 iteration 1575 : loss : 0.075544, loss_ce: 0.034536
2021-12-11 20:41:14,656 iteration 1576 : loss : 0.072681, loss_ce: 0.038643
2021-12-11 20:41:17,473 iteration 1577 : loss : 0.052337, loss_ce: 0.026236
2021-12-11 20:41:20,388 iteration 1578 : loss : 0.074534, loss_ce: 0.031631
2021-12-11 20:41:23,263 iteration 1579 : loss : 0.053703, loss_ce: 0.027048
2021-12-11 20:41:26,136 iteration 1580 : loss : 0.082286, loss_ce: 0.036822
2021-12-11 20:41:28,909 iteration 1581 : loss : 0.074288, loss_ce: 0.030723
 23%|██████▌                     | 93/400 [1:18:27<4:14:19, 49.71s/it]2021-12-11 20:41:31,849 iteration 1582 : loss : 0.071041, loss_ce: 0.030106
2021-12-11 20:41:34,549 iteration 1583 : loss : 0.100214, loss_ce: 0.033996
2021-12-11 20:41:37,417 iteration 1584 : loss : 0.066703, loss_ce: 0.039220
2021-12-11 20:41:40,248 iteration 1585 : loss : 0.056829, loss_ce: 0.025720
2021-12-11 20:41:42,918 iteration 1586 : loss : 0.061779, loss_ce: 0.028390
2021-12-11 20:41:45,582 iteration 1587 : loss : 0.044679, loss_ce: 0.025356
2021-12-11 20:41:48,334 iteration 1588 : loss : 0.072866, loss_ce: 0.036014
2021-12-11 20:41:51,250 iteration 1589 : loss : 0.051976, loss_ce: 0.027313
2021-12-11 20:41:54,204 iteration 1590 : loss : 0.099294, loss_ce: 0.040133
2021-12-11 20:41:57,006 iteration 1591 : loss : 0.054011, loss_ce: 0.026003
2021-12-11 20:41:59,692 iteration 1592 : loss : 0.092423, loss_ce: 0.037449
2021-12-11 20:42:02,585 iteration 1593 : loss : 0.087598, loss_ce: 0.041659
2021-12-11 20:42:05,477 iteration 1594 : loss : 0.071792, loss_ce: 0.030212
2021-12-11 20:42:08,197 iteration 1595 : loss : 0.050971, loss_ce: 0.023877
2021-12-11 20:42:11,136 iteration 1596 : loss : 0.075264, loss_ce: 0.038480
2021-12-11 20:42:13,846 iteration 1597 : loss : 0.122720, loss_ce: 0.050277
2021-12-11 20:42:16,582 iteration 1598 : loss : 0.071936, loss_ce: 0.031592
 24%|██████▌                     | 94/400 [1:19:14<4:10:23, 49.10s/it]2021-12-11 20:42:19,282 iteration 1599 : loss : 0.071689, loss_ce: 0.032523
2021-12-11 20:42:22,050 iteration 1600 : loss : 0.068396, loss_ce: 0.031746
2021-12-11 20:42:24,874 iteration 1601 : loss : 0.082909, loss_ce: 0.036684
2021-12-11 20:42:27,674 iteration 1602 : loss : 0.088289, loss_ce: 0.039127
2021-12-11 20:42:30,383 iteration 1603 : loss : 0.064773, loss_ce: 0.029022
2021-12-11 20:42:33,121 iteration 1604 : loss : 0.059580, loss_ce: 0.027475
2021-12-11 20:42:35,962 iteration 1605 : loss : 0.083926, loss_ce: 0.036377
2021-12-11 20:42:38,735 iteration 1606 : loss : 0.103359, loss_ce: 0.060432
2021-12-11 20:42:41,477 iteration 1607 : loss : 0.062808, loss_ce: 0.031573
2021-12-11 20:42:44,299 iteration 1608 : loss : 0.073115, loss_ce: 0.030182
2021-12-11 20:42:47,043 iteration 1609 : loss : 0.077570, loss_ce: 0.041059
2021-12-11 20:42:49,880 iteration 1610 : loss : 0.053357, loss_ce: 0.026522
2021-12-11 20:42:52,752 iteration 1611 : loss : 0.082290, loss_ce: 0.033160
2021-12-11 20:42:55,584 iteration 1612 : loss : 0.058105, loss_ce: 0.031453
2021-12-11 20:42:58,149 iteration 1613 : loss : 0.077185, loss_ce: 0.039320
2021-12-11 20:43:00,913 iteration 1614 : loss : 0.061675, loss_ce: 0.026073
2021-12-11 20:43:00,913 Training Data Eval:
2021-12-11 20:43:15,726   Average segmentation loss on training set: 0.0443
2021-12-11 20:43:15,726 Validation Data Eval:
2021-12-11 20:43:21,013   Average segmentation loss on validation set: 0.1050
2021-12-11 20:43:23,020 Found new lowest validation loss at iteration 1614! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_best_val_loss_seed2.pth
2021-12-11 20:43:25,387 iteration 1615 : loss : 0.076010, loss_ce: 0.033733
 24%|██████▋                     | 95/400 [1:20:23<4:39:38, 55.01s/it]2021-12-11 20:43:28,214 iteration 1616 : loss : 0.073987, loss_ce: 0.032841
2021-12-11 20:43:31,027 iteration 1617 : loss : 0.081936, loss_ce: 0.031482
2021-12-11 20:43:33,661 iteration 1618 : loss : 0.056896, loss_ce: 0.027898
2021-12-11 20:43:36,394 iteration 1619 : loss : 0.052716, loss_ce: 0.025836
2021-12-11 20:43:38,957 iteration 1620 : loss : 0.070262, loss_ce: 0.030135
2021-12-11 20:43:41,754 iteration 1621 : loss : 0.089749, loss_ce: 0.051737
2021-12-11 20:43:44,648 iteration 1622 : loss : 0.085244, loss_ce: 0.034132
2021-12-11 20:43:47,388 iteration 1623 : loss : 0.085923, loss_ce: 0.038174
2021-12-11 20:43:50,058 iteration 1624 : loss : 0.063023, loss_ce: 0.029102
2021-12-11 20:43:52,732 iteration 1625 : loss : 0.061574, loss_ce: 0.029914
2021-12-11 20:43:55,437 iteration 1626 : loss : 0.046433, loss_ce: 0.025264
2021-12-11 20:43:58,246 iteration 1627 : loss : 0.073113, loss_ce: 0.031830
2021-12-11 20:44:01,060 iteration 1628 : loss : 0.082951, loss_ce: 0.039884
2021-12-11 20:44:03,886 iteration 1629 : loss : 0.051055, loss_ce: 0.025889
2021-12-11 20:44:06,571 iteration 1630 : loss : 0.082443, loss_ce: 0.034044
2021-12-11 20:44:09,576 iteration 1631 : loss : 0.052311, loss_ce: 0.027970
2021-12-11 20:44:12,406 iteration 1632 : loss : 0.076355, loss_ce: 0.032059
 24%|██████▋                     | 96/400 [1:21:10<4:26:33, 52.61s/it]2021-12-11 20:44:15,316 iteration 1633 : loss : 0.055820, loss_ce: 0.027570
2021-12-11 20:44:18,129 iteration 1634 : loss : 0.092053, loss_ce: 0.036335
2021-12-11 20:44:20,823 iteration 1635 : loss : 0.068751, loss_ce: 0.037352
2021-12-11 20:44:23,710 iteration 1636 : loss : 0.088107, loss_ce: 0.044491
2021-12-11 20:44:26,397 iteration 1637 : loss : 0.083510, loss_ce: 0.033184
2021-12-11 20:44:29,228 iteration 1638 : loss : 0.082665, loss_ce: 0.034563
2021-12-11 20:44:32,005 iteration 1639 : loss : 0.100693, loss_ce: 0.031225
2021-12-11 20:44:34,959 iteration 1640 : loss : 0.085291, loss_ce: 0.029108
2021-12-11 20:44:37,766 iteration 1641 : loss : 0.048469, loss_ce: 0.028718
2021-12-11 20:44:40,582 iteration 1642 : loss : 0.085856, loss_ce: 0.034827
2021-12-11 20:44:43,511 iteration 1643 : loss : 0.089272, loss_ce: 0.032426
2021-12-11 20:44:46,198 iteration 1644 : loss : 0.066755, loss_ce: 0.029520
2021-12-11 20:44:49,020 iteration 1645 : loss : 0.085747, loss_ce: 0.036654
2021-12-11 20:44:51,795 iteration 1646 : loss : 0.085966, loss_ce: 0.032592
2021-12-11 20:44:54,590 iteration 1647 : loss : 0.089525, loss_ce: 0.036937
2021-12-11 20:44:57,246 iteration 1648 : loss : 0.090349, loss_ce: 0.044970
2021-12-11 20:44:59,901 iteration 1649 : loss : 0.081993, loss_ce: 0.038406
 24%|██████▊                     | 97/400 [1:21:58<4:17:56, 51.08s/it]2021-12-11 20:45:02,732 iteration 1650 : loss : 0.076956, loss_ce: 0.039452
2021-12-11 20:45:05,440 iteration 1651 : loss : 0.100735, loss_ce: 0.040028
2021-12-11 20:45:08,055 iteration 1652 : loss : 0.080451, loss_ce: 0.031636
2021-12-11 20:45:10,758 iteration 1653 : loss : 0.067481, loss_ce: 0.029647
2021-12-11 20:45:13,672 iteration 1654 : loss : 0.120006, loss_ce: 0.033875
2021-12-11 20:45:16,371 iteration 1655 : loss : 0.095979, loss_ce: 0.040324
2021-12-11 20:45:19,024 iteration 1656 : loss : 0.069746, loss_ce: 0.032758
2021-12-11 20:45:21,978 iteration 1657 : loss : 0.107407, loss_ce: 0.039932
2021-12-11 20:45:24,875 iteration 1658 : loss : 0.106781, loss_ce: 0.058616
2021-12-11 20:45:27,584 iteration 1659 : loss : 0.053816, loss_ce: 0.021862
2021-12-11 20:45:30,611 iteration 1660 : loss : 0.085160, loss_ce: 0.039794
2021-12-11 20:45:33,449 iteration 1661 : loss : 0.066812, loss_ce: 0.027422
2021-12-11 20:45:36,343 iteration 1662 : loss : 0.056856, loss_ce: 0.027720
2021-12-11 20:45:38,850 iteration 1663 : loss : 0.056258, loss_ce: 0.031756
2021-12-11 20:45:41,868 iteration 1664 : loss : 0.093190, loss_ce: 0.037445
2021-12-11 20:45:44,562 iteration 1665 : loss : 0.070324, loss_ce: 0.030466
2021-12-11 20:45:47,418 iteration 1666 : loss : 0.104484, loss_ce: 0.051690
 24%|██████▊                     | 98/400 [1:22:45<4:11:43, 50.01s/it]2021-12-11 20:45:50,118 iteration 1667 : loss : 0.108558, loss_ce: 0.041850
2021-12-11 20:45:52,806 iteration 1668 : loss : 0.067631, loss_ce: 0.027374
2021-12-11 20:45:55,409 iteration 1669 : loss : 0.049663, loss_ce: 0.025198
2021-12-11 20:45:58,336 iteration 1670 : loss : 0.085116, loss_ce: 0.041010
2021-12-11 20:46:01,009 iteration 1671 : loss : 0.060914, loss_ce: 0.029625
2021-12-11 20:46:03,732 iteration 1672 : loss : 0.070843, loss_ce: 0.033197
2021-12-11 20:46:06,667 iteration 1673 : loss : 0.065129, loss_ce: 0.030381
2021-12-11 20:46:09,358 iteration 1674 : loss : 0.125136, loss_ce: 0.080091
2021-12-11 20:46:12,195 iteration 1675 : loss : 0.122128, loss_ce: 0.041611
2021-12-11 20:46:14,948 iteration 1676 : loss : 0.154429, loss_ce: 0.056968
2021-12-11 20:46:17,905 iteration 1677 : loss : 0.102923, loss_ce: 0.052803
2021-12-11 20:46:20,574 iteration 1678 : loss : 0.056937, loss_ce: 0.031476
2021-12-11 20:46:23,310 iteration 1679 : loss : 0.053966, loss_ce: 0.021213
2021-12-11 20:46:26,167 iteration 1680 : loss : 0.076138, loss_ce: 0.038784
2021-12-11 20:46:28,958 iteration 1681 : loss : 0.080772, loss_ce: 0.045630
2021-12-11 20:46:31,822 iteration 1682 : loss : 0.072728, loss_ce: 0.030269
2021-12-11 20:46:34,590 iteration 1683 : loss : 0.050751, loss_ce: 0.024086
 25%|██████▉                     | 99/400 [1:23:32<4:06:37, 49.16s/it]2021-12-11 20:46:37,396 iteration 1684 : loss : 0.075612, loss_ce: 0.033026
2021-12-11 20:46:40,041 iteration 1685 : loss : 0.084812, loss_ce: 0.035616
2021-12-11 20:46:42,869 iteration 1686 : loss : 0.057092, loss_ce: 0.027104
2021-12-11 20:46:45,600 iteration 1687 : loss : 0.073645, loss_ce: 0.035146
2021-12-11 20:46:48,311 iteration 1688 : loss : 0.055411, loss_ce: 0.028423
2021-12-11 20:46:51,216 iteration 1689 : loss : 0.082311, loss_ce: 0.042467
2021-12-11 20:46:54,176 iteration 1690 : loss : 0.067436, loss_ce: 0.036467
2021-12-11 20:46:56,906 iteration 1691 : loss : 0.116366, loss_ce: 0.037216
2021-12-11 20:46:59,684 iteration 1692 : loss : 0.137992, loss_ce: 0.055457
2021-12-11 20:47:02,376 iteration 1693 : loss : 0.059471, loss_ce: 0.029913
2021-12-11 20:47:05,060 iteration 1694 : loss : 0.064914, loss_ce: 0.031803
2021-12-11 20:47:07,873 iteration 1695 : loss : 0.061322, loss_ce: 0.028702
2021-12-11 20:47:10,692 iteration 1696 : loss : 0.120894, loss_ce: 0.051001
2021-12-11 20:47:13,516 iteration 1697 : loss : 0.070447, loss_ce: 0.030499
2021-12-11 20:47:16,239 iteration 1698 : loss : 0.064777, loss_ce: 0.030523
2021-12-11 20:47:18,969 iteration 1699 : loss : 0.065360, loss_ce: 0.033481
2021-12-11 20:47:18,969 Training Data Eval:
2021-12-11 20:47:33,609   Average segmentation loss on training set: 0.0515
2021-12-11 20:47:33,610 Validation Data Eval:
2021-12-11 20:47:38,877   Average segmentation loss on validation set: 0.0830
2021-12-11 20:47:40,840 Found new lowest validation loss at iteration 1699! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_best_val_loss_seed2.pth
2021-12-11 20:47:43,136 iteration 1700 : loss : 0.070570, loss_ce: 0.028139
 25%|██████▊                    | 100/400 [1:24:41<4:34:52, 54.98s/it]2021-12-11 20:47:45,827 iteration 1701 : loss : 0.059961, loss_ce: 0.030930
2021-12-11 20:47:48,455 iteration 1702 : loss : 0.075149, loss_ce: 0.033700
2021-12-11 20:47:51,287 iteration 1703 : loss : 0.080432, loss_ce: 0.041613
2021-12-11 20:47:54,174 iteration 1704 : loss : 0.075885, loss_ce: 0.033589
2021-12-11 20:47:56,979 iteration 1705 : loss : 0.081790, loss_ce: 0.035849
2021-12-11 20:47:59,804 iteration 1706 : loss : 0.109921, loss_ce: 0.037169
2021-12-11 20:48:02,414 iteration 1707 : loss : 0.065197, loss_ce: 0.030710
2021-12-11 20:48:05,247 iteration 1708 : loss : 0.058243, loss_ce: 0.027933
2021-12-11 20:48:08,183 iteration 1709 : loss : 0.076257, loss_ce: 0.034019
2021-12-11 20:48:10,934 iteration 1710 : loss : 0.059042, loss_ce: 0.031864
2021-12-11 20:48:13,858 iteration 1711 : loss : 0.075224, loss_ce: 0.031722
2021-12-11 20:48:16,571 iteration 1712 : loss : 0.085575, loss_ce: 0.032758
2021-12-11 20:48:19,358 iteration 1713 : loss : 0.088199, loss_ce: 0.032267
2021-12-11 20:48:22,249 iteration 1714 : loss : 0.052796, loss_ce: 0.028258
2021-12-11 20:48:25,055 iteration 1715 : loss : 0.104826, loss_ce: 0.042706
2021-12-11 20:48:27,968 iteration 1716 : loss : 0.093024, loss_ce: 0.030841
2021-12-11 20:48:30,731 iteration 1717 : loss : 0.051632, loss_ce: 0.026596
 25%|██████▊                    | 101/400 [1:25:28<4:22:55, 52.76s/it]2021-12-11 20:48:33,588 iteration 1718 : loss : 0.077654, loss_ce: 0.031520
2021-12-11 20:48:36,569 iteration 1719 : loss : 0.057745, loss_ce: 0.021902
2021-12-11 20:48:39,369 iteration 1720 : loss : 0.063384, loss_ce: 0.027835
2021-12-11 20:48:42,074 iteration 1721 : loss : 0.088246, loss_ce: 0.042856
2021-12-11 20:48:44,831 iteration 1722 : loss : 0.052186, loss_ce: 0.024757
2021-12-11 20:48:47,688 iteration 1723 : loss : 0.076960, loss_ce: 0.034852
2021-12-11 20:48:50,382 iteration 1724 : loss : 0.059752, loss_ce: 0.029501
2021-12-11 20:48:53,170 iteration 1725 : loss : 0.066409, loss_ce: 0.031458
2021-12-11 20:48:55,985 iteration 1726 : loss : 0.093988, loss_ce: 0.041246
2021-12-11 20:48:58,932 iteration 1727 : loss : 0.067568, loss_ce: 0.029388
2021-12-11 20:49:01,725 iteration 1728 : loss : 0.080165, loss_ce: 0.041788
2021-12-11 20:49:04,432 iteration 1729 : loss : 0.067908, loss_ce: 0.030517
2021-12-11 20:49:07,306 iteration 1730 : loss : 0.058664, loss_ce: 0.027868
2021-12-11 20:49:09,932 iteration 1731 : loss : 0.064947, loss_ce: 0.031245
2021-12-11 20:49:12,871 iteration 1732 : loss : 0.060796, loss_ce: 0.031793
2021-12-11 20:49:15,682 iteration 1733 : loss : 0.059494, loss_ce: 0.024125
2021-12-11 20:49:18,380 iteration 1734 : loss : 0.082787, loss_ce: 0.029483
 26%|██████▉                    | 102/400 [1:26:16<4:14:24, 51.22s/it]2021-12-11 20:49:21,335 iteration 1735 : loss : 0.052153, loss_ce: 0.022724
2021-12-11 20:49:24,111 iteration 1736 : loss : 0.079905, loss_ce: 0.035248
2021-12-11 20:49:26,914 iteration 1737 : loss : 0.083028, loss_ce: 0.042301
2021-12-11 20:49:29,810 iteration 1738 : loss : 0.092895, loss_ce: 0.035043
2021-12-11 20:49:32,560 iteration 1739 : loss : 0.064701, loss_ce: 0.036787
2021-12-11 20:49:35,175 iteration 1740 : loss : 0.080760, loss_ce: 0.028563
2021-12-11 20:49:38,009 iteration 1741 : loss : 0.063742, loss_ce: 0.028726
2021-12-11 20:49:40,823 iteration 1742 : loss : 0.082960, loss_ce: 0.032631
2021-12-11 20:49:43,604 iteration 1743 : loss : 0.067973, loss_ce: 0.041687
2021-12-11 20:49:46,350 iteration 1744 : loss : 0.066931, loss_ce: 0.026585
2021-12-11 20:49:49,310 iteration 1745 : loss : 0.077723, loss_ce: 0.039440
2021-12-11 20:49:51,982 iteration 1746 : loss : 0.075737, loss_ce: 0.028686
2021-12-11 20:49:54,617 iteration 1747 : loss : 0.062767, loss_ce: 0.025140
2021-12-11 20:49:57,347 iteration 1748 : loss : 0.046724, loss_ce: 0.027283
2021-12-11 20:50:00,150 iteration 1749 : loss : 0.060908, loss_ce: 0.027533
2021-12-11 20:50:02,976 iteration 1750 : loss : 0.077251, loss_ce: 0.041614
2021-12-11 20:50:05,728 iteration 1751 : loss : 0.066460, loss_ce: 0.028795
 26%|██████▉                    | 103/400 [1:27:03<4:07:48, 50.06s/it]2021-12-11 20:50:08,588 iteration 1752 : loss : 0.049610, loss_ce: 0.025606
2021-12-11 20:50:11,407 iteration 1753 : loss : 0.082526, loss_ce: 0.041222
2021-12-11 20:50:14,257 iteration 1754 : loss : 0.051318, loss_ce: 0.027875
2021-12-11 20:50:16,902 iteration 1755 : loss : 0.057526, loss_ce: 0.023795
2021-12-11 20:50:19,848 iteration 1756 : loss : 0.112789, loss_ce: 0.072577
2021-12-11 20:50:22,536 iteration 1757 : loss : 0.064351, loss_ce: 0.022595
2021-12-11 20:50:25,292 iteration 1758 : loss : 0.080360, loss_ce: 0.037125
2021-12-11 20:50:27,998 iteration 1759 : loss : 0.073113, loss_ce: 0.033049
2021-12-11 20:50:30,748 iteration 1760 : loss : 0.062832, loss_ce: 0.034706
2021-12-11 20:50:33,502 iteration 1761 : loss : 0.051387, loss_ce: 0.026914
2021-12-11 20:50:36,236 iteration 1762 : loss : 0.116783, loss_ce: 0.057040
2021-12-11 20:50:39,065 iteration 1763 : loss : 0.068949, loss_ce: 0.026099
2021-12-11 20:50:41,925 iteration 1764 : loss : 0.141585, loss_ce: 0.038678
2021-12-11 20:50:44,844 iteration 1765 : loss : 0.091306, loss_ce: 0.030249
2021-12-11 20:50:47,562 iteration 1766 : loss : 0.046835, loss_ce: 0.025125
2021-12-11 20:50:50,366 iteration 1767 : loss : 0.058854, loss_ce: 0.030337
2021-12-11 20:50:53,341 iteration 1768 : loss : 0.091648, loss_ce: 0.038552
 26%|███████                    | 104/400 [1:27:51<4:03:21, 49.33s/it]2021-12-11 20:50:56,075 iteration 1769 : loss : 0.060510, loss_ce: 0.025419
2021-12-11 20:50:58,986 iteration 1770 : loss : 0.085485, loss_ce: 0.038195
2021-12-11 20:51:01,617 iteration 1771 : loss : 0.056568, loss_ce: 0.027936
2021-12-11 20:51:04,269 iteration 1772 : loss : 0.044113, loss_ce: 0.019629
2021-12-11 20:51:07,102 iteration 1773 : loss : 0.053675, loss_ce: 0.022248
2021-12-11 20:51:09,872 iteration 1774 : loss : 0.052748, loss_ce: 0.026281
2021-12-11 20:51:12,687 iteration 1775 : loss : 0.077426, loss_ce: 0.041265
2021-12-11 20:51:15,477 iteration 1776 : loss : 0.076434, loss_ce: 0.027675
2021-12-11 20:51:18,321 iteration 1777 : loss : 0.084906, loss_ce: 0.034133
2021-12-11 20:51:21,238 iteration 1778 : loss : 0.084968, loss_ce: 0.037461
2021-12-11 20:51:23,947 iteration 1779 : loss : 0.070138, loss_ce: 0.029851
2021-12-11 20:51:26,673 iteration 1780 : loss : 0.059318, loss_ce: 0.027550
2021-12-11 20:51:29,371 iteration 1781 : loss : 0.065654, loss_ce: 0.037536
2021-12-11 20:51:32,105 iteration 1782 : loss : 0.081108, loss_ce: 0.029938
2021-12-11 20:51:34,888 iteration 1783 : loss : 0.066423, loss_ce: 0.033154
2021-12-11 20:51:37,724 iteration 1784 : loss : 0.050218, loss_ce: 0.024961
2021-12-11 20:51:37,724 Training Data Eval:
2021-12-11 20:51:52,324   Average segmentation loss on training set: 0.0413
2021-12-11 20:51:52,325 Validation Data Eval:
2021-12-11 20:51:57,514   Average segmentation loss on validation set: 0.0837
2021-12-11 20:52:00,170 iteration 1785 : loss : 0.060211, loss_ce: 0.028807
 26%|███████                    | 105/400 [1:28:58<4:28:20, 54.58s/it]2021-12-11 20:52:03,158 iteration 1786 : loss : 0.090997, loss_ce: 0.052166
2021-12-11 20:52:06,086 iteration 1787 : loss : 0.069679, loss_ce: 0.030806
2021-12-11 20:52:08,883 iteration 1788 : loss : 0.067664, loss_ce: 0.032926
2021-12-11 20:52:11,640 iteration 1789 : loss : 0.079971, loss_ce: 0.034184
2021-12-11 20:52:14,343 iteration 1790 : loss : 0.051891, loss_ce: 0.027588
2021-12-11 20:52:16,974 iteration 1791 : loss : 0.046105, loss_ce: 0.027519
2021-12-11 20:52:19,666 iteration 1792 : loss : 0.074318, loss_ce: 0.031516
2021-12-11 20:52:22,506 iteration 1793 : loss : 0.054769, loss_ce: 0.029299
2021-12-11 20:52:25,279 iteration 1794 : loss : 0.050299, loss_ce: 0.028041
2021-12-11 20:52:27,967 iteration 1795 : loss : 0.066755, loss_ce: 0.030496
2021-12-11 20:52:30,912 iteration 1796 : loss : 0.057332, loss_ce: 0.025139
2021-12-11 20:52:33,887 iteration 1797 : loss : 0.056117, loss_ce: 0.024924
2021-12-11 20:52:36,647 iteration 1798 : loss : 0.066073, loss_ce: 0.028532
2021-12-11 20:52:39,473 iteration 1799 : loss : 0.098479, loss_ce: 0.040503
2021-12-11 20:52:42,229 iteration 1800 : loss : 0.085673, loss_ce: 0.043950
2021-12-11 20:52:45,023 iteration 1801 : loss : 0.052758, loss_ce: 0.026332
2021-12-11 20:52:47,724 iteration 1802 : loss : 0.077858, loss_ce: 0.028722
 26%|███████▏                   | 106/400 [1:29:45<4:17:06, 52.47s/it]2021-12-11 20:52:50,482 iteration 1803 : loss : 0.079335, loss_ce: 0.033195
2021-12-11 20:52:53,260 iteration 1804 : loss : 0.103008, loss_ce: 0.029218
2021-12-11 20:52:55,885 iteration 1805 : loss : 0.043414, loss_ce: 0.026201
2021-12-11 20:52:58,620 iteration 1806 : loss : 0.063432, loss_ce: 0.025963
2021-12-11 20:53:01,362 iteration 1807 : loss : 0.075346, loss_ce: 0.034963
2021-12-11 20:53:04,268 iteration 1808 : loss : 0.071950, loss_ce: 0.034004
2021-12-11 20:53:07,134 iteration 1809 : loss : 0.055598, loss_ce: 0.028681
2021-12-11 20:53:09,869 iteration 1810 : loss : 0.080539, loss_ce: 0.034086
2021-12-11 20:53:12,632 iteration 1811 : loss : 0.055435, loss_ce: 0.023931
2021-12-11 20:53:15,589 iteration 1812 : loss : 0.072931, loss_ce: 0.037737
2021-12-11 20:53:18,377 iteration 1813 : loss : 0.094659, loss_ce: 0.037271
2021-12-11 20:53:21,204 iteration 1814 : loss : 0.098530, loss_ce: 0.041906
2021-12-11 20:53:24,011 iteration 1815 : loss : 0.054998, loss_ce: 0.029572
2021-12-11 20:53:26,984 iteration 1816 : loss : 0.080553, loss_ce: 0.034850
2021-12-11 20:53:29,883 iteration 1817 : loss : 0.062334, loss_ce: 0.025376
2021-12-11 20:53:32,776 iteration 1818 : loss : 0.072247, loss_ce: 0.040464
2021-12-11 20:53:35,581 iteration 1819 : loss : 0.046777, loss_ce: 0.022316
 27%|███████▏                   | 107/400 [1:30:33<4:09:27, 51.08s/it]2021-12-11 20:53:38,396 iteration 1820 : loss : 0.094177, loss_ce: 0.031921
2021-12-11 20:53:41,107 iteration 1821 : loss : 0.088745, loss_ce: 0.029288
2021-12-11 20:53:43,996 iteration 1822 : loss : 0.054572, loss_ce: 0.030546
2021-12-11 20:53:46,844 iteration 1823 : loss : 0.084210, loss_ce: 0.035621
2021-12-11 20:53:49,574 iteration 1824 : loss : 0.063434, loss_ce: 0.028388
2021-12-11 20:53:52,374 iteration 1825 : loss : 0.059810, loss_ce: 0.026257
2021-12-11 20:53:55,034 iteration 1826 : loss : 0.051378, loss_ce: 0.026117
2021-12-11 20:53:57,763 iteration 1827 : loss : 0.077814, loss_ce: 0.027680
2021-12-11 20:54:00,447 iteration 1828 : loss : 0.054161, loss_ce: 0.028610
2021-12-11 20:54:03,285 iteration 1829 : loss : 0.087331, loss_ce: 0.029277
2021-12-11 20:54:05,992 iteration 1830 : loss : 0.060526, loss_ce: 0.030501
2021-12-11 20:54:08,777 iteration 1831 : loss : 0.066803, loss_ce: 0.033607
2021-12-11 20:54:11,541 iteration 1832 : loss : 0.050234, loss_ce: 0.022097
2021-12-11 20:54:14,249 iteration 1833 : loss : 0.051289, loss_ce: 0.032689
2021-12-11 20:54:17,201 iteration 1834 : loss : 0.060672, loss_ce: 0.027756
2021-12-11 20:54:19,786 iteration 1835 : loss : 0.053608, loss_ce: 0.024149
2021-12-11 20:54:22,467 iteration 1836 : loss : 0.053515, loss_ce: 0.024747
 27%|███████▎                   | 108/400 [1:31:20<4:02:29, 49.83s/it]2021-12-11 20:54:25,155 iteration 1837 : loss : 0.087676, loss_ce: 0.027285
2021-12-11 20:54:28,035 iteration 1838 : loss : 0.055444, loss_ce: 0.026919
2021-12-11 20:54:30,678 iteration 1839 : loss : 0.051865, loss_ce: 0.027831
2021-12-11 20:54:33,524 iteration 1840 : loss : 0.071835, loss_ce: 0.026504
2021-12-11 20:54:36,234 iteration 1841 : loss : 0.072341, loss_ce: 0.026778
2021-12-11 20:54:39,268 iteration 1842 : loss : 0.070710, loss_ce: 0.031007
2021-12-11 20:54:41,969 iteration 1843 : loss : 0.057470, loss_ce: 0.030664
2021-12-11 20:54:44,679 iteration 1844 : loss : 0.068866, loss_ce: 0.030932
2021-12-11 20:54:47,363 iteration 1845 : loss : 0.055648, loss_ce: 0.028033
2021-12-11 20:54:50,114 iteration 1846 : loss : 0.067363, loss_ce: 0.026406
2021-12-11 20:54:52,634 iteration 1847 : loss : 0.043109, loss_ce: 0.022676
2021-12-11 20:54:55,348 iteration 1848 : loss : 0.061248, loss_ce: 0.027220
2021-12-11 20:54:58,060 iteration 1849 : loss : 0.061964, loss_ce: 0.028319
2021-12-11 20:55:00,866 iteration 1850 : loss : 0.061474, loss_ce: 0.027483
2021-12-11 20:55:03,475 iteration 1851 : loss : 0.078993, loss_ce: 0.037451
2021-12-11 20:55:06,152 iteration 1852 : loss : 0.052783, loss_ce: 0.031343
2021-12-11 20:55:08,962 iteration 1853 : loss : 0.058433, loss_ce: 0.030102
 27%|███████▎                   | 109/400 [1:32:07<3:56:48, 48.83s/it]2021-12-11 20:55:11,931 iteration 1854 : loss : 0.079957, loss_ce: 0.031585
2021-12-11 20:55:14,676 iteration 1855 : loss : 0.070411, loss_ce: 0.033099
2021-12-11 20:55:17,393 iteration 1856 : loss : 0.064620, loss_ce: 0.035777
2021-12-11 20:55:19,989 iteration 1857 : loss : 0.060928, loss_ce: 0.030037
2021-12-11 20:55:22,687 iteration 1858 : loss : 0.047538, loss_ce: 0.025559
2021-12-11 20:55:25,595 iteration 1859 : loss : 0.061488, loss_ce: 0.027757
2021-12-11 20:55:28,473 iteration 1860 : loss : 0.079983, loss_ce: 0.032924
2021-12-11 20:55:31,163 iteration 1861 : loss : 0.056548, loss_ce: 0.030704
2021-12-11 20:55:33,937 iteration 1862 : loss : 0.049653, loss_ce: 0.028578
2021-12-11 20:55:36,889 iteration 1863 : loss : 0.069526, loss_ce: 0.027793
2021-12-11 20:55:39,754 iteration 1864 : loss : 0.045238, loss_ce: 0.023002
2021-12-11 20:55:42,578 iteration 1865 : loss : 0.073761, loss_ce: 0.029703
2021-12-11 20:55:45,282 iteration 1866 : loss : 0.050643, loss_ce: 0.025586
2021-12-11 20:55:48,075 iteration 1867 : loss : 0.062894, loss_ce: 0.022321
2021-12-11 20:55:50,822 iteration 1868 : loss : 0.056598, loss_ce: 0.024628
2021-12-11 20:55:53,680 iteration 1869 : loss : 0.060168, loss_ce: 0.033961
2021-12-11 20:55:53,681 Training Data Eval:
2021-12-11 20:56:08,493   Average segmentation loss on training set: 0.0367
2021-12-11 20:56:08,494 Validation Data Eval:
2021-12-11 20:56:13,613   Average segmentation loss on validation set: 0.0848
2021-12-11 20:56:16,425 iteration 1870 : loss : 0.085321, loss_ce: 0.038206
 28%|███████▍                   | 110/400 [1:33:14<4:23:01, 54.42s/it]2021-12-11 20:56:19,257 iteration 1871 : loss : 0.064781, loss_ce: 0.022867
2021-12-11 20:56:22,045 iteration 1872 : loss : 0.044347, loss_ce: 0.022196
2021-12-11 20:56:24,828 iteration 1873 : loss : 0.059738, loss_ce: 0.023514
2021-12-11 20:56:27,610 iteration 1874 : loss : 0.059355, loss_ce: 0.028120
2021-12-11 20:56:30,355 iteration 1875 : loss : 0.038844, loss_ce: 0.021471
2021-12-11 20:56:32,996 iteration 1876 : loss : 0.041308, loss_ce: 0.023363
2021-12-11 20:56:35,817 iteration 1877 : loss : 0.053451, loss_ce: 0.023454
2021-12-11 20:56:38,659 iteration 1878 : loss : 0.075051, loss_ce: 0.040908
2021-12-11 20:56:41,575 iteration 1879 : loss : 0.065458, loss_ce: 0.028858
2021-12-11 20:56:44,408 iteration 1880 : loss : 0.060232, loss_ce: 0.028819
2021-12-11 20:56:47,120 iteration 1881 : loss : 0.080336, loss_ce: 0.040260
2021-12-11 20:56:50,013 iteration 1882 : loss : 0.132917, loss_ce: 0.044831
2021-12-11 20:56:52,855 iteration 1883 : loss : 0.044189, loss_ce: 0.022749
2021-12-11 20:56:55,615 iteration 1884 : loss : 0.054252, loss_ce: 0.024767
2021-12-11 20:56:58,282 iteration 1885 : loss : 0.045516, loss_ce: 0.023011
2021-12-11 20:57:01,046 iteration 1886 : loss : 0.075253, loss_ce: 0.034128
2021-12-11 20:57:03,734 iteration 1887 : loss : 0.105709, loss_ce: 0.047672
 28%|███████▍                   | 111/400 [1:34:01<4:11:49, 52.28s/it]2021-12-11 20:57:06,598 iteration 1888 : loss : 0.073891, loss_ce: 0.026956
2021-12-11 20:57:09,452 iteration 1889 : loss : 0.081715, loss_ce: 0.030281
2021-12-11 20:57:12,142 iteration 1890 : loss : 0.058255, loss_ce: 0.030909
2021-12-11 20:57:14,796 iteration 1891 : loss : 0.051766, loss_ce: 0.024268
2021-12-11 20:57:17,469 iteration 1892 : loss : 0.050627, loss_ce: 0.028332
2021-12-11 20:57:20,150 iteration 1893 : loss : 0.071175, loss_ce: 0.030328
2021-12-11 20:57:23,095 iteration 1894 : loss : 0.066178, loss_ce: 0.030858
2021-12-11 20:57:25,956 iteration 1895 : loss : 0.045052, loss_ce: 0.022759
2021-12-11 20:57:28,835 iteration 1896 : loss : 0.080784, loss_ce: 0.036627
2021-12-11 20:57:31,609 iteration 1897 : loss : 0.077642, loss_ce: 0.033828
2021-12-11 20:57:34,587 iteration 1898 : loss : 0.072416, loss_ce: 0.033570
2021-12-11 20:57:37,292 iteration 1899 : loss : 0.062362, loss_ce: 0.026807
2021-12-11 20:57:39,988 iteration 1900 : loss : 0.087259, loss_ce: 0.040186
2021-12-11 20:57:42,814 iteration 1901 : loss : 0.053990, loss_ce: 0.023334
2021-12-11 20:57:45,514 iteration 1902 : loss : 0.053287, loss_ce: 0.025985
2021-12-11 20:57:48,495 iteration 1903 : loss : 0.064534, loss_ce: 0.027129
2021-12-11 20:57:51,493 iteration 1904 : loss : 0.054183, loss_ce: 0.026257
 28%|███████▌                   | 112/400 [1:34:49<4:04:27, 50.93s/it]2021-12-11 20:57:54,212 iteration 1905 : loss : 0.059856, loss_ce: 0.031892
2021-12-11 20:57:56,958 iteration 1906 : loss : 0.073563, loss_ce: 0.031190
2021-12-11 20:57:59,938 iteration 1907 : loss : 0.053248, loss_ce: 0.026086
2021-12-11 20:58:02,881 iteration 1908 : loss : 0.073601, loss_ce: 0.032429
2021-12-11 20:58:05,475 iteration 1909 : loss : 0.043877, loss_ce: 0.021949
2021-12-11 20:58:08,230 iteration 1910 : loss : 0.054874, loss_ce: 0.025054
2021-12-11 20:58:11,090 iteration 1911 : loss : 0.058353, loss_ce: 0.028712
2021-12-11 20:58:14,130 iteration 1912 : loss : 0.060202, loss_ce: 0.027410
2021-12-11 20:58:16,921 iteration 1913 : loss : 0.079601, loss_ce: 0.028823
2021-12-11 20:58:19,849 iteration 1914 : loss : 0.081502, loss_ce: 0.037433
2021-12-11 20:58:22,559 iteration 1915 : loss : 0.043374, loss_ce: 0.020964
2021-12-11 20:58:25,394 iteration 1916 : loss : 0.051510, loss_ce: 0.022728
2021-12-11 20:58:28,081 iteration 1917 : loss : 0.061480, loss_ce: 0.030597
2021-12-11 20:58:30,875 iteration 1918 : loss : 0.062101, loss_ce: 0.034250
2021-12-11 20:58:33,831 iteration 1919 : loss : 0.054198, loss_ce: 0.023590
2021-12-11 20:58:36,478 iteration 1920 : loss : 0.077114, loss_ce: 0.040026
2021-12-11 20:58:39,376 iteration 1921 : loss : 0.071769, loss_ce: 0.033661
 28%|███████▋                   | 113/400 [1:35:37<3:59:14, 50.02s/it]2021-12-11 20:58:42,426 iteration 1922 : loss : 0.085456, loss_ce: 0.037222
2021-12-11 20:58:45,201 iteration 1923 : loss : 0.044054, loss_ce: 0.022325
2021-12-11 20:58:48,106 iteration 1924 : loss : 0.085807, loss_ce: 0.031893
2021-12-11 20:58:50,847 iteration 1925 : loss : 0.116711, loss_ce: 0.039862
2021-12-11 20:58:53,754 iteration 1926 : loss : 0.082742, loss_ce: 0.036620
2021-12-11 20:58:56,544 iteration 1927 : loss : 0.054438, loss_ce: 0.029330
2021-12-11 20:58:59,312 iteration 1928 : loss : 0.073480, loss_ce: 0.025662
2021-12-11 20:59:02,210 iteration 1929 : loss : 0.082144, loss_ce: 0.044147
2021-12-11 20:59:05,146 iteration 1930 : loss : 0.059103, loss_ce: 0.030774
2021-12-11 20:59:08,089 iteration 1931 : loss : 0.076524, loss_ce: 0.033563
2021-12-11 20:59:10,846 iteration 1932 : loss : 0.085356, loss_ce: 0.033111
2021-12-11 20:59:13,533 iteration 1933 : loss : 0.069140, loss_ce: 0.030510
2021-12-11 20:59:16,382 iteration 1934 : loss : 0.076891, loss_ce: 0.030127
2021-12-11 20:59:19,088 iteration 1935 : loss : 0.061477, loss_ce: 0.024074
2021-12-11 20:59:21,721 iteration 1936 : loss : 0.060654, loss_ce: 0.028380
2021-12-11 20:59:24,334 iteration 1937 : loss : 0.047777, loss_ce: 0.021903
2021-12-11 20:59:27,290 iteration 1938 : loss : 0.064395, loss_ce: 0.030306
 28%|███████▋                   | 114/400 [1:36:25<3:55:23, 49.38s/it]2021-12-11 20:59:30,293 iteration 1939 : loss : 0.068557, loss_ce: 0.032814
2021-12-11 20:59:32,927 iteration 1940 : loss : 0.069987, loss_ce: 0.039150
2021-12-11 20:59:35,842 iteration 1941 : loss : 0.047410, loss_ce: 0.022913
2021-12-11 20:59:38,689 iteration 1942 : loss : 0.065704, loss_ce: 0.025006
2021-12-11 20:59:41,495 iteration 1943 : loss : 0.056880, loss_ce: 0.023839
2021-12-11 20:59:44,193 iteration 1944 : loss : 0.062730, loss_ce: 0.026951
2021-12-11 20:59:47,048 iteration 1945 : loss : 0.066098, loss_ce: 0.028122
2021-12-11 20:59:49,714 iteration 1946 : loss : 0.074049, loss_ce: 0.024104
2021-12-11 20:59:52,377 iteration 1947 : loss : 0.081754, loss_ce: 0.029353
2021-12-11 20:59:55,330 iteration 1948 : loss : 0.067050, loss_ce: 0.031559
2021-12-11 20:59:58,073 iteration 1949 : loss : 0.053053, loss_ce: 0.022196
2021-12-11 21:00:01,011 iteration 1950 : loss : 0.067660, loss_ce: 0.029067
2021-12-11 21:00:03,924 iteration 1951 : loss : 0.059364, loss_ce: 0.026674
2021-12-11 21:00:06,768 iteration 1952 : loss : 0.059691, loss_ce: 0.028526
2021-12-11 21:00:09,484 iteration 1953 : loss : 0.081912, loss_ce: 0.046511
2021-12-11 21:00:12,422 iteration 1954 : loss : 0.054249, loss_ce: 0.026293
2021-12-11 21:00:12,422 Training Data Eval:
2021-12-11 21:00:27,496   Average segmentation loss on training set: 0.0412
2021-12-11 21:00:27,496 Validation Data Eval:
2021-12-11 21:00:32,631   Average segmentation loss on validation set: 0.0982
2021-12-11 21:00:35,410 iteration 1955 : loss : 0.062069, loss_ce: 0.028329
 29%|███████▊                   | 115/400 [1:37:33<4:21:16, 55.00s/it]2021-12-11 21:00:38,153 iteration 1956 : loss : 0.078692, loss_ce: 0.034868
2021-12-11 21:00:41,002 iteration 1957 : loss : 0.063380, loss_ce: 0.035476
2021-12-11 21:00:43,835 iteration 1958 : loss : 0.082371, loss_ce: 0.026246
2021-12-11 21:00:46,527 iteration 1959 : loss : 0.072958, loss_ce: 0.024009
2021-12-11 21:00:49,240 iteration 1960 : loss : 0.068956, loss_ce: 0.034435
2021-12-11 21:00:51,887 iteration 1961 : loss : 0.039244, loss_ce: 0.020397
2021-12-11 21:00:54,729 iteration 1962 : loss : 0.056827, loss_ce: 0.027103
2021-12-11 21:00:57,374 iteration 1963 : loss : 0.050258, loss_ce: 0.022299
2021-12-11 21:01:00,204 iteration 1964 : loss : 0.066041, loss_ce: 0.026463
2021-12-11 21:01:03,073 iteration 1965 : loss : 0.052239, loss_ce: 0.026424
2021-12-11 21:01:05,840 iteration 1966 : loss : 0.071723, loss_ce: 0.029879
2021-12-11 21:01:08,807 iteration 1967 : loss : 0.055917, loss_ce: 0.026880
2021-12-11 21:01:11,494 iteration 1968 : loss : 0.132824, loss_ce: 0.064213
2021-12-11 21:01:14,216 iteration 1969 : loss : 0.067479, loss_ce: 0.030717
2021-12-11 21:01:16,840 iteration 1970 : loss : 0.044010, loss_ce: 0.022882
2021-12-11 21:01:19,656 iteration 1971 : loss : 0.061329, loss_ce: 0.026769
2021-12-11 21:01:22,581 iteration 1972 : loss : 0.063606, loss_ce: 0.023157
 29%|███████▊                   | 116/400 [1:38:20<4:09:14, 52.66s/it]2021-12-11 21:01:25,395 iteration 1973 : loss : 0.056300, loss_ce: 0.029282
2021-12-11 21:01:28,076 iteration 1974 : loss : 0.051209, loss_ce: 0.024688
2021-12-11 21:01:30,938 iteration 1975 : loss : 0.070279, loss_ce: 0.031371
2021-12-11 21:01:33,660 iteration 1976 : loss : 0.066932, loss_ce: 0.035653
2021-12-11 21:01:36,435 iteration 1977 : loss : 0.051024, loss_ce: 0.023822
2021-12-11 21:01:39,399 iteration 1978 : loss : 0.061379, loss_ce: 0.032982
2021-12-11 21:01:42,197 iteration 1979 : loss : 0.054289, loss_ce: 0.026586
2021-12-11 21:01:44,877 iteration 1980 : loss : 0.049834, loss_ce: 0.023209
2021-12-11 21:01:47,620 iteration 1981 : loss : 0.054495, loss_ce: 0.024723
2021-12-11 21:01:50,609 iteration 1982 : loss : 0.076080, loss_ce: 0.032702
2021-12-11 21:01:53,288 iteration 1983 : loss : 0.067743, loss_ce: 0.029499
2021-12-11 21:01:56,088 iteration 1984 : loss : 0.083161, loss_ce: 0.032725
2021-12-11 21:01:58,794 iteration 1985 : loss : 0.077347, loss_ce: 0.034119
2021-12-11 21:02:01,487 iteration 1986 : loss : 0.081773, loss_ce: 0.029103
2021-12-11 21:02:04,262 iteration 1987 : loss : 0.042830, loss_ce: 0.020490
2021-12-11 21:02:07,063 iteration 1988 : loss : 0.052549, loss_ce: 0.025517
2021-12-11 21:02:09,836 iteration 1989 : loss : 0.052569, loss_ce: 0.024589
 29%|███████▉                   | 117/400 [1:39:08<4:00:42, 51.03s/it]2021-12-11 21:02:12,704 iteration 1990 : loss : 0.068924, loss_ce: 0.024886
2021-12-11 21:02:15,543 iteration 1991 : loss : 0.040468, loss_ce: 0.020331
2021-12-11 21:02:18,239 iteration 1992 : loss : 0.046265, loss_ce: 0.021024
2021-12-11 21:02:21,100 iteration 1993 : loss : 0.057807, loss_ce: 0.026395
2021-12-11 21:02:23,920 iteration 1994 : loss : 0.048229, loss_ce: 0.024918
2021-12-11 21:02:26,729 iteration 1995 : loss : 0.044544, loss_ce: 0.022315
2021-12-11 21:02:29,605 iteration 1996 : loss : 0.046631, loss_ce: 0.022551
2021-12-11 21:02:32,276 iteration 1997 : loss : 0.052053, loss_ce: 0.025709
2021-12-11 21:02:35,018 iteration 1998 : loss : 0.057584, loss_ce: 0.027410
2021-12-11 21:02:37,813 iteration 1999 : loss : 0.046715, loss_ce: 0.023631
2021-12-11 21:02:40,555 iteration 2000 : loss : 0.075684, loss_ce: 0.039275
2021-12-11 21:02:43,252 iteration 2001 : loss : 0.080049, loss_ce: 0.034833
2021-12-11 21:02:46,106 iteration 2002 : loss : 0.075383, loss_ce: 0.031130
2021-12-11 21:02:48,721 iteration 2003 : loss : 0.049189, loss_ce: 0.020810
2021-12-11 21:02:51,591 iteration 2004 : loss : 0.053233, loss_ce: 0.027932
2021-12-11 21:02:54,292 iteration 2005 : loss : 0.057147, loss_ce: 0.028851
2021-12-11 21:02:57,166 iteration 2006 : loss : 0.057977, loss_ce: 0.024429
 30%|███████▉                   | 118/400 [1:39:55<3:54:37, 49.92s/it]2021-12-11 21:03:00,021 iteration 2007 : loss : 0.073701, loss_ce: 0.030706
2021-12-11 21:03:02,853 iteration 2008 : loss : 0.047562, loss_ce: 0.024436
2021-12-11 21:03:05,549 iteration 2009 : loss : 0.049518, loss_ce: 0.025825
2021-12-11 21:03:08,260 iteration 2010 : loss : 0.049691, loss_ce: 0.025422
2021-12-11 21:03:10,903 iteration 2011 : loss : 0.090443, loss_ce: 0.033220
2021-12-11 21:03:13,613 iteration 2012 : loss : 0.041790, loss_ce: 0.021577
2021-12-11 21:03:16,393 iteration 2013 : loss : 0.059771, loss_ce: 0.034809
2021-12-11 21:03:19,110 iteration 2014 : loss : 0.056429, loss_ce: 0.028402
2021-12-11 21:03:21,996 iteration 2015 : loss : 0.070832, loss_ce: 0.026690
2021-12-11 21:03:24,879 iteration 2016 : loss : 0.066103, loss_ce: 0.032618
2021-12-11 21:03:27,649 iteration 2017 : loss : 0.066419, loss_ce: 0.028684
2021-12-11 21:03:30,279 iteration 2018 : loss : 0.056760, loss_ce: 0.022148
2021-12-11 21:03:33,212 iteration 2019 : loss : 0.056124, loss_ce: 0.030269
2021-12-11 21:03:35,896 iteration 2020 : loss : 0.057029, loss_ce: 0.024423
2021-12-11 21:03:38,729 iteration 2021 : loss : 0.042711, loss_ce: 0.021769
2021-12-11 21:03:41,458 iteration 2022 : loss : 0.072112, loss_ce: 0.028754
2021-12-11 21:03:44,366 iteration 2023 : loss : 0.049123, loss_ce: 0.021242
 30%|████████                   | 119/400 [1:40:42<3:49:58, 49.10s/it]2021-12-11 21:03:47,358 iteration 2024 : loss : 0.079179, loss_ce: 0.039185
2021-12-11 21:03:50,341 iteration 2025 : loss : 0.080140, loss_ce: 0.032927
2021-12-11 21:03:53,053 iteration 2026 : loss : 0.061124, loss_ce: 0.024932
2021-12-11 21:03:55,975 iteration 2027 : loss : 0.062725, loss_ce: 0.025680
2021-12-11 21:03:58,552 iteration 2028 : loss : 0.055468, loss_ce: 0.029166
2021-12-11 21:04:01,251 iteration 2029 : loss : 0.062865, loss_ce: 0.024388
2021-12-11 21:04:04,166 iteration 2030 : loss : 0.056840, loss_ce: 0.023838
2021-12-11 21:04:06,947 iteration 2031 : loss : 0.060530, loss_ce: 0.027345
2021-12-11 21:04:09,714 iteration 2032 : loss : 0.059756, loss_ce: 0.029725
2021-12-11 21:04:12,286 iteration 2033 : loss : 0.036326, loss_ce: 0.019963
2021-12-11 21:04:14,987 iteration 2034 : loss : 0.061461, loss_ce: 0.026048
2021-12-11 21:04:17,660 iteration 2035 : loss : 0.066315, loss_ce: 0.031542
2021-12-11 21:04:20,386 iteration 2036 : loss : 0.052292, loss_ce: 0.031667
2021-12-11 21:04:23,018 iteration 2037 : loss : 0.047587, loss_ce: 0.023937
2021-12-11 21:04:25,945 iteration 2038 : loss : 0.091669, loss_ce: 0.032154
2021-12-11 21:04:28,664 iteration 2039 : loss : 0.050510, loss_ce: 0.022399
2021-12-11 21:04:28,664 Training Data Eval:
2021-12-11 21:04:43,720   Average segmentation loss on training set: 0.0341
2021-12-11 21:04:43,721 Validation Data Eval:
2021-12-11 21:04:48,857   Average segmentation loss on validation set: 0.1041
2021-12-11 21:04:51,818 iteration 2040 : loss : 0.060889, loss_ce: 0.030973
 30%|████████                   | 120/400 [1:41:49<4:14:51, 54.61s/it]2021-12-11 21:04:54,489 iteration 2041 : loss : 0.048748, loss_ce: 0.023589
2021-12-11 21:04:57,228 iteration 2042 : loss : 0.058337, loss_ce: 0.026795
2021-12-11 21:05:00,003 iteration 2043 : loss : 0.064877, loss_ce: 0.027143
2021-12-11 21:05:02,893 iteration 2044 : loss : 0.059516, loss_ce: 0.031936
2021-12-11 21:05:05,703 iteration 2045 : loss : 0.061090, loss_ce: 0.027038
2021-12-11 21:05:08,395 iteration 2046 : loss : 0.079946, loss_ce: 0.024692
2021-12-11 21:05:11,147 iteration 2047 : loss : 0.046516, loss_ce: 0.022421
2021-12-11 21:05:13,891 iteration 2048 : loss : 0.051118, loss_ce: 0.027106
2021-12-11 21:05:16,662 iteration 2049 : loss : 0.079882, loss_ce: 0.039887
2021-12-11 21:05:19,334 iteration 2050 : loss : 0.047299, loss_ce: 0.020245
2021-12-11 21:05:21,962 iteration 2051 : loss : 0.058533, loss_ce: 0.028982
2021-12-11 21:05:24,808 iteration 2052 : loss : 0.055753, loss_ce: 0.024122
2021-12-11 21:05:27,511 iteration 2053 : loss : 0.054942, loss_ce: 0.026273
2021-12-11 21:05:30,263 iteration 2054 : loss : 0.061886, loss_ce: 0.027779
2021-12-11 21:05:33,016 iteration 2055 : loss : 0.058744, loss_ce: 0.026730
2021-12-11 21:05:35,730 iteration 2056 : loss : 0.044831, loss_ce: 0.021068
2021-12-11 21:05:38,587 iteration 2057 : loss : 0.048170, loss_ce: 0.024317
 30%|████████▏                  | 121/400 [1:42:36<4:02:59, 52.26s/it]2021-12-11 21:05:41,461 iteration 2058 : loss : 0.061433, loss_ce: 0.032665
2021-12-11 21:05:44,250 iteration 2059 : loss : 0.064588, loss_ce: 0.025437
2021-12-11 21:05:47,167 iteration 2060 : loss : 0.050208, loss_ce: 0.022986
2021-12-11 21:05:49,908 iteration 2061 : loss : 0.052423, loss_ce: 0.027498
2021-12-11 21:05:52,657 iteration 2062 : loss : 0.061659, loss_ce: 0.027188
2021-12-11 21:05:55,481 iteration 2063 : loss : 0.067622, loss_ce: 0.032196
2021-12-11 21:05:58,445 iteration 2064 : loss : 0.060144, loss_ce: 0.027269
2021-12-11 21:06:01,323 iteration 2065 : loss : 0.083189, loss_ce: 0.032236
2021-12-11 21:06:04,105 iteration 2066 : loss : 0.060604, loss_ce: 0.026308
2021-12-11 21:06:06,851 iteration 2067 : loss : 0.089822, loss_ce: 0.040840
2021-12-11 21:06:09,496 iteration 2068 : loss : 0.044944, loss_ce: 0.021625
2021-12-11 21:06:12,338 iteration 2069 : loss : 0.067934, loss_ce: 0.035517
2021-12-11 21:06:15,221 iteration 2070 : loss : 0.054668, loss_ce: 0.026547
2021-12-11 21:06:17,980 iteration 2071 : loss : 0.043800, loss_ce: 0.021729
2021-12-11 21:06:20,778 iteration 2072 : loss : 0.046370, loss_ce: 0.025325
2021-12-11 21:06:23,576 iteration 2073 : loss : 0.053981, loss_ce: 0.024787
2021-12-11 21:06:26,278 iteration 2074 : loss : 0.053933, loss_ce: 0.022167
 30%|████████▏                  | 122/400 [1:43:24<3:55:47, 50.89s/it]2021-12-11 21:06:29,116 iteration 2075 : loss : 0.052849, loss_ce: 0.028863
2021-12-11 21:06:31,979 iteration 2076 : loss : 0.056358, loss_ce: 0.030002
2021-12-11 21:06:34,825 iteration 2077 : loss : 0.062949, loss_ce: 0.031955
2021-12-11 21:06:37,764 iteration 2078 : loss : 0.061752, loss_ce: 0.027068
2021-12-11 21:06:40,589 iteration 2079 : loss : 0.063307, loss_ce: 0.027769
2021-12-11 21:06:43,322 iteration 2080 : loss : 0.080319, loss_ce: 0.025823
2021-12-11 21:06:46,025 iteration 2081 : loss : 0.040382, loss_ce: 0.018567
2021-12-11 21:06:48,862 iteration 2082 : loss : 0.083222, loss_ce: 0.027001
2021-12-11 21:06:51,525 iteration 2083 : loss : 0.088604, loss_ce: 0.037621
2021-12-11 21:06:54,359 iteration 2084 : loss : 0.052657, loss_ce: 0.027465
2021-12-11 21:06:57,147 iteration 2085 : loss : 0.088574, loss_ce: 0.022835
2021-12-11 21:07:00,091 iteration 2086 : loss : 0.066211, loss_ce: 0.028220
2021-12-11 21:07:02,726 iteration 2087 : loss : 0.047959, loss_ce: 0.022506
2021-12-11 21:07:05,624 iteration 2088 : loss : 0.045052, loss_ce: 0.020958
2021-12-11 21:07:08,314 iteration 2089 : loss : 0.049176, loss_ce: 0.023951
2021-12-11 21:07:10,983 iteration 2090 : loss : 0.054103, loss_ce: 0.028749
2021-12-11 21:07:13,947 iteration 2091 : loss : 0.070405, loss_ce: 0.030108
 31%|████████▎                  | 123/400 [1:44:12<3:50:28, 49.92s/it]2021-12-11 21:07:16,981 iteration 2092 : loss : 0.046981, loss_ce: 0.019936
2021-12-11 21:07:19,670 iteration 2093 : loss : 0.036931, loss_ce: 0.019154
2021-12-11 21:07:22,411 iteration 2094 : loss : 0.050059, loss_ce: 0.022930
2021-12-11 21:07:25,418 iteration 2095 : loss : 0.061478, loss_ce: 0.028163
2021-12-11 21:07:28,264 iteration 2096 : loss : 0.045554, loss_ce: 0.023212
2021-12-11 21:07:31,168 iteration 2097 : loss : 0.079509, loss_ce: 0.031065
2021-12-11 21:07:34,021 iteration 2098 : loss : 0.054556, loss_ce: 0.024155
2021-12-11 21:07:36,668 iteration 2099 : loss : 0.050125, loss_ce: 0.026300
2021-12-11 21:07:39,622 iteration 2100 : loss : 0.063359, loss_ce: 0.030044
2021-12-11 21:07:42,259 iteration 2101 : loss : 0.053382, loss_ce: 0.023816
2021-12-11 21:07:44,978 iteration 2102 : loss : 0.064508, loss_ce: 0.032117
2021-12-11 21:07:47,685 iteration 2103 : loss : 0.072695, loss_ce: 0.041537
2021-12-11 21:07:50,393 iteration 2104 : loss : 0.049441, loss_ce: 0.022280
2021-12-11 21:07:53,344 iteration 2105 : loss : 0.071410, loss_ce: 0.038314
2021-12-11 21:07:56,162 iteration 2106 : loss : 0.055079, loss_ce: 0.025906
2021-12-11 21:07:58,913 iteration 2107 : loss : 0.056050, loss_ce: 0.024408
2021-12-11 21:08:01,605 iteration 2108 : loss : 0.047795, loss_ce: 0.022382
 31%|████████▎                  | 124/400 [1:44:59<3:46:31, 49.25s/it]2021-12-11 21:08:04,521 iteration 2109 : loss : 0.103063, loss_ce: 0.031516
2021-12-11 21:08:07,445 iteration 2110 : loss : 0.084902, loss_ce: 0.050308
2021-12-11 21:08:10,162 iteration 2111 : loss : 0.043947, loss_ce: 0.020470
2021-12-11 21:08:13,156 iteration 2112 : loss : 0.053102, loss_ce: 0.026378
2021-12-11 21:08:16,085 iteration 2113 : loss : 0.061520, loss_ce: 0.023870
2021-12-11 21:08:18,861 iteration 2114 : loss : 0.051248, loss_ce: 0.020417
2021-12-11 21:08:21,743 iteration 2115 : loss : 0.058344, loss_ce: 0.023037
2021-12-11 21:08:24,434 iteration 2116 : loss : 0.079191, loss_ce: 0.033351
2021-12-11 21:08:27,289 iteration 2117 : loss : 0.064207, loss_ce: 0.024955
2021-12-11 21:08:29,993 iteration 2118 : loss : 0.053807, loss_ce: 0.024805
2021-12-11 21:08:33,044 iteration 2119 : loss : 0.061179, loss_ce: 0.028812
2021-12-11 21:08:35,921 iteration 2120 : loss : 0.052594, loss_ce: 0.031296
2021-12-11 21:08:38,716 iteration 2121 : loss : 0.065153, loss_ce: 0.035042
2021-12-11 21:08:41,660 iteration 2122 : loss : 0.111952, loss_ce: 0.071179
2021-12-11 21:08:44,322 iteration 2123 : loss : 0.075152, loss_ce: 0.028989
2021-12-11 21:08:47,167 iteration 2124 : loss : 0.059076, loss_ce: 0.028868
2021-12-11 21:08:47,167 Training Data Eval:
2021-12-11 21:09:02,126   Average segmentation loss on training set: 0.0534
2021-12-11 21:09:02,127 Validation Data Eval:
2021-12-11 21:09:07,297   Average segmentation loss on validation set: 0.0932
2021-12-11 21:09:09,951 iteration 2125 : loss : 0.043967, loss_ce: 0.024511
 31%|████████▍                  | 125/400 [1:46:08<4:11:57, 54.97s/it]2021-12-11 21:09:12,936 iteration 2126 : loss : 0.103920, loss_ce: 0.040130
2021-12-11 21:09:15,687 iteration 2127 : loss : 0.046575, loss_ce: 0.023502
2021-12-11 21:09:18,374 iteration 2128 : loss : 0.042638, loss_ce: 0.021615
2021-12-11 21:09:21,131 iteration 2129 : loss : 0.060459, loss_ce: 0.027391
2021-12-11 21:09:24,040 iteration 2130 : loss : 0.055200, loss_ce: 0.029390
2021-12-11 21:09:26,751 iteration 2131 : loss : 0.043086, loss_ce: 0.019791
2021-12-11 21:09:29,628 iteration 2132 : loss : 0.065026, loss_ce: 0.023097
2021-12-11 21:09:32,366 iteration 2133 : loss : 0.076513, loss_ce: 0.039827
2021-12-11 21:09:35,331 iteration 2134 : loss : 0.081165, loss_ce: 0.035414
2021-12-11 21:09:38,015 iteration 2135 : loss : 0.054632, loss_ce: 0.020980
2021-12-11 21:09:40,772 iteration 2136 : loss : 0.068402, loss_ce: 0.030019
2021-12-11 21:09:43,674 iteration 2137 : loss : 0.048350, loss_ce: 0.025062
2021-12-11 21:09:46,548 iteration 2138 : loss : 0.053452, loss_ce: 0.020691
2021-12-11 21:09:49,352 iteration 2139 : loss : 0.048780, loss_ce: 0.028128
2021-12-11 21:09:52,045 iteration 2140 : loss : 0.084608, loss_ce: 0.049371
2021-12-11 21:09:54,982 iteration 2141 : loss : 0.049149, loss_ce: 0.022120
2021-12-11 21:09:57,826 iteration 2142 : loss : 0.104508, loss_ce: 0.041604
 32%|████████▌                  | 126/400 [1:46:56<4:01:19, 52.85s/it]2021-12-11 21:10:00,545 iteration 2143 : loss : 0.055187, loss_ce: 0.030299
2021-12-11 21:10:03,292 iteration 2144 : loss : 0.041439, loss_ce: 0.021893
2021-12-11 21:10:06,202 iteration 2145 : loss : 0.060699, loss_ce: 0.028140
2021-12-11 21:10:08,935 iteration 2146 : loss : 0.070711, loss_ce: 0.032455
2021-12-11 21:10:11,541 iteration 2147 : loss : 0.073208, loss_ce: 0.031886
2021-12-11 21:10:14,272 iteration 2148 : loss : 0.044258, loss_ce: 0.021500
2021-12-11 21:10:17,125 iteration 2149 : loss : 0.065695, loss_ce: 0.027808
2021-12-11 21:10:19,985 iteration 2150 : loss : 0.049713, loss_ce: 0.024049
2021-12-11 21:10:22,658 iteration 2151 : loss : 0.056515, loss_ce: 0.025650
2021-12-11 21:10:25,332 iteration 2152 : loss : 0.058640, loss_ce: 0.032982
2021-12-11 21:10:28,096 iteration 2153 : loss : 0.073800, loss_ce: 0.029398
2021-12-11 21:10:30,951 iteration 2154 : loss : 0.057246, loss_ce: 0.022567
2021-12-11 21:10:33,898 iteration 2155 : loss : 0.059347, loss_ce: 0.035457
2021-12-11 21:10:36,686 iteration 2156 : loss : 0.049649, loss_ce: 0.022465
2021-12-11 21:10:39,437 iteration 2157 : loss : 0.068999, loss_ce: 0.033108
2021-12-11 21:10:42,242 iteration 2158 : loss : 0.066133, loss_ce: 0.025034
2021-12-11 21:10:45,104 iteration 2159 : loss : 0.062752, loss_ce: 0.026516
 32%|████████▌                  | 127/400 [1:47:43<3:52:49, 51.17s/it]2021-12-11 21:10:47,970 iteration 2160 : loss : 0.048035, loss_ce: 0.022108
2021-12-11 21:10:50,825 iteration 2161 : loss : 0.082447, loss_ce: 0.038064
2021-12-11 21:10:53,781 iteration 2162 : loss : 0.064994, loss_ce: 0.029687
2021-12-11 21:10:56,433 iteration 2163 : loss : 0.053849, loss_ce: 0.023382
2021-12-11 21:10:59,240 iteration 2164 : loss : 0.059756, loss_ce: 0.026675
2021-12-11 21:11:02,054 iteration 2165 : loss : 0.046988, loss_ce: 0.024865
2021-12-11 21:11:04,760 iteration 2166 : loss : 0.053521, loss_ce: 0.019712
2021-12-11 21:11:07,718 iteration 2167 : loss : 0.042077, loss_ce: 0.019162
2021-12-11 21:11:10,537 iteration 2168 : loss : 0.059968, loss_ce: 0.028870
2021-12-11 21:11:13,393 iteration 2169 : loss : 0.055728, loss_ce: 0.023565
2021-12-11 21:11:16,094 iteration 2170 : loss : 0.075820, loss_ce: 0.038158
2021-12-11 21:11:18,760 iteration 2171 : loss : 0.061104, loss_ce: 0.026976
2021-12-11 21:11:21,672 iteration 2172 : loss : 0.068196, loss_ce: 0.025423
2021-12-11 21:11:24,448 iteration 2173 : loss : 0.046870, loss_ce: 0.020378
2021-12-11 21:11:27,117 iteration 2174 : loss : 0.058459, loss_ce: 0.032047
2021-12-11 21:11:29,729 iteration 2175 : loss : 0.040079, loss_ce: 0.018627
2021-12-11 21:11:32,684 iteration 2176 : loss : 0.074795, loss_ce: 0.034446
 32%|████████▋                  | 128/400 [1:48:30<3:47:06, 50.10s/it]2021-12-11 21:11:35,654 iteration 2177 : loss : 0.052061, loss_ce: 0.023024
2021-12-11 21:11:38,609 iteration 2178 : loss : 0.062030, loss_ce: 0.029423
2021-12-11 21:11:41,285 iteration 2179 : loss : 0.055486, loss_ce: 0.030537
2021-12-11 21:11:43,964 iteration 2180 : loss : 0.039979, loss_ce: 0.020288
2021-12-11 21:11:46,740 iteration 2181 : loss : 0.057893, loss_ce: 0.030667
2021-12-11 21:11:49,642 iteration 2182 : loss : 0.055905, loss_ce: 0.018909
2021-12-11 21:11:52,251 iteration 2183 : loss : 0.055432, loss_ce: 0.029732
2021-12-11 21:11:55,140 iteration 2184 : loss : 0.045909, loss_ce: 0.019214
2021-12-11 21:11:58,029 iteration 2185 : loss : 0.052138, loss_ce: 0.024911
2021-12-11 21:12:00,766 iteration 2186 : loss : 0.063819, loss_ce: 0.025758
2021-12-11 21:12:03,434 iteration 2187 : loss : 0.035330, loss_ce: 0.019480
2021-12-11 21:12:06,155 iteration 2188 : loss : 0.050668, loss_ce: 0.020560
2021-12-11 21:12:09,139 iteration 2189 : loss : 0.054708, loss_ce: 0.025480
2021-12-11 21:12:11,823 iteration 2190 : loss : 0.056308, loss_ce: 0.024501
2021-12-11 21:12:14,694 iteration 2191 : loss : 0.104173, loss_ce: 0.034456
2021-12-11 21:12:17,416 iteration 2192 : loss : 0.044575, loss_ce: 0.020218
2021-12-11 21:12:20,160 iteration 2193 : loss : 0.068581, loss_ce: 0.032644
 32%|████████▋                  | 129/400 [1:49:18<3:42:42, 49.31s/it]2021-12-11 21:12:22,853 iteration 2194 : loss : 0.053261, loss_ce: 0.031531
2021-12-11 21:12:25,690 iteration 2195 : loss : 0.101723, loss_ce: 0.039454
2021-12-11 21:12:28,460 iteration 2196 : loss : 0.048292, loss_ce: 0.022253
2021-12-11 21:12:31,360 iteration 2197 : loss : 0.053709, loss_ce: 0.030527
2021-12-11 21:12:34,173 iteration 2198 : loss : 0.057088, loss_ce: 0.027890
2021-12-11 21:12:36,979 iteration 2199 : loss : 0.049606, loss_ce: 0.027771
2021-12-11 21:12:39,610 iteration 2200 : loss : 0.050058, loss_ce: 0.023053
2021-12-11 21:12:42,486 iteration 2201 : loss : 0.053016, loss_ce: 0.024702
2021-12-11 21:12:45,185 iteration 2202 : loss : 0.107999, loss_ce: 0.028689
2021-12-11 21:12:48,060 iteration 2203 : loss : 0.052747, loss_ce: 0.029820
2021-12-11 21:12:50,779 iteration 2204 : loss : 0.050511, loss_ce: 0.028179
2021-12-11 21:12:53,394 iteration 2205 : loss : 0.040271, loss_ce: 0.020309
2021-12-11 21:12:56,227 iteration 2206 : loss : 0.058310, loss_ce: 0.019628
2021-12-11 21:12:59,005 iteration 2207 : loss : 0.063968, loss_ce: 0.022817
2021-12-11 21:13:01,765 iteration 2208 : loss : 0.050486, loss_ce: 0.020301
2021-12-11 21:13:04,435 iteration 2209 : loss : 0.055872, loss_ce: 0.024700
2021-12-11 21:13:04,436 Training Data Eval:
2021-12-11 21:13:19,558   Average segmentation loss on training set: 0.0313
2021-12-11 21:13:19,558 Validation Data Eval:
2021-12-11 21:13:24,753   Average segmentation loss on validation set: 0.0929
2021-12-11 21:13:27,513 iteration 2210 : loss : 0.057489, loss_ce: 0.024428
 32%|████████▊                  | 130/400 [1:50:25<4:06:15, 54.72s/it]2021-12-11 21:13:30,454 iteration 2211 : loss : 0.057823, loss_ce: 0.025647
2021-12-11 21:13:33,291 iteration 2212 : loss : 0.053575, loss_ce: 0.026473
2021-12-11 21:13:36,114 iteration 2213 : loss : 0.064527, loss_ce: 0.029223
2021-12-11 21:13:39,037 iteration 2214 : loss : 0.046372, loss_ce: 0.021318
2021-12-11 21:13:41,833 iteration 2215 : loss : 0.071536, loss_ce: 0.033330
2021-12-11 21:13:44,540 iteration 2216 : loss : 0.050799, loss_ce: 0.021807
2021-12-11 21:13:47,274 iteration 2217 : loss : 0.055587, loss_ce: 0.027314
2021-12-11 21:13:50,122 iteration 2218 : loss : 0.062956, loss_ce: 0.031382
2021-12-11 21:13:52,909 iteration 2219 : loss : 0.044668, loss_ce: 0.021928
2021-12-11 21:13:55,480 iteration 2220 : loss : 0.069323, loss_ce: 0.027862
2021-12-11 21:13:58,313 iteration 2221 : loss : 0.041894, loss_ce: 0.017934
2021-12-11 21:14:01,083 iteration 2222 : loss : 0.051668, loss_ce: 0.026611
2021-12-11 21:14:03,809 iteration 2223 : loss : 0.068432, loss_ce: 0.025345
2021-12-11 21:14:06,695 iteration 2224 : loss : 0.063668, loss_ce: 0.034542
2021-12-11 21:14:09,453 iteration 2225 : loss : 0.031907, loss_ce: 0.016610
2021-12-11 21:14:12,415 iteration 2226 : loss : 0.076002, loss_ce: 0.039433
2021-12-11 21:14:15,055 iteration 2227 : loss : 0.058593, loss_ce: 0.028517
 33%|████████▊                  | 131/400 [1:51:13<3:55:41, 52.57s/it]2021-12-11 21:14:17,971 iteration 2228 : loss : 0.052564, loss_ce: 0.026984
2021-12-11 21:14:20,829 iteration 2229 : loss : 0.060122, loss_ce: 0.027393
2021-12-11 21:14:23,654 iteration 2230 : loss : 0.052449, loss_ce: 0.022916
2021-12-11 21:14:26,318 iteration 2231 : loss : 0.030533, loss_ce: 0.017721
2021-12-11 21:14:29,057 iteration 2232 : loss : 0.054973, loss_ce: 0.025883
2021-12-11 21:14:31,759 iteration 2233 : loss : 0.056858, loss_ce: 0.024535
2021-12-11 21:14:34,395 iteration 2234 : loss : 0.053662, loss_ce: 0.023160
2021-12-11 21:14:37,141 iteration 2235 : loss : 0.098128, loss_ce: 0.023713
2021-12-11 21:14:40,081 iteration 2236 : loss : 0.054374, loss_ce: 0.026498
2021-12-11 21:14:42,877 iteration 2237 : loss : 0.054625, loss_ce: 0.025197
2021-12-11 21:14:45,814 iteration 2238 : loss : 0.059451, loss_ce: 0.032130
2021-12-11 21:14:48,658 iteration 2239 : loss : 0.040158, loss_ce: 0.019236
2021-12-11 21:14:51,486 iteration 2240 : loss : 0.068822, loss_ce: 0.030367
2021-12-11 21:14:54,314 iteration 2241 : loss : 0.052043, loss_ce: 0.020105
2021-12-11 21:14:57,011 iteration 2242 : loss : 0.049940, loss_ce: 0.023267
2021-12-11 21:14:59,885 iteration 2243 : loss : 0.051456, loss_ce: 0.020186
2021-12-11 21:15:02,630 iteration 2244 : loss : 0.054321, loss_ce: 0.026246
 33%|████████▉                  | 132/400 [1:52:00<3:48:06, 51.07s/it]2021-12-11 21:15:05,492 iteration 2245 : loss : 0.075273, loss_ce: 0.030372
2021-12-11 21:15:08,232 iteration 2246 : loss : 0.036722, loss_ce: 0.018916
2021-12-11 21:15:11,163 iteration 2247 : loss : 0.091514, loss_ce: 0.031524
2021-12-11 21:15:13,846 iteration 2248 : loss : 0.050725, loss_ce: 0.026246
2021-12-11 21:15:16,550 iteration 2249 : loss : 0.045308, loss_ce: 0.019988
2021-12-11 21:15:19,336 iteration 2250 : loss : 0.069767, loss_ce: 0.031898
2021-12-11 21:15:21,981 iteration 2251 : loss : 0.047420, loss_ce: 0.022167
2021-12-11 21:15:24,862 iteration 2252 : loss : 0.057215, loss_ce: 0.024337
2021-12-11 21:15:27,694 iteration 2253 : loss : 0.064207, loss_ce: 0.029405
2021-12-11 21:15:30,525 iteration 2254 : loss : 0.047471, loss_ce: 0.024956
2021-12-11 21:15:33,135 iteration 2255 : loss : 0.056238, loss_ce: 0.023164
2021-12-11 21:15:35,928 iteration 2256 : loss : 0.056472, loss_ce: 0.027557
2021-12-11 21:15:38,824 iteration 2257 : loss : 0.055110, loss_ce: 0.024239
2021-12-11 21:15:41,657 iteration 2258 : loss : 0.037160, loss_ce: 0.019543
2021-12-11 21:15:44,439 iteration 2259 : loss : 0.057508, loss_ce: 0.025690
2021-12-11 21:15:47,207 iteration 2260 : loss : 0.052380, loss_ce: 0.025681
2021-12-11 21:15:50,014 iteration 2261 : loss : 0.060045, loss_ce: 0.029025
 33%|████████▉                  | 133/400 [1:52:48<3:42:20, 49.96s/it]2021-12-11 21:15:52,990 iteration 2262 : loss : 0.058722, loss_ce: 0.026490
2021-12-11 21:15:55,781 iteration 2263 : loss : 0.051133, loss_ce: 0.024790
2021-12-11 21:15:58,525 iteration 2264 : loss : 0.067051, loss_ce: 0.029348
2021-12-11 21:16:01,298 iteration 2265 : loss : 0.046088, loss_ce: 0.021439
2021-12-11 21:16:03,965 iteration 2266 : loss : 0.049883, loss_ce: 0.019420
2021-12-11 21:16:06,557 iteration 2267 : loss : 0.031558, loss_ce: 0.016893
2021-12-11 21:16:09,385 iteration 2268 : loss : 0.053660, loss_ce: 0.025687
2021-12-11 21:16:12,131 iteration 2269 : loss : 0.045787, loss_ce: 0.025710
2021-12-11 21:16:14,840 iteration 2270 : loss : 0.028815, loss_ce: 0.014528
2021-12-11 21:16:17,581 iteration 2271 : loss : 0.087127, loss_ce: 0.039189
2021-12-11 21:16:20,401 iteration 2272 : loss : 0.052036, loss_ce: 0.025529
2021-12-11 21:16:23,245 iteration 2273 : loss : 0.053386, loss_ce: 0.022435
2021-12-11 21:16:25,955 iteration 2274 : loss : 0.076054, loss_ce: 0.038371
2021-12-11 21:16:28,637 iteration 2275 : loss : 0.059461, loss_ce: 0.022691
2021-12-11 21:16:31,386 iteration 2276 : loss : 0.067012, loss_ce: 0.031444
2021-12-11 21:16:34,121 iteration 2277 : loss : 0.039557, loss_ce: 0.016041
2021-12-11 21:16:36,869 iteration 2278 : loss : 0.064559, loss_ce: 0.028372
 34%|█████████                  | 134/400 [1:53:35<3:37:22, 49.03s/it]2021-12-11 21:16:39,506 iteration 2279 : loss : 0.033360, loss_ce: 0.016923
2021-12-11 21:16:42,242 iteration 2280 : loss : 0.053373, loss_ce: 0.026775
2021-12-11 21:16:44,971 iteration 2281 : loss : 0.068955, loss_ce: 0.028476
2021-12-11 21:16:47,701 iteration 2282 : loss : 0.052247, loss_ce: 0.023504
2021-12-11 21:16:50,481 iteration 2283 : loss : 0.062102, loss_ce: 0.022516
2021-12-11 21:16:53,171 iteration 2284 : loss : 0.044486, loss_ce: 0.024133
2021-12-11 21:16:56,082 iteration 2285 : loss : 0.044976, loss_ce: 0.024311
2021-12-11 21:16:58,799 iteration 2286 : loss : 0.064437, loss_ce: 0.031322
2021-12-11 21:17:01,541 iteration 2287 : loss : 0.039649, loss_ce: 0.015807
2021-12-11 21:17:04,467 iteration 2288 : loss : 0.075835, loss_ce: 0.025638
2021-12-11 21:17:07,124 iteration 2289 : loss : 0.049873, loss_ce: 0.018263
2021-12-11 21:17:10,005 iteration 2290 : loss : 0.074796, loss_ce: 0.029593
2021-12-11 21:17:12,849 iteration 2291 : loss : 0.057856, loss_ce: 0.025802
2021-12-11 21:17:15,728 iteration 2292 : loss : 0.049555, loss_ce: 0.022115
2021-12-11 21:17:18,614 iteration 2293 : loss : 0.062468, loss_ce: 0.028564
2021-12-11 21:17:21,239 iteration 2294 : loss : 0.052416, loss_ce: 0.029751
2021-12-11 21:17:21,239 Training Data Eval:
2021-12-11 21:17:36,279   Average segmentation loss on training set: 0.0383
2021-12-11 21:17:36,279 Validation Data Eval:
2021-12-11 21:17:41,550   Average segmentation loss on validation set: 0.0738
2021-12-11 21:17:43,524 Found new lowest validation loss at iteration 2294! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_best_val_loss_seed2.pth
2021-12-11 21:17:45,866 iteration 2295 : loss : 0.065147, loss_ce: 0.022445
 34%|█████████                  | 135/400 [1:54:44<4:02:59, 55.02s/it]2021-12-11 21:17:48,643 iteration 2296 : loss : 0.048075, loss_ce: 0.024060
2021-12-11 21:17:51,521 iteration 2297 : loss : 0.045098, loss_ce: 0.018563
2021-12-11 21:17:54,336 iteration 2298 : loss : 0.057506, loss_ce: 0.026177
2021-12-11 21:17:57,097 iteration 2299 : loss : 0.055486, loss_ce: 0.022368
2021-12-11 21:17:59,786 iteration 2300 : loss : 0.038194, loss_ce: 0.018369
2021-12-11 21:18:02,536 iteration 2301 : loss : 0.050551, loss_ce: 0.023161
2021-12-11 21:18:05,503 iteration 2302 : loss : 0.055445, loss_ce: 0.024813
2021-12-11 21:18:08,287 iteration 2303 : loss : 0.055587, loss_ce: 0.021832
2021-12-11 21:18:11,212 iteration 2304 : loss : 0.065127, loss_ce: 0.029988
2021-12-11 21:18:13,903 iteration 2305 : loss : 0.072440, loss_ce: 0.033377
2021-12-11 21:18:16,727 iteration 2306 : loss : 0.057522, loss_ce: 0.027723
2021-12-11 21:18:19,439 iteration 2307 : loss : 0.050889, loss_ce: 0.023198
2021-12-11 21:18:22,177 iteration 2308 : loss : 0.048605, loss_ce: 0.023414
2021-12-11 21:18:25,140 iteration 2309 : loss : 0.053963, loss_ce: 0.025896
2021-12-11 21:18:27,994 iteration 2310 : loss : 0.055606, loss_ce: 0.029641
2021-12-11 21:18:30,752 iteration 2311 : loss : 0.064257, loss_ce: 0.028606
2021-12-11 21:18:33,500 iteration 2312 : loss : 0.046224, loss_ce: 0.021218
 34%|█████████▏                 | 136/400 [1:55:31<3:52:21, 52.81s/it]2021-12-11 21:18:36,353 iteration 2313 : loss : 0.089572, loss_ce: 0.032393
2021-12-11 21:18:39,278 iteration 2314 : loss : 0.039065, loss_ce: 0.017832
2021-12-11 21:18:42,295 iteration 2315 : loss : 0.055383, loss_ce: 0.021731
2021-12-11 21:18:44,969 iteration 2316 : loss : 0.059188, loss_ce: 0.025899
2021-12-11 21:18:47,797 iteration 2317 : loss : 0.078984, loss_ce: 0.028726
2021-12-11 21:18:50,725 iteration 2318 : loss : 0.055529, loss_ce: 0.028243
2021-12-11 21:18:53,628 iteration 2319 : loss : 0.043553, loss_ce: 0.022248
2021-12-11 21:18:56,423 iteration 2320 : loss : 0.049057, loss_ce: 0.025210
2021-12-11 21:18:59,114 iteration 2321 : loss : 0.071500, loss_ce: 0.027640
2021-12-11 21:19:02,011 iteration 2322 : loss : 0.066048, loss_ce: 0.039478
2021-12-11 21:19:04,838 iteration 2323 : loss : 0.055174, loss_ce: 0.028524
2021-12-11 21:19:07,694 iteration 2324 : loss : 0.043986, loss_ce: 0.017355
2021-12-11 21:19:10,380 iteration 2325 : loss : 0.042012, loss_ce: 0.021221
2021-12-11 21:19:13,201 iteration 2326 : loss : 0.050756, loss_ce: 0.025767
2021-12-11 21:19:15,919 iteration 2327 : loss : 0.038191, loss_ce: 0.018956
2021-12-11 21:19:18,827 iteration 2328 : loss : 0.076872, loss_ce: 0.039461
2021-12-11 21:19:21,547 iteration 2329 : loss : 0.038949, loss_ce: 0.018830
 34%|█████████▏                 | 137/400 [1:56:19<3:45:11, 51.38s/it]2021-12-11 21:19:24,312 iteration 2330 : loss : 0.053762, loss_ce: 0.023734
2021-12-11 21:19:27,042 iteration 2331 : loss : 0.072245, loss_ce: 0.033574
2021-12-11 21:19:30,011 iteration 2332 : loss : 0.058771, loss_ce: 0.027385
2021-12-11 21:19:32,774 iteration 2333 : loss : 0.048477, loss_ce: 0.026517
2021-12-11 21:19:35,520 iteration 2334 : loss : 0.044793, loss_ce: 0.022382
2021-12-11 21:19:38,299 iteration 2335 : loss : 0.063090, loss_ce: 0.023855
2021-12-11 21:19:41,226 iteration 2336 : loss : 0.082767, loss_ce: 0.029223
2021-12-11 21:19:43,992 iteration 2337 : loss : 0.038826, loss_ce: 0.022941
2021-12-11 21:19:46,563 iteration 2338 : loss : 0.055517, loss_ce: 0.023569
2021-12-11 21:19:49,507 iteration 2339 : loss : 0.046503, loss_ce: 0.020591
2021-12-11 21:19:52,316 iteration 2340 : loss : 0.067409, loss_ce: 0.031977
2021-12-11 21:19:55,060 iteration 2341 : loss : 0.051939, loss_ce: 0.020583
2021-12-11 21:19:57,682 iteration 2342 : loss : 0.041627, loss_ce: 0.021661
2021-12-11 21:20:00,526 iteration 2343 : loss : 0.063935, loss_ce: 0.028898
2021-12-11 21:20:03,463 iteration 2344 : loss : 0.055904, loss_ce: 0.025702
2021-12-11 21:20:06,242 iteration 2345 : loss : 0.055592, loss_ce: 0.021982
2021-12-11 21:20:08,919 iteration 2346 : loss : 0.043709, loss_ce: 0.021895
 34%|█████████▎                 | 138/400 [1:57:07<3:39:05, 50.18s/it]2021-12-11 21:20:11,811 iteration 2347 : loss : 0.038240, loss_ce: 0.017606
2021-12-11 21:20:14,445 iteration 2348 : loss : 0.048292, loss_ce: 0.021641
2021-12-11 21:20:17,332 iteration 2349 : loss : 0.043583, loss_ce: 0.023335
2021-12-11 21:20:20,003 iteration 2350 : loss : 0.045881, loss_ce: 0.021809
2021-12-11 21:20:22,905 iteration 2351 : loss : 0.138659, loss_ce: 0.030235
2021-12-11 21:20:25,687 iteration 2352 : loss : 0.040837, loss_ce: 0.017254
2021-12-11 21:20:28,518 iteration 2353 : loss : 0.054193, loss_ce: 0.026617
2021-12-11 21:20:31,521 iteration 2354 : loss : 0.046117, loss_ce: 0.023859
2021-12-11 21:20:34,209 iteration 2355 : loss : 0.051126, loss_ce: 0.024106
2021-12-11 21:20:37,044 iteration 2356 : loss : 0.042840, loss_ce: 0.020865
2021-12-11 21:20:39,790 iteration 2357 : loss : 0.070153, loss_ce: 0.027796
2021-12-11 21:20:42,657 iteration 2358 : loss : 0.055328, loss_ce: 0.027344
2021-12-11 21:20:45,664 iteration 2359 : loss : 0.051141, loss_ce: 0.025050
2021-12-11 21:20:48,330 iteration 2360 : loss : 0.056012, loss_ce: 0.029600
2021-12-11 21:20:51,181 iteration 2361 : loss : 0.046922, loss_ce: 0.019803
2021-12-11 21:20:54,245 iteration 2362 : loss : 0.067184, loss_ce: 0.034716
2021-12-11 21:20:56,887 iteration 2363 : loss : 0.037107, loss_ce: 0.019762
 35%|█████████▍                 | 139/400 [1:57:55<3:35:23, 49.51s/it]2021-12-11 21:20:59,674 iteration 2364 : loss : 0.091962, loss_ce: 0.022307
2021-12-11 21:21:02,453 iteration 2365 : loss : 0.054854, loss_ce: 0.017937
2021-12-11 21:21:05,099 iteration 2366 : loss : 0.062157, loss_ce: 0.029915
2021-12-11 21:21:07,932 iteration 2367 : loss : 0.076640, loss_ce: 0.042960
2021-12-11 21:21:10,567 iteration 2368 : loss : 0.057585, loss_ce: 0.025522
2021-12-11 21:21:13,510 iteration 2369 : loss : 0.055845, loss_ce: 0.025244
2021-12-11 21:21:16,167 iteration 2370 : loss : 0.057189, loss_ce: 0.026066
2021-12-11 21:21:18,916 iteration 2371 : loss : 0.048313, loss_ce: 0.021807
2021-12-11 21:21:21,592 iteration 2372 : loss : 0.051355, loss_ce: 0.024432
2021-12-11 21:21:24,324 iteration 2373 : loss : 0.047191, loss_ce: 0.022655
2021-12-11 21:21:26,997 iteration 2374 : loss : 0.048099, loss_ce: 0.025528
2021-12-11 21:21:29,903 iteration 2375 : loss : 0.059585, loss_ce: 0.025489
2021-12-11 21:21:32,793 iteration 2376 : loss : 0.055141, loss_ce: 0.025783
2021-12-11 21:21:35,567 iteration 2377 : loss : 0.042516, loss_ce: 0.019671
2021-12-11 21:21:38,286 iteration 2378 : loss : 0.044222, loss_ce: 0.022163
2021-12-11 21:21:41,096 iteration 2379 : loss : 0.052865, loss_ce: 0.028881
2021-12-11 21:21:41,096 Training Data Eval:
2021-12-11 21:21:56,210   Average segmentation loss on training set: 0.0305
2021-12-11 21:21:56,210 Validation Data Eval:
2021-12-11 21:22:01,430   Average segmentation loss on validation set: 0.0962
2021-12-11 21:22:04,119 iteration 2380 : loss : 0.036384, loss_ce: 0.019449
 35%|█████████▍                 | 140/400 [1:59:02<3:57:35, 54.83s/it]2021-12-11 21:22:07,007 iteration 2381 : loss : 0.080274, loss_ce: 0.030421
2021-12-11 21:22:09,769 iteration 2382 : loss : 0.070388, loss_ce: 0.030075
2021-12-11 21:22:12,656 iteration 2383 : loss : 0.047024, loss_ce: 0.023150
2021-12-11 21:22:15,277 iteration 2384 : loss : 0.048813, loss_ce: 0.022402
2021-12-11 21:22:18,247 iteration 2385 : loss : 0.044727, loss_ce: 0.019542
2021-12-11 21:22:21,020 iteration 2386 : loss : 0.054064, loss_ce: 0.022245
2021-12-11 21:22:23,963 iteration 2387 : loss : 0.040992, loss_ce: 0.019250
2021-12-11 21:22:26,796 iteration 2388 : loss : 0.045055, loss_ce: 0.025156
2021-12-11 21:22:29,529 iteration 2389 : loss : 0.044231, loss_ce: 0.020375
2021-12-11 21:22:32,265 iteration 2390 : loss : 0.047441, loss_ce: 0.023282
2021-12-11 21:22:34,985 iteration 2391 : loss : 0.058439, loss_ce: 0.025228
2021-12-11 21:22:38,043 iteration 2392 : loss : 0.062606, loss_ce: 0.022494
2021-12-11 21:22:40,914 iteration 2393 : loss : 0.057848, loss_ce: 0.025202
2021-12-11 21:22:43,765 iteration 2394 : loss : 0.067120, loss_ce: 0.027856
2021-12-11 21:22:46,514 iteration 2395 : loss : 0.048172, loss_ce: 0.024944
2021-12-11 21:22:49,297 iteration 2396 : loss : 0.045681, loss_ce: 0.022980
2021-12-11 21:22:52,042 iteration 2397 : loss : 0.051101, loss_ce: 0.030102
 35%|█████████▌                 | 141/400 [1:59:50<3:47:44, 52.76s/it]2021-12-11 21:22:54,881 iteration 2398 : loss : 0.045790, loss_ce: 0.026210
2021-12-11 21:22:57,929 iteration 2399 : loss : 0.051828, loss_ce: 0.023886
2021-12-11 21:23:00,611 iteration 2400 : loss : 0.060756, loss_ce: 0.027668
2021-12-11 21:23:03,676 iteration 2401 : loss : 0.089048, loss_ce: 0.033576
2021-12-11 21:23:06,330 iteration 2402 : loss : 0.060278, loss_ce: 0.026301
2021-12-11 21:23:09,076 iteration 2403 : loss : 0.050136, loss_ce: 0.023397
2021-12-11 21:23:11,848 iteration 2404 : loss : 0.041845, loss_ce: 0.018422
2021-12-11 21:23:14,708 iteration 2405 : loss : 0.048421, loss_ce: 0.021907
2021-12-11 21:23:17,306 iteration 2406 : loss : 0.077400, loss_ce: 0.043512
2021-12-11 21:23:20,181 iteration 2407 : loss : 0.044522, loss_ce: 0.020061
2021-12-11 21:23:23,139 iteration 2408 : loss : 0.080403, loss_ce: 0.030151
2021-12-11 21:23:25,799 iteration 2409 : loss : 0.050017, loss_ce: 0.022710
2021-12-11 21:23:28,463 iteration 2410 : loss : 0.048666, loss_ce: 0.022005
2021-12-11 21:23:31,129 iteration 2411 : loss : 0.044655, loss_ce: 0.020879
2021-12-11 21:23:34,008 iteration 2412 : loss : 0.045409, loss_ce: 0.020067
2021-12-11 21:23:36,811 iteration 2413 : loss : 0.041312, loss_ce: 0.020840
2021-12-11 21:23:39,553 iteration 2414 : loss : 0.038958, loss_ce: 0.020667
 36%|█████████▌                 | 142/400 [2:00:37<3:40:05, 51.18s/it]2021-12-11 21:23:42,520 iteration 2415 : loss : 0.047378, loss_ce: 0.020919
2021-12-11 21:23:45,349 iteration 2416 : loss : 0.060604, loss_ce: 0.030357
2021-12-11 21:23:48,094 iteration 2417 : loss : 0.057312, loss_ce: 0.029969
2021-12-11 21:23:50,767 iteration 2418 : loss : 0.040265, loss_ce: 0.017334
2021-12-11 21:23:53,596 iteration 2419 : loss : 0.077950, loss_ce: 0.031765
2021-12-11 21:23:56,396 iteration 2420 : loss : 0.054838, loss_ce: 0.027977
2021-12-11 21:23:59,198 iteration 2421 : loss : 0.042682, loss_ce: 0.023884
2021-12-11 21:24:02,071 iteration 2422 : loss : 0.045030, loss_ce: 0.024470
2021-12-11 21:24:04,897 iteration 2423 : loss : 0.038743, loss_ce: 0.021462
2021-12-11 21:24:07,603 iteration 2424 : loss : 0.042706, loss_ce: 0.022091
2021-12-11 21:24:10,373 iteration 2425 : loss : 0.049401, loss_ce: 0.020217
2021-12-11 21:24:13,300 iteration 2426 : loss : 0.061755, loss_ce: 0.026141
2021-12-11 21:24:16,057 iteration 2427 : loss : 0.043831, loss_ce: 0.022922
2021-12-11 21:24:18,854 iteration 2428 : loss : 0.047976, loss_ce: 0.019635
2021-12-11 21:24:21,621 iteration 2429 : loss : 0.081472, loss_ce: 0.031625
2021-12-11 21:24:24,272 iteration 2430 : loss : 0.048202, loss_ce: 0.024263
2021-12-11 21:24:27,023 iteration 2431 : loss : 0.046744, loss_ce: 0.018524
 36%|█████████▋                 | 143/400 [2:01:25<3:34:27, 50.07s/it]2021-12-11 21:24:29,818 iteration 2432 : loss : 0.052174, loss_ce: 0.021596
2021-12-11 21:24:32,548 iteration 2433 : loss : 0.037469, loss_ce: 0.021239
2021-12-11 21:24:35,356 iteration 2434 : loss : 0.046696, loss_ce: 0.022786
2021-12-11 21:24:38,133 iteration 2435 : loss : 0.033507, loss_ce: 0.017746
2021-12-11 21:24:41,024 iteration 2436 : loss : 0.040435, loss_ce: 0.019585
2021-12-11 21:24:43,805 iteration 2437 : loss : 0.050501, loss_ce: 0.023153
2021-12-11 21:24:46,479 iteration 2438 : loss : 0.037823, loss_ce: 0.018906
2021-12-11 21:24:49,263 iteration 2439 : loss : 0.048491, loss_ce: 0.021856
2021-12-11 21:24:51,938 iteration 2440 : loss : 0.037180, loss_ce: 0.018134
2021-12-11 21:24:54,861 iteration 2441 : loss : 0.076442, loss_ce: 0.027630
2021-12-11 21:24:57,666 iteration 2442 : loss : 0.044816, loss_ce: 0.019245
2021-12-11 21:25:00,418 iteration 2443 : loss : 0.055791, loss_ce: 0.026496
2021-12-11 21:25:03,329 iteration 2444 : loss : 0.048199, loss_ce: 0.021289
2021-12-11 21:25:06,342 iteration 2445 : loss : 0.042542, loss_ce: 0.021620
2021-12-11 21:25:09,162 iteration 2446 : loss : 0.039257, loss_ce: 0.020831
2021-12-11 21:25:11,968 iteration 2447 : loss : 0.043642, loss_ce: 0.019803
2021-12-11 21:25:14,771 iteration 2448 : loss : 0.066916, loss_ce: 0.036759
 36%|█████████▋                 | 144/400 [2:02:12<3:30:39, 49.37s/it]2021-12-11 21:25:17,757 iteration 2449 : loss : 0.056430, loss_ce: 0.021051
2021-12-11 21:25:20,488 iteration 2450 : loss : 0.040399, loss_ce: 0.019478
2021-12-11 21:25:23,145 iteration 2451 : loss : 0.038987, loss_ce: 0.019154
2021-12-11 21:25:25,809 iteration 2452 : loss : 0.051547, loss_ce: 0.027900
2021-12-11 21:25:28,512 iteration 2453 : loss : 0.051736, loss_ce: 0.025887
2021-12-11 21:25:31,279 iteration 2454 : loss : 0.048711, loss_ce: 0.020494
2021-12-11 21:25:34,251 iteration 2455 : loss : 0.070186, loss_ce: 0.028920
2021-12-11 21:25:37,020 iteration 2456 : loss : 0.041302, loss_ce: 0.021068
2021-12-11 21:25:39,706 iteration 2457 : loss : 0.067507, loss_ce: 0.031668
2021-12-11 21:25:42,550 iteration 2458 : loss : 0.056257, loss_ce: 0.022051
2021-12-11 21:25:45,266 iteration 2459 : loss : 0.034266, loss_ce: 0.016702
2021-12-11 21:25:48,092 iteration 2460 : loss : 0.055832, loss_ce: 0.021620
2021-12-11 21:25:50,964 iteration 2461 : loss : 0.048538, loss_ce: 0.017230
2021-12-11 21:25:53,828 iteration 2462 : loss : 0.056372, loss_ce: 0.026816
2021-12-11 21:25:56,709 iteration 2463 : loss : 0.073537, loss_ce: 0.033578
2021-12-11 21:25:59,604 iteration 2464 : loss : 0.058225, loss_ce: 0.036548
2021-12-11 21:25:59,604 Training Data Eval:
2021-12-11 21:26:14,408   Average segmentation loss on training set: 0.0302
2021-12-11 21:26:14,408 Validation Data Eval:
2021-12-11 21:26:19,541   Average segmentation loss on validation set: 0.1052
2021-12-11 21:26:22,280 iteration 2465 : loss : 0.051324, loss_ce: 0.026409
 36%|█████████▊                 | 145/400 [2:03:20<3:52:57, 54.81s/it]2021-12-11 21:26:25,103 iteration 2466 : loss : 0.052696, loss_ce: 0.024969
2021-12-11 21:26:27,911 iteration 2467 : loss : 0.034734, loss_ce: 0.018383
2021-12-11 21:26:30,769 iteration 2468 : loss : 0.064099, loss_ce: 0.027619
2021-12-11 21:26:33,478 iteration 2469 : loss : 0.046586, loss_ce: 0.023673
2021-12-11 21:26:36,299 iteration 2470 : loss : 0.053531, loss_ce: 0.024702
2021-12-11 21:26:39,038 iteration 2471 : loss : 0.054683, loss_ce: 0.029511
2021-12-11 21:26:41,783 iteration 2472 : loss : 0.038412, loss_ce: 0.017418
2021-12-11 21:26:44,628 iteration 2473 : loss : 0.049187, loss_ce: 0.021970
2021-12-11 21:26:47,345 iteration 2474 : loss : 0.057856, loss_ce: 0.032712
2021-12-11 21:26:50,177 iteration 2475 : loss : 0.069225, loss_ce: 0.046081
2021-12-11 21:26:53,034 iteration 2476 : loss : 0.057519, loss_ce: 0.020458
2021-12-11 21:26:55,830 iteration 2477 : loss : 0.045117, loss_ce: 0.020147
2021-12-11 21:26:58,751 iteration 2478 : loss : 0.045986, loss_ce: 0.020636
2021-12-11 21:27:01,533 iteration 2479 : loss : 0.044240, loss_ce: 0.019306
2021-12-11 21:27:04,248 iteration 2480 : loss : 0.032452, loss_ce: 0.016008
2021-12-11 21:27:06,981 iteration 2481 : loss : 0.040820, loss_ce: 0.020502
2021-12-11 21:27:09,961 iteration 2482 : loss : 0.090799, loss_ce: 0.037545
 36%|█████████▊                 | 146/400 [2:04:08<3:42:58, 52.67s/it]2021-12-11 21:27:12,938 iteration 2483 : loss : 0.054338, loss_ce: 0.026518
2021-12-11 21:27:15,729 iteration 2484 : loss : 0.127298, loss_ce: 0.036112
2021-12-11 21:27:18,508 iteration 2485 : loss : 0.079858, loss_ce: 0.029112
2021-12-11 21:27:21,375 iteration 2486 : loss : 0.064520, loss_ce: 0.032304
2021-12-11 21:27:24,049 iteration 2487 : loss : 0.041079, loss_ce: 0.020027
2021-12-11 21:27:27,038 iteration 2488 : loss : 0.068590, loss_ce: 0.039035
2021-12-11 21:27:29,856 iteration 2489 : loss : 0.059179, loss_ce: 0.028961
2021-12-11 21:27:32,806 iteration 2490 : loss : 0.055575, loss_ce: 0.026870
2021-12-11 21:27:35,652 iteration 2491 : loss : 0.059721, loss_ce: 0.024938
2021-12-11 21:27:38,369 iteration 2492 : loss : 0.066151, loss_ce: 0.028948
2021-12-11 21:27:41,121 iteration 2493 : loss : 0.041423, loss_ce: 0.020984
2021-12-11 21:27:43,903 iteration 2494 : loss : 0.047313, loss_ce: 0.020048
2021-12-11 21:27:46,913 iteration 2495 : loss : 0.041597, loss_ce: 0.021849
2021-12-11 21:27:49,511 iteration 2496 : loss : 0.033135, loss_ce: 0.016534
2021-12-11 21:27:52,218 iteration 2497 : loss : 0.043143, loss_ce: 0.018635
2021-12-11 21:27:55,246 iteration 2498 : loss : 0.059559, loss_ce: 0.025518
2021-12-11 21:27:58,026 iteration 2499 : loss : 0.040008, loss_ce: 0.022396
 37%|█████████▉                 | 147/400 [2:04:56<3:36:16, 51.29s/it]2021-12-11 21:28:00,884 iteration 2500 : loss : 0.046608, loss_ce: 0.020447
2021-12-11 21:28:03,619 iteration 2501 : loss : 0.036224, loss_ce: 0.017661
2021-12-11 21:28:06,541 iteration 2502 : loss : 0.049075, loss_ce: 0.026376
2021-12-11 21:28:09,167 iteration 2503 : loss : 0.046743, loss_ce: 0.025141
2021-12-11 21:28:12,099 iteration 2504 : loss : 0.044109, loss_ce: 0.019896
2021-12-11 21:28:14,802 iteration 2505 : loss : 0.056493, loss_ce: 0.025352
2021-12-11 21:28:17,459 iteration 2506 : loss : 0.036043, loss_ce: 0.020660
2021-12-11 21:28:20,275 iteration 2507 : loss : 0.035096, loss_ce: 0.017099
2021-12-11 21:28:22,998 iteration 2508 : loss : 0.044182, loss_ce: 0.023503
2021-12-11 21:28:25,660 iteration 2509 : loss : 0.037576, loss_ce: 0.020694
2021-12-11 21:28:28,442 iteration 2510 : loss : 0.063793, loss_ce: 0.022767
2021-12-11 21:28:31,260 iteration 2511 : loss : 0.050685, loss_ce: 0.027328
2021-12-11 21:28:33,835 iteration 2512 : loss : 0.047423, loss_ce: 0.023933
2021-12-11 21:28:36,584 iteration 2513 : loss : 0.066735, loss_ce: 0.026281
2021-12-11 21:28:39,511 iteration 2514 : loss : 0.049224, loss_ce: 0.021749
2021-12-11 21:28:42,295 iteration 2515 : loss : 0.047542, loss_ce: 0.018120
2021-12-11 21:28:45,031 iteration 2516 : loss : 0.045007, loss_ce: 0.022447
 37%|█████████▉                 | 148/400 [2:05:43<3:30:01, 50.01s/it]2021-12-11 21:28:47,950 iteration 2517 : loss : 0.041519, loss_ce: 0.018754
2021-12-11 21:28:50,797 iteration 2518 : loss : 0.052072, loss_ce: 0.024194
2021-12-11 21:28:53,622 iteration 2519 : loss : 0.039557, loss_ce: 0.019637
2021-12-11 21:28:56,515 iteration 2520 : loss : 0.033692, loss_ce: 0.015845
2021-12-11 21:28:59,190 iteration 2521 : loss : 0.045853, loss_ce: 0.024678
2021-12-11 21:29:01,953 iteration 2522 : loss : 0.038888, loss_ce: 0.020408
2021-12-11 21:29:04,725 iteration 2523 : loss : 0.052403, loss_ce: 0.018167
2021-12-11 21:29:07,457 iteration 2524 : loss : 0.048060, loss_ce: 0.026581
2021-12-11 21:29:10,193 iteration 2525 : loss : 0.046707, loss_ce: 0.023102
2021-12-11 21:29:13,022 iteration 2526 : loss : 0.057709, loss_ce: 0.022201
2021-12-11 21:29:15,968 iteration 2527 : loss : 0.038418, loss_ce: 0.017631
2021-12-11 21:29:18,608 iteration 2528 : loss : 0.054306, loss_ce: 0.023303
2021-12-11 21:29:21,524 iteration 2529 : loss : 0.035968, loss_ce: 0.018007
2021-12-11 21:29:24,305 iteration 2530 : loss : 0.059008, loss_ce: 0.022699
2021-12-11 21:29:27,022 iteration 2531 : loss : 0.072047, loss_ce: 0.027735
2021-12-11 21:29:29,947 iteration 2532 : loss : 0.060576, loss_ce: 0.024349
2021-12-11 21:29:32,691 iteration 2533 : loss : 0.050433, loss_ce: 0.031057
 37%|██████████                 | 149/400 [2:06:30<3:26:15, 49.30s/it]2021-12-11 21:29:35,420 iteration 2534 : loss : 0.045551, loss_ce: 0.022509
2021-12-11 21:29:38,326 iteration 2535 : loss : 0.041591, loss_ce: 0.017893
2021-12-11 21:29:41,111 iteration 2536 : loss : 0.048543, loss_ce: 0.026219
2021-12-11 21:29:44,048 iteration 2537 : loss : 0.071301, loss_ce: 0.028075
2021-12-11 21:29:46,739 iteration 2538 : loss : 0.067022, loss_ce: 0.029974
2021-12-11 21:29:49,390 iteration 2539 : loss : 0.043872, loss_ce: 0.021161
2021-12-11 21:29:52,222 iteration 2540 : loss : 0.072951, loss_ce: 0.028897
2021-12-11 21:29:55,111 iteration 2541 : loss : 0.096937, loss_ce: 0.031741
2021-12-11 21:29:57,778 iteration 2542 : loss : 0.040985, loss_ce: 0.019233
2021-12-11 21:30:00,635 iteration 2543 : loss : 0.047541, loss_ce: 0.020343
2021-12-11 21:30:03,481 iteration 2544 : loss : 0.056876, loss_ce: 0.024356
2021-12-11 21:30:06,310 iteration 2545 : loss : 0.052110, loss_ce: 0.028440
2021-12-11 21:30:09,033 iteration 2546 : loss : 0.039396, loss_ce: 0.020502
2021-12-11 21:30:11,760 iteration 2547 : loss : 0.091152, loss_ce: 0.028899
2021-12-11 21:30:14,527 iteration 2548 : loss : 0.051221, loss_ce: 0.023138
2021-12-11 21:30:17,447 iteration 2549 : loss : 0.067944, loss_ce: 0.029824
2021-12-11 21:30:17,447 Training Data Eval:
2021-12-11 21:30:32,470   Average segmentation loss on training set: 0.0288
2021-12-11 21:30:32,470 Validation Data Eval:
2021-12-11 21:30:37,581   Average segmentation loss on validation set: 0.0793
2021-12-11 21:30:40,191 iteration 2550 : loss : 0.056807, loss_ce: 0.032036
2021-12-11 21:30:42,160 save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_seed2epoch_149.pth
 38%|██████████▏                | 150/400 [2:07:40<3:50:33, 55.33s/it]2021-12-11 21:30:44,515 iteration 2551 : loss : 0.046470, loss_ce: 0.020617
2021-12-11 21:30:47,373 iteration 2552 : loss : 0.041723, loss_ce: 0.020191
2021-12-11 21:30:50,014 iteration 2553 : loss : 0.059106, loss_ce: 0.025031
2021-12-11 21:30:52,685 iteration 2554 : loss : 0.119719, loss_ce: 0.035791
2021-12-11 21:30:55,456 iteration 2555 : loss : 0.061843, loss_ce: 0.021915
2021-12-11 21:30:58,124 iteration 2556 : loss : 0.047920, loss_ce: 0.024365
2021-12-11 21:31:01,081 iteration 2557 : loss : 0.078967, loss_ce: 0.033760
2021-12-11 21:31:03,934 iteration 2558 : loss : 0.038846, loss_ce: 0.021239
2021-12-11 21:31:06,801 iteration 2559 : loss : 0.084461, loss_ce: 0.021857
2021-12-11 21:31:09,664 iteration 2560 : loss : 0.042911, loss_ce: 0.020599
2021-12-11 21:31:12,511 iteration 2561 : loss : 0.065922, loss_ce: 0.032976
2021-12-11 21:31:15,132 iteration 2562 : loss : 0.045314, loss_ce: 0.024652
2021-12-11 21:31:17,888 iteration 2563 : loss : 0.045717, loss_ce: 0.020329
2021-12-11 21:31:20,711 iteration 2564 : loss : 0.052011, loss_ce: 0.028253
2021-12-11 21:31:23,502 iteration 2565 : loss : 0.046715, loss_ce: 0.017298
2021-12-11 21:31:26,208 iteration 2566 : loss : 0.050903, loss_ce: 0.027617
2021-12-11 21:31:29,067 iteration 2567 : loss : 0.054117, loss_ce: 0.027091
 38%|██████████▏                | 151/400 [2:08:27<3:39:13, 52.82s/it]2021-12-11 21:31:32,026 iteration 2568 : loss : 0.039734, loss_ce: 0.020969
2021-12-11 21:31:34,894 iteration 2569 : loss : 0.056792, loss_ce: 0.022937
2021-12-11 21:31:37,619 iteration 2570 : loss : 0.073290, loss_ce: 0.024419
2021-12-11 21:31:40,359 iteration 2571 : loss : 0.054251, loss_ce: 0.025704
2021-12-11 21:31:43,135 iteration 2572 : loss : 0.052904, loss_ce: 0.021341
2021-12-11 21:31:45,883 iteration 2573 : loss : 0.033190, loss_ce: 0.017079
2021-12-11 21:31:48,594 iteration 2574 : loss : 0.071242, loss_ce: 0.028213
2021-12-11 21:31:51,430 iteration 2575 : loss : 0.047201, loss_ce: 0.020726
2021-12-11 21:31:54,261 iteration 2576 : loss : 0.049876, loss_ce: 0.024718
2021-12-11 21:31:57,005 iteration 2577 : loss : 0.044669, loss_ce: 0.020397
2021-12-11 21:31:59,783 iteration 2578 : loss : 0.062364, loss_ce: 0.031797
2021-12-11 21:32:02,682 iteration 2579 : loss : 0.057847, loss_ce: 0.022119
2021-12-11 21:32:05,361 iteration 2580 : loss : 0.045312, loss_ce: 0.017576
2021-12-11 21:32:08,244 iteration 2581 : loss : 0.049752, loss_ce: 0.024265
2021-12-11 21:32:11,026 iteration 2582 : loss : 0.050822, loss_ce: 0.024928
2021-12-11 21:32:13,786 iteration 2583 : loss : 0.037059, loss_ce: 0.017542
2021-12-11 21:32:16,631 iteration 2584 : loss : 0.031184, loss_ce: 0.016033
 38%|██████████▎                | 152/400 [2:09:14<3:31:48, 51.24s/it]2021-12-11 21:32:19,467 iteration 2585 : loss : 0.058211, loss_ce: 0.028251
2021-12-11 21:32:22,386 iteration 2586 : loss : 0.067844, loss_ce: 0.032542
2021-12-11 21:32:25,125 iteration 2587 : loss : 0.036291, loss_ce: 0.016963
2021-12-11 21:32:27,898 iteration 2588 : loss : 0.053308, loss_ce: 0.025533
2021-12-11 21:32:30,616 iteration 2589 : loss : 0.031483, loss_ce: 0.016667
2021-12-11 21:32:33,487 iteration 2590 : loss : 0.046895, loss_ce: 0.018769
2021-12-11 21:32:36,191 iteration 2591 : loss : 0.043516, loss_ce: 0.021978
2021-12-11 21:32:38,779 iteration 2592 : loss : 0.045793, loss_ce: 0.023367
2021-12-11 21:32:41,598 iteration 2593 : loss : 0.045414, loss_ce: 0.019538
2021-12-11 21:32:44,389 iteration 2594 : loss : 0.047028, loss_ce: 0.023022
2021-12-11 21:32:47,225 iteration 2595 : loss : 0.067862, loss_ce: 0.026325
2021-12-11 21:32:49,998 iteration 2596 : loss : 0.039100, loss_ce: 0.016524
2021-12-11 21:32:52,763 iteration 2597 : loss : 0.071611, loss_ce: 0.028770
2021-12-11 21:32:55,503 iteration 2598 : loss : 0.038694, loss_ce: 0.020766
2021-12-11 21:32:58,361 iteration 2599 : loss : 0.038296, loss_ce: 0.021053
2021-12-11 21:33:01,170 iteration 2600 : loss : 0.052829, loss_ce: 0.021741
2021-12-11 21:33:03,937 iteration 2601 : loss : 0.038846, loss_ce: 0.019347
 38%|██████████▎                | 153/400 [2:10:02<3:26:05, 50.06s/it]2021-12-11 21:33:06,739 iteration 2602 : loss : 0.047075, loss_ce: 0.020116
2021-12-11 21:33:09,517 iteration 2603 : loss : 0.056442, loss_ce: 0.027313
2021-12-11 21:33:12,324 iteration 2604 : loss : 0.051954, loss_ce: 0.028605
2021-12-11 21:33:14,821 iteration 2605 : loss : 0.030347, loss_ce: 0.013496
2021-12-11 21:33:17,703 iteration 2606 : loss : 0.063905, loss_ce: 0.025012
2021-12-11 21:33:20,481 iteration 2607 : loss : 0.054172, loss_ce: 0.026067
2021-12-11 21:33:23,385 iteration 2608 : loss : 0.050430, loss_ce: 0.026051
2021-12-11 21:33:26,175 iteration 2609 : loss : 0.082705, loss_ce: 0.029652
2021-12-11 21:33:29,025 iteration 2610 : loss : 0.041549, loss_ce: 0.018632
2021-12-11 21:33:31,780 iteration 2611 : loss : 0.045879, loss_ce: 0.020840
2021-12-11 21:33:34,607 iteration 2612 : loss : 0.055969, loss_ce: 0.029286
2021-12-11 21:33:37,348 iteration 2613 : loss : 0.046781, loss_ce: 0.021886
2021-12-11 21:33:40,073 iteration 2614 : loss : 0.051534, loss_ce: 0.021790
2021-12-11 21:33:42,828 iteration 2615 : loss : 0.064492, loss_ce: 0.027076
2021-12-11 21:33:45,551 iteration 2616 : loss : 0.060183, loss_ce: 0.031387
2021-12-11 21:33:48,256 iteration 2617 : loss : 0.067410, loss_ce: 0.026895
2021-12-11 21:33:51,033 iteration 2618 : loss : 0.038832, loss_ce: 0.019074
 38%|██████████▍                | 154/400 [2:10:49<3:21:36, 49.17s/it]2021-12-11 21:33:53,701 iteration 2619 : loss : 0.061550, loss_ce: 0.023064
2021-12-11 21:33:56,376 iteration 2620 : loss : 0.033249, loss_ce: 0.018550
2021-12-11 21:33:59,200 iteration 2621 : loss : 0.046377, loss_ce: 0.020398
2021-12-11 21:34:02,127 iteration 2622 : loss : 0.062968, loss_ce: 0.020734
2021-12-11 21:34:04,971 iteration 2623 : loss : 0.051748, loss_ce: 0.026386
2021-12-11 21:34:07,720 iteration 2624 : loss : 0.041182, loss_ce: 0.017930
2021-12-11 21:34:10,466 iteration 2625 : loss : 0.071848, loss_ce: 0.019142
2021-12-11 21:34:13,387 iteration 2626 : loss : 0.057717, loss_ce: 0.028003
2021-12-11 21:34:16,182 iteration 2627 : loss : 0.061100, loss_ce: 0.028122
2021-12-11 21:34:18,733 iteration 2628 : loss : 0.046544, loss_ce: 0.022689
2021-12-11 21:34:21,727 iteration 2629 : loss : 0.057838, loss_ce: 0.028823
2021-12-11 21:34:24,546 iteration 2630 : loss : 0.044450, loss_ce: 0.021480
2021-12-11 21:34:27,329 iteration 2631 : loss : 0.047699, loss_ce: 0.024530
2021-12-11 21:34:30,016 iteration 2632 : loss : 0.044570, loss_ce: 0.019570
2021-12-11 21:34:32,848 iteration 2633 : loss : 0.049550, loss_ce: 0.027446
2021-12-11 21:34:35,680 iteration 2634 : loss : 0.053843, loss_ce: 0.023064
2021-12-11 21:34:35,680 Training Data Eval:
2021-12-11 21:34:50,668   Average segmentation loss on training set: 0.0279
2021-12-11 21:34:50,669 Validation Data Eval:
2021-12-11 21:34:55,823   Average segmentation loss on validation set: 0.0772
2021-12-11 21:34:58,444 iteration 2635 : loss : 0.044716, loss_ce: 0.022261
 39%|██████████▍                | 155/400 [2:11:56<3:43:07, 54.64s/it]2021-12-11 21:35:01,314 iteration 2636 : loss : 0.061923, loss_ce: 0.030282
2021-12-11 21:35:04,140 iteration 2637 : loss : 0.053393, loss_ce: 0.026050
2021-12-11 21:35:06,904 iteration 2638 : loss : 0.089526, loss_ce: 0.037307
2021-12-11 21:35:09,595 iteration 2639 : loss : 0.045087, loss_ce: 0.024213
2021-12-11 21:35:12,288 iteration 2640 : loss : 0.044502, loss_ce: 0.019165
2021-12-11 21:35:15,042 iteration 2641 : loss : 0.061733, loss_ce: 0.030710
2021-12-11 21:35:17,871 iteration 2642 : loss : 0.034865, loss_ce: 0.016257
2021-12-11 21:35:20,736 iteration 2643 : loss : 0.040558, loss_ce: 0.018408
2021-12-11 21:35:23,526 iteration 2644 : loss : 0.047580, loss_ce: 0.021000
2021-12-11 21:35:26,102 iteration 2645 : loss : 0.048501, loss_ce: 0.021084
2021-12-11 21:35:29,180 iteration 2646 : loss : 0.041388, loss_ce: 0.018454
2021-12-11 21:35:32,136 iteration 2647 : loss : 0.090048, loss_ce: 0.041424
2021-12-11 21:35:34,816 iteration 2648 : loss : 0.049366, loss_ce: 0.022087
2021-12-11 21:35:37,746 iteration 2649 : loss : 0.054347, loss_ce: 0.026542
2021-12-11 21:35:40,546 iteration 2650 : loss : 0.041751, loss_ce: 0.018116
2021-12-11 21:35:43,273 iteration 2651 : loss : 0.050474, loss_ce: 0.024697
2021-12-11 21:35:46,027 iteration 2652 : loss : 0.040444, loss_ce: 0.019370
 39%|██████████▌                | 156/400 [2:12:44<3:33:36, 52.53s/it]2021-12-11 21:35:48,848 iteration 2653 : loss : 0.045910, loss_ce: 0.024165
2021-12-11 21:35:51,559 iteration 2654 : loss : 0.051742, loss_ce: 0.021928
2021-12-11 21:35:54,389 iteration 2655 : loss : 0.058261, loss_ce: 0.035260
2021-12-11 21:35:57,137 iteration 2656 : loss : 0.062101, loss_ce: 0.023762
2021-12-11 21:35:59,858 iteration 2657 : loss : 0.047142, loss_ce: 0.021476
2021-12-11 21:36:02,536 iteration 2658 : loss : 0.042361, loss_ce: 0.022517
2021-12-11 21:36:05,267 iteration 2659 : loss : 0.060079, loss_ce: 0.017633
2021-12-11 21:36:08,030 iteration 2660 : loss : 0.047020, loss_ce: 0.018100
2021-12-11 21:36:10,658 iteration 2661 : loss : 0.038420, loss_ce: 0.019980
2021-12-11 21:36:13,371 iteration 2662 : loss : 0.057554, loss_ce: 0.029787
2021-12-11 21:36:16,222 iteration 2663 : loss : 0.064242, loss_ce: 0.023134
2021-12-11 21:36:19,158 iteration 2664 : loss : 0.058394, loss_ce: 0.031146
2021-12-11 21:36:21,889 iteration 2665 : loss : 0.065427, loss_ce: 0.030584
2021-12-11 21:36:24,479 iteration 2666 : loss : 0.029691, loss_ce: 0.014844
2021-12-11 21:36:27,363 iteration 2667 : loss : 0.058065, loss_ce: 0.023615
2021-12-11 21:36:30,054 iteration 2668 : loss : 0.055469, loss_ce: 0.022157
2021-12-11 21:36:32,831 iteration 2669 : loss : 0.049627, loss_ce: 0.021740
 39%|██████████▌                | 157/400 [2:13:31<3:25:46, 50.81s/it]2021-12-11 21:36:35,562 iteration 2670 : loss : 0.061878, loss_ce: 0.026681
2021-12-11 21:36:38,588 iteration 2671 : loss : 0.069311, loss_ce: 0.028305
2021-12-11 21:36:41,378 iteration 2672 : loss : 0.058079, loss_ce: 0.025690
2021-12-11 21:36:44,074 iteration 2673 : loss : 0.041595, loss_ce: 0.023231
2021-12-11 21:36:46,916 iteration 2674 : loss : 0.056754, loss_ce: 0.027494
2021-12-11 21:36:49,622 iteration 2675 : loss : 0.045862, loss_ce: 0.022935
2021-12-11 21:36:52,499 iteration 2676 : loss : 0.080608, loss_ce: 0.025055
2021-12-11 21:36:55,357 iteration 2677 : loss : 0.063356, loss_ce: 0.027368
2021-12-11 21:36:58,053 iteration 2678 : loss : 0.049674, loss_ce: 0.024589
2021-12-11 21:37:00,842 iteration 2679 : loss : 0.055868, loss_ce: 0.021574
2021-12-11 21:37:03,703 iteration 2680 : loss : 0.044365, loss_ce: 0.021183
2021-12-11 21:37:06,662 iteration 2681 : loss : 0.039861, loss_ce: 0.016818
2021-12-11 21:37:09,437 iteration 2682 : loss : 0.047556, loss_ce: 0.019598
2021-12-11 21:37:12,254 iteration 2683 : loss : 0.076771, loss_ce: 0.023425
2021-12-11 21:37:14,930 iteration 2684 : loss : 0.062756, loss_ce: 0.033676
2021-12-11 21:37:17,672 iteration 2685 : loss : 0.035628, loss_ce: 0.018136
2021-12-11 21:37:20,472 iteration 2686 : loss : 0.054168, loss_ce: 0.026909
 40%|██████████▋                | 158/400 [2:14:18<3:21:05, 49.86s/it]2021-12-11 21:37:23,245 iteration 2687 : loss : 0.042328, loss_ce: 0.016457
2021-12-11 21:37:25,889 iteration 2688 : loss : 0.051934, loss_ce: 0.021840
2021-12-11 21:37:28,697 iteration 2689 : loss : 0.053509, loss_ce: 0.024039
2021-12-11 21:37:31,423 iteration 2690 : loss : 0.063511, loss_ce: 0.033089
2021-12-11 21:37:34,098 iteration 2691 : loss : 0.040870, loss_ce: 0.019491
2021-12-11 21:37:36,901 iteration 2692 : loss : 0.040653, loss_ce: 0.017539
2021-12-11 21:37:39,688 iteration 2693 : loss : 0.049387, loss_ce: 0.021471
2021-12-11 21:37:42,703 iteration 2694 : loss : 0.063092, loss_ce: 0.025421
2021-12-11 21:37:45,418 iteration 2695 : loss : 0.040759, loss_ce: 0.020248
2021-12-11 21:37:48,183 iteration 2696 : loss : 0.045878, loss_ce: 0.024393
2021-12-11 21:37:50,823 iteration 2697 : loss : 0.035344, loss_ce: 0.019622
2021-12-11 21:37:53,509 iteration 2698 : loss : 0.033324, loss_ce: 0.018713
2021-12-11 21:37:56,215 iteration 2699 : loss : 0.046125, loss_ce: 0.020633
2021-12-11 21:37:58,896 iteration 2700 : loss : 0.052954, loss_ce: 0.029392
2021-12-11 21:38:01,748 iteration 2701 : loss : 0.045024, loss_ce: 0.021240
2021-12-11 21:38:04,534 iteration 2702 : loss : 0.100272, loss_ce: 0.020514
2021-12-11 21:38:07,310 iteration 2703 : loss : 0.052356, loss_ce: 0.022227
 40%|██████████▋                | 159/400 [2:15:05<3:16:37, 48.95s/it]2021-12-11 21:38:10,345 iteration 2704 : loss : 0.049718, loss_ce: 0.022146
2021-12-11 21:38:13,194 iteration 2705 : loss : 0.044564, loss_ce: 0.018309
2021-12-11 21:38:15,978 iteration 2706 : loss : 0.045827, loss_ce: 0.025968
2021-12-11 21:38:18,873 iteration 2707 : loss : 0.052091, loss_ce: 0.021316
2021-12-11 21:38:21,415 iteration 2708 : loss : 0.027722, loss_ce: 0.014951
2021-12-11 21:38:24,159 iteration 2709 : loss : 0.050533, loss_ce: 0.020082
2021-12-11 21:38:26,760 iteration 2710 : loss : 0.042859, loss_ce: 0.023496
2021-12-11 21:38:29,719 iteration 2711 : loss : 0.058257, loss_ce: 0.024277
2021-12-11 21:38:32,343 iteration 2712 : loss : 0.033895, loss_ce: 0.017921
2021-12-11 21:38:35,242 iteration 2713 : loss : 0.052387, loss_ce: 0.023953
2021-12-11 21:38:37,927 iteration 2714 : loss : 0.047626, loss_ce: 0.020018
2021-12-11 21:38:40,865 iteration 2715 : loss : 0.059683, loss_ce: 0.031180
2021-12-11 21:38:43,488 iteration 2716 : loss : 0.044657, loss_ce: 0.019333
2021-12-11 21:38:46,125 iteration 2717 : loss : 0.040755, loss_ce: 0.023272
2021-12-11 21:38:48,811 iteration 2718 : loss : 0.045304, loss_ce: 0.019838
2021-12-11 21:38:51,507 iteration 2719 : loss : 0.072020, loss_ce: 0.024882
2021-12-11 21:38:51,507 Training Data Eval:
2021-12-11 21:39:06,510   Average segmentation loss on training set: 0.0248
2021-12-11 21:39:06,511 Validation Data Eval:
2021-12-11 21:39:11,659   Average segmentation loss on validation set: 0.0821
2021-12-11 21:39:14,417 iteration 2720 : loss : 0.033634, loss_ce: 0.018208
 40%|██████████▊                | 160/400 [2:16:12<3:37:35, 54.40s/it]2021-12-11 21:39:17,271 iteration 2721 : loss : 0.042877, loss_ce: 0.021027
2021-12-11 21:39:20,180 iteration 2722 : loss : 0.071235, loss_ce: 0.031387
2021-12-11 21:39:22,885 iteration 2723 : loss : 0.049826, loss_ce: 0.023906
2021-12-11 21:39:25,743 iteration 2724 : loss : 0.040331, loss_ce: 0.018439
2021-12-11 21:39:28,533 iteration 2725 : loss : 0.045438, loss_ce: 0.024012
2021-12-11 21:39:31,215 iteration 2726 : loss : 0.049339, loss_ce: 0.020109
2021-12-11 21:39:33,878 iteration 2727 : loss : 0.033338, loss_ce: 0.015211
2021-12-11 21:39:36,722 iteration 2728 : loss : 0.064389, loss_ce: 0.031454
2021-12-11 21:39:39,381 iteration 2729 : loss : 0.064062, loss_ce: 0.027279
2021-12-11 21:39:42,268 iteration 2730 : loss : 0.035821, loss_ce: 0.018590
2021-12-11 21:39:45,173 iteration 2731 : loss : 0.040969, loss_ce: 0.020951
2021-12-11 21:39:47,972 iteration 2732 : loss : 0.046108, loss_ce: 0.022769
2021-12-11 21:39:50,907 iteration 2733 : loss : 0.042535, loss_ce: 0.018707
2021-12-11 21:39:53,554 iteration 2734 : loss : 0.046477, loss_ce: 0.016690
2021-12-11 21:39:56,336 iteration 2735 : loss : 0.039310, loss_ce: 0.019422
2021-12-11 21:39:59,188 iteration 2736 : loss : 0.053249, loss_ce: 0.030176
2021-12-11 21:40:02,083 iteration 2737 : loss : 0.040119, loss_ce: 0.018546
 40%|██████████▊                | 161/400 [2:17:00<3:28:38, 52.38s/it]2021-12-11 21:40:04,936 iteration 2738 : loss : 0.041564, loss_ce: 0.023382
2021-12-11 21:40:07,700 iteration 2739 : loss : 0.057120, loss_ce: 0.030956
2021-12-11 21:40:10,475 iteration 2740 : loss : 0.043485, loss_ce: 0.019978
2021-12-11 21:40:13,282 iteration 2741 : loss : 0.039326, loss_ce: 0.016627
2021-12-11 21:40:15,993 iteration 2742 : loss : 0.043853, loss_ce: 0.021272
2021-12-11 21:40:18,717 iteration 2743 : loss : 0.066349, loss_ce: 0.036757
2021-12-11 21:40:21,625 iteration 2744 : loss : 0.045392, loss_ce: 0.019038
2021-12-11 21:40:24,305 iteration 2745 : loss : 0.043418, loss_ce: 0.020407
2021-12-11 21:40:27,159 iteration 2746 : loss : 0.073093, loss_ce: 0.023365
2021-12-11 21:40:30,027 iteration 2747 : loss : 0.040699, loss_ce: 0.016700
2021-12-11 21:40:32,700 iteration 2748 : loss : 0.062231, loss_ce: 0.028971
2021-12-11 21:40:35,361 iteration 2749 : loss : 0.040080, loss_ce: 0.021304
2021-12-11 21:40:38,159 iteration 2750 : loss : 0.051594, loss_ce: 0.020286
2021-12-11 21:40:40,938 iteration 2751 : loss : 0.046749, loss_ce: 0.023514
2021-12-11 21:40:43,684 iteration 2752 : loss : 0.075985, loss_ce: 0.017718
2021-12-11 21:40:46,408 iteration 2753 : loss : 0.054134, loss_ce: 0.025690
2021-12-11 21:40:49,234 iteration 2754 : loss : 0.035636, loss_ce: 0.018428
 40%|██████████▉                | 162/400 [2:17:47<3:21:33, 50.81s/it]2021-12-11 21:40:52,049 iteration 2755 : loss : 0.048243, loss_ce: 0.021783
2021-12-11 21:40:54,927 iteration 2756 : loss : 0.060756, loss_ce: 0.035359
2021-12-11 21:40:57,685 iteration 2757 : loss : 0.051103, loss_ce: 0.024499
2021-12-11 21:41:00,460 iteration 2758 : loss : 0.064173, loss_ce: 0.026541
2021-12-11 21:41:03,173 iteration 2759 : loss : 0.062559, loss_ce: 0.024000
2021-12-11 21:41:06,029 iteration 2760 : loss : 0.050464, loss_ce: 0.022759
2021-12-11 21:41:08,637 iteration 2761 : loss : 0.055825, loss_ce: 0.023201
2021-12-11 21:41:11,300 iteration 2762 : loss : 0.031688, loss_ce: 0.015752
2021-12-11 21:41:14,186 iteration 2763 : loss : 0.073241, loss_ce: 0.026675
2021-12-11 21:41:16,968 iteration 2764 : loss : 0.051611, loss_ce: 0.021576
2021-12-11 21:41:19,822 iteration 2765 : loss : 0.048480, loss_ce: 0.022676
2021-12-11 21:41:22,423 iteration 2766 : loss : 0.037644, loss_ce: 0.019312
2021-12-11 21:41:25,330 iteration 2767 : loss : 0.040170, loss_ce: 0.019143
2021-12-11 21:41:28,022 iteration 2768 : loss : 0.032515, loss_ce: 0.014540
2021-12-11 21:41:30,850 iteration 2769 : loss : 0.039674, loss_ce: 0.018772
2021-12-11 21:41:33,544 iteration 2770 : loss : 0.040806, loss_ce: 0.022119
2021-12-11 21:41:36,366 iteration 2771 : loss : 0.050099, loss_ce: 0.023574
 41%|███████████                | 163/400 [2:18:34<3:16:21, 49.71s/it]2021-12-11 21:41:39,327 iteration 2772 : loss : 0.050638, loss_ce: 0.024838
2021-12-11 21:41:42,032 iteration 2773 : loss : 0.035820, loss_ce: 0.018429
2021-12-11 21:41:44,793 iteration 2774 : loss : 0.050207, loss_ce: 0.018874
2021-12-11 21:41:47,593 iteration 2775 : loss : 0.051707, loss_ce: 0.023664
2021-12-11 21:41:50,221 iteration 2776 : loss : 0.035309, loss_ce: 0.016466
2021-12-11 21:41:53,129 iteration 2777 : loss : 0.029279, loss_ce: 0.015059
2021-12-11 21:41:56,039 iteration 2778 : loss : 0.056277, loss_ce: 0.025836
2021-12-11 21:41:59,065 iteration 2779 : loss : 0.046600, loss_ce: 0.022519
2021-12-11 21:42:01,749 iteration 2780 : loss : 0.031019, loss_ce: 0.014424
2021-12-11 21:42:04,509 iteration 2781 : loss : 0.052658, loss_ce: 0.022617
2021-12-11 21:42:07,318 iteration 2782 : loss : 0.044166, loss_ce: 0.020055
2021-12-11 21:42:09,920 iteration 2783 : loss : 0.065709, loss_ce: 0.032554
2021-12-11 21:42:12,788 iteration 2784 : loss : 0.069390, loss_ce: 0.025220
2021-12-11 21:42:15,704 iteration 2785 : loss : 0.058113, loss_ce: 0.021267
2021-12-11 21:42:18,536 iteration 2786 : loss : 0.040075, loss_ce: 0.021956
2021-12-11 21:42:21,232 iteration 2787 : loss : 0.032929, loss_ce: 0.016963
2021-12-11 21:42:24,076 iteration 2788 : loss : 0.035152, loss_ce: 0.020833
 41%|███████████                | 164/400 [2:19:22<3:13:09, 49.11s/it]2021-12-11 21:42:26,864 iteration 2789 : loss : 0.045556, loss_ce: 0.022181
2021-12-11 21:42:29,788 iteration 2790 : loss : 0.059267, loss_ce: 0.022910
2021-12-11 21:42:32,688 iteration 2791 : loss : 0.037353, loss_ce: 0.018540
2021-12-11 21:42:35,532 iteration 2792 : loss : 0.065084, loss_ce: 0.029032
2021-12-11 21:42:38,255 iteration 2793 : loss : 0.038304, loss_ce: 0.018809
2021-12-11 21:42:41,102 iteration 2794 : loss : 0.038743, loss_ce: 0.016763
2021-12-11 21:42:43,857 iteration 2795 : loss : 0.035960, loss_ce: 0.015372
2021-12-11 21:42:46,647 iteration 2796 : loss : 0.042645, loss_ce: 0.018991
2021-12-11 21:42:49,498 iteration 2797 : loss : 0.036174, loss_ce: 0.018661
2021-12-11 21:42:52,304 iteration 2798 : loss : 0.063146, loss_ce: 0.033973
2021-12-11 21:42:55,173 iteration 2799 : loss : 0.030327, loss_ce: 0.016771
2021-12-11 21:42:57,909 iteration 2800 : loss : 0.076849, loss_ce: 0.027343
2021-12-11 21:43:00,610 iteration 2801 : loss : 0.033780, loss_ce: 0.014540
2021-12-11 21:43:03,461 iteration 2802 : loss : 0.049909, loss_ce: 0.019118
2021-12-11 21:43:06,254 iteration 2803 : loss : 0.036989, loss_ce: 0.018948
2021-12-11 21:43:09,006 iteration 2804 : loss : 0.033322, loss_ce: 0.016110
2021-12-11 21:43:09,006 Training Data Eval:
2021-12-11 21:43:23,717   Average segmentation loss on training set: 0.0242
2021-12-11 21:43:23,717 Validation Data Eval:
2021-12-11 21:43:28,998   Average segmentation loss on validation set: 0.0831
2021-12-11 21:43:31,649 iteration 2805 : loss : 0.042416, loss_ce: 0.020736
 41%|███████████▏               | 165/400 [2:20:29<3:34:01, 54.65s/it]2021-12-11 21:43:34,647 iteration 2806 : loss : 0.057013, loss_ce: 0.029183
2021-12-11 21:43:37,380 iteration 2807 : loss : 0.052924, loss_ce: 0.023479
2021-12-11 21:43:39,999 iteration 2808 : loss : 0.045028, loss_ce: 0.020178
2021-12-11 21:43:42,924 iteration 2809 : loss : 0.033804, loss_ce: 0.015461
2021-12-11 21:43:45,635 iteration 2810 : loss : 0.047489, loss_ce: 0.022968
2021-12-11 21:43:48,594 iteration 2811 : loss : 0.041769, loss_ce: 0.017613
2021-12-11 21:43:51,350 iteration 2812 : loss : 0.056571, loss_ce: 0.023307
2021-12-11 21:43:54,056 iteration 2813 : loss : 0.031166, loss_ce: 0.016139
2021-12-11 21:43:56,858 iteration 2814 : loss : 0.045688, loss_ce: 0.025814
2021-12-11 21:43:59,635 iteration 2815 : loss : 0.046557, loss_ce: 0.021155
2021-12-11 21:44:02,417 iteration 2816 : loss : 0.032299, loss_ce: 0.014980
2021-12-11 21:44:05,034 iteration 2817 : loss : 0.054520, loss_ce: 0.027268
2021-12-11 21:44:07,958 iteration 2818 : loss : 0.052168, loss_ce: 0.021012
2021-12-11 21:44:10,731 iteration 2819 : loss : 0.042481, loss_ce: 0.021620
2021-12-11 21:44:13,430 iteration 2820 : loss : 0.040300, loss_ce: 0.019588
2021-12-11 21:44:16,095 iteration 2821 : loss : 0.027776, loss_ce: 0.015052
2021-12-11 21:44:18,787 iteration 2822 : loss : 0.075675, loss_ce: 0.022809
 42%|███████████▏               | 166/400 [2:21:16<3:24:19, 52.39s/it]2021-12-11 21:44:21,803 iteration 2823 : loss : 0.039429, loss_ce: 0.016704
2021-12-11 21:44:24,633 iteration 2824 : loss : 0.053628, loss_ce: 0.023103
2021-12-11 21:44:27,354 iteration 2825 : loss : 0.058945, loss_ce: 0.023961
2021-12-11 21:44:30,093 iteration 2826 : loss : 0.049113, loss_ce: 0.020262
2021-12-11 21:44:32,825 iteration 2827 : loss : 0.048200, loss_ce: 0.026797
2021-12-11 21:44:35,711 iteration 2828 : loss : 0.061685, loss_ce: 0.032345
2021-12-11 21:44:38,400 iteration 2829 : loss : 0.046817, loss_ce: 0.018669
2021-12-11 21:44:41,112 iteration 2830 : loss : 0.039265, loss_ce: 0.020208
2021-12-11 21:44:43,811 iteration 2831 : loss : 0.051355, loss_ce: 0.020303
2021-12-11 21:44:46,572 iteration 2832 : loss : 0.034910, loss_ce: 0.017679
2021-12-11 21:44:49,297 iteration 2833 : loss : 0.032370, loss_ce: 0.017249
2021-12-11 21:44:52,180 iteration 2834 : loss : 0.039675, loss_ce: 0.019330
2021-12-11 21:44:54,970 iteration 2835 : loss : 0.053674, loss_ce: 0.023772
2021-12-11 21:44:57,923 iteration 2836 : loss : 0.039272, loss_ce: 0.018790
2021-12-11 21:45:00,697 iteration 2837 : loss : 0.041279, loss_ce: 0.016790
2021-12-11 21:45:03,616 iteration 2838 : loss : 0.044206, loss_ce: 0.018662
2021-12-11 21:45:06,272 iteration 2839 : loss : 0.041245, loss_ce: 0.020026
 42%|███████████▎               | 167/400 [2:22:04<3:17:44, 50.92s/it]2021-12-11 21:45:09,102 iteration 2840 : loss : 0.049770, loss_ce: 0.020511
2021-12-11 21:45:11,795 iteration 2841 : loss : 0.037552, loss_ce: 0.020872
2021-12-11 21:45:14,473 iteration 2842 : loss : 0.031171, loss_ce: 0.015412
2021-12-11 21:45:17,203 iteration 2843 : loss : 0.044652, loss_ce: 0.016369
2021-12-11 21:45:20,041 iteration 2844 : loss : 0.043934, loss_ce: 0.022348
2021-12-11 21:45:22,835 iteration 2845 : loss : 0.036809, loss_ce: 0.016080
2021-12-11 21:45:25,619 iteration 2846 : loss : 0.043819, loss_ce: 0.018753
2021-12-11 21:45:28,423 iteration 2847 : loss : 0.047710, loss_ce: 0.022973
2021-12-11 21:45:31,104 iteration 2848 : loss : 0.056466, loss_ce: 0.028358
2021-12-11 21:45:33,788 iteration 2849 : loss : 0.050116, loss_ce: 0.022157
2021-12-11 21:45:36,487 iteration 2850 : loss : 0.046455, loss_ce: 0.019085
2021-12-11 21:45:39,124 iteration 2851 : loss : 0.036488, loss_ce: 0.018062
2021-12-11 21:45:41,830 iteration 2852 : loss : 0.056411, loss_ce: 0.026279
2021-12-11 21:45:44,386 iteration 2853 : loss : 0.042990, loss_ce: 0.021492
2021-12-11 21:45:47,283 iteration 2854 : loss : 0.041825, loss_ce: 0.022274
2021-12-11 21:45:50,134 iteration 2855 : loss : 0.052997, loss_ce: 0.025181
2021-12-11 21:45:53,022 iteration 2856 : loss : 0.041040, loss_ce: 0.018616
 42%|███████████▎               | 168/400 [2:22:51<3:12:03, 49.67s/it]2021-12-11 21:45:56,048 iteration 2857 : loss : 0.069034, loss_ce: 0.022601
2021-12-11 21:45:58,865 iteration 2858 : loss : 0.050911, loss_ce: 0.021182
2021-12-11 21:46:01,506 iteration 2859 : loss : 0.035748, loss_ce: 0.017219
2021-12-11 21:46:04,465 iteration 2860 : loss : 0.059744, loss_ce: 0.027162
2021-12-11 21:46:07,219 iteration 2861 : loss : 0.041477, loss_ce: 0.019129
2021-12-11 21:46:10,146 iteration 2862 : loss : 0.038252, loss_ce: 0.018050
2021-12-11 21:46:12,986 iteration 2863 : loss : 0.055601, loss_ce: 0.024129
2021-12-11 21:46:15,913 iteration 2864 : loss : 0.035284, loss_ce: 0.018264
2021-12-11 21:46:18,495 iteration 2865 : loss : 0.038637, loss_ce: 0.017365
2021-12-11 21:46:21,222 iteration 2866 : loss : 0.041742, loss_ce: 0.016569
2021-12-11 21:46:24,113 iteration 2867 : loss : 0.048272, loss_ce: 0.023353
2021-12-11 21:46:26,886 iteration 2868 : loss : 0.039317, loss_ce: 0.019753
2021-12-11 21:46:29,476 iteration 2869 : loss : 0.040313, loss_ce: 0.020055
2021-12-11 21:46:32,154 iteration 2870 : loss : 0.067281, loss_ce: 0.035453
2021-12-11 21:46:34,879 iteration 2871 : loss : 0.036993, loss_ce: 0.020880
2021-12-11 21:46:37,796 iteration 2872 : loss : 0.043724, loss_ce: 0.019225
2021-12-11 21:46:40,716 iteration 2873 : loss : 0.053392, loss_ce: 0.026551
 42%|███████████▍               | 169/400 [2:23:38<3:08:57, 49.08s/it]2021-12-11 21:46:43,380 iteration 2874 : loss : 0.041532, loss_ce: 0.017279
2021-12-11 21:46:45,950 iteration 2875 : loss : 0.030710, loss_ce: 0.015772
2021-12-11 21:46:48,612 iteration 2876 : loss : 0.039216, loss_ce: 0.020162
2021-12-11 21:46:51,479 iteration 2877 : loss : 0.044063, loss_ce: 0.019245
2021-12-11 21:46:54,421 iteration 2878 : loss : 0.066383, loss_ce: 0.025061
2021-12-11 21:46:57,159 iteration 2879 : loss : 0.038524, loss_ce: 0.021596
2021-12-11 21:47:00,115 iteration 2880 : loss : 0.070194, loss_ce: 0.031163
2021-12-11 21:47:02,885 iteration 2881 : loss : 0.053504, loss_ce: 0.025670
2021-12-11 21:47:05,726 iteration 2882 : loss : 0.083142, loss_ce: 0.027868
2021-12-11 21:47:08,532 iteration 2883 : loss : 0.042961, loss_ce: 0.021752
2021-12-11 21:47:11,590 iteration 2884 : loss : 0.061309, loss_ce: 0.023426
2021-12-11 21:47:14,495 iteration 2885 : loss : 0.041654, loss_ce: 0.023950
2021-12-11 21:47:17,153 iteration 2886 : loss : 0.061832, loss_ce: 0.023697
2021-12-11 21:47:19,894 iteration 2887 : loss : 0.037823, loss_ce: 0.018165
2021-12-11 21:47:22,699 iteration 2888 : loss : 0.041324, loss_ce: 0.020440
2021-12-11 21:47:25,488 iteration 2889 : loss : 0.048649, loss_ce: 0.023546
2021-12-11 21:47:25,489 Training Data Eval:
2021-12-11 21:47:40,369   Average segmentation loss on training set: 0.0274
2021-12-11 21:47:40,370 Validation Data Eval:
2021-12-11 21:47:45,486   Average segmentation loss on validation set: 0.1175
2021-12-11 21:47:48,399 iteration 2890 : loss : 0.068037, loss_ce: 0.030392
 42%|███████████▍               | 170/400 [2:24:46<3:29:31, 54.66s/it]2021-12-11 21:47:51,233 iteration 2891 : loss : 0.054633, loss_ce: 0.032241
2021-12-11 21:47:53,917 iteration 2892 : loss : 0.041512, loss_ce: 0.023362
2021-12-11 21:47:56,637 iteration 2893 : loss : 0.036816, loss_ce: 0.015928
2021-12-11 21:47:59,562 iteration 2894 : loss : 0.028547, loss_ce: 0.014785
2021-12-11 21:48:02,419 iteration 2895 : loss : 0.043361, loss_ce: 0.016959
2021-12-11 21:48:05,193 iteration 2896 : loss : 0.036661, loss_ce: 0.016870
2021-12-11 21:48:07,963 iteration 2897 : loss : 0.051022, loss_ce: 0.023519
2021-12-11 21:48:10,709 iteration 2898 : loss : 0.051665, loss_ce: 0.032184
2021-12-11 21:48:13,462 iteration 2899 : loss : 0.062701, loss_ce: 0.027893
2021-12-11 21:48:16,077 iteration 2900 : loss : 0.039034, loss_ce: 0.016493
2021-12-11 21:48:18,990 iteration 2901 : loss : 0.037896, loss_ce: 0.016477
2021-12-11 21:48:21,699 iteration 2902 : loss : 0.034355, loss_ce: 0.014512
2021-12-11 21:48:24,464 iteration 2903 : loss : 0.039291, loss_ce: 0.021437
2021-12-11 21:48:27,363 iteration 2904 : loss : 0.084790, loss_ce: 0.027141
2021-12-11 21:48:30,262 iteration 2905 : loss : 0.053730, loss_ce: 0.021192
2021-12-11 21:48:33,064 iteration 2906 : loss : 0.057792, loss_ce: 0.026128
2021-12-11 21:48:35,673 iteration 2907 : loss : 0.029752, loss_ce: 0.016003
 43%|███████████▌               | 171/400 [2:25:33<3:20:09, 52.44s/it]2021-12-11 21:48:38,649 iteration 2908 : loss : 0.048627, loss_ce: 0.021797
2021-12-11 21:48:41,307 iteration 2909 : loss : 0.047906, loss_ce: 0.014967
2021-12-11 21:48:44,177 iteration 2910 : loss : 0.046555, loss_ce: 0.019548
2021-12-11 21:48:46,973 iteration 2911 : loss : 0.057345, loss_ce: 0.026365
2021-12-11 21:48:49,892 iteration 2912 : loss : 0.040029, loss_ce: 0.017882
2021-12-11 21:48:52,783 iteration 2913 : loss : 0.051469, loss_ce: 0.022528
2021-12-11 21:48:55,439 iteration 2914 : loss : 0.055329, loss_ce: 0.025175
2021-12-11 21:48:58,275 iteration 2915 : loss : 0.037832, loss_ce: 0.017054
2021-12-11 21:49:01,175 iteration 2916 : loss : 0.037050, loss_ce: 0.021366
2021-12-11 21:49:04,044 iteration 2917 : loss : 0.060545, loss_ce: 0.029004
2021-12-11 21:49:06,761 iteration 2918 : loss : 0.042973, loss_ce: 0.017721
2021-12-11 21:49:09,614 iteration 2919 : loss : 0.051732, loss_ce: 0.026770
2021-12-11 21:49:12,273 iteration 2920 : loss : 0.050680, loss_ce: 0.016899
2021-12-11 21:49:15,155 iteration 2921 : loss : 0.059474, loss_ce: 0.022815
2021-12-11 21:49:17,817 iteration 2922 : loss : 0.058198, loss_ce: 0.029085
2021-12-11 21:49:20,600 iteration 2923 : loss : 0.044402, loss_ce: 0.022823
2021-12-11 21:49:23,323 iteration 2924 : loss : 0.033093, loss_ce: 0.015690
 43%|███████████▌               | 172/400 [2:26:21<3:13:49, 51.01s/it]2021-12-11 21:49:26,398 iteration 2925 : loss : 0.054632, loss_ce: 0.024357
2021-12-11 21:49:29,052 iteration 2926 : loss : 0.040028, loss_ce: 0.016473
2021-12-11 21:49:31,937 iteration 2927 : loss : 0.052674, loss_ce: 0.025533
2021-12-11 21:49:34,724 iteration 2928 : loss : 0.049274, loss_ce: 0.016764
2021-12-11 21:49:37,547 iteration 2929 : loss : 0.037127, loss_ce: 0.016777
2021-12-11 21:49:40,272 iteration 2930 : loss : 0.051063, loss_ce: 0.025258
2021-12-11 21:49:42,959 iteration 2931 : loss : 0.046519, loss_ce: 0.022387
2021-12-11 21:49:45,678 iteration 2932 : loss : 0.039702, loss_ce: 0.017903
2021-12-11 21:49:48,487 iteration 2933 : loss : 0.052333, loss_ce: 0.024168
2021-12-11 21:49:51,298 iteration 2934 : loss : 0.057056, loss_ce: 0.029426
2021-12-11 21:49:54,249 iteration 2935 : loss : 0.048166, loss_ce: 0.018778
2021-12-11 21:49:57,026 iteration 2936 : loss : 0.055216, loss_ce: 0.027251
2021-12-11 21:49:59,784 iteration 2937 : loss : 0.035609, loss_ce: 0.019314
2021-12-11 21:50:02,558 iteration 2938 : loss : 0.037224, loss_ce: 0.017395
2021-12-11 21:50:05,226 iteration 2939 : loss : 0.041438, loss_ce: 0.020351
2021-12-11 21:50:08,175 iteration 2940 : loss : 0.040851, loss_ce: 0.016255
2021-12-11 21:50:10,817 iteration 2941 : loss : 0.045008, loss_ce: 0.018650
 43%|███████████▋               | 173/400 [2:27:08<3:08:59, 49.95s/it]2021-12-11 21:50:13,651 iteration 2942 : loss : 0.045104, loss_ce: 0.019903
2021-12-11 21:50:16,463 iteration 2943 : loss : 0.035014, loss_ce: 0.017506
2021-12-11 21:50:19,029 iteration 2944 : loss : 0.046085, loss_ce: 0.018751
2021-12-11 21:50:21,927 iteration 2945 : loss : 0.050845, loss_ce: 0.024846
2021-12-11 21:50:24,538 iteration 2946 : loss : 0.040413, loss_ce: 0.019473
2021-12-11 21:50:27,284 iteration 2947 : loss : 0.030186, loss_ce: 0.014888
2021-12-11 21:50:30,037 iteration 2948 : loss : 0.044850, loss_ce: 0.018230
2021-12-11 21:50:32,980 iteration 2949 : loss : 0.036438, loss_ce: 0.017223
2021-12-11 21:50:35,752 iteration 2950 : loss : 0.047578, loss_ce: 0.025588
2021-12-11 21:50:38,410 iteration 2951 : loss : 0.041984, loss_ce: 0.019699
2021-12-11 21:50:41,136 iteration 2952 : loss : 0.048417, loss_ce: 0.017662
2021-12-11 21:50:44,030 iteration 2953 : loss : 0.046302, loss_ce: 0.021395
2021-12-11 21:50:46,802 iteration 2954 : loss : 0.063913, loss_ce: 0.023990
2021-12-11 21:50:49,477 iteration 2955 : loss : 0.033611, loss_ce: 0.018006
2021-12-11 21:50:52,230 iteration 2956 : loss : 0.031616, loss_ce: 0.018582
2021-12-11 21:50:54,992 iteration 2957 : loss : 0.066641, loss_ce: 0.022043
2021-12-11 21:50:57,875 iteration 2958 : loss : 0.055668, loss_ce: 0.022046
 44%|███████████▋               | 174/400 [2:27:56<3:04:52, 49.08s/it]2021-12-11 21:51:00,693 iteration 2959 : loss : 0.033904, loss_ce: 0.018579
2021-12-11 21:51:03,633 iteration 2960 : loss : 0.049144, loss_ce: 0.020612
2021-12-11 21:51:06,340 iteration 2961 : loss : 0.031960, loss_ce: 0.018337
2021-12-11 21:51:09,099 iteration 2962 : loss : 0.037710, loss_ce: 0.017156
2021-12-11 21:51:11,854 iteration 2963 : loss : 0.037656, loss_ce: 0.017184
2021-12-11 21:51:14,669 iteration 2964 : loss : 0.045160, loss_ce: 0.019888
2021-12-11 21:51:17,397 iteration 2965 : loss : 0.033432, loss_ce: 0.017538
2021-12-11 21:51:20,232 iteration 2966 : loss : 0.040333, loss_ce: 0.017400
2021-12-11 21:51:22,952 iteration 2967 : loss : 0.057193, loss_ce: 0.021648
2021-12-11 21:51:25,585 iteration 2968 : loss : 0.061514, loss_ce: 0.023209
2021-12-11 21:51:28,275 iteration 2969 : loss : 0.045052, loss_ce: 0.017168
2021-12-11 21:51:31,009 iteration 2970 : loss : 0.032499, loss_ce: 0.017033
2021-12-11 21:51:33,926 iteration 2971 : loss : 0.046746, loss_ce: 0.021408
2021-12-11 21:51:36,658 iteration 2972 : loss : 0.043693, loss_ce: 0.019456
2021-12-11 21:51:39,533 iteration 2973 : loss : 0.067866, loss_ce: 0.031406
2021-12-11 21:51:42,370 iteration 2974 : loss : 0.058451, loss_ce: 0.026646
2021-12-11 21:51:42,371 Training Data Eval:
2021-12-11 21:51:57,335   Average segmentation loss on training set: 0.0233
2021-12-11 21:51:57,335 Validation Data Eval:
2021-12-11 21:52:02,465   Average segmentation loss on validation set: 0.0975
2021-12-11 21:52:05,201 iteration 2975 : loss : 0.062080, loss_ce: 0.023941
 44%|███████████▊               | 175/400 [2:29:03<3:24:35, 54.56s/it]2021-12-11 21:52:08,026 iteration 2976 : loss : 0.045924, loss_ce: 0.022165
2021-12-11 21:52:10,948 iteration 2977 : loss : 0.050174, loss_ce: 0.021879
2021-12-11 21:52:13,644 iteration 2978 : loss : 0.053239, loss_ce: 0.023249
2021-12-11 21:52:16,513 iteration 2979 : loss : 0.084167, loss_ce: 0.028136
2021-12-11 21:52:19,349 iteration 2980 : loss : 0.049003, loss_ce: 0.023813
2021-12-11 21:52:22,262 iteration 2981 : loss : 0.034953, loss_ce: 0.018658
2021-12-11 21:52:24,972 iteration 2982 : loss : 0.046255, loss_ce: 0.019959
2021-12-11 21:52:27,788 iteration 2983 : loss : 0.047142, loss_ce: 0.015869
2021-12-11 21:52:30,527 iteration 2984 : loss : 0.045333, loss_ce: 0.023514
2021-12-11 21:52:33,179 iteration 2985 : loss : 0.059606, loss_ce: 0.027961
2021-12-11 21:52:35,818 iteration 2986 : loss : 0.034134, loss_ce: 0.016903
2021-12-11 21:52:38,460 iteration 2987 : loss : 0.047226, loss_ce: 0.019748
2021-12-11 21:52:41,331 iteration 2988 : loss : 0.033537, loss_ce: 0.016594
2021-12-11 21:52:44,134 iteration 2989 : loss : 0.030184, loss_ce: 0.014531
2021-12-11 21:52:46,824 iteration 2990 : loss : 0.052010, loss_ce: 0.027175
2021-12-11 21:52:49,482 iteration 2991 : loss : 0.031319, loss_ce: 0.013316
2021-12-11 21:52:52,431 iteration 2992 : loss : 0.049051, loss_ce: 0.024476
 44%|███████████▉               | 176/400 [2:29:50<3:15:28, 52.36s/it]2021-12-11 21:52:55,213 iteration 2993 : loss : 0.036314, loss_ce: 0.017270
2021-12-11 21:52:57,990 iteration 2994 : loss : 0.038028, loss_ce: 0.018049
2021-12-11 21:53:00,855 iteration 2995 : loss : 0.050829, loss_ce: 0.023304
2021-12-11 21:53:03,672 iteration 2996 : loss : 0.048718, loss_ce: 0.022069
2021-12-11 21:53:06,290 iteration 2997 : loss : 0.039340, loss_ce: 0.016397
2021-12-11 21:53:09,229 iteration 2998 : loss : 0.035388, loss_ce: 0.020697
2021-12-11 21:53:11,921 iteration 2999 : loss : 0.044727, loss_ce: 0.020081
2021-12-11 21:53:14,649 iteration 3000 : loss : 0.045187, loss_ce: 0.020990
2021-12-11 21:53:17,575 iteration 3001 : loss : 0.042755, loss_ce: 0.022138
2021-12-11 21:53:20,275 iteration 3002 : loss : 0.053800, loss_ce: 0.022161
2021-12-11 21:53:22,919 iteration 3003 : loss : 0.036902, loss_ce: 0.017802
2021-12-11 21:53:25,689 iteration 3004 : loss : 0.039609, loss_ce: 0.022931
2021-12-11 21:53:28,310 iteration 3005 : loss : 0.033481, loss_ce: 0.018096
2021-12-11 21:53:30,924 iteration 3006 : loss : 0.044839, loss_ce: 0.020697
2021-12-11 21:53:33,821 iteration 3007 : loss : 0.037956, loss_ce: 0.015973
2021-12-11 21:53:36,454 iteration 3008 : loss : 0.039264, loss_ce: 0.016983
2021-12-11 21:53:39,056 iteration 3009 : loss : 0.038555, loss_ce: 0.015148
 44%|███████████▉               | 177/400 [2:30:37<3:08:12, 50.64s/it]2021-12-11 21:53:42,030 iteration 3010 : loss : 0.071712, loss_ce: 0.026886
2021-12-11 21:53:44,684 iteration 3011 : loss : 0.048058, loss_ce: 0.017841
2021-12-11 21:53:47,364 iteration 3012 : loss : 0.039025, loss_ce: 0.017387
2021-12-11 21:53:50,388 iteration 3013 : loss : 0.048327, loss_ce: 0.019351
2021-12-11 21:53:53,175 iteration 3014 : loss : 0.051844, loss_ce: 0.023982
2021-12-11 21:53:56,100 iteration 3015 : loss : 0.045200, loss_ce: 0.018151
2021-12-11 21:53:58,930 iteration 3016 : loss : 0.035670, loss_ce: 0.017006
2021-12-11 21:54:01,694 iteration 3017 : loss : 0.076575, loss_ce: 0.042244
2021-12-11 21:54:04,497 iteration 3018 : loss : 0.051283, loss_ce: 0.031348
2021-12-11 21:54:07,558 iteration 3019 : loss : 0.052540, loss_ce: 0.025829
2021-12-11 21:54:10,297 iteration 3020 : loss : 0.038767, loss_ce: 0.016538
2021-12-11 21:54:13,029 iteration 3021 : loss : 0.042173, loss_ce: 0.019742
2021-12-11 21:54:15,805 iteration 3022 : loss : 0.042287, loss_ce: 0.022399
2021-12-11 21:54:18,522 iteration 3023 : loss : 0.033405, loss_ce: 0.016490
2021-12-11 21:54:21,276 iteration 3024 : loss : 0.051850, loss_ce: 0.024821
2021-12-11 21:54:24,144 iteration 3025 : loss : 0.044991, loss_ce: 0.020237
2021-12-11 21:54:26,869 iteration 3026 : loss : 0.048136, loss_ce: 0.020325
 44%|████████████               | 178/400 [2:31:25<3:04:13, 49.79s/it]2021-12-11 21:54:29,645 iteration 3027 : loss : 0.053437, loss_ce: 0.027762
2021-12-11 21:54:32,460 iteration 3028 : loss : 0.051438, loss_ce: 0.021792
2021-12-11 21:54:35,249 iteration 3029 : loss : 0.034988, loss_ce: 0.015054
2021-12-11 21:54:38,114 iteration 3030 : loss : 0.039201, loss_ce: 0.016930
2021-12-11 21:54:40,925 iteration 3031 : loss : 0.060045, loss_ce: 0.021156
2021-12-11 21:54:43,801 iteration 3032 : loss : 0.030182, loss_ce: 0.015579
2021-12-11 21:54:46,707 iteration 3033 : loss : 0.042204, loss_ce: 0.020160
2021-12-11 21:54:49,476 iteration 3034 : loss : 0.032683, loss_ce: 0.014983
2021-12-11 21:54:52,240 iteration 3035 : loss : 0.037938, loss_ce: 0.018891
2021-12-11 21:54:55,189 iteration 3036 : loss : 0.044603, loss_ce: 0.023047
2021-12-11 21:54:57,915 iteration 3037 : loss : 0.046476, loss_ce: 0.023288
2021-12-11 21:55:00,592 iteration 3038 : loss : 0.038412, loss_ce: 0.021231
2021-12-11 21:55:03,276 iteration 3039 : loss : 0.032453, loss_ce: 0.015042
2021-12-11 21:55:06,300 iteration 3040 : loss : 0.063043, loss_ce: 0.022527
2021-12-11 21:55:08,970 iteration 3041 : loss : 0.029231, loss_ce: 0.015035
2021-12-11 21:55:11,556 iteration 3042 : loss : 0.045887, loss_ce: 0.025936
2021-12-11 21:55:14,248 iteration 3043 : loss : 0.048506, loss_ce: 0.020451
 45%|████████████               | 179/400 [2:32:12<3:00:44, 49.07s/it]2021-12-11 21:55:17,154 iteration 3044 : loss : 0.038274, loss_ce: 0.018010
2021-12-11 21:55:19,907 iteration 3045 : loss : 0.044776, loss_ce: 0.017701
2021-12-11 21:55:22,807 iteration 3046 : loss : 0.046402, loss_ce: 0.023399
2021-12-11 21:55:25,612 iteration 3047 : loss : 0.036741, loss_ce: 0.015893
2021-12-11 21:55:28,383 iteration 3048 : loss : 0.042513, loss_ce: 0.018831
2021-12-11 21:55:31,312 iteration 3049 : loss : 0.031303, loss_ce: 0.014324
2021-12-11 21:55:34,129 iteration 3050 : loss : 0.047273, loss_ce: 0.016459
2021-12-11 21:55:36,946 iteration 3051 : loss : 0.048096, loss_ce: 0.022611
2021-12-11 21:55:39,689 iteration 3052 : loss : 0.048353, loss_ce: 0.024433
2021-12-11 21:55:42,385 iteration 3053 : loss : 0.028904, loss_ce: 0.014083
2021-12-11 21:55:45,185 iteration 3054 : loss : 0.042373, loss_ce: 0.020354
2021-12-11 21:55:47,776 iteration 3055 : loss : 0.055723, loss_ce: 0.026598
2021-12-11 21:55:50,431 iteration 3056 : loss : 0.028633, loss_ce: 0.013796
2021-12-11 21:55:53,450 iteration 3057 : loss : 0.051461, loss_ce: 0.022778
2021-12-11 21:55:56,192 iteration 3058 : loss : 0.033786, loss_ce: 0.015997
2021-12-11 21:55:58,769 iteration 3059 : loss : 0.033646, loss_ce: 0.020751
2021-12-11 21:55:58,769 Training Data Eval:
2021-12-11 21:56:13,427   Average segmentation loss on training set: 0.0228
2021-12-11 21:56:13,428 Validation Data Eval:
2021-12-11 21:56:18,647   Average segmentation loss on validation set: 0.0888
2021-12-11 21:56:21,424 iteration 3060 : loss : 0.051614, loss_ce: 0.023177
 45%|████████████▏              | 180/400 [2:33:19<3:19:49, 54.50s/it]2021-12-11 21:56:24,295 iteration 3061 : loss : 0.040627, loss_ce: 0.015420
2021-12-11 21:56:27,194 iteration 3062 : loss : 0.054038, loss_ce: 0.026576
2021-12-11 21:56:29,921 iteration 3063 : loss : 0.040862, loss_ce: 0.019866
2021-12-11 21:56:32,600 iteration 3064 : loss : 0.045170, loss_ce: 0.025249
2021-12-11 21:56:35,457 iteration 3065 : loss : 0.051888, loss_ce: 0.020863
2021-12-11 21:56:38,328 iteration 3066 : loss : 0.041711, loss_ce: 0.016538
2021-12-11 21:56:41,203 iteration 3067 : loss : 0.051309, loss_ce: 0.025138
2021-12-11 21:56:44,212 iteration 3068 : loss : 0.044344, loss_ce: 0.022128
2021-12-11 21:56:46,849 iteration 3069 : loss : 0.020273, loss_ce: 0.011785
2021-12-11 21:56:49,767 iteration 3070 : loss : 0.043069, loss_ce: 0.018065
2021-12-11 21:56:52,709 iteration 3071 : loss : 0.052933, loss_ce: 0.021503
2021-12-11 21:56:55,423 iteration 3072 : loss : 0.039011, loss_ce: 0.020716
2021-12-11 21:56:58,150 iteration 3073 : loss : 0.048209, loss_ce: 0.019221
2021-12-11 21:57:00,788 iteration 3074 : loss : 0.041669, loss_ce: 0.020995
2021-12-11 21:57:03,604 iteration 3075 : loss : 0.039802, loss_ce: 0.016355
2021-12-11 21:57:06,395 iteration 3076 : loss : 0.029443, loss_ce: 0.014515
2021-12-11 21:57:09,166 iteration 3077 : loss : 0.058202, loss_ce: 0.022102
 45%|████████████▏              | 181/400 [2:34:07<3:11:31, 52.47s/it]2021-12-11 21:57:12,085 iteration 3078 : loss : 0.073723, loss_ce: 0.014830
2021-12-11 21:57:14,807 iteration 3079 : loss : 0.037806, loss_ce: 0.017047
2021-12-11 21:57:17,618 iteration 3080 : loss : 0.040539, loss_ce: 0.017128
2021-12-11 21:57:20,343 iteration 3081 : loss : 0.034619, loss_ce: 0.017420
2021-12-11 21:57:23,253 iteration 3082 : loss : 0.053672, loss_ce: 0.022199
2021-12-11 21:57:26,062 iteration 3083 : loss : 0.067975, loss_ce: 0.025526
2021-12-11 21:57:28,861 iteration 3084 : loss : 0.042693, loss_ce: 0.018981
2021-12-11 21:57:31,732 iteration 3085 : loss : 0.039236, loss_ce: 0.018655
2021-12-11 21:57:34,470 iteration 3086 : loss : 0.056696, loss_ce: 0.029188
2021-12-11 21:57:37,354 iteration 3087 : loss : 0.045643, loss_ce: 0.020182
2021-12-11 21:57:40,146 iteration 3088 : loss : 0.047209, loss_ce: 0.023671
2021-12-11 21:57:42,967 iteration 3089 : loss : 0.072062, loss_ce: 0.028897
2021-12-11 21:57:45,709 iteration 3090 : loss : 0.045818, loss_ce: 0.020066
2021-12-11 21:57:48,471 iteration 3091 : loss : 0.050470, loss_ce: 0.025447
2021-12-11 21:57:51,033 iteration 3092 : loss : 0.035853, loss_ce: 0.020545
2021-12-11 21:57:53,691 iteration 3093 : loss : 0.031822, loss_ce: 0.015968
2021-12-11 21:57:56,498 iteration 3094 : loss : 0.087442, loss_ce: 0.030056
 46%|████████████▎              | 182/400 [2:34:54<3:05:02, 50.93s/it]2021-12-11 21:57:59,308 iteration 3095 : loss : 0.032728, loss_ce: 0.016829
2021-12-11 21:58:02,230 iteration 3096 : loss : 0.040258, loss_ce: 0.017889
2021-12-11 21:58:04,895 iteration 3097 : loss : 0.054606, loss_ce: 0.028550
2021-12-11 21:58:07,830 iteration 3098 : loss : 0.044576, loss_ce: 0.017598
2021-12-11 21:58:10,498 iteration 3099 : loss : 0.047763, loss_ce: 0.024867
2021-12-11 21:58:13,265 iteration 3100 : loss : 0.032726, loss_ce: 0.015046
2021-12-11 21:58:16,037 iteration 3101 : loss : 0.031624, loss_ce: 0.014248
2021-12-11 21:58:18,762 iteration 3102 : loss : 0.035690, loss_ce: 0.017640
2021-12-11 21:58:21,660 iteration 3103 : loss : 0.049805, loss_ce: 0.026231
2021-12-11 21:58:24,403 iteration 3104 : loss : 0.052740, loss_ce: 0.020228
2021-12-11 21:58:27,270 iteration 3105 : loss : 0.041741, loss_ce: 0.021384
2021-12-11 21:58:29,929 iteration 3106 : loss : 0.035523, loss_ce: 0.017021
2021-12-11 21:58:32,797 iteration 3107 : loss : 0.037613, loss_ce: 0.017284
2021-12-11 21:58:35,577 iteration 3108 : loss : 0.050557, loss_ce: 0.021658
2021-12-11 21:58:38,283 iteration 3109 : loss : 0.034551, loss_ce: 0.018086
2021-12-11 21:58:41,161 iteration 3110 : loss : 0.039075, loss_ce: 0.019243
2021-12-11 21:58:43,898 iteration 3111 : loss : 0.063152, loss_ce: 0.021644
 46%|████████████▎              | 183/400 [2:35:42<3:00:22, 49.87s/it]2021-12-11 21:58:46,633 iteration 3112 : loss : 0.034191, loss_ce: 0.016153
2021-12-11 21:58:49,358 iteration 3113 : loss : 0.038502, loss_ce: 0.017929
2021-12-11 21:58:51,970 iteration 3114 : loss : 0.040095, loss_ce: 0.021250
2021-12-11 21:58:54,850 iteration 3115 : loss : 0.037476, loss_ce: 0.019157
2021-12-11 21:58:57,556 iteration 3116 : loss : 0.040108, loss_ce: 0.018834
2021-12-11 21:59:00,257 iteration 3117 : loss : 0.068294, loss_ce: 0.022428
2021-12-11 21:59:03,051 iteration 3118 : loss : 0.032741, loss_ce: 0.018341
2021-12-11 21:59:05,842 iteration 3119 : loss : 0.032623, loss_ce: 0.017444
2021-12-11 21:59:08,695 iteration 3120 : loss : 0.037251, loss_ce: 0.016458
2021-12-11 21:59:11,545 iteration 3121 : loss : 0.076775, loss_ce: 0.024239
2021-12-11 21:59:14,178 iteration 3122 : loss : 0.042446, loss_ce: 0.021960
2021-12-11 21:59:17,018 iteration 3123 : loss : 0.067698, loss_ce: 0.026404
2021-12-11 21:59:19,735 iteration 3124 : loss : 0.040191, loss_ce: 0.019332
2021-12-11 21:59:22,337 iteration 3125 : loss : 0.028269, loss_ce: 0.015705
2021-12-11 21:59:24,962 iteration 3126 : loss : 0.034377, loss_ce: 0.015985
2021-12-11 21:59:27,713 iteration 3127 : loss : 0.036929, loss_ce: 0.019473
2021-12-11 21:59:30,377 iteration 3128 : loss : 0.062637, loss_ce: 0.018266
 46%|████████████▍              | 184/400 [2:36:28<2:55:52, 48.85s/it]2021-12-11 21:59:33,147 iteration 3129 : loss : 0.057592, loss_ce: 0.020735
2021-12-11 21:59:35,943 iteration 3130 : loss : 0.032023, loss_ce: 0.015899
2021-12-11 21:59:38,673 iteration 3131 : loss : 0.054945, loss_ce: 0.026944
2021-12-11 21:59:41,599 iteration 3132 : loss : 0.047426, loss_ce: 0.019496
2021-12-11 21:59:44,201 iteration 3133 : loss : 0.034683, loss_ce: 0.017530
2021-12-11 21:59:47,130 iteration 3134 : loss : 0.039377, loss_ce: 0.019127
2021-12-11 21:59:49,826 iteration 3135 : loss : 0.060044, loss_ce: 0.017587
2021-12-11 21:59:52,573 iteration 3136 : loss : 0.032712, loss_ce: 0.018888
2021-12-11 21:59:55,268 iteration 3137 : loss : 0.029067, loss_ce: 0.015680
2021-12-11 21:59:57,966 iteration 3138 : loss : 0.043488, loss_ce: 0.020369
2021-12-11 22:00:00,883 iteration 3139 : loss : 0.057811, loss_ce: 0.021810
2021-12-11 22:00:03,816 iteration 3140 : loss : 0.034482, loss_ce: 0.016364
2021-12-11 22:00:06,541 iteration 3141 : loss : 0.045761, loss_ce: 0.023595
2021-12-11 22:00:09,237 iteration 3142 : loss : 0.045960, loss_ce: 0.022567
2021-12-11 22:00:12,026 iteration 3143 : loss : 0.082157, loss_ce: 0.029017
2021-12-11 22:00:14,993 iteration 3144 : loss : 0.071138, loss_ce: 0.022887
2021-12-11 22:00:14,994 Training Data Eval:
2021-12-11 22:00:29,626   Average segmentation loss on training set: 0.0234
2021-12-11 22:00:29,627 Validation Data Eval:
2021-12-11 22:00:34,853   Average segmentation loss on validation set: 0.0836
2021-12-11 22:00:37,462 iteration 3145 : loss : 0.034468, loss_ce: 0.018189
 46%|████████████▍              | 185/400 [2:37:35<3:14:39, 54.32s/it]2021-12-11 22:00:40,383 iteration 3146 : loss : 0.047599, loss_ce: 0.022432
2021-12-11 22:00:43,195 iteration 3147 : loss : 0.049768, loss_ce: 0.023092
2021-12-11 22:00:45,961 iteration 3148 : loss : 0.035041, loss_ce: 0.015766
2021-12-11 22:00:48,592 iteration 3149 : loss : 0.054671, loss_ce: 0.018373
2021-12-11 22:00:51,370 iteration 3150 : loss : 0.058958, loss_ce: 0.019702
2021-12-11 22:00:54,038 iteration 3151 : loss : 0.037556, loss_ce: 0.020538
2021-12-11 22:00:56,981 iteration 3152 : loss : 0.040998, loss_ce: 0.025071
2021-12-11 22:00:59,602 iteration 3153 : loss : 0.042768, loss_ce: 0.022761
2021-12-11 22:01:02,410 iteration 3154 : loss : 0.040760, loss_ce: 0.015302
2021-12-11 22:01:05,159 iteration 3155 : loss : 0.036045, loss_ce: 0.016736
2021-12-11 22:01:07,958 iteration 3156 : loss : 0.041351, loss_ce: 0.018715
2021-12-11 22:01:10,651 iteration 3157 : loss : 0.046585, loss_ce: 0.015543
2021-12-11 22:01:13,547 iteration 3158 : loss : 0.030986, loss_ce: 0.015579
2021-12-11 22:01:16,212 iteration 3159 : loss : 0.050671, loss_ce: 0.026473
2021-12-11 22:01:19,045 iteration 3160 : loss : 0.038832, loss_ce: 0.019887
2021-12-11 22:01:21,883 iteration 3161 : loss : 0.042701, loss_ce: 0.017943
2021-12-11 22:01:24,682 iteration 3162 : loss : 0.043922, loss_ce: 0.025924
 46%|████████████▌              | 186/400 [2:38:22<3:06:09, 52.19s/it]2021-12-11 22:01:27,419 iteration 3163 : loss : 0.041490, loss_ce: 0.017203
2021-12-11 22:01:30,122 iteration 3164 : loss : 0.047522, loss_ce: 0.017454
2021-12-11 22:01:32,850 iteration 3165 : loss : 0.026187, loss_ce: 0.014119
2021-12-11 22:01:35,633 iteration 3166 : loss : 0.047123, loss_ce: 0.024820
2021-12-11 22:01:38,563 iteration 3167 : loss : 0.079942, loss_ce: 0.031275
2021-12-11 22:01:41,215 iteration 3168 : loss : 0.036673, loss_ce: 0.020916
2021-12-11 22:01:43,953 iteration 3169 : loss : 0.029439, loss_ce: 0.013133
2021-12-11 22:01:46,888 iteration 3170 : loss : 0.043051, loss_ce: 0.019651
2021-12-11 22:01:49,757 iteration 3171 : loss : 0.058853, loss_ce: 0.020227
2021-12-11 22:01:52,499 iteration 3172 : loss : 0.044144, loss_ce: 0.022684
2021-12-11 22:01:55,203 iteration 3173 : loss : 0.041781, loss_ce: 0.018922
2021-12-11 22:01:58,043 iteration 3174 : loss : 0.039365, loss_ce: 0.016326
2021-12-11 22:02:00,905 iteration 3175 : loss : 0.049826, loss_ce: 0.024847
2021-12-11 22:02:03,691 iteration 3176 : loss : 0.041812, loss_ce: 0.022743
2021-12-11 22:02:06,430 iteration 3177 : loss : 0.034264, loss_ce: 0.017929
2021-12-11 22:02:09,205 iteration 3178 : loss : 0.048896, loss_ce: 0.017492
2021-12-11 22:02:12,154 iteration 3179 : loss : 0.066467, loss_ce: 0.023299
 47%|████████████▌              | 187/400 [2:39:10<3:00:15, 50.78s/it]2021-12-11 22:02:14,994 iteration 3180 : loss : 0.034596, loss_ce: 0.017612
2021-12-11 22:02:17,752 iteration 3181 : loss : 0.042772, loss_ce: 0.019916
2021-12-11 22:02:20,537 iteration 3182 : loss : 0.039844, loss_ce: 0.019120
2021-12-11 22:02:23,344 iteration 3183 : loss : 0.041293, loss_ce: 0.019822
2021-12-11 22:02:26,099 iteration 3184 : loss : 0.035109, loss_ce: 0.019042
2021-12-11 22:02:28,741 iteration 3185 : loss : 0.045199, loss_ce: 0.020473
2021-12-11 22:02:31,675 iteration 3186 : loss : 0.042760, loss_ce: 0.023637
2021-12-11 22:02:34,292 iteration 3187 : loss : 0.029954, loss_ce: 0.013724
2021-12-11 22:02:37,067 iteration 3188 : loss : 0.039197, loss_ce: 0.015160
2021-12-11 22:02:39,836 iteration 3189 : loss : 0.028683, loss_ce: 0.016227
2021-12-11 22:02:42,697 iteration 3190 : loss : 0.067772, loss_ce: 0.028309
2021-12-11 22:02:45,468 iteration 3191 : loss : 0.035157, loss_ce: 0.015957
2021-12-11 22:02:48,188 iteration 3192 : loss : 0.030510, loss_ce: 0.016080
2021-12-11 22:02:50,957 iteration 3193 : loss : 0.035879, loss_ce: 0.018868
2021-12-11 22:02:53,689 iteration 3194 : loss : 0.042221, loss_ce: 0.017205
2021-12-11 22:02:56,438 iteration 3195 : loss : 0.042324, loss_ce: 0.017651
2021-12-11 22:02:59,214 iteration 3196 : loss : 0.034055, loss_ce: 0.014737
 47%|████████████▋              | 188/400 [2:39:57<2:55:28, 49.66s/it]2021-12-11 22:03:02,016 iteration 3197 : loss : 0.038397, loss_ce: 0.017003
2021-12-11 22:03:04,618 iteration 3198 : loss : 0.037439, loss_ce: 0.017724
2021-12-11 22:03:07,462 iteration 3199 : loss : 0.033863, loss_ce: 0.016818
2021-12-11 22:03:10,153 iteration 3200 : loss : 0.037496, loss_ce: 0.017777
2021-12-11 22:03:12,980 iteration 3201 : loss : 0.041481, loss_ce: 0.019295
2021-12-11 22:03:15,775 iteration 3202 : loss : 0.057066, loss_ce: 0.028171
2021-12-11 22:03:18,556 iteration 3203 : loss : 0.089031, loss_ce: 0.023778
2021-12-11 22:03:21,389 iteration 3204 : loss : 0.047349, loss_ce: 0.024808
2021-12-11 22:03:24,232 iteration 3205 : loss : 0.037363, loss_ce: 0.016278
2021-12-11 22:03:26,882 iteration 3206 : loss : 0.034804, loss_ce: 0.017216
2021-12-11 22:03:29,552 iteration 3207 : loss : 0.036526, loss_ce: 0.014546
2021-12-11 22:03:32,371 iteration 3208 : loss : 0.050203, loss_ce: 0.020706
2021-12-11 22:03:35,249 iteration 3209 : loss : 0.039415, loss_ce: 0.018744
2021-12-11 22:03:38,057 iteration 3210 : loss : 0.049012, loss_ce: 0.018501
2021-12-11 22:03:40,903 iteration 3211 : loss : 0.051719, loss_ce: 0.025090
2021-12-11 22:03:43,626 iteration 3212 : loss : 0.047010, loss_ce: 0.025677
2021-12-11 22:03:46,582 iteration 3213 : loss : 0.048445, loss_ce: 0.026912
 47%|████████████▊              | 189/400 [2:40:44<2:52:13, 48.97s/it]2021-12-11 22:03:49,452 iteration 3214 : loss : 0.037096, loss_ce: 0.018991
2021-12-11 22:03:52,206 iteration 3215 : loss : 0.029669, loss_ce: 0.015513
2021-12-11 22:03:54,887 iteration 3216 : loss : 0.047924, loss_ce: 0.022836
2021-12-11 22:03:57,620 iteration 3217 : loss : 0.067932, loss_ce: 0.023272
2021-12-11 22:04:00,309 iteration 3218 : loss : 0.037217, loss_ce: 0.019265
2021-12-11 22:04:03,039 iteration 3219 : loss : 0.032252, loss_ce: 0.013747
2021-12-11 22:04:05,790 iteration 3220 : loss : 0.048033, loss_ce: 0.025702
2021-12-11 22:04:08,594 iteration 3221 : loss : 0.053176, loss_ce: 0.021627
2021-12-11 22:04:11,393 iteration 3222 : loss : 0.024899, loss_ce: 0.012473
2021-12-11 22:04:14,183 iteration 3223 : loss : 0.072275, loss_ce: 0.022010
2021-12-11 22:04:16,925 iteration 3224 : loss : 0.031840, loss_ce: 0.014463
2021-12-11 22:04:19,755 iteration 3225 : loss : 0.042452, loss_ce: 0.017508
2021-12-11 22:04:22,466 iteration 3226 : loss : 0.035290, loss_ce: 0.017901
2021-12-11 22:04:25,424 iteration 3227 : loss : 0.040368, loss_ce: 0.019057
2021-12-11 22:04:28,226 iteration 3228 : loss : 0.058095, loss_ce: 0.027402
2021-12-11 22:04:31,039 iteration 3229 : loss : 0.045938, loss_ce: 0.016970
2021-12-11 22:04:31,039 Training Data Eval:
2021-12-11 22:04:45,788   Average segmentation loss on training set: 0.0258
2021-12-11 22:04:45,789 Validation Data Eval:
2021-12-11 22:04:50,920   Average segmentation loss on validation set: 0.1197
2021-12-11 22:04:53,524 iteration 3230 : loss : 0.042648, loss_ce: 0.023341
 48%|████████████▊              | 190/400 [2:41:51<3:10:16, 54.36s/it]2021-12-11 22:04:56,176 iteration 3231 : loss : 0.031466, loss_ce: 0.015256
2021-12-11 22:04:59,005 iteration 3232 : loss : 0.032691, loss_ce: 0.017381
2021-12-11 22:05:01,804 iteration 3233 : loss : 0.038997, loss_ce: 0.014994
2021-12-11 22:05:04,539 iteration 3234 : loss : 0.036905, loss_ce: 0.017845
2021-12-11 22:05:07,286 iteration 3235 : loss : 0.033138, loss_ce: 0.015917
2021-12-11 22:05:09,831 iteration 3236 : loss : 0.045497, loss_ce: 0.026429
2021-12-11 22:05:12,615 iteration 3237 : loss : 0.028031, loss_ce: 0.014979
2021-12-11 22:05:15,399 iteration 3238 : loss : 0.043677, loss_ce: 0.020220
2021-12-11 22:05:18,231 iteration 3239 : loss : 0.032962, loss_ce: 0.016907
2021-12-11 22:05:21,160 iteration 3240 : loss : 0.064412, loss_ce: 0.032389
2021-12-11 22:05:23,996 iteration 3241 : loss : 0.045867, loss_ce: 0.020043
2021-12-11 22:05:26,810 iteration 3242 : loss : 0.043095, loss_ce: 0.021640
2021-12-11 22:05:29,748 iteration 3243 : loss : 0.034074, loss_ce: 0.017780
2021-12-11 22:05:32,374 iteration 3244 : loss : 0.044992, loss_ce: 0.017658
2021-12-11 22:05:35,296 iteration 3245 : loss : 0.050830, loss_ce: 0.019194
2021-12-11 22:05:38,132 iteration 3246 : loss : 0.048213, loss_ce: 0.019657
2021-12-11 22:05:40,971 iteration 3247 : loss : 0.057001, loss_ce: 0.023797
 48%|████████████▉              | 191/400 [2:42:39<3:02:08, 52.29s/it]2021-12-11 22:05:43,718 iteration 3248 : loss : 0.078929, loss_ce: 0.023522
2021-12-11 22:05:46,677 iteration 3249 : loss : 0.045593, loss_ce: 0.019039
2021-12-11 22:05:49,370 iteration 3250 : loss : 0.043114, loss_ce: 0.018185
2021-12-11 22:05:52,100 iteration 3251 : loss : 0.032742, loss_ce: 0.013919
2021-12-11 22:05:54,870 iteration 3252 : loss : 0.027580, loss_ce: 0.015757
2021-12-11 22:05:57,458 iteration 3253 : loss : 0.046303, loss_ce: 0.017907
2021-12-11 22:06:00,308 iteration 3254 : loss : 0.041903, loss_ce: 0.022289
2021-12-11 22:06:03,185 iteration 3255 : loss : 0.044420, loss_ce: 0.018988
2021-12-11 22:06:06,024 iteration 3256 : loss : 0.043013, loss_ce: 0.021140
2021-12-11 22:06:08,793 iteration 3257 : loss : 0.052380, loss_ce: 0.024111
2021-12-11 22:06:11,466 iteration 3258 : loss : 0.033347, loss_ce: 0.012345
2021-12-11 22:06:14,243 iteration 3259 : loss : 0.044462, loss_ce: 0.020622
2021-12-11 22:06:17,029 iteration 3260 : loss : 0.034116, loss_ce: 0.017190
2021-12-11 22:06:19,683 iteration 3261 : loss : 0.040178, loss_ce: 0.018123
2021-12-11 22:06:22,584 iteration 3262 : loss : 0.054174, loss_ce: 0.022330
2021-12-11 22:06:25,457 iteration 3263 : loss : 0.042206, loss_ce: 0.018015
2021-12-11 22:06:28,256 iteration 3264 : loss : 0.058144, loss_ce: 0.023495
 48%|████████████▉              | 192/400 [2:43:26<2:56:03, 50.79s/it]2021-12-11 22:06:31,102 iteration 3265 : loss : 0.050429, loss_ce: 0.022877
2021-12-11 22:06:33,810 iteration 3266 : loss : 0.039070, loss_ce: 0.017019
2021-12-11 22:06:36,740 iteration 3267 : loss : 0.027441, loss_ce: 0.012721
2021-12-11 22:06:39,469 iteration 3268 : loss : 0.040626, loss_ce: 0.018335
2021-12-11 22:06:42,058 iteration 3269 : loss : 0.039790, loss_ce: 0.021694
2021-12-11 22:06:44,712 iteration 3270 : loss : 0.039098, loss_ce: 0.017985
2021-12-11 22:06:47,390 iteration 3271 : loss : 0.047666, loss_ce: 0.018134
2021-12-11 22:06:50,226 iteration 3272 : loss : 0.048783, loss_ce: 0.021314
2021-12-11 22:06:52,874 iteration 3273 : loss : 0.035757, loss_ce: 0.018115
2021-12-11 22:06:55,499 iteration 3274 : loss : 0.064901, loss_ce: 0.026810
2021-12-11 22:06:58,304 iteration 3275 : loss : 0.034089, loss_ce: 0.017503
2021-12-11 22:07:01,095 iteration 3276 : loss : 0.054861, loss_ce: 0.018948
2021-12-11 22:07:03,703 iteration 3277 : loss : 0.044332, loss_ce: 0.017514
2021-12-11 22:07:06,471 iteration 3278 : loss : 0.028973, loss_ce: 0.015237
2021-12-11 22:07:09,025 iteration 3279 : loss : 0.051371, loss_ce: 0.033819
2021-12-11 22:07:11,986 iteration 3280 : loss : 0.058324, loss_ce: 0.026784
2021-12-11 22:07:14,806 iteration 3281 : loss : 0.030236, loss_ce: 0.013048
 48%|█████████████              | 193/400 [2:44:12<2:50:49, 49.51s/it]2021-12-11 22:07:17,674 iteration 3282 : loss : 0.034962, loss_ce: 0.013506
2021-12-11 22:07:20,451 iteration 3283 : loss : 0.046984, loss_ce: 0.020400
2021-12-11 22:07:23,257 iteration 3284 : loss : 0.040465, loss_ce: 0.020827
2021-12-11 22:07:26,023 iteration 3285 : loss : 0.056701, loss_ce: 0.022728
2021-12-11 22:07:28,883 iteration 3286 : loss : 0.039093, loss_ce: 0.015899
2021-12-11 22:07:31,735 iteration 3287 : loss : 0.061575, loss_ce: 0.030291
2021-12-11 22:07:34,441 iteration 3288 : loss : 0.036234, loss_ce: 0.018457
2021-12-11 22:07:37,382 iteration 3289 : loss : 0.033258, loss_ce: 0.015207
2021-12-11 22:07:40,244 iteration 3290 : loss : 0.042898, loss_ce: 0.019109
2021-12-11 22:07:43,057 iteration 3291 : loss : 0.073504, loss_ce: 0.037487
2021-12-11 22:07:45,954 iteration 3292 : loss : 0.039994, loss_ce: 0.016951
2021-12-11 22:07:48,751 iteration 3293 : loss : 0.050798, loss_ce: 0.019013
2021-12-11 22:07:51,583 iteration 3294 : loss : 0.042703, loss_ce: 0.021793
2021-12-11 22:07:54,446 iteration 3295 : loss : 0.036535, loss_ce: 0.019414
2021-12-11 22:07:57,213 iteration 3296 : loss : 0.033460, loss_ce: 0.015641
2021-12-11 22:07:59,824 iteration 3297 : loss : 0.034109, loss_ce: 0.016096
2021-12-11 22:08:02,741 iteration 3298 : loss : 0.036579, loss_ce: 0.020751
 48%|█████████████              | 194/400 [2:45:00<2:48:22, 49.04s/it]2021-12-11 22:08:05,500 iteration 3299 : loss : 0.031093, loss_ce: 0.012350
2021-12-11 22:08:08,321 iteration 3300 : loss : 0.037664, loss_ce: 0.018243
2021-12-11 22:08:11,142 iteration 3301 : loss : 0.042732, loss_ce: 0.022493
2021-12-11 22:08:13,927 iteration 3302 : loss : 0.039339, loss_ce: 0.015083
2021-12-11 22:08:16,697 iteration 3303 : loss : 0.048273, loss_ce: 0.023712
2021-12-11 22:08:19,527 iteration 3304 : loss : 0.031248, loss_ce: 0.015209
2021-12-11 22:08:22,214 iteration 3305 : loss : 0.037336, loss_ce: 0.016195
2021-12-11 22:08:24,984 iteration 3306 : loss : 0.035024, loss_ce: 0.013982
2021-12-11 22:08:27,793 iteration 3307 : loss : 0.056358, loss_ce: 0.026804
2021-12-11 22:08:30,747 iteration 3308 : loss : 0.044663, loss_ce: 0.023426
2021-12-11 22:08:33,592 iteration 3309 : loss : 0.030681, loss_ce: 0.015950
2021-12-11 22:08:36,265 iteration 3310 : loss : 0.032934, loss_ce: 0.019940
2021-12-11 22:08:39,234 iteration 3311 : loss : 0.057611, loss_ce: 0.024801
2021-12-11 22:08:41,945 iteration 3312 : loss : 0.055930, loss_ce: 0.017913
2021-12-11 22:08:44,796 iteration 3313 : loss : 0.047948, loss_ce: 0.023254
2021-12-11 22:08:47,528 iteration 3314 : loss : 0.044730, loss_ce: 0.017953
2021-12-11 22:08:47,529 Training Data Eval:
2021-12-11 22:09:02,232   Average segmentation loss on training set: 0.0210
2021-12-11 22:09:02,233 Validation Data Eval:
2021-12-11 22:09:07,359   Average segmentation loss on validation set: 0.0837
2021-12-11 22:09:10,125 iteration 3315 : loss : 0.029516, loss_ce: 0.014302
 49%|█████████████▏             | 195/400 [2:46:08<3:06:21, 54.54s/it]2021-12-11 22:09:12,920 iteration 3316 : loss : 0.029746, loss_ce: 0.015549
2021-12-11 22:09:15,634 iteration 3317 : loss : 0.045626, loss_ce: 0.023846
2021-12-11 22:09:18,396 iteration 3318 : loss : 0.060646, loss_ce: 0.028429
2021-12-11 22:09:21,184 iteration 3319 : loss : 0.035610, loss_ce: 0.016448
2021-12-11 22:09:24,040 iteration 3320 : loss : 0.060887, loss_ce: 0.025699
2021-12-11 22:09:26,696 iteration 3321 : loss : 0.037160, loss_ce: 0.016028
2021-12-11 22:09:29,569 iteration 3322 : loss : 0.054820, loss_ce: 0.019651
2021-12-11 22:09:32,435 iteration 3323 : loss : 0.034956, loss_ce: 0.014627
2021-12-11 22:09:35,235 iteration 3324 : loss : 0.057070, loss_ce: 0.029107
2021-12-11 22:09:38,037 iteration 3325 : loss : 0.043793, loss_ce: 0.019986
2021-12-11 22:09:40,800 iteration 3326 : loss : 0.042022, loss_ce: 0.018782
2021-12-11 22:09:43,551 iteration 3327 : loss : 0.034360, loss_ce: 0.014968
2021-12-11 22:09:46,327 iteration 3328 : loss : 0.034041, loss_ce: 0.015315
2021-12-11 22:09:49,147 iteration 3329 : loss : 0.034806, loss_ce: 0.017139
2021-12-11 22:09:51,728 iteration 3330 : loss : 0.033479, loss_ce: 0.020132
2021-12-11 22:09:54,397 iteration 3331 : loss : 0.038745, loss_ce: 0.019908
2021-12-11 22:09:57,197 iteration 3332 : loss : 0.036618, loss_ce: 0.015130
 49%|█████████████▏             | 196/400 [2:46:55<2:57:49, 52.30s/it]2021-12-11 22:10:00,184 iteration 3333 : loss : 0.045394, loss_ce: 0.020136
2021-12-11 22:10:03,115 iteration 3334 : loss : 0.051894, loss_ce: 0.028171
2021-12-11 22:10:05,977 iteration 3335 : loss : 0.051802, loss_ce: 0.022801
2021-12-11 22:10:08,650 iteration 3336 : loss : 0.041617, loss_ce: 0.017400
2021-12-11 22:10:11,417 iteration 3337 : loss : 0.066777, loss_ce: 0.024048
2021-12-11 22:10:14,250 iteration 3338 : loss : 0.050152, loss_ce: 0.023445
2021-12-11 22:10:16,906 iteration 3339 : loss : 0.042050, loss_ce: 0.019845
2021-12-11 22:10:19,621 iteration 3340 : loss : 0.050319, loss_ce: 0.015564
2021-12-11 22:10:22,380 iteration 3341 : loss : 0.047379, loss_ce: 0.021636
2021-12-11 22:10:25,088 iteration 3342 : loss : 0.030601, loss_ce: 0.017431
2021-12-11 22:10:27,938 iteration 3343 : loss : 0.029082, loss_ce: 0.016023
2021-12-11 22:10:30,783 iteration 3344 : loss : 0.037404, loss_ce: 0.016417
2021-12-11 22:10:33,686 iteration 3345 : loss : 0.043448, loss_ce: 0.022586
2021-12-11 22:10:36,413 iteration 3346 : loss : 0.034822, loss_ce: 0.018940
2021-12-11 22:10:39,181 iteration 3347 : loss : 0.029899, loss_ce: 0.014888
2021-12-11 22:10:41,906 iteration 3348 : loss : 0.040194, loss_ce: 0.017537
2021-12-11 22:10:44,765 iteration 3349 : loss : 0.058744, loss_ce: 0.019431
 49%|█████████████▎             | 197/400 [2:47:42<2:52:09, 50.88s/it]2021-12-11 22:10:47,571 iteration 3350 : loss : 0.050801, loss_ce: 0.021284
2021-12-11 22:10:50,518 iteration 3351 : loss : 0.037999, loss_ce: 0.015629
2021-12-11 22:10:53,362 iteration 3352 : loss : 0.060792, loss_ce: 0.028326
2021-12-11 22:10:56,260 iteration 3353 : loss : 0.038447, loss_ce: 0.015629
2021-12-11 22:10:58,970 iteration 3354 : loss : 0.029715, loss_ce: 0.015849
2021-12-11 22:11:01,818 iteration 3355 : loss : 0.043668, loss_ce: 0.022716
2021-12-11 22:11:04,535 iteration 3356 : loss : 0.043631, loss_ce: 0.019503
2021-12-11 22:11:07,443 iteration 3357 : loss : 0.050597, loss_ce: 0.017799
2021-12-11 22:11:10,333 iteration 3358 : loss : 0.032787, loss_ce: 0.017444
2021-12-11 22:11:13,035 iteration 3359 : loss : 0.035035, loss_ce: 0.014803
2021-12-11 22:11:15,635 iteration 3360 : loss : 0.054115, loss_ce: 0.023539
2021-12-11 22:11:18,469 iteration 3361 : loss : 0.042712, loss_ce: 0.017394
2021-12-11 22:11:21,371 iteration 3362 : loss : 0.040000, loss_ce: 0.017370
2021-12-11 22:11:24,267 iteration 3363 : loss : 0.056585, loss_ce: 0.021773
2021-12-11 22:11:26,947 iteration 3364 : loss : 0.035780, loss_ce: 0.017394
2021-12-11 22:11:29,795 iteration 3365 : loss : 0.055594, loss_ce: 0.024841
2021-12-11 22:11:32,467 iteration 3366 : loss : 0.026725, loss_ce: 0.014266
 50%|█████████████▎             | 198/400 [2:48:30<2:48:05, 49.93s/it]2021-12-11 22:11:35,295 iteration 3367 : loss : 0.032666, loss_ce: 0.015183
2021-12-11 22:11:38,061 iteration 3368 : loss : 0.029582, loss_ce: 0.017752
2021-12-11 22:11:40,803 iteration 3369 : loss : 0.039991, loss_ce: 0.015274
2021-12-11 22:11:43,580 iteration 3370 : loss : 0.049443, loss_ce: 0.025434
2021-12-11 22:11:46,476 iteration 3371 : loss : 0.043446, loss_ce: 0.020006
2021-12-11 22:11:49,076 iteration 3372 : loss : 0.025338, loss_ce: 0.013741
2021-12-11 22:11:51,770 iteration 3373 : loss : 0.038225, loss_ce: 0.016992
2021-12-11 22:11:54,573 iteration 3374 : loss : 0.033145, loss_ce: 0.015010
2021-12-11 22:11:57,198 iteration 3375 : loss : 0.047098, loss_ce: 0.021150
2021-12-11 22:12:00,143 iteration 3376 : loss : 0.038943, loss_ce: 0.015924
2021-12-11 22:12:02,694 iteration 3377 : loss : 0.023968, loss_ce: 0.013480
2021-12-11 22:12:05,481 iteration 3378 : loss : 0.060909, loss_ce: 0.018926
2021-12-11 22:12:08,370 iteration 3379 : loss : 0.057744, loss_ce: 0.028661
2021-12-11 22:12:11,202 iteration 3380 : loss : 0.045304, loss_ce: 0.018773
2021-12-11 22:12:13,969 iteration 3381 : loss : 0.067203, loss_ce: 0.026735
2021-12-11 22:12:16,760 iteration 3382 : loss : 0.039721, loss_ce: 0.018480
2021-12-11 22:12:19,480 iteration 3383 : loss : 0.032587, loss_ce: 0.014981
 50%|█████████████▍             | 199/400 [2:49:17<2:44:19, 49.05s/it]2021-12-11 22:12:22,389 iteration 3384 : loss : 0.041207, loss_ce: 0.017539
2021-12-11 22:12:25,133 iteration 3385 : loss : 0.044365, loss_ce: 0.019978
2021-12-11 22:12:27,965 iteration 3386 : loss : 0.032343, loss_ce: 0.013324
2021-12-11 22:12:30,783 iteration 3387 : loss : 0.046929, loss_ce: 0.022160
2021-12-11 22:12:33,660 iteration 3388 : loss : 0.040407, loss_ce: 0.021464
2021-12-11 22:12:36,279 iteration 3389 : loss : 0.027084, loss_ce: 0.014470
2021-12-11 22:12:39,036 iteration 3390 : loss : 0.023258, loss_ce: 0.012476
2021-12-11 22:12:41,946 iteration 3391 : loss : 0.045748, loss_ce: 0.021224
2021-12-11 22:12:44,721 iteration 3392 : loss : 0.049122, loss_ce: 0.019653
2021-12-11 22:12:47,607 iteration 3393 : loss : 0.038298, loss_ce: 0.019385
2021-12-11 22:12:50,460 iteration 3394 : loss : 0.034196, loss_ce: 0.018502
2021-12-11 22:12:53,348 iteration 3395 : loss : 0.028964, loss_ce: 0.014697
2021-12-11 22:12:56,173 iteration 3396 : loss : 0.042302, loss_ce: 0.019161
2021-12-11 22:12:59,015 iteration 3397 : loss : 0.040711, loss_ce: 0.019388
2021-12-11 22:13:01,944 iteration 3398 : loss : 0.033272, loss_ce: 0.017622
2021-12-11 22:13:04,713 iteration 3399 : loss : 0.045737, loss_ce: 0.020963
2021-12-11 22:13:04,713 Training Data Eval:
2021-12-11 22:13:19,288   Average segmentation loss on training set: 0.0208
2021-12-11 22:13:19,288 Validation Data Eval:
2021-12-11 22:13:24,496   Average segmentation loss on validation set: 0.0899
2021-12-11 22:13:27,085 iteration 3400 : loss : 0.041050, loss_ce: 0.017195
2021-12-11 22:13:29,103 save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_seed2epoch_199.pth
 50%|█████████████▌             | 200/400 [2:50:27<3:04:01, 55.21s/it]2021-12-11 22:13:31,420 iteration 3401 : loss : 0.044299, loss_ce: 0.025196
2021-12-11 22:13:34,168 iteration 3402 : loss : 0.031074, loss_ce: 0.014951
2021-12-11 22:13:37,042 iteration 3403 : loss : 0.056788, loss_ce: 0.025708
2021-12-11 22:13:39,847 iteration 3404 : loss : 0.039914, loss_ce: 0.018436
2021-12-11 22:13:42,379 iteration 3405 : loss : 0.039685, loss_ce: 0.015128
2021-12-11 22:13:45,351 iteration 3406 : loss : 0.033031, loss_ce: 0.015147
2021-12-11 22:13:48,136 iteration 3407 : loss : 0.045715, loss_ce: 0.021061
2021-12-11 22:13:50,911 iteration 3408 : loss : 0.038560, loss_ce: 0.016540
2021-12-11 22:13:53,732 iteration 3409 : loss : 0.027178, loss_ce: 0.012984
2021-12-11 22:13:56,585 iteration 3410 : loss : 0.034810, loss_ce: 0.017158
2021-12-11 22:13:59,524 iteration 3411 : loss : 0.060856, loss_ce: 0.027601
2021-12-11 22:14:02,410 iteration 3412 : loss : 0.036281, loss_ce: 0.018689
2021-12-11 22:14:05,056 iteration 3413 : loss : 0.034799, loss_ce: 0.011452
2021-12-11 22:14:07,877 iteration 3414 : loss : 0.040555, loss_ce: 0.019307
2021-12-11 22:14:10,808 iteration 3415 : loss : 0.029882, loss_ce: 0.015945
2021-12-11 22:14:13,639 iteration 3416 : loss : 0.029025, loss_ce: 0.013906
2021-12-11 22:14:16,505 iteration 3417 : loss : 0.045616, loss_ce: 0.019144
 50%|█████████████▌             | 201/400 [2:51:14<2:55:23, 52.88s/it]2021-12-11 22:14:19,366 iteration 3418 : loss : 0.025837, loss_ce: 0.012328
2021-12-11 22:14:22,087 iteration 3419 : loss : 0.029399, loss_ce: 0.016089
2021-12-11 22:14:24,834 iteration 3420 : loss : 0.033167, loss_ce: 0.015003
2021-12-11 22:14:27,480 iteration 3421 : loss : 0.029982, loss_ce: 0.015105
2021-12-11 22:14:30,172 iteration 3422 : loss : 0.031557, loss_ce: 0.014361
2021-12-11 22:14:32,911 iteration 3423 : loss : 0.047265, loss_ce: 0.025475
2021-12-11 22:14:35,808 iteration 3424 : loss : 0.043601, loss_ce: 0.020008
2021-12-11 22:14:38,654 iteration 3425 : loss : 0.031297, loss_ce: 0.015549
2021-12-11 22:14:41,368 iteration 3426 : loss : 0.037695, loss_ce: 0.017631
2021-12-11 22:14:44,118 iteration 3427 : loss : 0.048585, loss_ce: 0.017500
2021-12-11 22:14:47,030 iteration 3428 : loss : 0.035385, loss_ce: 0.016578
2021-12-11 22:14:49,751 iteration 3429 : loss : 0.036248, loss_ce: 0.016391
2021-12-11 22:14:52,518 iteration 3430 : loss : 0.036519, loss_ce: 0.017044
2021-12-11 22:14:55,457 iteration 3431 : loss : 0.050317, loss_ce: 0.021544
2021-12-11 22:14:58,260 iteration 3432 : loss : 0.052107, loss_ce: 0.023720
2021-12-11 22:15:01,016 iteration 3433 : loss : 0.065477, loss_ce: 0.021943
2021-12-11 22:15:03,795 iteration 3434 : loss : 0.039998, loss_ce: 0.015307
 50%|█████████████▋             | 202/400 [2:52:01<2:48:58, 51.20s/it]2021-12-11 22:15:06,728 iteration 3435 : loss : 0.031185, loss_ce: 0.015402
2021-12-11 22:15:09,603 iteration 3436 : loss : 0.042060, loss_ce: 0.018947
2021-12-11 22:15:12,375 iteration 3437 : loss : 0.048340, loss_ce: 0.021317
2021-12-11 22:15:15,329 iteration 3438 : loss : 0.051033, loss_ce: 0.023646
2021-12-11 22:15:18,141 iteration 3439 : loss : 0.034150, loss_ce: 0.017107
2021-12-11 22:15:21,049 iteration 3440 : loss : 0.034631, loss_ce: 0.016176
2021-12-11 22:15:23,714 iteration 3441 : loss : 0.043176, loss_ce: 0.018348
2021-12-11 22:15:26,473 iteration 3442 : loss : 0.036275, loss_ce: 0.016521
2021-12-11 22:15:29,396 iteration 3443 : loss : 0.039404, loss_ce: 0.018326
2021-12-11 22:15:32,151 iteration 3444 : loss : 0.040696, loss_ce: 0.017924
2021-12-11 22:15:34,852 iteration 3445 : loss : 0.042365, loss_ce: 0.020314
2021-12-11 22:15:37,795 iteration 3446 : loss : 0.048467, loss_ce: 0.023583
2021-12-11 22:15:40,609 iteration 3447 : loss : 0.065645, loss_ce: 0.025505
2021-12-11 22:15:43,172 iteration 3448 : loss : 0.037831, loss_ce: 0.017401
2021-12-11 22:15:45,870 iteration 3449 : loss : 0.035331, loss_ce: 0.016997
2021-12-11 22:15:48,659 iteration 3450 : loss : 0.045314, loss_ce: 0.018351
2021-12-11 22:15:51,342 iteration 3451 : loss : 0.029108, loss_ce: 0.018339
 51%|█████████████▋             | 203/400 [2:52:49<2:44:31, 50.11s/it]2021-12-11 22:15:53,984 iteration 3452 : loss : 0.029512, loss_ce: 0.015694
2021-12-11 22:15:56,801 iteration 3453 : loss : 0.041387, loss_ce: 0.018747
2021-12-11 22:15:59,765 iteration 3454 : loss : 0.049709, loss_ce: 0.023851
2021-12-11 22:16:02,496 iteration 3455 : loss : 0.047384, loss_ce: 0.022889
2021-12-11 22:16:05,215 iteration 3456 : loss : 0.045820, loss_ce: 0.015863
2021-12-11 22:16:07,996 iteration 3457 : loss : 0.035438, loss_ce: 0.017284
2021-12-11 22:16:10,790 iteration 3458 : loss : 0.053707, loss_ce: 0.019255
2021-12-11 22:16:13,651 iteration 3459 : loss : 0.040295, loss_ce: 0.019376
2021-12-11 22:16:16,338 iteration 3460 : loss : 0.032830, loss_ce: 0.015947
2021-12-11 22:16:19,175 iteration 3461 : loss : 0.040264, loss_ce: 0.018050
2021-12-11 22:16:21,813 iteration 3462 : loss : 0.026760, loss_ce: 0.014878
2021-12-11 22:16:24,754 iteration 3463 : loss : 0.037425, loss_ce: 0.018534
2021-12-11 22:16:27,562 iteration 3464 : loss : 0.035384, loss_ce: 0.015274
2021-12-11 22:16:30,316 iteration 3465 : loss : 0.034548, loss_ce: 0.016550
2021-12-11 22:16:33,123 iteration 3466 : loss : 0.049918, loss_ce: 0.022789
2021-12-11 22:16:35,768 iteration 3467 : loss : 0.025732, loss_ce: 0.013059
2021-12-11 22:16:38,648 iteration 3468 : loss : 0.034952, loss_ce: 0.014412
 51%|█████████████▊             | 204/400 [2:53:36<2:40:55, 49.26s/it]2021-12-11 22:16:41,543 iteration 3469 : loss : 0.032265, loss_ce: 0.016781
2021-12-11 22:16:44,321 iteration 3470 : loss : 0.049458, loss_ce: 0.021256
2021-12-11 22:16:47,157 iteration 3471 : loss : 0.034850, loss_ce: 0.017436
2021-12-11 22:16:49,947 iteration 3472 : loss : 0.039389, loss_ce: 0.016051
2021-12-11 22:16:52,713 iteration 3473 : loss : 0.030772, loss_ce: 0.017321
2021-12-11 22:16:55,545 iteration 3474 : loss : 0.046780, loss_ce: 0.020462
2021-12-11 22:16:58,289 iteration 3475 : loss : 0.027595, loss_ce: 0.015312
2021-12-11 22:17:00,927 iteration 3476 : loss : 0.057461, loss_ce: 0.030777
2021-12-11 22:17:03,807 iteration 3477 : loss : 0.033903, loss_ce: 0.014464
2021-12-11 22:17:06,513 iteration 3478 : loss : 0.037538, loss_ce: 0.019719
2021-12-11 22:17:09,322 iteration 3479 : loss : 0.041604, loss_ce: 0.020464
2021-12-11 22:17:12,016 iteration 3480 : loss : 0.034092, loss_ce: 0.019590
2021-12-11 22:17:14,727 iteration 3481 : loss : 0.039716, loss_ce: 0.014484
2021-12-11 22:17:17,396 iteration 3482 : loss : 0.037072, loss_ce: 0.017311
2021-12-11 22:17:20,209 iteration 3483 : loss : 0.040330, loss_ce: 0.020847
2021-12-11 22:17:23,081 iteration 3484 : loss : 0.035384, loss_ce: 0.015901
2021-12-11 22:17:23,082 Training Data Eval:
2021-12-11 22:17:37,706   Average segmentation loss on training set: 0.0211
2021-12-11 22:17:37,707 Validation Data Eval:
2021-12-11 22:17:42,841   Average segmentation loss on validation set: 0.0806
2021-12-11 22:17:45,577 iteration 3485 : loss : 0.027509, loss_ce: 0.014311
 51%|█████████████▊             | 205/400 [2:54:43<2:57:20, 54.57s/it]2021-12-11 22:17:48,472 iteration 3486 : loss : 0.052471, loss_ce: 0.019269
2021-12-11 22:17:51,093 iteration 3487 : loss : 0.035944, loss_ce: 0.017974
2021-12-11 22:17:53,984 iteration 3488 : loss : 0.033334, loss_ce: 0.014022
2021-12-11 22:17:56,671 iteration 3489 : loss : 0.038995, loss_ce: 0.018873
2021-12-11 22:17:59,472 iteration 3490 : loss : 0.040027, loss_ce: 0.017779
2021-12-11 22:18:02,223 iteration 3491 : loss : 0.048229, loss_ce: 0.024886
2021-12-11 22:18:04,871 iteration 3492 : loss : 0.048051, loss_ce: 0.016749
2021-12-11 22:18:07,664 iteration 3493 : loss : 0.052772, loss_ce: 0.020439
2021-12-11 22:18:10,345 iteration 3494 : loss : 0.030087, loss_ce: 0.014993
2021-12-11 22:18:13,266 iteration 3495 : loss : 0.034790, loss_ce: 0.014652
2021-12-11 22:18:16,152 iteration 3496 : loss : 0.056377, loss_ce: 0.022344
2021-12-11 22:18:18,976 iteration 3497 : loss : 0.032813, loss_ce: 0.016184
2021-12-11 22:18:21,804 iteration 3498 : loss : 0.041995, loss_ce: 0.018540
2021-12-11 22:18:24,581 iteration 3499 : loss : 0.037868, loss_ce: 0.024266
2021-12-11 22:18:27,386 iteration 3500 : loss : 0.035730, loss_ce: 0.017919
2021-12-11 22:18:30,220 iteration 3501 : loss : 0.032504, loss_ce: 0.014382
2021-12-11 22:18:32,936 iteration 3502 : loss : 0.037427, loss_ce: 0.021334
 52%|█████████████▉             | 206/400 [2:55:31<2:49:26, 52.40s/it]2021-12-11 22:18:35,595 iteration 3503 : loss : 0.053687, loss_ce: 0.018442
2021-12-11 22:18:38,314 iteration 3504 : loss : 0.033363, loss_ce: 0.016203
2021-12-11 22:18:40,933 iteration 3505 : loss : 0.026391, loss_ce: 0.015422
2021-12-11 22:18:43,823 iteration 3506 : loss : 0.043847, loss_ce: 0.016953
2021-12-11 22:18:46,423 iteration 3507 : loss : 0.037297, loss_ce: 0.017735
2021-12-11 22:18:49,285 iteration 3508 : loss : 0.048001, loss_ce: 0.018223
2021-12-11 22:18:52,211 iteration 3509 : loss : 0.039668, loss_ce: 0.018684
2021-12-11 22:18:55,081 iteration 3510 : loss : 0.031947, loss_ce: 0.015456
2021-12-11 22:18:57,962 iteration 3511 : loss : 0.063858, loss_ce: 0.031661
2021-12-11 22:19:00,588 iteration 3512 : loss : 0.033551, loss_ce: 0.016958
2021-12-11 22:19:03,252 iteration 3513 : loss : 0.037969, loss_ce: 0.019501
2021-12-11 22:19:05,885 iteration 3514 : loss : 0.036740, loss_ce: 0.018074
2021-12-11 22:19:08,747 iteration 3515 : loss : 0.047514, loss_ce: 0.025026
2021-12-11 22:19:11,474 iteration 3516 : loss : 0.046556, loss_ce: 0.027198
2021-12-11 22:19:14,273 iteration 3517 : loss : 0.035513, loss_ce: 0.019542
2021-12-11 22:19:17,192 iteration 3518 : loss : 0.066604, loss_ce: 0.022825
2021-12-11 22:19:19,990 iteration 3519 : loss : 0.041607, loss_ce: 0.020175
 52%|█████████████▉             | 207/400 [2:56:18<2:43:24, 50.80s/it]2021-12-11 22:19:22,891 iteration 3520 : loss : 0.038003, loss_ce: 0.017002
2021-12-11 22:19:25,566 iteration 3521 : loss : 0.049051, loss_ce: 0.017950
2021-12-11 22:19:28,295 iteration 3522 : loss : 0.044986, loss_ce: 0.016085
2021-12-11 22:19:31,063 iteration 3523 : loss : 0.039015, loss_ce: 0.022234
2021-12-11 22:19:33,962 iteration 3524 : loss : 0.035146, loss_ce: 0.016613
2021-12-11 22:19:36,733 iteration 3525 : loss : 0.051966, loss_ce: 0.020249
2021-12-11 22:19:39,530 iteration 3526 : loss : 0.033367, loss_ce: 0.021269
2021-12-11 22:19:42,336 iteration 3527 : loss : 0.045159, loss_ce: 0.021382
2021-12-11 22:19:45,213 iteration 3528 : loss : 0.081282, loss_ce: 0.021229
2021-12-11 22:19:48,014 iteration 3529 : loss : 0.040983, loss_ce: 0.017660
2021-12-11 22:19:50,683 iteration 3530 : loss : 0.027559, loss_ce: 0.013994
2021-12-11 22:19:53,410 iteration 3531 : loss : 0.034876, loss_ce: 0.016026
2021-12-11 22:19:56,075 iteration 3532 : loss : 0.033615, loss_ce: 0.017930
2021-12-11 22:19:58,861 iteration 3533 : loss : 0.049865, loss_ce: 0.023566
2021-12-11 22:20:01,527 iteration 3534 : loss : 0.044662, loss_ce: 0.019679
2021-12-11 22:20:04,448 iteration 3535 : loss : 0.043094, loss_ce: 0.019865
2021-12-11 22:20:07,230 iteration 3536 : loss : 0.041102, loss_ce: 0.019074
 52%|██████████████             | 208/400 [2:57:05<2:39:07, 49.73s/it]2021-12-11 22:20:10,141 iteration 3537 : loss : 0.068228, loss_ce: 0.034731
2021-12-11 22:20:12,782 iteration 3538 : loss : 0.038596, loss_ce: 0.017781
2021-12-11 22:20:15,648 iteration 3539 : loss : 0.038286, loss_ce: 0.015605
2021-12-11 22:20:18,588 iteration 3540 : loss : 0.067666, loss_ce: 0.034101
2021-12-11 22:20:21,297 iteration 3541 : loss : 0.042051, loss_ce: 0.018400
2021-12-11 22:20:23,937 iteration 3542 : loss : 0.041663, loss_ce: 0.018922
2021-12-11 22:20:26,803 iteration 3543 : loss : 0.038791, loss_ce: 0.020185
2021-12-11 22:20:29,649 iteration 3544 : loss : 0.076972, loss_ce: 0.018686
2021-12-11 22:20:32,449 iteration 3545 : loss : 0.046123, loss_ce: 0.018205
2021-12-11 22:20:35,240 iteration 3546 : loss : 0.054061, loss_ce: 0.022758
2021-12-11 22:20:38,096 iteration 3547 : loss : 0.046528, loss_ce: 0.021184
2021-12-11 22:20:40,740 iteration 3548 : loss : 0.024687, loss_ce: 0.011524
2021-12-11 22:20:43,622 iteration 3549 : loss : 0.063632, loss_ce: 0.032320
2021-12-11 22:20:46,393 iteration 3550 : loss : 0.028021, loss_ce: 0.013592
2021-12-11 22:20:49,152 iteration 3551 : loss : 0.038096, loss_ce: 0.017907
2021-12-11 22:20:51,783 iteration 3552 : loss : 0.045189, loss_ce: 0.024677
2021-12-11 22:20:54,391 iteration 3553 : loss : 0.030839, loss_ce: 0.014876
 52%|██████████████             | 209/400 [2:57:52<2:35:51, 48.96s/it]2021-12-11 22:20:57,357 iteration 3554 : loss : 0.046757, loss_ce: 0.021553
2021-12-11 22:21:00,144 iteration 3555 : loss : 0.056595, loss_ce: 0.026577
2021-12-11 22:21:02,887 iteration 3556 : loss : 0.047816, loss_ce: 0.022259
2021-12-11 22:21:05,749 iteration 3557 : loss : 0.046623, loss_ce: 0.023846
2021-12-11 22:21:08,416 iteration 3558 : loss : 0.042333, loss_ce: 0.016766
2021-12-11 22:21:11,284 iteration 3559 : loss : 0.056582, loss_ce: 0.020888
2021-12-11 22:21:13,971 iteration 3560 : loss : 0.035245, loss_ce: 0.016596
2021-12-11 22:21:16,810 iteration 3561 : loss : 0.028851, loss_ce: 0.016675
2021-12-11 22:21:19,675 iteration 3562 : loss : 0.041019, loss_ce: 0.021107
2021-12-11 22:21:22,338 iteration 3563 : loss : 0.042464, loss_ce: 0.020175
2021-12-11 22:21:25,278 iteration 3564 : loss : 0.073291, loss_ce: 0.023751
2021-12-11 22:21:27,991 iteration 3565 : loss : 0.062071, loss_ce: 0.020780
2021-12-11 22:21:30,687 iteration 3566 : loss : 0.040993, loss_ce: 0.018778
2021-12-11 22:21:33,510 iteration 3567 : loss : 0.051639, loss_ce: 0.025729
2021-12-11 22:21:36,419 iteration 3568 : loss : 0.041621, loss_ce: 0.017401
2021-12-11 22:21:39,042 iteration 3569 : loss : 0.038304, loss_ce: 0.017312
2021-12-11 22:21:39,042 Training Data Eval:
2021-12-11 22:21:53,724   Average segmentation loss on training set: 0.0227
2021-12-11 22:21:53,724 Validation Data Eval:
2021-12-11 22:21:58,972   Average segmentation loss on validation set: 0.1315
2021-12-11 22:22:01,801 iteration 3570 : loss : 0.040806, loss_ce: 0.018390
 52%|██████████████▏            | 210/400 [2:58:59<2:52:34, 54.50s/it]2021-12-11 22:22:04,613 iteration 3571 : loss : 0.058969, loss_ce: 0.031624
2021-12-11 22:22:07,172 iteration 3572 : loss : 0.031277, loss_ce: 0.014331
2021-12-11 22:22:10,007 iteration 3573 : loss : 0.041550, loss_ce: 0.015978
2021-12-11 22:22:12,866 iteration 3574 : loss : 0.033707, loss_ce: 0.013678
2021-12-11 22:22:15,528 iteration 3575 : loss : 0.029028, loss_ce: 0.013330
2021-12-11 22:22:18,262 iteration 3576 : loss : 0.044667, loss_ce: 0.017914
2021-12-11 22:22:21,037 iteration 3577 : loss : 0.040567, loss_ce: 0.019604
2021-12-11 22:22:23,687 iteration 3578 : loss : 0.039060, loss_ce: 0.017075
2021-12-11 22:22:26,465 iteration 3579 : loss : 0.040891, loss_ce: 0.018916
2021-12-11 22:22:29,094 iteration 3580 : loss : 0.046208, loss_ce: 0.019833
2021-12-11 22:22:31,974 iteration 3581 : loss : 0.045673, loss_ce: 0.026776
2021-12-11 22:22:34,586 iteration 3582 : loss : 0.032687, loss_ce: 0.015179
2021-12-11 22:22:37,367 iteration 3583 : loss : 0.035919, loss_ce: 0.015365
2021-12-11 22:22:40,205 iteration 3584 : loss : 0.045269, loss_ce: 0.018680
2021-12-11 22:22:42,823 iteration 3585 : loss : 0.057918, loss_ce: 0.021923
2021-12-11 22:22:45,636 iteration 3586 : loss : 0.044357, loss_ce: 0.023073
2021-12-11 22:22:48,257 iteration 3587 : loss : 0.029892, loss_ce: 0.015313
 53%|██████████████▏            | 211/400 [2:59:46<2:44:04, 52.09s/it]2021-12-11 22:22:51,215 iteration 3588 : loss : 0.025666, loss_ce: 0.013739
2021-12-11 22:22:53,803 iteration 3589 : loss : 0.033465, loss_ce: 0.015512
2021-12-11 22:22:56,487 iteration 3590 : loss : 0.028462, loss_ce: 0.014047
2021-12-11 22:22:59,354 iteration 3591 : loss : 0.028564, loss_ce: 0.013377
2021-12-11 22:23:02,297 iteration 3592 : loss : 0.051792, loss_ce: 0.022100
2021-12-11 22:23:05,055 iteration 3593 : loss : 0.046569, loss_ce: 0.018887
2021-12-11 22:23:07,849 iteration 3594 : loss : 0.042742, loss_ce: 0.019636
2021-12-11 22:23:10,629 iteration 3595 : loss : 0.068528, loss_ce: 0.025077
2021-12-11 22:23:13,324 iteration 3596 : loss : 0.028427, loss_ce: 0.014270
2021-12-11 22:23:16,018 iteration 3597 : loss : 0.061254, loss_ce: 0.025878
2021-12-11 22:23:18,861 iteration 3598 : loss : 0.029079, loss_ce: 0.016396
2021-12-11 22:23:21,660 iteration 3599 : loss : 0.039644, loss_ce: 0.018017
2021-12-11 22:23:24,386 iteration 3600 : loss : 0.028170, loss_ce: 0.013914
2021-12-11 22:23:27,017 iteration 3601 : loss : 0.035279, loss_ce: 0.017151
2021-12-11 22:23:29,748 iteration 3602 : loss : 0.036012, loss_ce: 0.015427
2021-12-11 22:23:32,371 iteration 3603 : loss : 0.034662, loss_ce: 0.019799
2021-12-11 22:23:35,098 iteration 3604 : loss : 0.032021, loss_ce: 0.017257
 53%|██████████████▎            | 212/400 [3:00:33<2:38:16, 50.51s/it]2021-12-11 22:23:37,986 iteration 3605 : loss : 0.034041, loss_ce: 0.015655
2021-12-11 22:23:40,739 iteration 3606 : loss : 0.042945, loss_ce: 0.023447
2021-12-11 22:23:43,604 iteration 3607 : loss : 0.055006, loss_ce: 0.022911
2021-12-11 22:23:46,456 iteration 3608 : loss : 0.031277, loss_ce: 0.014542
2021-12-11 22:23:49,224 iteration 3609 : loss : 0.033372, loss_ce: 0.018707
2021-12-11 22:23:51,829 iteration 3610 : loss : 0.031743, loss_ce: 0.015672
2021-12-11 22:23:54,563 iteration 3611 : loss : 0.035961, loss_ce: 0.012966
2021-12-11 22:23:57,207 iteration 3612 : loss : 0.029530, loss_ce: 0.013631
2021-12-11 22:24:00,214 iteration 3613 : loss : 0.040720, loss_ce: 0.020020
2021-12-11 22:24:02,909 iteration 3614 : loss : 0.026579, loss_ce: 0.013433
2021-12-11 22:24:05,547 iteration 3615 : loss : 0.029919, loss_ce: 0.013986
2021-12-11 22:24:08,180 iteration 3616 : loss : 0.040222, loss_ce: 0.019325
2021-12-11 22:24:10,967 iteration 3617 : loss : 0.038047, loss_ce: 0.018625
2021-12-11 22:24:13,708 iteration 3618 : loss : 0.034095, loss_ce: 0.016876
2021-12-11 22:24:16,505 iteration 3619 : loss : 0.045492, loss_ce: 0.017188
2021-12-11 22:24:19,253 iteration 3620 : loss : 0.039754, loss_ce: 0.015685
2021-12-11 22:24:22,056 iteration 3621 : loss : 0.058664, loss_ce: 0.023973
 53%|██████████████▍            | 213/400 [3:01:20<2:34:05, 49.44s/it]2021-12-11 22:24:25,003 iteration 3622 : loss : 0.033224, loss_ce: 0.018478
2021-12-11 22:24:27,847 iteration 3623 : loss : 0.043592, loss_ce: 0.015375
2021-12-11 22:24:30,594 iteration 3624 : loss : 0.033833, loss_ce: 0.016678
2021-12-11 22:24:33,314 iteration 3625 : loss : 0.042016, loss_ce: 0.018903
2021-12-11 22:24:36,018 iteration 3626 : loss : 0.046468, loss_ce: 0.022563
2021-12-11 22:24:38,831 iteration 3627 : loss : 0.042739, loss_ce: 0.019149
2021-12-11 22:24:41,778 iteration 3628 : loss : 0.052734, loss_ce: 0.017959
2021-12-11 22:24:44,715 iteration 3629 : loss : 0.035299, loss_ce: 0.017330
2021-12-11 22:24:47,335 iteration 3630 : loss : 0.030865, loss_ce: 0.016404
2021-12-11 22:24:50,122 iteration 3631 : loss : 0.045107, loss_ce: 0.016994
2021-12-11 22:24:52,959 iteration 3632 : loss : 0.044288, loss_ce: 0.018960
2021-12-11 22:24:55,698 iteration 3633 : loss : 0.035982, loss_ce: 0.018053
2021-12-11 22:24:58,476 iteration 3634 : loss : 0.035940, loss_ce: 0.019330
2021-12-11 22:25:01,267 iteration 3635 : loss : 0.046432, loss_ce: 0.023240
2021-12-11 22:25:04,147 iteration 3636 : loss : 0.058477, loss_ce: 0.020563
2021-12-11 22:25:06,807 iteration 3637 : loss : 0.045598, loss_ce: 0.019198
2021-12-11 22:25:09,540 iteration 3638 : loss : 0.026905, loss_ce: 0.014775
 54%|██████████████▍            | 214/400 [3:02:07<2:31:27, 48.86s/it]2021-12-11 22:25:12,344 iteration 3639 : loss : 0.030177, loss_ce: 0.015418
2021-12-11 22:25:14,977 iteration 3640 : loss : 0.037686, loss_ce: 0.016591
2021-12-11 22:25:17,600 iteration 3641 : loss : 0.029648, loss_ce: 0.014305
2021-12-11 22:25:20,322 iteration 3642 : loss : 0.043792, loss_ce: 0.017239
2021-12-11 22:25:23,064 iteration 3643 : loss : 0.035185, loss_ce: 0.016604
2021-12-11 22:25:25,862 iteration 3644 : loss : 0.021312, loss_ce: 0.011605
2021-12-11 22:25:28,632 iteration 3645 : loss : 0.029138, loss_ce: 0.013626
2021-12-11 22:25:31,538 iteration 3646 : loss : 0.047265, loss_ce: 0.022383
2021-12-11 22:25:34,370 iteration 3647 : loss : 0.038587, loss_ce: 0.013201
2021-12-11 22:25:37,218 iteration 3648 : loss : 0.041678, loss_ce: 0.020313
2021-12-11 22:25:39,874 iteration 3649 : loss : 0.032080, loss_ce: 0.012348
2021-12-11 22:25:42,680 iteration 3650 : loss : 0.040943, loss_ce: 0.025733
2021-12-11 22:25:45,434 iteration 3651 : loss : 0.045268, loss_ce: 0.018562
2021-12-11 22:25:48,303 iteration 3652 : loss : 0.061917, loss_ce: 0.023247
2021-12-11 22:25:51,045 iteration 3653 : loss : 0.046500, loss_ce: 0.028425
2021-12-11 22:25:53,785 iteration 3654 : loss : 0.028868, loss_ce: 0.016107
2021-12-11 22:25:53,785 Training Data Eval:
2021-12-11 22:26:08,851   Average segmentation loss on training set: 0.0204
2021-12-11 22:26:08,852 Validation Data Eval:
2021-12-11 22:26:14,021   Average segmentation loss on validation set: 0.1057
2021-12-11 22:26:16,792 iteration 3655 : loss : 0.027718, loss_ce: 0.013983
 54%|██████████████▌            | 215/400 [3:03:14<2:47:39, 54.38s/it]2021-12-11 22:26:19,676 iteration 3656 : loss : 0.033649, loss_ce: 0.016816
2021-12-11 22:26:22,448 iteration 3657 : loss : 0.029814, loss_ce: 0.017860
2021-12-11 22:26:25,205 iteration 3658 : loss : 0.039504, loss_ce: 0.021424
2021-12-11 22:26:28,029 iteration 3659 : loss : 0.045376, loss_ce: 0.018287
2021-12-11 22:26:30,949 iteration 3660 : loss : 0.056054, loss_ce: 0.018971
2021-12-11 22:26:33,553 iteration 3661 : loss : 0.029622, loss_ce: 0.015032
2021-12-11 22:26:36,464 iteration 3662 : loss : 0.032342, loss_ce: 0.014875
2021-12-11 22:26:39,138 iteration 3663 : loss : 0.041298, loss_ce: 0.016751
2021-12-11 22:26:41,993 iteration 3664 : loss : 0.026147, loss_ce: 0.012479
2021-12-11 22:26:44,752 iteration 3665 : loss : 0.036422, loss_ce: 0.020461
2021-12-11 22:26:47,479 iteration 3666 : loss : 0.033572, loss_ce: 0.015458
2021-12-11 22:26:50,184 iteration 3667 : loss : 0.052392, loss_ce: 0.027602
2021-12-11 22:26:53,013 iteration 3668 : loss : 0.040293, loss_ce: 0.012836
2021-12-11 22:26:55,970 iteration 3669 : loss : 0.038938, loss_ce: 0.019734
2021-12-11 22:26:58,736 iteration 3670 : loss : 0.030232, loss_ce: 0.015936
2021-12-11 22:27:01,439 iteration 3671 : loss : 0.029017, loss_ce: 0.016813
2021-12-11 22:27:04,353 iteration 3672 : loss : 0.072592, loss_ce: 0.023944
 54%|██████████████▌            | 216/400 [3:04:02<2:40:29, 52.33s/it]2021-12-11 22:27:07,136 iteration 3673 : loss : 0.039008, loss_ce: 0.019346
2021-12-11 22:27:10,031 iteration 3674 : loss : 0.038679, loss_ce: 0.014059
2021-12-11 22:27:13,002 iteration 3675 : loss : 0.046972, loss_ce: 0.017426
2021-12-11 22:27:15,639 iteration 3676 : loss : 0.028253, loss_ce: 0.014685
2021-12-11 22:27:18,465 iteration 3677 : loss : 0.040672, loss_ce: 0.015920
2021-12-11 22:27:21,302 iteration 3678 : loss : 0.067399, loss_ce: 0.019885
2021-12-11 22:27:24,164 iteration 3679 : loss : 0.035049, loss_ce: 0.015767
2021-12-11 22:27:26,879 iteration 3680 : loss : 0.049969, loss_ce: 0.021614
2021-12-11 22:27:29,688 iteration 3681 : loss : 0.032365, loss_ce: 0.017124
2021-12-11 22:27:32,367 iteration 3682 : loss : 0.029847, loss_ce: 0.014722
2021-12-11 22:27:35,094 iteration 3683 : loss : 0.026914, loss_ce: 0.012579
2021-12-11 22:27:37,964 iteration 3684 : loss : 0.025700, loss_ce: 0.013543
2021-12-11 22:27:40,788 iteration 3685 : loss : 0.029873, loss_ce: 0.012885
2021-12-11 22:27:43,594 iteration 3686 : loss : 0.038882, loss_ce: 0.017227
2021-12-11 22:27:46,473 iteration 3687 : loss : 0.047327, loss_ce: 0.019505
2021-12-11 22:27:49,340 iteration 3688 : loss : 0.035286, loss_ce: 0.020083
2021-12-11 22:27:52,223 iteration 3689 : loss : 0.045405, loss_ce: 0.017335
 54%|██████████████▋            | 217/400 [3:04:50<2:35:31, 50.99s/it]2021-12-11 22:27:55,104 iteration 3690 : loss : 0.060884, loss_ce: 0.019196
2021-12-11 22:27:57,811 iteration 3691 : loss : 0.043011, loss_ce: 0.014056
2021-12-11 22:28:00,494 iteration 3692 : loss : 0.044353, loss_ce: 0.025193
2021-12-11 22:28:03,116 iteration 3693 : loss : 0.023352, loss_ce: 0.012446
2021-12-11 22:28:05,891 iteration 3694 : loss : 0.050484, loss_ce: 0.024268
2021-12-11 22:28:08,629 iteration 3695 : loss : 0.041277, loss_ce: 0.023711
2021-12-11 22:28:11,304 iteration 3696 : loss : 0.052711, loss_ce: 0.021020
2021-12-11 22:28:13,975 iteration 3697 : loss : 0.040134, loss_ce: 0.015667
2021-12-11 22:28:16,807 iteration 3698 : loss : 0.047730, loss_ce: 0.023499
2021-12-11 22:28:19,602 iteration 3699 : loss : 0.027613, loss_ce: 0.014202
2021-12-11 22:28:22,294 iteration 3700 : loss : 0.035886, loss_ce: 0.013728
2021-12-11 22:28:25,031 iteration 3701 : loss : 0.029584, loss_ce: 0.015519
2021-12-11 22:28:27,759 iteration 3702 : loss : 0.029682, loss_ce: 0.013364
2021-12-11 22:28:30,577 iteration 3703 : loss : 0.032275, loss_ce: 0.017599
2021-12-11 22:28:33,245 iteration 3704 : loss : 0.032185, loss_ce: 0.015257
2021-12-11 22:28:36,142 iteration 3705 : loss : 0.042728, loss_ce: 0.018522
2021-12-11 22:28:38,939 iteration 3706 : loss : 0.025366, loss_ce: 0.012048
 55%|██████████████▋            | 218/400 [3:05:37<2:30:46, 49.71s/it]2021-12-11 22:28:41,893 iteration 3707 : loss : 0.048332, loss_ce: 0.022423
2021-12-11 22:28:44,682 iteration 3708 : loss : 0.034258, loss_ce: 0.014591
2021-12-11 22:28:47,540 iteration 3709 : loss : 0.059488, loss_ce: 0.024587
2021-12-11 22:28:50,163 iteration 3710 : loss : 0.051212, loss_ce: 0.020531
2021-12-11 22:28:52,952 iteration 3711 : loss : 0.032347, loss_ce: 0.014256
2021-12-11 22:28:55,758 iteration 3712 : loss : 0.033176, loss_ce: 0.017652
2021-12-11 22:28:58,623 iteration 3713 : loss : 0.043508, loss_ce: 0.017763
2021-12-11 22:29:01,365 iteration 3714 : loss : 0.048594, loss_ce: 0.021824
2021-12-11 22:29:04,155 iteration 3715 : loss : 0.024532, loss_ce: 0.013855
2021-12-11 22:29:07,019 iteration 3716 : loss : 0.042185, loss_ce: 0.025016
2021-12-11 22:29:09,807 iteration 3717 : loss : 0.028659, loss_ce: 0.013868
2021-12-11 22:29:12,635 iteration 3718 : loss : 0.032831, loss_ce: 0.014882
2021-12-11 22:29:15,456 iteration 3719 : loss : 0.035697, loss_ce: 0.017453
2021-12-11 22:29:18,279 iteration 3720 : loss : 0.055160, loss_ce: 0.020032
2021-12-11 22:29:21,128 iteration 3721 : loss : 0.028977, loss_ce: 0.014681
2021-12-11 22:29:23,898 iteration 3722 : loss : 0.052145, loss_ce: 0.025320
2021-12-11 22:29:26,853 iteration 3723 : loss : 0.065955, loss_ce: 0.023640
 55%|██████████████▊            | 219/400 [3:06:25<2:28:19, 49.17s/it]2021-12-11 22:29:29,662 iteration 3724 : loss : 0.038443, loss_ce: 0.019618
2021-12-11 22:29:32,544 iteration 3725 : loss : 0.064167, loss_ce: 0.029842
2021-12-11 22:29:35,453 iteration 3726 : loss : 0.044566, loss_ce: 0.018712
2021-12-11 22:29:38,179 iteration 3727 : loss : 0.034262, loss_ce: 0.014822
2021-12-11 22:29:40,852 iteration 3728 : loss : 0.044743, loss_ce: 0.025862
2021-12-11 22:29:43,751 iteration 3729 : loss : 0.045906, loss_ce: 0.018253
2021-12-11 22:29:46,368 iteration 3730 : loss : 0.040927, loss_ce: 0.016686
2021-12-11 22:29:49,031 iteration 3731 : loss : 0.037675, loss_ce: 0.016325
2021-12-11 22:29:51,812 iteration 3732 : loss : 0.045381, loss_ce: 0.021705
2021-12-11 22:29:54,640 iteration 3733 : loss : 0.038152, loss_ce: 0.015451
2021-12-11 22:29:57,557 iteration 3734 : loss : 0.034439, loss_ce: 0.017284
2021-12-11 22:30:00,236 iteration 3735 : loss : 0.047583, loss_ce: 0.026207
2021-12-11 22:30:02,916 iteration 3736 : loss : 0.060012, loss_ce: 0.018796
2021-12-11 22:30:05,701 iteration 3737 : loss : 0.033077, loss_ce: 0.015376
2021-12-11 22:30:08,581 iteration 3738 : loss : 0.054001, loss_ce: 0.025755
2021-12-11 22:30:11,342 iteration 3739 : loss : 0.036712, loss_ce: 0.015501
2021-12-11 22:30:11,342 Training Data Eval:
2021-12-11 22:30:26,106   Average segmentation loss on training set: 0.0188
2021-12-11 22:30:26,107 Validation Data Eval:
2021-12-11 22:30:31,388   Average segmentation loss on validation set: 0.1009
2021-12-11 22:30:34,168 iteration 3740 : loss : 0.034193, loss_ce: 0.014936
 55%|██████████████▊            | 220/400 [3:07:32<2:43:50, 54.61s/it]2021-12-11 22:30:36,934 iteration 3741 : loss : 0.030860, loss_ce: 0.015166
2021-12-11 22:30:39,586 iteration 3742 : loss : 0.039983, loss_ce: 0.021037
2021-12-11 22:30:42,216 iteration 3743 : loss : 0.039226, loss_ce: 0.017519
2021-12-11 22:30:44,906 iteration 3744 : loss : 0.044772, loss_ce: 0.022217
2021-12-11 22:30:47,552 iteration 3745 : loss : 0.030393, loss_ce: 0.014512
2021-12-11 22:30:50,457 iteration 3746 : loss : 0.026166, loss_ce: 0.013112
2021-12-11 22:30:53,230 iteration 3747 : loss : 0.098954, loss_ce: 0.021719
2021-12-11 22:30:55,938 iteration 3748 : loss : 0.041549, loss_ce: 0.020474
2021-12-11 22:30:58,774 iteration 3749 : loss : 0.040811, loss_ce: 0.020163
2021-12-11 22:31:01,577 iteration 3750 : loss : 0.041615, loss_ce: 0.020935
2021-12-11 22:31:04,308 iteration 3751 : loss : 0.042236, loss_ce: 0.014127
2021-12-11 22:31:07,211 iteration 3752 : loss : 0.054180, loss_ce: 0.025845
2021-12-11 22:31:09,830 iteration 3753 : loss : 0.038070, loss_ce: 0.014896
2021-12-11 22:31:12,531 iteration 3754 : loss : 0.032032, loss_ce: 0.012800
2021-12-11 22:31:15,214 iteration 3755 : loss : 0.039369, loss_ce: 0.020856
2021-12-11 22:31:17,975 iteration 3756 : loss : 0.035890, loss_ce: 0.015718
2021-12-11 22:31:20,676 iteration 3757 : loss : 0.043103, loss_ce: 0.018144
 55%|██████████████▉            | 221/400 [3:08:18<2:35:40, 52.18s/it]2021-12-11 22:31:23,644 iteration 3758 : loss : 0.051985, loss_ce: 0.023687
2021-12-11 22:31:26,389 iteration 3759 : loss : 0.028015, loss_ce: 0.013234
2021-12-11 22:31:29,097 iteration 3760 : loss : 0.034506, loss_ce: 0.013993
2021-12-11 22:31:31,843 iteration 3761 : loss : 0.040187, loss_ce: 0.019817
2021-12-11 22:31:34,710 iteration 3762 : loss : 0.035156, loss_ce: 0.016269
2021-12-11 22:31:37,479 iteration 3763 : loss : 0.044197, loss_ce: 0.021056
2021-12-11 22:31:40,421 iteration 3764 : loss : 0.038757, loss_ce: 0.018703
2021-12-11 22:31:43,068 iteration 3765 : loss : 0.041112, loss_ce: 0.018842
2021-12-11 22:31:45,817 iteration 3766 : loss : 0.038132, loss_ce: 0.017619
2021-12-11 22:31:48,638 iteration 3767 : loss : 0.048230, loss_ce: 0.020377
2021-12-11 22:31:51,462 iteration 3768 : loss : 0.034654, loss_ce: 0.018541
2021-12-11 22:31:54,204 iteration 3769 : loss : 0.031238, loss_ce: 0.014905
2021-12-11 22:31:56,781 iteration 3770 : loss : 0.030413, loss_ce: 0.013083
2021-12-11 22:31:59,436 iteration 3771 : loss : 0.026096, loss_ce: 0.013190
2021-12-11 22:32:02,107 iteration 3772 : loss : 0.036551, loss_ce: 0.016242
2021-12-11 22:32:04,827 iteration 3773 : loss : 0.026868, loss_ce: 0.012711
2021-12-11 22:32:07,584 iteration 3774 : loss : 0.039583, loss_ce: 0.018790
 56%|██████████████▉            | 222/400 [3:09:05<2:30:06, 50.60s/it]2021-12-11 22:32:10,461 iteration 3775 : loss : 0.037743, loss_ce: 0.013938
2021-12-11 22:32:13,404 iteration 3776 : loss : 0.043215, loss_ce: 0.020521
2021-12-11 22:32:16,226 iteration 3777 : loss : 0.037405, loss_ce: 0.016967
2021-12-11 22:32:19,009 iteration 3778 : loss : 0.025729, loss_ce: 0.015541
2021-12-11 22:32:21,943 iteration 3779 : loss : 0.026385, loss_ce: 0.013300
2021-12-11 22:32:24,588 iteration 3780 : loss : 0.033517, loss_ce: 0.020067
2021-12-11 22:32:27,486 iteration 3781 : loss : 0.029253, loss_ce: 0.013605
2021-12-11 22:32:30,156 iteration 3782 : loss : 0.037961, loss_ce: 0.016883
2021-12-11 22:32:32,825 iteration 3783 : loss : 0.027004, loss_ce: 0.012095
2021-12-11 22:32:35,695 iteration 3784 : loss : 0.042882, loss_ce: 0.023544
2021-12-11 22:32:38,471 iteration 3785 : loss : 0.033208, loss_ce: 0.014973
2021-12-11 22:32:41,166 iteration 3786 : loss : 0.036545, loss_ce: 0.017637
2021-12-11 22:32:44,114 iteration 3787 : loss : 0.066333, loss_ce: 0.027511
2021-12-11 22:32:47,002 iteration 3788 : loss : 0.040038, loss_ce: 0.017262
2021-12-11 22:32:49,784 iteration 3789 : loss : 0.037283, loss_ce: 0.014816
2021-12-11 22:32:52,438 iteration 3790 : loss : 0.040277, loss_ce: 0.017240
2021-12-11 22:32:55,000 iteration 3791 : loss : 0.030948, loss_ce: 0.014943
 56%|███████████████            | 223/400 [3:09:53<2:26:27, 49.64s/it]2021-12-11 22:32:57,952 iteration 3792 : loss : 0.028344, loss_ce: 0.014602
2021-12-11 22:33:00,592 iteration 3793 : loss : 0.027576, loss_ce: 0.013973
2021-12-11 22:33:03,160 iteration 3794 : loss : 0.025260, loss_ce: 0.012913
2021-12-11 22:33:06,184 iteration 3795 : loss : 0.054352, loss_ce: 0.025952
2021-12-11 22:33:08,900 iteration 3796 : loss : 0.051219, loss_ce: 0.015746
2021-12-11 22:33:11,694 iteration 3797 : loss : 0.032960, loss_ce: 0.014644
2021-12-11 22:33:14,628 iteration 3798 : loss : 0.039057, loss_ce: 0.019250
2021-12-11 22:33:17,418 iteration 3799 : loss : 0.035228, loss_ce: 0.021505
2021-12-11 22:33:20,332 iteration 3800 : loss : 0.041057, loss_ce: 0.015973
2021-12-11 22:33:23,041 iteration 3801 : loss : 0.024551, loss_ce: 0.012548
2021-12-11 22:33:25,810 iteration 3802 : loss : 0.040734, loss_ce: 0.016002
2021-12-11 22:33:28,689 iteration 3803 : loss : 0.026736, loss_ce: 0.013848
2021-12-11 22:33:31,431 iteration 3804 : loss : 0.039514, loss_ce: 0.020121
2021-12-11 22:33:34,252 iteration 3805 : loss : 0.036849, loss_ce: 0.018961
2021-12-11 22:33:36,904 iteration 3806 : loss : 0.034050, loss_ce: 0.012678
2021-12-11 22:33:39,683 iteration 3807 : loss : 0.034453, loss_ce: 0.016687
2021-12-11 22:33:42,430 iteration 3808 : loss : 0.031726, loss_ce: 0.014581
 56%|███████████████            | 224/400 [3:10:40<2:23:40, 48.98s/it]2021-12-11 22:33:45,338 iteration 3809 : loss : 0.029818, loss_ce: 0.014733
2021-12-11 22:33:48,038 iteration 3810 : loss : 0.033297, loss_ce: 0.015361
2021-12-11 22:33:51,067 iteration 3811 : loss : 0.034453, loss_ce: 0.014444
2021-12-11 22:33:53,905 iteration 3812 : loss : 0.033176, loss_ce: 0.015554
2021-12-11 22:33:56,531 iteration 3813 : loss : 0.044089, loss_ce: 0.015964
2021-12-11 22:33:59,060 iteration 3814 : loss : 0.030573, loss_ce: 0.013257
2021-12-11 22:34:01,918 iteration 3815 : loss : 0.055162, loss_ce: 0.021898
2021-12-11 22:34:04,805 iteration 3816 : loss : 0.040494, loss_ce: 0.015848
2021-12-11 22:34:07,505 iteration 3817 : loss : 0.041285, loss_ce: 0.021600
2021-12-11 22:34:10,511 iteration 3818 : loss : 0.044921, loss_ce: 0.022480
2021-12-11 22:34:13,162 iteration 3819 : loss : 0.034232, loss_ce: 0.018818
2021-12-11 22:34:15,849 iteration 3820 : loss : 0.046878, loss_ce: 0.021718
2021-12-11 22:34:18,612 iteration 3821 : loss : 0.031985, loss_ce: 0.016872
2021-12-11 22:34:21,137 iteration 3822 : loss : 0.026340, loss_ce: 0.013715
2021-12-11 22:34:23,894 iteration 3823 : loss : 0.028346, loss_ce: 0.013149
2021-12-11 22:34:26,665 iteration 3824 : loss : 0.039397, loss_ce: 0.018645
2021-12-11 22:34:26,665 Training Data Eval:
2021-12-11 22:34:41,636   Average segmentation loss on training set: 0.0195
2021-12-11 22:34:41,636 Validation Data Eval:
2021-12-11 22:34:46,659   Average segmentation loss on validation set: 0.0791
2021-12-11 22:34:49,229 iteration 3825 : loss : 0.036902, loss_ce: 0.017460
 56%|███████████████▏           | 225/400 [3:11:47<2:38:27, 54.33s/it]2021-12-11 22:34:51,883 iteration 3826 : loss : 0.034382, loss_ce: 0.018384
2021-12-11 22:34:54,681 iteration 3827 : loss : 0.034519, loss_ce: 0.015898
2021-12-11 22:34:57,421 iteration 3828 : loss : 0.029789, loss_ce: 0.014344
2021-12-11 22:35:00,238 iteration 3829 : loss : 0.044806, loss_ce: 0.023707
2021-12-11 22:35:03,124 iteration 3830 : loss : 0.049882, loss_ce: 0.026099
2021-12-11 22:35:06,005 iteration 3831 : loss : 0.042460, loss_ce: 0.013574
2021-12-11 22:35:08,627 iteration 3832 : loss : 0.025736, loss_ce: 0.012691
2021-12-11 22:35:11,382 iteration 3833 : loss : 0.033713, loss_ce: 0.014970
2021-12-11 22:35:14,128 iteration 3834 : loss : 0.025341, loss_ce: 0.015240
2021-12-11 22:35:16,923 iteration 3835 : loss : 0.041314, loss_ce: 0.023406
2021-12-11 22:35:19,858 iteration 3836 : loss : 0.032564, loss_ce: 0.014344
2021-12-11 22:35:22,554 iteration 3837 : loss : 0.028009, loss_ce: 0.014500
2021-12-11 22:35:25,186 iteration 3838 : loss : 0.040331, loss_ce: 0.016577
2021-12-11 22:35:27,969 iteration 3839 : loss : 0.032552, loss_ce: 0.015230
2021-12-11 22:35:30,656 iteration 3840 : loss : 0.041403, loss_ce: 0.016181
2021-12-11 22:35:33,380 iteration 3841 : loss : 0.038320, loss_ce: 0.015512
2021-12-11 22:35:36,043 iteration 3842 : loss : 0.036576, loss_ce: 0.012749
 56%|███████████████▎           | 226/400 [3:12:34<2:31:00, 52.07s/it]2021-12-11 22:35:38,981 iteration 3843 : loss : 0.029175, loss_ce: 0.014319
2021-12-11 22:35:41,796 iteration 3844 : loss : 0.039805, loss_ce: 0.021464
2021-12-11 22:35:44,435 iteration 3845 : loss : 0.044017, loss_ce: 0.018918
2021-12-11 22:35:47,214 iteration 3846 : loss : 0.029515, loss_ce: 0.011802
2021-12-11 22:35:50,079 iteration 3847 : loss : 0.028436, loss_ce: 0.013915
2021-12-11 22:35:52,897 iteration 3848 : loss : 0.027528, loss_ce: 0.014268
2021-12-11 22:35:55,526 iteration 3849 : loss : 0.026708, loss_ce: 0.013331
2021-12-11 22:35:58,255 iteration 3850 : loss : 0.064536, loss_ce: 0.018124
2021-12-11 22:36:00,926 iteration 3851 : loss : 0.060642, loss_ce: 0.034580
2021-12-11 22:36:03,803 iteration 3852 : loss : 0.057855, loss_ce: 0.021721
2021-12-11 22:36:06,617 iteration 3853 : loss : 0.042406, loss_ce: 0.024355
2021-12-11 22:36:09,456 iteration 3854 : loss : 0.032831, loss_ce: 0.017163
2021-12-11 22:36:12,184 iteration 3855 : loss : 0.044287, loss_ce: 0.020713
2021-12-11 22:36:14,863 iteration 3856 : loss : 0.050769, loss_ce: 0.021623
2021-12-11 22:36:17,546 iteration 3857 : loss : 0.037661, loss_ce: 0.019358
2021-12-11 22:36:20,332 iteration 3858 : loss : 0.030906, loss_ce: 0.015083
2021-12-11 22:36:23,194 iteration 3859 : loss : 0.031793, loss_ce: 0.012439
 57%|███████████████▎           | 227/400 [3:13:21<2:25:53, 50.60s/it]2021-12-11 22:36:25,877 iteration 3860 : loss : 0.028881, loss_ce: 0.015665
2021-12-11 22:36:28,756 iteration 3861 : loss : 0.048333, loss_ce: 0.024085
2021-12-11 22:36:31,595 iteration 3862 : loss : 0.045526, loss_ce: 0.022163
2021-12-11 22:36:34,343 iteration 3863 : loss : 0.037563, loss_ce: 0.021142
2021-12-11 22:36:37,257 iteration 3864 : loss : 0.038942, loss_ce: 0.016389
2021-12-11 22:36:40,033 iteration 3865 : loss : 0.052783, loss_ce: 0.023084
2021-12-11 22:36:43,066 iteration 3866 : loss : 0.043075, loss_ce: 0.021215
2021-12-11 22:36:45,851 iteration 3867 : loss : 0.024607, loss_ce: 0.012997
2021-12-11 22:36:48,609 iteration 3868 : loss : 0.032558, loss_ce: 0.016023
2021-12-11 22:36:51,428 iteration 3869 : loss : 0.037245, loss_ce: 0.016556
2021-12-11 22:36:54,380 iteration 3870 : loss : 0.029613, loss_ce: 0.013426
2021-12-11 22:36:57,182 iteration 3871 : loss : 0.051893, loss_ce: 0.018249
2021-12-11 22:36:59,930 iteration 3872 : loss : 0.039087, loss_ce: 0.018923
2021-12-11 22:37:02,694 iteration 3873 : loss : 0.036106, loss_ce: 0.016731
2021-12-11 22:37:05,638 iteration 3874 : loss : 0.035140, loss_ce: 0.016768
2021-12-11 22:37:08,451 iteration 3875 : loss : 0.033850, loss_ce: 0.015737
2021-12-11 22:37:11,163 iteration 3876 : loss : 0.053635, loss_ce: 0.015740
 57%|███████████████▍           | 228/400 [3:14:09<2:22:47, 49.81s/it]2021-12-11 22:37:13,897 iteration 3877 : loss : 0.031371, loss_ce: 0.014963
2021-12-11 22:37:16,659 iteration 3878 : loss : 0.038482, loss_ce: 0.018439
2021-12-11 22:37:19,387 iteration 3879 : loss : 0.054858, loss_ce: 0.024490
2021-12-11 22:37:22,193 iteration 3880 : loss : 0.025929, loss_ce: 0.013634
2021-12-11 22:37:24,890 iteration 3881 : loss : 0.033605, loss_ce: 0.019310
2021-12-11 22:37:27,746 iteration 3882 : loss : 0.073284, loss_ce: 0.028352
2021-12-11 22:37:30,537 iteration 3883 : loss : 0.043405, loss_ce: 0.022240
2021-12-11 22:37:33,152 iteration 3884 : loss : 0.049670, loss_ce: 0.013445
2021-12-11 22:37:35,903 iteration 3885 : loss : 0.031937, loss_ce: 0.013083
2021-12-11 22:37:38,588 iteration 3886 : loss : 0.038633, loss_ce: 0.016575
2021-12-11 22:37:41,331 iteration 3887 : loss : 0.046380, loss_ce: 0.021271
2021-12-11 22:37:44,133 iteration 3888 : loss : 0.032586, loss_ce: 0.014790
2021-12-11 22:37:46,933 iteration 3889 : loss : 0.039931, loss_ce: 0.019580
2021-12-11 22:37:49,566 iteration 3890 : loss : 0.030943, loss_ce: 0.014444
2021-12-11 22:37:52,320 iteration 3891 : loss : 0.023516, loss_ce: 0.011076
2021-12-11 22:37:55,024 iteration 3892 : loss : 0.043867, loss_ce: 0.018953
2021-12-11 22:37:57,942 iteration 3893 : loss : 0.046657, loss_ce: 0.024108
 57%|███████████████▍           | 229/400 [3:14:56<2:19:21, 48.90s/it]2021-12-11 22:38:00,673 iteration 3894 : loss : 0.029899, loss_ce: 0.016218
2021-12-11 22:38:03,406 iteration 3895 : loss : 0.035659, loss_ce: 0.017736
2021-12-11 22:38:06,217 iteration 3896 : loss : 0.049584, loss_ce: 0.017013
2021-12-11 22:38:08,970 iteration 3897 : loss : 0.057406, loss_ce: 0.029170
2021-12-11 22:38:11,749 iteration 3898 : loss : 0.033361, loss_ce: 0.016372
2021-12-11 22:38:14,538 iteration 3899 : loss : 0.037038, loss_ce: 0.017988
2021-12-11 22:38:17,314 iteration 3900 : loss : 0.041652, loss_ce: 0.016559
2021-12-11 22:38:19,914 iteration 3901 : loss : 0.033214, loss_ce: 0.014148
2021-12-11 22:38:22,809 iteration 3902 : loss : 0.029629, loss_ce: 0.015531
2021-12-11 22:38:25,601 iteration 3903 : loss : 0.024157, loss_ce: 0.012063
2021-12-11 22:38:28,403 iteration 3904 : loss : 0.041015, loss_ce: 0.017041
2021-12-11 22:38:31,077 iteration 3905 : loss : 0.050517, loss_ce: 0.025959
2021-12-11 22:38:33,818 iteration 3906 : loss : 0.033053, loss_ce: 0.018326
2021-12-11 22:38:36,538 iteration 3907 : loss : 0.035133, loss_ce: 0.014930
2021-12-11 22:38:39,368 iteration 3908 : loss : 0.054959, loss_ce: 0.022757
2021-12-11 22:38:42,122 iteration 3909 : loss : 0.034594, loss_ce: 0.017673
2021-12-11 22:38:42,122 Training Data Eval:
2021-12-11 22:38:56,911   Average segmentation loss on training set: 0.0190
2021-12-11 22:38:56,912 Validation Data Eval:
2021-12-11 22:39:01,974   Average segmentation loss on validation set: 0.0919
2021-12-11 22:39:04,604 iteration 3910 : loss : 0.026976, loss_ce: 0.012374
 57%|███████████████▌           | 230/400 [3:16:02<2:33:38, 54.23s/it]2021-12-11 22:39:07,399 iteration 3911 : loss : 0.029295, loss_ce: 0.012392
2021-12-11 22:39:10,212 iteration 3912 : loss : 0.042376, loss_ce: 0.019781
2021-12-11 22:39:12,922 iteration 3913 : loss : 0.030045, loss_ce: 0.013085
2021-12-11 22:39:15,599 iteration 3914 : loss : 0.024781, loss_ce: 0.011911
2021-12-11 22:39:18,262 iteration 3915 : loss : 0.029331, loss_ce: 0.013371
2021-12-11 22:39:21,150 iteration 3916 : loss : 0.047500, loss_ce: 0.018879
2021-12-11 22:39:24,046 iteration 3917 : loss : 0.082907, loss_ce: 0.023465
2021-12-11 22:39:26,749 iteration 3918 : loss : 0.041979, loss_ce: 0.020833
2021-12-11 22:39:29,586 iteration 3919 : loss : 0.037334, loss_ce: 0.016177
2021-12-11 22:39:32,408 iteration 3920 : loss : 0.049426, loss_ce: 0.024261
2021-12-11 22:39:35,062 iteration 3921 : loss : 0.045100, loss_ce: 0.019887
2021-12-11 22:39:37,812 iteration 3922 : loss : 0.021929, loss_ce: 0.011541
2021-12-11 22:39:40,624 iteration 3923 : loss : 0.038474, loss_ce: 0.019045
2021-12-11 22:39:43,406 iteration 3924 : loss : 0.045580, loss_ce: 0.021202
2021-12-11 22:39:46,270 iteration 3925 : loss : 0.052796, loss_ce: 0.022959
2021-12-11 22:39:49,022 iteration 3926 : loss : 0.038970, loss_ce: 0.019431
2021-12-11 22:39:51,942 iteration 3927 : loss : 0.033716, loss_ce: 0.016800
 58%|███████████████▌           | 231/400 [3:16:50<2:26:55, 52.16s/it]2021-12-11 22:39:54,750 iteration 3928 : loss : 0.031180, loss_ce: 0.015801
2021-12-11 22:39:57,642 iteration 3929 : loss : 0.040013, loss_ce: 0.019471
2021-12-11 22:40:00,532 iteration 3930 : loss : 0.073763, loss_ce: 0.028240
2021-12-11 22:40:03,343 iteration 3931 : loss : 0.040130, loss_ce: 0.020427
2021-12-11 22:40:06,284 iteration 3932 : loss : 0.039518, loss_ce: 0.014282
2021-12-11 22:40:09,017 iteration 3933 : loss : 0.051514, loss_ce: 0.018285
2021-12-11 22:40:11,830 iteration 3934 : loss : 0.036206, loss_ce: 0.015470
2021-12-11 22:40:14,715 iteration 3935 : loss : 0.035751, loss_ce: 0.015969
2021-12-11 22:40:17,449 iteration 3936 : loss : 0.031904, loss_ce: 0.015502
2021-12-11 22:40:20,242 iteration 3937 : loss : 0.032556, loss_ce: 0.012622
2021-12-11 22:40:22,886 iteration 3938 : loss : 0.033456, loss_ce: 0.017759
2021-12-11 22:40:25,718 iteration 3939 : loss : 0.039771, loss_ce: 0.017075
2021-12-11 22:40:28,432 iteration 3940 : loss : 0.032740, loss_ce: 0.012240
2021-12-11 22:40:31,055 iteration 3941 : loss : 0.038767, loss_ce: 0.022702
2021-12-11 22:40:33,688 iteration 3942 : loss : 0.047683, loss_ce: 0.024046
2021-12-11 22:40:36,540 iteration 3943 : loss : 0.040243, loss_ce: 0.020807
2021-12-11 22:40:39,372 iteration 3944 : loss : 0.033777, loss_ce: 0.013762
 58%|███████████████▋           | 232/400 [3:17:37<2:22:04, 50.74s/it]2021-12-11 22:40:42,206 iteration 3945 : loss : 0.026319, loss_ce: 0.014236
2021-12-11 22:40:44,811 iteration 3946 : loss : 0.020256, loss_ce: 0.011219
2021-12-11 22:40:47,583 iteration 3947 : loss : 0.030969, loss_ce: 0.014023
2021-12-11 22:40:50,270 iteration 3948 : loss : 0.045370, loss_ce: 0.022066
2021-12-11 22:40:53,112 iteration 3949 : loss : 0.030358, loss_ce: 0.017217
2021-12-11 22:40:55,831 iteration 3950 : loss : 0.032390, loss_ce: 0.014929
2021-12-11 22:40:58,481 iteration 3951 : loss : 0.034224, loss_ce: 0.017020
2021-12-11 22:41:01,117 iteration 3952 : loss : 0.029145, loss_ce: 0.016260
2021-12-11 22:41:03,852 iteration 3953 : loss : 0.029769, loss_ce: 0.012975
2021-12-11 22:41:06,507 iteration 3954 : loss : 0.035977, loss_ce: 0.017768
2021-12-11 22:41:09,149 iteration 3955 : loss : 0.044093, loss_ce: 0.020233
2021-12-11 22:41:11,850 iteration 3956 : loss : 0.034359, loss_ce: 0.017864
2021-12-11 22:41:14,629 iteration 3957 : loss : 0.042591, loss_ce: 0.020574
2021-12-11 22:41:17,521 iteration 3958 : loss : 0.042932, loss_ce: 0.020827
2021-12-11 22:41:20,311 iteration 3959 : loss : 0.031776, loss_ce: 0.012745
2021-12-11 22:41:23,222 iteration 3960 : loss : 0.050906, loss_ce: 0.016537
2021-12-11 22:41:26,101 iteration 3961 : loss : 0.027521, loss_ce: 0.014549
 58%|███████████████▋           | 233/400 [3:18:24<2:17:53, 49.54s/it]2021-12-11 22:41:28,933 iteration 3962 : loss : 0.049945, loss_ce: 0.022474
2021-12-11 22:41:31,623 iteration 3963 : loss : 0.030251, loss_ce: 0.016891
2021-12-11 22:41:34,560 iteration 3964 : loss : 0.042931, loss_ce: 0.020066
2021-12-11 22:41:37,564 iteration 3965 : loss : 0.063925, loss_ce: 0.022032
2021-12-11 22:41:40,258 iteration 3966 : loss : 0.035160, loss_ce: 0.015310
2021-12-11 22:41:43,003 iteration 3967 : loss : 0.037037, loss_ce: 0.013749
2021-12-11 22:41:45,783 iteration 3968 : loss : 0.035566, loss_ce: 0.016369
2021-12-11 22:41:48,653 iteration 3969 : loss : 0.038781, loss_ce: 0.019753
2021-12-11 22:41:51,585 iteration 3970 : loss : 0.044835, loss_ce: 0.025891
2021-12-11 22:41:54,195 iteration 3971 : loss : 0.034727, loss_ce: 0.015424
2021-12-11 22:41:57,049 iteration 3972 : loss : 0.047859, loss_ce: 0.021125
2021-12-11 22:41:59,959 iteration 3973 : loss : 0.033661, loss_ce: 0.015384
2021-12-11 22:42:02,970 iteration 3974 : loss : 0.054417, loss_ce: 0.026417
2021-12-11 22:42:05,901 iteration 3975 : loss : 0.035529, loss_ce: 0.015694
2021-12-11 22:42:08,931 iteration 3976 : loss : 0.044290, loss_ce: 0.022309
2021-12-11 22:42:11,623 iteration 3977 : loss : 0.032964, loss_ce: 0.015533
2021-12-11 22:42:14,484 iteration 3978 : loss : 0.027886, loss_ce: 0.014673
 58%|███████████████▊           | 234/400 [3:19:12<2:16:05, 49.19s/it]2021-12-11 22:42:17,261 iteration 3979 : loss : 0.045465, loss_ce: 0.017808
2021-12-11 22:42:20,206 iteration 3980 : loss : 0.035547, loss_ce: 0.020183
2021-12-11 22:42:22,975 iteration 3981 : loss : 0.035989, loss_ce: 0.016000
2021-12-11 22:42:25,913 iteration 3982 : loss : 0.033215, loss_ce: 0.014176
2021-12-11 22:42:28,626 iteration 3983 : loss : 0.032399, loss_ce: 0.012076
2021-12-11 22:42:31,501 iteration 3984 : loss : 0.033278, loss_ce: 0.015493
2021-12-11 22:42:34,414 iteration 3985 : loss : 0.035946, loss_ce: 0.016419
2021-12-11 22:42:37,230 iteration 3986 : loss : 0.035554, loss_ce: 0.015461
2021-12-11 22:42:40,039 iteration 3987 : loss : 0.026420, loss_ce: 0.013994
2021-12-11 22:42:42,878 iteration 3988 : loss : 0.041263, loss_ce: 0.023335
2021-12-11 22:42:45,635 iteration 3989 : loss : 0.036682, loss_ce: 0.016950
2021-12-11 22:42:48,308 iteration 3990 : loss : 0.030543, loss_ce: 0.014066
2021-12-11 22:42:51,047 iteration 3991 : loss : 0.036053, loss_ce: 0.016230
2021-12-11 22:42:53,937 iteration 3992 : loss : 0.063861, loss_ce: 0.021044
2021-12-11 22:42:56,828 iteration 3993 : loss : 0.041046, loss_ce: 0.024347
2021-12-11 22:42:59,658 iteration 3994 : loss : 0.041848, loss_ce: 0.021779
2021-12-11 22:42:59,658 Training Data Eval:
2021-12-11 22:43:14,530   Average segmentation loss on training set: 0.0180
2021-12-11 22:43:14,530 Validation Data Eval:
2021-12-11 22:43:19,540   Average segmentation loss on validation set: 0.0931
2021-12-11 22:43:22,304 iteration 3995 : loss : 0.039594, loss_ce: 0.015619
 59%|███████████████▊           | 235/400 [3:20:20<2:30:38, 54.78s/it]2021-12-11 22:43:25,168 iteration 3996 : loss : 0.047629, loss_ce: 0.024643
2021-12-11 22:43:27,939 iteration 3997 : loss : 0.036514, loss_ce: 0.016303
2021-12-11 22:43:30,841 iteration 3998 : loss : 0.042631, loss_ce: 0.019313
2021-12-11 22:43:33,587 iteration 3999 : loss : 0.034069, loss_ce: 0.013224
2021-12-11 22:43:36,425 iteration 4000 : loss : 0.032200, loss_ce: 0.015589
2021-12-11 22:43:39,280 iteration 4001 : loss : 0.048773, loss_ce: 0.017421
2021-12-11 22:43:42,031 iteration 4002 : loss : 0.029106, loss_ce: 0.015643
2021-12-11 22:43:44,769 iteration 4003 : loss : 0.037481, loss_ce: 0.017246
2021-12-11 22:43:47,582 iteration 4004 : loss : 0.041314, loss_ce: 0.019308
2021-12-11 22:43:50,504 iteration 4005 : loss : 0.042124, loss_ce: 0.021360
2021-12-11 22:43:53,360 iteration 4006 : loss : 0.038112, loss_ce: 0.015161
2021-12-11 22:43:56,074 iteration 4007 : loss : 0.030076, loss_ce: 0.013693
2021-12-11 22:43:58,829 iteration 4008 : loss : 0.041429, loss_ce: 0.016435
2021-12-11 22:44:01,561 iteration 4009 : loss : 0.028210, loss_ce: 0.013887
2021-12-11 22:44:04,259 iteration 4010 : loss : 0.020427, loss_ce: 0.010863
2021-12-11 22:44:07,073 iteration 4011 : loss : 0.036917, loss_ce: 0.013660
2021-12-11 22:44:09,894 iteration 4012 : loss : 0.046635, loss_ce: 0.024462
 59%|███████████████▉           | 236/400 [3:21:08<2:23:50, 52.62s/it]2021-12-11 22:44:12,853 iteration 4013 : loss : 0.042549, loss_ce: 0.014488
2021-12-11 22:44:15,530 iteration 4014 : loss : 0.038816, loss_ce: 0.015070
2021-12-11 22:44:18,352 iteration 4015 : loss : 0.043638, loss_ce: 0.024422
2021-12-11 22:44:21,151 iteration 4016 : loss : 0.044448, loss_ce: 0.018065
2021-12-11 22:44:23,905 iteration 4017 : loss : 0.043148, loss_ce: 0.017300
2021-12-11 22:44:26,789 iteration 4018 : loss : 0.036833, loss_ce: 0.015482
2021-12-11 22:44:29,597 iteration 4019 : loss : 0.055403, loss_ce: 0.026805
2021-12-11 22:44:32,493 iteration 4020 : loss : 0.026048, loss_ce: 0.012307
2021-12-11 22:44:35,384 iteration 4021 : loss : 0.029058, loss_ce: 0.013418
2021-12-11 22:44:38,162 iteration 4022 : loss : 0.062314, loss_ce: 0.021551
2021-12-11 22:44:40,927 iteration 4023 : loss : 0.049135, loss_ce: 0.025283
2021-12-11 22:44:43,770 iteration 4024 : loss : 0.045865, loss_ce: 0.016899
2021-12-11 22:44:46,443 iteration 4025 : loss : 0.036258, loss_ce: 0.017227
2021-12-11 22:44:49,202 iteration 4026 : loss : 0.022073, loss_ce: 0.012306
2021-12-11 22:44:51,996 iteration 4027 : loss : 0.035004, loss_ce: 0.015243
2021-12-11 22:44:54,696 iteration 4028 : loss : 0.033739, loss_ce: 0.013595
2021-12-11 22:44:57,527 iteration 4029 : loss : 0.033387, loss_ce: 0.015700
 59%|███████████████▉           | 237/400 [3:21:55<2:18:53, 51.12s/it]2021-12-11 22:45:00,324 iteration 4030 : loss : 0.040922, loss_ce: 0.019253
2021-12-11 22:45:03,105 iteration 4031 : loss : 0.028372, loss_ce: 0.013206
2021-12-11 22:45:05,871 iteration 4032 : loss : 0.030562, loss_ce: 0.014102
2021-12-11 22:45:08,753 iteration 4033 : loss : 0.053738, loss_ce: 0.018637
2021-12-11 22:45:11,532 iteration 4034 : loss : 0.037667, loss_ce: 0.014975
2021-12-11 22:45:14,336 iteration 4035 : loss : 0.042656, loss_ce: 0.018637
2021-12-11 22:45:17,116 iteration 4036 : loss : 0.023989, loss_ce: 0.011158
2021-12-11 22:45:19,987 iteration 4037 : loss : 0.032270, loss_ce: 0.014213
2021-12-11 22:45:22,702 iteration 4038 : loss : 0.051181, loss_ce: 0.029350
2021-12-11 22:45:25,533 iteration 4039 : loss : 0.037350, loss_ce: 0.019813
2021-12-11 22:45:28,210 iteration 4040 : loss : 0.055016, loss_ce: 0.016714
2021-12-11 22:45:31,144 iteration 4041 : loss : 0.044453, loss_ce: 0.023718
2021-12-11 22:45:33,868 iteration 4042 : loss : 0.031650, loss_ce: 0.016782
2021-12-11 22:45:36,670 iteration 4043 : loss : 0.030439, loss_ce: 0.014280
2021-12-11 22:45:39,476 iteration 4044 : loss : 0.036663, loss_ce: 0.018520
2021-12-11 22:45:42,146 iteration 4045 : loss : 0.034144, loss_ce: 0.016344
2021-12-11 22:45:44,949 iteration 4046 : loss : 0.036662, loss_ce: 0.014815
 60%|████████████████           | 238/400 [3:22:43<2:15:02, 50.02s/it]2021-12-11 22:45:47,856 iteration 4047 : loss : 0.027784, loss_ce: 0.014116
2021-12-11 22:45:50,645 iteration 4048 : loss : 0.026773, loss_ce: 0.011372
2021-12-11 22:45:53,449 iteration 4049 : loss : 0.042413, loss_ce: 0.020555
2021-12-11 22:45:56,206 iteration 4050 : loss : 0.036037, loss_ce: 0.014894
2021-12-11 22:45:59,124 iteration 4051 : loss : 0.023526, loss_ce: 0.011462
2021-12-11 22:46:01,885 iteration 4052 : loss : 0.028469, loss_ce: 0.012456
2021-12-11 22:46:04,496 iteration 4053 : loss : 0.048333, loss_ce: 0.025526
2021-12-11 22:46:07,111 iteration 4054 : loss : 0.034482, loss_ce: 0.017472
2021-12-11 22:46:09,992 iteration 4055 : loss : 0.046938, loss_ce: 0.017337
2021-12-11 22:46:12,764 iteration 4056 : loss : 0.055095, loss_ce: 0.019813
2021-12-11 22:46:15,578 iteration 4057 : loss : 0.050299, loss_ce: 0.025461
2021-12-11 22:46:18,240 iteration 4058 : loss : 0.036284, loss_ce: 0.018283
2021-12-11 22:46:21,249 iteration 4059 : loss : 0.042727, loss_ce: 0.024290
2021-12-11 22:46:24,111 iteration 4060 : loss : 0.030773, loss_ce: 0.014795
2021-12-11 22:46:26,933 iteration 4061 : loss : 0.086238, loss_ce: 0.019250
2021-12-11 22:46:29,593 iteration 4062 : loss : 0.042038, loss_ce: 0.022099
2021-12-11 22:46:32,357 iteration 4063 : loss : 0.077744, loss_ce: 0.032315
 60%|████████████████▏          | 239/400 [3:23:30<2:12:06, 49.23s/it]2021-12-11 22:46:35,184 iteration 4064 : loss : 0.036272, loss_ce: 0.014884
2021-12-11 22:46:37,822 iteration 4065 : loss : 0.034320, loss_ce: 0.014311
2021-12-11 22:46:40,619 iteration 4066 : loss : 0.043656, loss_ce: 0.016439
2021-12-11 22:46:43,181 iteration 4067 : loss : 0.043584, loss_ce: 0.014571
2021-12-11 22:46:46,019 iteration 4068 : loss : 0.034158, loss_ce: 0.015474
2021-12-11 22:46:48,744 iteration 4069 : loss : 0.040327, loss_ce: 0.017323
2021-12-11 22:46:51,566 iteration 4070 : loss : 0.044639, loss_ce: 0.025871
2021-12-11 22:46:54,162 iteration 4071 : loss : 0.035780, loss_ce: 0.017145
2021-12-11 22:46:56,878 iteration 4072 : loss : 0.031862, loss_ce: 0.016703
2021-12-11 22:46:59,540 iteration 4073 : loss : 0.049132, loss_ce: 0.027246
2021-12-11 22:47:02,217 iteration 4074 : loss : 0.037088, loss_ce: 0.015186
2021-12-11 22:47:05,078 iteration 4075 : loss : 0.049724, loss_ce: 0.020202
2021-12-11 22:47:07,721 iteration 4076 : loss : 0.028891, loss_ce: 0.012622
2021-12-11 22:47:10,584 iteration 4077 : loss : 0.028043, loss_ce: 0.016075
2021-12-11 22:47:13,417 iteration 4078 : loss : 0.056478, loss_ce: 0.018016
2021-12-11 22:47:16,274 iteration 4079 : loss : 0.025782, loss_ce: 0.012231
2021-12-11 22:47:16,274 Training Data Eval:
2021-12-11 22:47:31,012   Average segmentation loss on training set: 0.0199
2021-12-11 22:47:31,013 Validation Data Eval:
2021-12-11 22:47:36,071   Average segmentation loss on validation set: 0.0846
2021-12-11 22:47:38,861 iteration 4080 : loss : 0.054592, loss_ce: 0.017269
 60%|████████████████▏          | 240/400 [3:24:37<2:25:06, 54.41s/it]2021-12-11 22:47:41,688 iteration 4081 : loss : 0.033991, loss_ce: 0.018987
2021-12-11 22:47:44,460 iteration 4082 : loss : 0.038105, loss_ce: 0.017657
2021-12-11 22:47:47,200 iteration 4083 : loss : 0.039322, loss_ce: 0.020340
2021-12-11 22:47:49,969 iteration 4084 : loss : 0.042048, loss_ce: 0.017169
2021-12-11 22:47:52,788 iteration 4085 : loss : 0.040778, loss_ce: 0.013230
2021-12-11 22:47:55,508 iteration 4086 : loss : 0.034191, loss_ce: 0.014134
2021-12-11 22:47:58,374 iteration 4087 : loss : 0.040546, loss_ce: 0.015161
2021-12-11 22:48:01,231 iteration 4088 : loss : 0.032097, loss_ce: 0.017777
2021-12-11 22:48:04,027 iteration 4089 : loss : 0.040321, loss_ce: 0.018373
2021-12-11 22:48:06,684 iteration 4090 : loss : 0.042586, loss_ce: 0.024108
2021-12-11 22:48:09,625 iteration 4091 : loss : 0.040974, loss_ce: 0.016006
2021-12-11 22:48:12,382 iteration 4092 : loss : 0.036065, loss_ce: 0.013373
2021-12-11 22:48:15,026 iteration 4093 : loss : 0.027555, loss_ce: 0.014402
2021-12-11 22:48:18,004 iteration 4094 : loss : 0.034136, loss_ce: 0.012667
2021-12-11 22:48:20,895 iteration 4095 : loss : 0.043004, loss_ce: 0.016515
2021-12-11 22:48:23,672 iteration 4096 : loss : 0.035373, loss_ce: 0.016821
2021-12-11 22:48:26,522 iteration 4097 : loss : 0.045389, loss_ce: 0.020203
 60%|████████████████▎          | 241/400 [3:25:24<2:18:49, 52.39s/it]2021-12-11 22:48:29,386 iteration 4098 : loss : 0.026799, loss_ce: 0.013781
2021-12-11 22:48:32,315 iteration 4099 : loss : 0.042008, loss_ce: 0.016335
2021-12-11 22:48:35,037 iteration 4100 : loss : 0.051052, loss_ce: 0.021478
2021-12-11 22:48:37,791 iteration 4101 : loss : 0.029769, loss_ce: 0.016637
2021-12-11 22:48:40,695 iteration 4102 : loss : 0.043295, loss_ce: 0.017783
2021-12-11 22:48:43,313 iteration 4103 : loss : 0.027065, loss_ce: 0.013186
2021-12-11 22:48:46,063 iteration 4104 : loss : 0.052486, loss_ce: 0.021593
2021-12-11 22:48:48,839 iteration 4105 : loss : 0.041437, loss_ce: 0.018002
2021-12-11 22:48:51,671 iteration 4106 : loss : 0.041644, loss_ce: 0.018368
2021-12-11 22:48:54,441 iteration 4107 : loss : 0.043688, loss_ce: 0.021004
2021-12-11 22:48:57,255 iteration 4108 : loss : 0.034486, loss_ce: 0.017237
2021-12-11 22:48:59,993 iteration 4109 : loss : 0.041803, loss_ce: 0.016235
2021-12-11 22:49:02,891 iteration 4110 : loss : 0.039570, loss_ce: 0.017943
2021-12-11 22:49:05,710 iteration 4111 : loss : 0.036773, loss_ce: 0.014938
2021-12-11 22:49:08,449 iteration 4112 : loss : 0.027525, loss_ce: 0.013812
2021-12-11 22:49:11,335 iteration 4113 : loss : 0.040563, loss_ce: 0.017194
2021-12-11 22:49:13,954 iteration 4114 : loss : 0.039234, loss_ce: 0.018076
 60%|████████████████▎          | 242/400 [3:26:12<2:14:02, 50.90s/it]2021-12-11 22:49:16,745 iteration 4115 : loss : 0.036748, loss_ce: 0.013941
2021-12-11 22:49:19,383 iteration 4116 : loss : 0.026186, loss_ce: 0.013397
2021-12-11 22:49:22,236 iteration 4117 : loss : 0.035097, loss_ce: 0.020085
2021-12-11 22:49:24,889 iteration 4118 : loss : 0.040337, loss_ce: 0.015968
2021-12-11 22:49:27,530 iteration 4119 : loss : 0.024163, loss_ce: 0.012214
2021-12-11 22:49:30,367 iteration 4120 : loss : 0.033362, loss_ce: 0.012825
2021-12-11 22:49:33,183 iteration 4121 : loss : 0.028848, loss_ce: 0.014059
2021-12-11 22:49:35,953 iteration 4122 : loss : 0.035486, loss_ce: 0.015147
2021-12-11 22:49:38,740 iteration 4123 : loss : 0.043092, loss_ce: 0.020590
2021-12-11 22:49:41,477 iteration 4124 : loss : 0.026657, loss_ce: 0.012857
2021-12-11 22:49:44,217 iteration 4125 : loss : 0.041043, loss_ce: 0.016330
2021-12-11 22:49:47,280 iteration 4126 : loss : 0.039201, loss_ce: 0.024450
2021-12-11 22:49:50,097 iteration 4127 : loss : 0.043359, loss_ce: 0.022141
2021-12-11 22:49:52,904 iteration 4128 : loss : 0.029170, loss_ce: 0.014990
2021-12-11 22:49:55,800 iteration 4129 : loss : 0.052803, loss_ce: 0.016407
2021-12-11 22:49:58,581 iteration 4130 : loss : 0.029804, loss_ce: 0.013193
2021-12-11 22:50:01,402 iteration 4131 : loss : 0.037986, loss_ce: 0.019388
 61%|████████████████▍          | 243/400 [3:26:59<2:10:28, 49.86s/it]2021-12-11 22:50:04,322 iteration 4132 : loss : 0.040272, loss_ce: 0.017683
2021-12-11 22:50:06,965 iteration 4133 : loss : 0.035663, loss_ce: 0.018427
2021-12-11 22:50:09,838 iteration 4134 : loss : 0.024653, loss_ce: 0.013571
2021-12-11 22:50:12,484 iteration 4135 : loss : 0.038379, loss_ce: 0.016729
2021-12-11 22:50:15,127 iteration 4136 : loss : 0.030372, loss_ce: 0.013755
2021-12-11 22:50:17,970 iteration 4137 : loss : 0.049145, loss_ce: 0.026347
2021-12-11 22:50:20,802 iteration 4138 : loss : 0.042451, loss_ce: 0.022136
2021-12-11 22:50:23,542 iteration 4139 : loss : 0.054153, loss_ce: 0.019366
2021-12-11 22:50:26,417 iteration 4140 : loss : 0.040037, loss_ce: 0.015702
2021-12-11 22:50:29,297 iteration 4141 : loss : 0.040800, loss_ce: 0.012909
2021-12-11 22:50:32,101 iteration 4142 : loss : 0.029400, loss_ce: 0.015873
2021-12-11 22:50:34,840 iteration 4143 : loss : 0.034822, loss_ce: 0.016636
2021-12-11 22:50:37,890 iteration 4144 : loss : 0.050556, loss_ce: 0.019375
2021-12-11 22:50:40,708 iteration 4145 : loss : 0.041989, loss_ce: 0.019740
2021-12-11 22:50:43,212 iteration 4146 : loss : 0.030441, loss_ce: 0.011870
2021-12-11 22:50:46,045 iteration 4147 : loss : 0.042787, loss_ce: 0.017230
2021-12-11 22:50:48,754 iteration 4148 : loss : 0.039499, loss_ce: 0.015126
 61%|████████████████▍          | 244/400 [3:27:46<2:07:41, 49.11s/it]2021-12-11 22:50:51,637 iteration 4149 : loss : 0.037619, loss_ce: 0.017808
2021-12-11 22:50:54,255 iteration 4150 : loss : 0.033515, loss_ce: 0.014803
2021-12-11 22:50:56,973 iteration 4151 : loss : 0.031650, loss_ce: 0.016201
2021-12-11 22:50:59,871 iteration 4152 : loss : 0.038272, loss_ce: 0.017402
2021-12-11 22:51:02,697 iteration 4153 : loss : 0.041462, loss_ce: 0.021493
2021-12-11 22:51:05,491 iteration 4154 : loss : 0.026358, loss_ce: 0.014202
2021-12-11 22:51:08,089 iteration 4155 : loss : 0.026784, loss_ce: 0.015317
2021-12-11 22:51:10,871 iteration 4156 : loss : 0.036893, loss_ce: 0.015417
2021-12-11 22:51:13,517 iteration 4157 : loss : 0.036765, loss_ce: 0.016208
2021-12-11 22:51:16,248 iteration 4158 : loss : 0.026894, loss_ce: 0.013323
2021-12-11 22:51:19,032 iteration 4159 : loss : 0.032295, loss_ce: 0.015944
2021-12-11 22:51:21,687 iteration 4160 : loss : 0.040534, loss_ce: 0.017420
2021-12-11 22:51:24,357 iteration 4161 : loss : 0.038467, loss_ce: 0.014635
2021-12-11 22:51:27,260 iteration 4162 : loss : 0.052368, loss_ce: 0.021682
2021-12-11 22:51:30,114 iteration 4163 : loss : 0.031276, loss_ce: 0.014291
2021-12-11 22:51:32,865 iteration 4164 : loss : 0.026002, loss_ce: 0.011877
2021-12-11 22:51:32,865 Training Data Eval:
2021-12-11 22:51:47,602   Average segmentation loss on training set: 0.0171
2021-12-11 22:51:47,602 Validation Data Eval:
2021-12-11 22:51:52,824   Average segmentation loss on validation set: 0.0831
2021-12-11 22:51:55,590 iteration 4165 : loss : 0.031776, loss_ce: 0.012589
 61%|████████████████▌          | 245/400 [3:28:53<2:20:36, 54.43s/it]2021-12-11 22:51:58,534 iteration 4166 : loss : 0.053895, loss_ce: 0.015481
2021-12-11 22:52:01,299 iteration 4167 : loss : 0.024602, loss_ce: 0.012765
2021-12-11 22:52:03,936 iteration 4168 : loss : 0.028786, loss_ce: 0.012674
2021-12-11 22:52:06,540 iteration 4169 : loss : 0.032824, loss_ce: 0.015287
2021-12-11 22:52:09,331 iteration 4170 : loss : 0.036979, loss_ce: 0.016124
2021-12-11 22:52:12,312 iteration 4171 : loss : 0.039880, loss_ce: 0.016670
2021-12-11 22:52:15,002 iteration 4172 : loss : 0.033590, loss_ce: 0.011326
2021-12-11 22:52:17,693 iteration 4173 : loss : 0.048346, loss_ce: 0.020491
2021-12-11 22:52:20,524 iteration 4174 : loss : 0.037927, loss_ce: 0.018863
2021-12-11 22:52:23,329 iteration 4175 : loss : 0.039557, loss_ce: 0.016441
2021-12-11 22:52:26,138 iteration 4176 : loss : 0.041654, loss_ce: 0.019547
2021-12-11 22:52:29,086 iteration 4177 : loss : 0.045501, loss_ce: 0.018438
2021-12-11 22:52:31,968 iteration 4178 : loss : 0.031663, loss_ce: 0.016880
2021-12-11 22:52:34,935 iteration 4179 : loss : 0.034289, loss_ce: 0.016931
2021-12-11 22:52:37,722 iteration 4180 : loss : 0.056645, loss_ce: 0.024719
2021-12-11 22:52:40,497 iteration 4181 : loss : 0.065779, loss_ce: 0.019591
2021-12-11 22:52:43,369 iteration 4182 : loss : 0.036425, loss_ce: 0.017268
 62%|████████████████▌          | 246/400 [3:29:41<2:14:34, 52.43s/it]2021-12-11 22:52:46,227 iteration 4183 : loss : 0.033709, loss_ce: 0.015942
2021-12-11 22:52:48,997 iteration 4184 : loss : 0.038653, loss_ce: 0.018188
2021-12-11 22:52:51,898 iteration 4185 : loss : 0.032591, loss_ce: 0.014463
2021-12-11 22:52:54,615 iteration 4186 : loss : 0.031275, loss_ce: 0.013809
2021-12-11 22:52:57,563 iteration 4187 : loss : 0.038342, loss_ce: 0.016438
2021-12-11 22:53:00,421 iteration 4188 : loss : 0.041827, loss_ce: 0.023524
2021-12-11 22:53:03,073 iteration 4189 : loss : 0.032346, loss_ce: 0.012743
2021-12-11 22:53:06,091 iteration 4190 : loss : 0.045791, loss_ce: 0.019312
2021-12-11 22:53:08,978 iteration 4191 : loss : 0.037013, loss_ce: 0.014991
2021-12-11 22:53:11,792 iteration 4192 : loss : 0.037473, loss_ce: 0.014948
2021-12-11 22:53:14,683 iteration 4193 : loss : 0.029414, loss_ce: 0.012478
2021-12-11 22:53:17,417 iteration 4194 : loss : 0.040771, loss_ce: 0.018948
2021-12-11 22:53:20,384 iteration 4195 : loss : 0.061367, loss_ce: 0.020745
2021-12-11 22:53:23,316 iteration 4196 : loss : 0.026451, loss_ce: 0.015975
2021-12-11 22:53:26,145 iteration 4197 : loss : 0.030976, loss_ce: 0.012098
2021-12-11 22:53:28,971 iteration 4198 : loss : 0.035395, loss_ce: 0.017569
2021-12-11 22:53:31,582 iteration 4199 : loss : 0.040922, loss_ce: 0.021023
 62%|████████████████▋          | 247/400 [3:30:29<2:10:28, 51.17s/it]2021-12-11 22:53:34,328 iteration 4200 : loss : 0.033791, loss_ce: 0.013987
2021-12-11 22:53:37,298 iteration 4201 : loss : 0.034681, loss_ce: 0.018466
2021-12-11 22:53:39,905 iteration 4202 : loss : 0.028398, loss_ce: 0.015382
2021-12-11 22:53:42,598 iteration 4203 : loss : 0.027622, loss_ce: 0.014515
2021-12-11 22:53:45,415 iteration 4204 : loss : 0.034049, loss_ce: 0.015554
2021-12-11 22:53:48,238 iteration 4205 : loss : 0.031969, loss_ce: 0.014151
2021-12-11 22:53:51,026 iteration 4206 : loss : 0.035199, loss_ce: 0.013115
2021-12-11 22:53:53,778 iteration 4207 : loss : 0.025509, loss_ce: 0.013666
2021-12-11 22:53:56,506 iteration 4208 : loss : 0.035895, loss_ce: 0.016614
2021-12-11 22:53:59,344 iteration 4209 : loss : 0.045584, loss_ce: 0.014783
2021-12-11 22:54:02,302 iteration 4210 : loss : 0.043968, loss_ce: 0.020184
2021-12-11 22:54:04,883 iteration 4211 : loss : 0.022109, loss_ce: 0.011939
2021-12-11 22:54:07,536 iteration 4212 : loss : 0.033210, loss_ce: 0.018713
2021-12-11 22:54:10,420 iteration 4213 : loss : 0.043421, loss_ce: 0.021172
2021-12-11 22:54:13,230 iteration 4214 : loss : 0.048474, loss_ce: 0.019028
2021-12-11 22:54:15,970 iteration 4215 : loss : 0.030020, loss_ce: 0.013300
2021-12-11 22:54:18,830 iteration 4216 : loss : 0.037901, loss_ce: 0.015615
 62%|████████████████▋          | 248/400 [3:31:17<2:06:38, 49.99s/it]2021-12-11 22:54:21,710 iteration 4217 : loss : 0.033401, loss_ce: 0.017122
2021-12-11 22:54:24,416 iteration 4218 : loss : 0.026867, loss_ce: 0.015838
2021-12-11 22:54:27,172 iteration 4219 : loss : 0.035104, loss_ce: 0.017597
2021-12-11 22:54:30,076 iteration 4220 : loss : 0.043220, loss_ce: 0.018457
2021-12-11 22:54:32,747 iteration 4221 : loss : 0.040149, loss_ce: 0.016653
2021-12-11 22:54:35,677 iteration 4222 : loss : 0.035267, loss_ce: 0.017016
2021-12-11 22:54:38,535 iteration 4223 : loss : 0.034960, loss_ce: 0.015925
2021-12-11 22:54:41,189 iteration 4224 : loss : 0.029119, loss_ce: 0.012965
2021-12-11 22:54:43,955 iteration 4225 : loss : 0.038764, loss_ce: 0.015894
2021-12-11 22:54:46,678 iteration 4226 : loss : 0.031591, loss_ce: 0.013260
2021-12-11 22:54:49,578 iteration 4227 : loss : 0.041330, loss_ce: 0.018239
2021-12-11 22:54:52,221 iteration 4228 : loss : 0.029449, loss_ce: 0.013649
2021-12-11 22:54:54,908 iteration 4229 : loss : 0.035168, loss_ce: 0.017101
2021-12-11 22:54:57,687 iteration 4230 : loss : 0.039608, loss_ce: 0.021537
2021-12-11 22:55:00,596 iteration 4231 : loss : 0.036627, loss_ce: 0.015594
2021-12-11 22:55:03,303 iteration 4232 : loss : 0.040357, loss_ce: 0.019681
2021-12-11 22:55:06,127 iteration 4233 : loss : 0.041113, loss_ce: 0.021447
 62%|████████████████▊          | 249/400 [3:32:04<2:03:46, 49.18s/it]2021-12-11 22:55:09,021 iteration 4234 : loss : 0.061150, loss_ce: 0.032609
2021-12-11 22:55:11,901 iteration 4235 : loss : 0.051320, loss_ce: 0.017615
2021-12-11 22:55:14,550 iteration 4236 : loss : 0.032372, loss_ce: 0.014016
2021-12-11 22:55:17,157 iteration 4237 : loss : 0.031927, loss_ce: 0.015850
2021-12-11 22:55:19,936 iteration 4238 : loss : 0.032676, loss_ce: 0.013781
2021-12-11 22:55:22,766 iteration 4239 : loss : 0.030227, loss_ce: 0.014406
2021-12-11 22:55:25,505 iteration 4240 : loss : 0.049984, loss_ce: 0.022093
2021-12-11 22:55:28,302 iteration 4241 : loss : 0.033707, loss_ce: 0.015963
2021-12-11 22:55:30,939 iteration 4242 : loss : 0.021592, loss_ce: 0.011467
2021-12-11 22:55:33,739 iteration 4243 : loss : 0.041749, loss_ce: 0.017755
2021-12-11 22:55:36,460 iteration 4244 : loss : 0.029634, loss_ce: 0.013840
2021-12-11 22:55:39,149 iteration 4245 : loss : 0.028974, loss_ce: 0.014418
2021-12-11 22:55:41,879 iteration 4246 : loss : 0.033560, loss_ce: 0.015287
2021-12-11 22:55:44,486 iteration 4247 : loss : 0.025830, loss_ce: 0.011524
2021-12-11 22:55:47,118 iteration 4248 : loss : 0.042107, loss_ce: 0.021073
2021-12-11 22:55:49,987 iteration 4249 : loss : 0.037270, loss_ce: 0.016953
2021-12-11 22:55:49,987 Training Data Eval:
2021-12-11 22:56:04,769   Average segmentation loss on training set: 0.0175
2021-12-11 22:56:04,770 Validation Data Eval:
2021-12-11 22:56:09,911   Average segmentation loss on validation set: 0.0923
2021-12-11 22:56:12,739 iteration 4250 : loss : 0.041545, loss_ce: 0.017293
2021-12-11 22:56:14,768 save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_seed2epoch_249.pth
 62%|████████████████▉          | 250/400 [3:33:12<2:17:30, 55.00s/it]2021-12-11 22:56:17,080 iteration 4251 : loss : 0.038808, loss_ce: 0.016905
2021-12-11 22:56:19,817 iteration 4252 : loss : 0.037112, loss_ce: 0.012958
2021-12-11 22:56:22,657 iteration 4253 : loss : 0.028380, loss_ce: 0.015849
2021-12-11 22:56:25,405 iteration 4254 : loss : 0.031962, loss_ce: 0.015274
2021-12-11 22:56:28,115 iteration 4255 : loss : 0.066065, loss_ce: 0.020565
2021-12-11 22:56:30,902 iteration 4256 : loss : 0.036308, loss_ce: 0.017105
2021-12-11 22:56:33,718 iteration 4257 : loss : 0.032274, loss_ce: 0.017542
2021-12-11 22:56:36,698 iteration 4258 : loss : 0.048919, loss_ce: 0.024144
2021-12-11 22:56:39,328 iteration 4259 : loss : 0.024020, loss_ce: 0.011150
2021-12-11 22:56:42,043 iteration 4260 : loss : 0.032744, loss_ce: 0.014326
2021-12-11 22:56:44,848 iteration 4261 : loss : 0.022225, loss_ce: 0.010542
2021-12-11 22:56:47,529 iteration 4262 : loss : 0.032468, loss_ce: 0.016108
2021-12-11 22:56:50,365 iteration 4263 : loss : 0.042549, loss_ce: 0.019489
2021-12-11 22:56:53,099 iteration 4264 : loss : 0.034510, loss_ce: 0.018484
2021-12-11 22:56:55,738 iteration 4265 : loss : 0.031907, loss_ce: 0.016506
2021-12-11 22:56:58,537 iteration 4266 : loss : 0.038900, loss_ce: 0.014605
2021-12-11 22:57:01,502 iteration 4267 : loss : 0.035887, loss_ce: 0.016127
 63%|████████████████▉          | 251/400 [3:33:59<2:10:28, 52.54s/it]2021-12-11 22:57:04,305 iteration 4268 : loss : 0.036084, loss_ce: 0.017702
2021-12-11 22:57:07,053 iteration 4269 : loss : 0.027222, loss_ce: 0.011292
2021-12-11 22:57:09,892 iteration 4270 : loss : 0.048886, loss_ce: 0.018236
2021-12-11 22:57:12,546 iteration 4271 : loss : 0.036475, loss_ce: 0.016922
2021-12-11 22:57:15,419 iteration 4272 : loss : 0.035141, loss_ce: 0.017072
2021-12-11 22:57:18,120 iteration 4273 : loss : 0.030144, loss_ce: 0.015980
2021-12-11 22:57:20,891 iteration 4274 : loss : 0.031744, loss_ce: 0.017878
2021-12-11 22:57:23,804 iteration 4275 : loss : 0.030961, loss_ce: 0.014449
2021-12-11 22:57:26,670 iteration 4276 : loss : 0.034806, loss_ce: 0.015854
2021-12-11 22:57:29,396 iteration 4277 : loss : 0.036524, loss_ce: 0.018465
2021-12-11 22:57:32,198 iteration 4278 : loss : 0.037237, loss_ce: 0.013527
2021-12-11 22:57:34,896 iteration 4279 : loss : 0.029496, loss_ce: 0.015975
2021-12-11 22:57:37,763 iteration 4280 : loss : 0.057121, loss_ce: 0.021061
2021-12-11 22:57:40,612 iteration 4281 : loss : 0.037223, loss_ce: 0.017585
2021-12-11 22:57:43,463 iteration 4282 : loss : 0.051334, loss_ce: 0.021975
2021-12-11 22:57:46,153 iteration 4283 : loss : 0.036075, loss_ce: 0.019872
2021-12-11 22:57:49,163 iteration 4284 : loss : 0.042693, loss_ce: 0.018628
 63%|█████████████████          | 252/400 [3:34:47<2:05:59, 51.08s/it]2021-12-11 22:57:51,969 iteration 4285 : loss : 0.030600, loss_ce: 0.018908
2021-12-11 22:57:54,674 iteration 4286 : loss : 0.033938, loss_ce: 0.015208
2021-12-11 22:57:57,379 iteration 4287 : loss : 0.058769, loss_ce: 0.030879
2021-12-11 22:58:00,073 iteration 4288 : loss : 0.023183, loss_ce: 0.010384
2021-12-11 22:58:02,869 iteration 4289 : loss : 0.028759, loss_ce: 0.015411
2021-12-11 22:58:05,649 iteration 4290 : loss : 0.035766, loss_ce: 0.017402
2021-12-11 22:58:08,567 iteration 4291 : loss : 0.034308, loss_ce: 0.017170
2021-12-11 22:58:11,517 iteration 4292 : loss : 0.044230, loss_ce: 0.023562
2021-12-11 22:58:14,115 iteration 4293 : loss : 0.028491, loss_ce: 0.013333
2021-12-11 22:58:16,879 iteration 4294 : loss : 0.033913, loss_ce: 0.018240
2021-12-11 22:58:19,913 iteration 4295 : loss : 0.051605, loss_ce: 0.024186
2021-12-11 22:58:22,658 iteration 4296 : loss : 0.030139, loss_ce: 0.013807
2021-12-11 22:58:25,390 iteration 4297 : loss : 0.024921, loss_ce: 0.011087
2021-12-11 22:58:28,267 iteration 4298 : loss : 0.039536, loss_ce: 0.015626
2021-12-11 22:58:31,176 iteration 4299 : loss : 0.050601, loss_ce: 0.018484
2021-12-11 22:58:34,016 iteration 4300 : loss : 0.039020, loss_ce: 0.016304
2021-12-11 22:58:36,695 iteration 4301 : loss : 0.053718, loss_ce: 0.021103
 63%|█████████████████          | 253/400 [3:35:34<2:02:31, 50.01s/it]2021-12-11 22:58:39,556 iteration 4302 : loss : 0.032714, loss_ce: 0.015872
2021-12-11 22:58:42,417 iteration 4303 : loss : 0.043376, loss_ce: 0.026748
2021-12-11 22:58:45,210 iteration 4304 : loss : 0.034090, loss_ce: 0.017498
2021-12-11 22:58:47,900 iteration 4305 : loss : 0.045601, loss_ce: 0.015993
2021-12-11 22:58:50,691 iteration 4306 : loss : 0.030331, loss_ce: 0.013516
2021-12-11 22:58:53,673 iteration 4307 : loss : 0.045504, loss_ce: 0.020559
2021-12-11 22:58:56,487 iteration 4308 : loss : 0.035300, loss_ce: 0.016121
2021-12-11 22:58:59,235 iteration 4309 : loss : 0.034862, loss_ce: 0.014907
2021-12-11 22:59:01,922 iteration 4310 : loss : 0.028063, loss_ce: 0.012200
2021-12-11 22:59:04,749 iteration 4311 : loss : 0.053029, loss_ce: 0.023226
2021-12-11 22:59:07,450 iteration 4312 : loss : 0.039962, loss_ce: 0.020781
2021-12-11 22:59:10,259 iteration 4313 : loss : 0.030750, loss_ce: 0.015992
2021-12-11 22:59:13,037 iteration 4314 : loss : 0.037767, loss_ce: 0.015482
2021-12-11 22:59:15,629 iteration 4315 : loss : 0.025325, loss_ce: 0.012421
2021-12-11 22:59:18,455 iteration 4316 : loss : 0.037715, loss_ce: 0.018071
2021-12-11 22:59:21,317 iteration 4317 : loss : 0.028320, loss_ce: 0.014787
2021-12-11 22:59:24,181 iteration 4318 : loss : 0.026628, loss_ce: 0.012481
 64%|█████████████████▏         | 254/400 [3:36:22<1:59:51, 49.26s/it]2021-12-11 22:59:27,022 iteration 4319 : loss : 0.032966, loss_ce: 0.016224
2021-12-11 22:59:29,720 iteration 4320 : loss : 0.027823, loss_ce: 0.013002
2021-12-11 22:59:32,521 iteration 4321 : loss : 0.045477, loss_ce: 0.025500
2021-12-11 22:59:35,438 iteration 4322 : loss : 0.043663, loss_ce: 0.020313
2021-12-11 22:59:38,250 iteration 4323 : loss : 0.037916, loss_ce: 0.015507
2021-12-11 22:59:40,985 iteration 4324 : loss : 0.021302, loss_ce: 0.012255
2021-12-11 22:59:43,607 iteration 4325 : loss : 0.045052, loss_ce: 0.016095
2021-12-11 22:59:46,576 iteration 4326 : loss : 0.045013, loss_ce: 0.017389
2021-12-11 22:59:49,570 iteration 4327 : loss : 0.023581, loss_ce: 0.012801
2021-12-11 22:59:52,254 iteration 4328 : loss : 0.029264, loss_ce: 0.015196
2021-12-11 22:59:54,932 iteration 4329 : loss : 0.031036, loss_ce: 0.014241
2021-12-11 22:59:57,695 iteration 4330 : loss : 0.032783, loss_ce: 0.015686
2021-12-11 23:00:00,492 iteration 4331 : loss : 0.031530, loss_ce: 0.015429
2021-12-11 23:00:03,120 iteration 4332 : loss : 0.030043, loss_ce: 0.013351
2021-12-11 23:00:05,737 iteration 4333 : loss : 0.023103, loss_ce: 0.012058
2021-12-11 23:00:08,639 iteration 4334 : loss : 0.034788, loss_ce: 0.014502
2021-12-11 23:00:08,639 Training Data Eval:
2021-12-11 23:00:23,300   Average segmentation loss on training set: 0.0180
2021-12-11 23:00:23,301 Validation Data Eval:
2021-12-11 23:00:28,409   Average segmentation loss on validation set: 0.0898
2021-12-11 23:00:31,268 iteration 4335 : loss : 0.053872, loss_ce: 0.017902
 64%|█████████████████▏         | 255/400 [3:37:29<2:11:57, 54.60s/it]2021-12-11 23:00:34,029 iteration 4336 : loss : 0.031495, loss_ce: 0.015101
2021-12-11 23:00:36,716 iteration 4337 : loss : 0.031016, loss_ce: 0.014359
2021-12-11 23:00:39,559 iteration 4338 : loss : 0.037508, loss_ce: 0.017112
2021-12-11 23:00:42,316 iteration 4339 : loss : 0.037610, loss_ce: 0.013693
2021-12-11 23:00:45,152 iteration 4340 : loss : 0.040389, loss_ce: 0.012356
2021-12-11 23:00:47,814 iteration 4341 : loss : 0.033637, loss_ce: 0.015940
2021-12-11 23:00:50,466 iteration 4342 : loss : 0.031999, loss_ce: 0.015835
2021-12-11 23:00:53,286 iteration 4343 : loss : 0.043228, loss_ce: 0.015679
2021-12-11 23:00:56,079 iteration 4344 : loss : 0.027318, loss_ce: 0.013352
2021-12-11 23:00:58,685 iteration 4345 : loss : 0.023225, loss_ce: 0.012532
2021-12-11 23:01:01,228 iteration 4346 : loss : 0.021849, loss_ce: 0.010639
2021-12-11 23:01:04,191 iteration 4347 : loss : 0.056263, loss_ce: 0.019677
2021-12-11 23:01:07,027 iteration 4348 : loss : 0.036801, loss_ce: 0.018379
2021-12-11 23:01:09,676 iteration 4349 : loss : 0.030286, loss_ce: 0.015526
2021-12-11 23:01:12,404 iteration 4350 : loss : 0.025724, loss_ce: 0.012502
2021-12-11 23:01:15,185 iteration 4351 : loss : 0.033876, loss_ce: 0.016731
2021-12-11 23:01:17,969 iteration 4352 : loss : 0.037832, loss_ce: 0.016505
 64%|█████████████████▎         | 256/400 [3:38:16<2:05:21, 52.24s/it]2021-12-11 23:01:20,661 iteration 4353 : loss : 0.027545, loss_ce: 0.013291
2021-12-11 23:01:23,601 iteration 4354 : loss : 0.025968, loss_ce: 0.011060
2021-12-11 23:01:26,391 iteration 4355 : loss : 0.024817, loss_ce: 0.011788
2021-12-11 23:01:29,168 iteration 4356 : loss : 0.034613, loss_ce: 0.016433
2021-12-11 23:01:32,032 iteration 4357 : loss : 0.039918, loss_ce: 0.016231
2021-12-11 23:01:34,696 iteration 4358 : loss : 0.029873, loss_ce: 0.015413
2021-12-11 23:01:37,509 iteration 4359 : loss : 0.038174, loss_ce: 0.014101
2021-12-11 23:01:40,245 iteration 4360 : loss : 0.028287, loss_ce: 0.014016
2021-12-11 23:01:43,032 iteration 4361 : loss : 0.047379, loss_ce: 0.020266
2021-12-11 23:01:45,762 iteration 4362 : loss : 0.032682, loss_ce: 0.012763
2021-12-11 23:01:48,594 iteration 4363 : loss : 0.031882, loss_ce: 0.015548
2021-12-11 23:01:51,388 iteration 4364 : loss : 0.040460, loss_ce: 0.019367
2021-12-11 23:01:54,331 iteration 4365 : loss : 0.031080, loss_ce: 0.013557
2021-12-11 23:01:57,073 iteration 4366 : loss : 0.026788, loss_ce: 0.013263
2021-12-11 23:01:59,677 iteration 4367 : loss : 0.031428, loss_ce: 0.016431
2021-12-11 23:02:02,474 iteration 4368 : loss : 0.031703, loss_ce: 0.016586
2021-12-11 23:02:05,143 iteration 4369 : loss : 0.036343, loss_ce: 0.017317
 64%|█████████████████▎         | 257/400 [3:39:03<2:00:52, 50.72s/it]2021-12-11 23:02:07,875 iteration 4370 : loss : 0.033660, loss_ce: 0.015383
2021-12-11 23:02:10,758 iteration 4371 : loss : 0.055190, loss_ce: 0.022882
2021-12-11 23:02:13,600 iteration 4372 : loss : 0.044844, loss_ce: 0.014808
2021-12-11 23:02:16,401 iteration 4373 : loss : 0.052291, loss_ce: 0.021762
2021-12-11 23:02:19,098 iteration 4374 : loss : 0.033474, loss_ce: 0.017153
2021-12-11 23:02:21,864 iteration 4375 : loss : 0.025658, loss_ce: 0.013018
2021-12-11 23:02:24,673 iteration 4376 : loss : 0.037862, loss_ce: 0.016920
2021-12-11 23:02:27,439 iteration 4377 : loss : 0.030516, loss_ce: 0.012549
2021-12-11 23:02:30,208 iteration 4378 : loss : 0.034142, loss_ce: 0.019783
2021-12-11 23:02:32,988 iteration 4379 : loss : 0.042954, loss_ce: 0.017519
2021-12-11 23:02:35,810 iteration 4380 : loss : 0.039311, loss_ce: 0.017145
2021-12-11 23:02:38,518 iteration 4381 : loss : 0.040810, loss_ce: 0.014871
2021-12-11 23:02:41,469 iteration 4382 : loss : 0.054014, loss_ce: 0.023018
2021-12-11 23:02:44,227 iteration 4383 : loss : 0.053598, loss_ce: 0.021812
2021-12-11 23:02:47,187 iteration 4384 : loss : 0.035210, loss_ce: 0.015520
2021-12-11 23:02:49,992 iteration 4385 : loss : 0.038292, loss_ce: 0.019643
2021-12-11 23:02:52,681 iteration 4386 : loss : 0.039715, loss_ce: 0.014226
 64%|█████████████████▍         | 258/400 [3:39:50<1:57:46, 49.76s/it]2021-12-11 23:02:55,586 iteration 4387 : loss : 0.041828, loss_ce: 0.014261
2021-12-11 23:02:58,300 iteration 4388 : loss : 0.030164, loss_ce: 0.015524
2021-12-11 23:03:01,061 iteration 4389 : loss : 0.031427, loss_ce: 0.016247
2021-12-11 23:03:03,790 iteration 4390 : loss : 0.039987, loss_ce: 0.019670
2021-12-11 23:03:06,657 iteration 4391 : loss : 0.054174, loss_ce: 0.024972
2021-12-11 23:03:09,354 iteration 4392 : loss : 0.027972, loss_ce: 0.014400
2021-12-11 23:03:12,357 iteration 4393 : loss : 0.033326, loss_ce: 0.017427
2021-12-11 23:03:14,960 iteration 4394 : loss : 0.025652, loss_ce: 0.013335
2021-12-11 23:03:17,718 iteration 4395 : loss : 0.024515, loss_ce: 0.013339
2021-12-11 23:03:20,404 iteration 4396 : loss : 0.052660, loss_ce: 0.025188
2021-12-11 23:03:23,144 iteration 4397 : loss : 0.026646, loss_ce: 0.011611
2021-12-11 23:03:26,109 iteration 4398 : loss : 0.025165, loss_ce: 0.013098
2021-12-11 23:03:28,929 iteration 4399 : loss : 0.041861, loss_ce: 0.016569
2021-12-11 23:03:31,545 iteration 4400 : loss : 0.031445, loss_ce: 0.013381
2021-12-11 23:03:34,473 iteration 4401 : loss : 0.038351, loss_ce: 0.020844
2021-12-11 23:03:37,071 iteration 4402 : loss : 0.025433, loss_ce: 0.011839
2021-12-11 23:03:39,993 iteration 4403 : loss : 0.033627, loss_ce: 0.013485
 65%|█████████████████▍         | 259/400 [3:40:38<1:55:13, 49.03s/it]2021-12-11 23:03:42,905 iteration 4404 : loss : 0.049442, loss_ce: 0.023680
2021-12-11 23:03:45,656 iteration 4405 : loss : 0.041081, loss_ce: 0.018844
2021-12-11 23:03:48,495 iteration 4406 : loss : 0.030615, loss_ce: 0.014673
2021-12-11 23:03:51,298 iteration 4407 : loss : 0.051196, loss_ce: 0.018205
2021-12-11 23:03:54,026 iteration 4408 : loss : 0.032730, loss_ce: 0.018562
2021-12-11 23:03:56,636 iteration 4409 : loss : 0.034815, loss_ce: 0.016133
2021-12-11 23:03:59,664 iteration 4410 : loss : 0.048590, loss_ce: 0.019476
2021-12-11 23:04:02,367 iteration 4411 : loss : 0.044886, loss_ce: 0.017269
2021-12-11 23:04:05,137 iteration 4412 : loss : 0.025381, loss_ce: 0.012300
2021-12-11 23:04:08,072 iteration 4413 : loss : 0.025262, loss_ce: 0.013665
2021-12-11 23:04:10,733 iteration 4414 : loss : 0.040430, loss_ce: 0.015517
2021-12-11 23:04:13,391 iteration 4415 : loss : 0.025450, loss_ce: 0.013925
2021-12-11 23:04:16,292 iteration 4416 : loss : 0.038629, loss_ce: 0.016346
2021-12-11 23:04:19,072 iteration 4417 : loss : 0.037676, loss_ce: 0.019354
2021-12-11 23:04:21,930 iteration 4418 : loss : 0.028688, loss_ce: 0.014898
2021-12-11 23:04:24,821 iteration 4419 : loss : 0.026582, loss_ce: 0.012113
2021-12-11 23:04:24,821 Training Data Eval:
2021-12-11 23:04:39,684   Average segmentation loss on training set: 0.0176
2021-12-11 23:04:39,685 Validation Data Eval:
2021-12-11 23:04:44,965   Average segmentation loss on validation set: 0.0842
2021-12-11 23:04:47,781 iteration 4420 : loss : 0.036554, loss_ce: 0.013536
 65%|█████████████████▌         | 260/400 [3:41:45<2:07:31, 54.65s/it]2021-12-11 23:04:50,581 iteration 4421 : loss : 0.045506, loss_ce: 0.017763
2021-12-11 23:04:53,423 iteration 4422 : loss : 0.041341, loss_ce: 0.017563
2021-12-11 23:04:56,145 iteration 4423 : loss : 0.025452, loss_ce: 0.012333
2021-12-11 23:04:59,061 iteration 4424 : loss : 0.038746, loss_ce: 0.017160
2021-12-11 23:05:01,844 iteration 4425 : loss : 0.052771, loss_ce: 0.018902
2021-12-11 23:05:04,634 iteration 4426 : loss : 0.026753, loss_ce: 0.010859
2021-12-11 23:05:07,463 iteration 4427 : loss : 0.034840, loss_ce: 0.017143
2021-12-11 23:05:10,307 iteration 4428 : loss : 0.042009, loss_ce: 0.020635
2021-12-11 23:05:12,826 iteration 4429 : loss : 0.033244, loss_ce: 0.016892
2021-12-11 23:05:15,677 iteration 4430 : loss : 0.047068, loss_ce: 0.023452
2021-12-11 23:05:18,549 iteration 4431 : loss : 0.041587, loss_ce: 0.016643
2021-12-11 23:05:21,383 iteration 4432 : loss : 0.040680, loss_ce: 0.020884
2021-12-11 23:05:24,083 iteration 4433 : loss : 0.033563, loss_ce: 0.012230
2021-12-11 23:05:26,821 iteration 4434 : loss : 0.040159, loss_ce: 0.018905
2021-12-11 23:05:29,543 iteration 4435 : loss : 0.030369, loss_ce: 0.012356
2021-12-11 23:05:32,439 iteration 4436 : loss : 0.033460, loss_ce: 0.019489
2021-12-11 23:05:35,218 iteration 4437 : loss : 0.035369, loss_ce: 0.014316
 65%|█████████████████▌         | 261/400 [3:42:33<2:01:35, 52.49s/it]2021-12-11 23:05:38,180 iteration 4438 : loss : 0.039198, loss_ce: 0.019345
2021-12-11 23:05:40,902 iteration 4439 : loss : 0.023236, loss_ce: 0.012947
2021-12-11 23:05:43,625 iteration 4440 : loss : 0.038605, loss_ce: 0.020784
2021-12-11 23:05:46,560 iteration 4441 : loss : 0.028090, loss_ce: 0.013567
2021-12-11 23:05:49,490 iteration 4442 : loss : 0.038527, loss_ce: 0.016872
2021-12-11 23:05:52,461 iteration 4443 : loss : 0.034961, loss_ce: 0.015503
2021-12-11 23:05:55,146 iteration 4444 : loss : 0.029105, loss_ce: 0.014568
2021-12-11 23:05:57,878 iteration 4445 : loss : 0.034254, loss_ce: 0.014997
2021-12-11 23:06:00,827 iteration 4446 : loss : 0.063393, loss_ce: 0.018818
2021-12-11 23:06:03,696 iteration 4447 : loss : 0.049773, loss_ce: 0.019988
2021-12-11 23:06:06,534 iteration 4448 : loss : 0.032616, loss_ce: 0.013998
2021-12-11 23:06:09,459 iteration 4449 : loss : 0.034242, loss_ce: 0.018305
2021-12-11 23:06:12,155 iteration 4450 : loss : 0.039550, loss_ce: 0.021403
2021-12-11 23:06:14,955 iteration 4451 : loss : 0.027784, loss_ce: 0.013953
2021-12-11 23:06:17,744 iteration 4452 : loss : 0.048740, loss_ce: 0.017893
2021-12-11 23:06:20,460 iteration 4453 : loss : 0.028867, loss_ce: 0.014691
2021-12-11 23:06:23,199 iteration 4454 : loss : 0.038235, loss_ce: 0.017545
 66%|█████████████████▋         | 262/400 [3:43:21<1:57:36, 51.14s/it]2021-12-11 23:06:26,096 iteration 4455 : loss : 0.043690, loss_ce: 0.018114
2021-12-11 23:06:28,832 iteration 4456 : loss : 0.032317, loss_ce: 0.013820
2021-12-11 23:06:31,571 iteration 4457 : loss : 0.034831, loss_ce: 0.017969
2021-12-11 23:06:34,273 iteration 4458 : loss : 0.035743, loss_ce: 0.016748
2021-12-11 23:06:37,126 iteration 4459 : loss : 0.058993, loss_ce: 0.019007
2021-12-11 23:06:39,854 iteration 4460 : loss : 0.021979, loss_ce: 0.011295
2021-12-11 23:06:42,478 iteration 4461 : loss : 0.034378, loss_ce: 0.017983
2021-12-11 23:06:45,320 iteration 4462 : loss : 0.044779, loss_ce: 0.019037
2021-12-11 23:06:48,083 iteration 4463 : loss : 0.027884, loss_ce: 0.013654
2021-12-11 23:06:50,669 iteration 4464 : loss : 0.025138, loss_ce: 0.013496
2021-12-11 23:06:53,553 iteration 4465 : loss : 0.083336, loss_ce: 0.020891
2021-12-11 23:06:56,195 iteration 4466 : loss : 0.032365, loss_ce: 0.015194
2021-12-11 23:06:59,051 iteration 4467 : loss : 0.030706, loss_ce: 0.015806
2021-12-11 23:07:01,742 iteration 4468 : loss : 0.066481, loss_ce: 0.027387
2021-12-11 23:07:04,593 iteration 4469 : loss : 0.036594, loss_ce: 0.013487
2021-12-11 23:07:07,229 iteration 4470 : loss : 0.056718, loss_ce: 0.025287
2021-12-11 23:07:09,974 iteration 4471 : loss : 0.054028, loss_ce: 0.019499
 66%|█████████████████▊         | 263/400 [3:44:08<1:53:46, 49.83s/it]2021-12-11 23:07:12,734 iteration 4472 : loss : 0.037915, loss_ce: 0.019167
2021-12-11 23:07:15,687 iteration 4473 : loss : 0.034973, loss_ce: 0.015964
2021-12-11 23:07:18,532 iteration 4474 : loss : 0.043457, loss_ce: 0.019809
2021-12-11 23:07:21,198 iteration 4475 : loss : 0.035912, loss_ce: 0.017352
2021-12-11 23:07:24,044 iteration 4476 : loss : 0.041080, loss_ce: 0.017931
2021-12-11 23:07:26,820 iteration 4477 : loss : 0.028889, loss_ce: 0.014849
2021-12-11 23:07:29,570 iteration 4478 : loss : 0.037564, loss_ce: 0.021207
2021-12-11 23:07:32,453 iteration 4479 : loss : 0.041239, loss_ce: 0.022549
2021-12-11 23:07:35,061 iteration 4480 : loss : 0.030871, loss_ce: 0.014452
2021-12-11 23:07:38,021 iteration 4481 : loss : 0.046193, loss_ce: 0.015319
2021-12-11 23:07:40,889 iteration 4482 : loss : 0.024323, loss_ce: 0.011376
2021-12-11 23:07:43,530 iteration 4483 : loss : 0.036126, loss_ce: 0.012928
2021-12-11 23:07:46,366 iteration 4484 : loss : 0.036270, loss_ce: 0.016611
2021-12-11 23:07:49,214 iteration 4485 : loss : 0.035202, loss_ce: 0.018944
2021-12-11 23:07:51,968 iteration 4486 : loss : 0.040751, loss_ce: 0.016020
2021-12-11 23:07:54,713 iteration 4487 : loss : 0.039707, loss_ce: 0.013142
2021-12-11 23:07:57,580 iteration 4488 : loss : 0.040780, loss_ce: 0.017864
 66%|█████████████████▊         | 264/400 [3:44:55<1:51:25, 49.16s/it]2021-12-11 23:08:00,323 iteration 4489 : loss : 0.035470, loss_ce: 0.018671
2021-12-11 23:08:02,980 iteration 4490 : loss : 0.026633, loss_ce: 0.017970
2021-12-11 23:08:05,797 iteration 4491 : loss : 0.045205, loss_ce: 0.019347
2021-12-11 23:08:08,709 iteration 4492 : loss : 0.032092, loss_ce: 0.017703
2021-12-11 23:08:11,456 iteration 4493 : loss : 0.039888, loss_ce: 0.018661
2021-12-11 23:08:14,281 iteration 4494 : loss : 0.033127, loss_ce: 0.013249
2021-12-11 23:08:17,125 iteration 4495 : loss : 0.050478, loss_ce: 0.019877
2021-12-11 23:08:19,837 iteration 4496 : loss : 0.026181, loss_ce: 0.014893
2021-12-11 23:08:22,620 iteration 4497 : loss : 0.037534, loss_ce: 0.018482
2021-12-11 23:08:25,349 iteration 4498 : loss : 0.039001, loss_ce: 0.015592
2021-12-11 23:08:28,167 iteration 4499 : loss : 0.046674, loss_ce: 0.018459
2021-12-11 23:08:31,023 iteration 4500 : loss : 0.025866, loss_ce: 0.011747
2021-12-11 23:08:33,824 iteration 4501 : loss : 0.028334, loss_ce: 0.011644
2021-12-11 23:08:36,607 iteration 4502 : loss : 0.049118, loss_ce: 0.021931
2021-12-11 23:08:39,450 iteration 4503 : loss : 0.034643, loss_ce: 0.017983
2021-12-11 23:08:42,258 iteration 4504 : loss : 0.034224, loss_ce: 0.016485
2021-12-11 23:08:42,258 Training Data Eval:
2021-12-11 23:08:57,274   Average segmentation loss on training set: 0.0169
2021-12-11 23:08:57,275 Validation Data Eval:
2021-12-11 23:09:02,448   Average segmentation loss on validation set: 0.0890
2021-12-11 23:09:05,397 iteration 4505 : loss : 0.029331, loss_ce: 0.014060
 66%|█████████████████▉         | 265/400 [3:46:03<2:03:12, 54.76s/it]2021-12-11 23:09:08,239 iteration 4506 : loss : 0.038405, loss_ce: 0.019437
2021-12-11 23:09:10,903 iteration 4507 : loss : 0.027879, loss_ce: 0.014728
2021-12-11 23:09:13,677 iteration 4508 : loss : 0.027146, loss_ce: 0.012486
2021-12-11 23:09:16,283 iteration 4509 : loss : 0.039605, loss_ce: 0.012309
2021-12-11 23:09:19,083 iteration 4510 : loss : 0.040903, loss_ce: 0.018153
2021-12-11 23:09:21,874 iteration 4511 : loss : 0.031093, loss_ce: 0.012272
2021-12-11 23:09:24,665 iteration 4512 : loss : 0.045185, loss_ce: 0.023008
2021-12-11 23:09:27,572 iteration 4513 : loss : 0.041466, loss_ce: 0.017442
2021-12-11 23:09:30,267 iteration 4514 : loss : 0.029494, loss_ce: 0.012231
2021-12-11 23:09:33,053 iteration 4515 : loss : 0.032560, loss_ce: 0.014464
2021-12-11 23:09:35,799 iteration 4516 : loss : 0.025984, loss_ce: 0.013020
2021-12-11 23:09:38,528 iteration 4517 : loss : 0.027862, loss_ce: 0.013919
2021-12-11 23:09:41,335 iteration 4518 : loss : 0.037406, loss_ce: 0.016401
2021-12-11 23:09:43,961 iteration 4519 : loss : 0.027537, loss_ce: 0.012730
2021-12-11 23:09:46,632 iteration 4520 : loss : 0.047622, loss_ce: 0.021999
2021-12-11 23:09:49,481 iteration 4521 : loss : 0.026184, loss_ce: 0.013591
2021-12-11 23:09:52,230 iteration 4522 : loss : 0.034865, loss_ce: 0.015701
 66%|█████████████████▉         | 266/400 [3:46:50<1:56:59, 52.38s/it]2021-12-11 23:09:55,120 iteration 4523 : loss : 0.028046, loss_ce: 0.014334
2021-12-11 23:09:57,876 iteration 4524 : loss : 0.036342, loss_ce: 0.015207
2021-12-11 23:10:00,461 iteration 4525 : loss : 0.038148, loss_ce: 0.020291
2021-12-11 23:10:03,158 iteration 4526 : loss : 0.040452, loss_ce: 0.015472
2021-12-11 23:10:06,036 iteration 4527 : loss : 0.036958, loss_ce: 0.016767
2021-12-11 23:10:08,797 iteration 4528 : loss : 0.025584, loss_ce: 0.012577
2021-12-11 23:10:11,409 iteration 4529 : loss : 0.027209, loss_ce: 0.013095
2021-12-11 23:10:14,229 iteration 4530 : loss : 0.037132, loss_ce: 0.016616
2021-12-11 23:10:17,078 iteration 4531 : loss : 0.043851, loss_ce: 0.017066
2021-12-11 23:10:19,771 iteration 4532 : loss : 0.022410, loss_ce: 0.011213
2021-12-11 23:10:22,777 iteration 4533 : loss : 0.048125, loss_ce: 0.018982
2021-12-11 23:10:25,547 iteration 4534 : loss : 0.026914, loss_ce: 0.014399
2021-12-11 23:10:28,411 iteration 4535 : loss : 0.046189, loss_ce: 0.024567
2021-12-11 23:10:31,164 iteration 4536 : loss : 0.031846, loss_ce: 0.012457
2021-12-11 23:10:33,948 iteration 4537 : loss : 0.036247, loss_ce: 0.013328
2021-12-11 23:10:36,765 iteration 4538 : loss : 0.032105, loss_ce: 0.015603
2021-12-11 23:10:39,387 iteration 4539 : loss : 0.027388, loss_ce: 0.013328
 67%|██████████████████         | 267/400 [3:47:37<1:52:38, 50.82s/it]2021-12-11 23:10:42,265 iteration 4540 : loss : 0.029000, loss_ce: 0.016559
2021-12-11 23:10:45,120 iteration 4541 : loss : 0.039326, loss_ce: 0.018439
2021-12-11 23:10:47,918 iteration 4542 : loss : 0.042070, loss_ce: 0.015724
2021-12-11 23:10:50,842 iteration 4543 : loss : 0.031615, loss_ce: 0.015296
2021-12-11 23:10:53,533 iteration 4544 : loss : 0.036202, loss_ce: 0.017730
2021-12-11 23:10:56,294 iteration 4545 : loss : 0.022424, loss_ce: 0.011740
2021-12-11 23:10:59,025 iteration 4546 : loss : 0.038475, loss_ce: 0.018522
2021-12-11 23:11:01,775 iteration 4547 : loss : 0.038860, loss_ce: 0.017437
2021-12-11 23:11:04,490 iteration 4548 : loss : 0.032595, loss_ce: 0.018593
2021-12-11 23:11:07,213 iteration 4549 : loss : 0.036015, loss_ce: 0.015178
2021-12-11 23:11:10,208 iteration 4550 : loss : 0.045934, loss_ce: 0.018623
2021-12-11 23:11:13,209 iteration 4551 : loss : 0.036525, loss_ce: 0.016758
2021-12-11 23:11:15,910 iteration 4552 : loss : 0.034120, loss_ce: 0.014273
2021-12-11 23:11:18,747 iteration 4553 : loss : 0.028836, loss_ce: 0.013247
2021-12-11 23:11:21,559 iteration 4554 : loss : 0.034375, loss_ce: 0.013155
2021-12-11 23:11:24,246 iteration 4555 : loss : 0.041497, loss_ce: 0.016583
2021-12-11 23:11:26,991 iteration 4556 : loss : 0.032470, loss_ce: 0.015276
 67%|██████████████████         | 268/400 [3:48:25<1:49:40, 49.85s/it]2021-12-11 23:11:29,797 iteration 4557 : loss : 0.024352, loss_ce: 0.014356
2021-12-11 23:11:32,723 iteration 4558 : loss : 0.045060, loss_ce: 0.021234
2021-12-11 23:11:35,589 iteration 4559 : loss : 0.037012, loss_ce: 0.013110
2021-12-11 23:11:38,336 iteration 4560 : loss : 0.040865, loss_ce: 0.016808
2021-12-11 23:11:41,277 iteration 4561 : loss : 0.047860, loss_ce: 0.015715
2021-12-11 23:11:43,973 iteration 4562 : loss : 0.033222, loss_ce: 0.013534
2021-12-11 23:11:46,782 iteration 4563 : loss : 0.035174, loss_ce: 0.017349
2021-12-11 23:11:49,628 iteration 4564 : loss : 0.046383, loss_ce: 0.019237
2021-12-11 23:11:52,463 iteration 4565 : loss : 0.039913, loss_ce: 0.016817
2021-12-11 23:11:55,415 iteration 4566 : loss : 0.042278, loss_ce: 0.021798
2021-12-11 23:11:58,286 iteration 4567 : loss : 0.043091, loss_ce: 0.017282
2021-12-11 23:12:01,078 iteration 4568 : loss : 0.033548, loss_ce: 0.014762
2021-12-11 23:12:03,836 iteration 4569 : loss : 0.059220, loss_ce: 0.015136
2021-12-11 23:12:06,867 iteration 4570 : loss : 0.055981, loss_ce: 0.030071
2021-12-11 23:12:09,552 iteration 4571 : loss : 0.040723, loss_ce: 0.020927
2021-12-11 23:12:12,316 iteration 4572 : loss : 0.033865, loss_ce: 0.015600
2021-12-11 23:12:15,027 iteration 4573 : loss : 0.041133, loss_ce: 0.020661
 67%|██████████████████▏        | 269/400 [3:49:13<1:47:39, 49.31s/it]2021-12-11 23:12:17,781 iteration 4574 : loss : 0.026922, loss_ce: 0.015296
2021-12-11 23:12:20,684 iteration 4575 : loss : 0.031923, loss_ce: 0.011914
2021-12-11 23:12:23,610 iteration 4576 : loss : 0.033793, loss_ce: 0.015329
2021-12-11 23:12:26,351 iteration 4577 : loss : 0.034276, loss_ce: 0.015642
2021-12-11 23:12:29,009 iteration 4578 : loss : 0.032515, loss_ce: 0.016571
2021-12-11 23:12:31,817 iteration 4579 : loss : 0.029054, loss_ce: 0.012588
2021-12-11 23:12:34,585 iteration 4580 : loss : 0.031565, loss_ce: 0.014720
2021-12-11 23:12:37,495 iteration 4581 : loss : 0.030379, loss_ce: 0.014153
2021-12-11 23:12:40,226 iteration 4582 : loss : 0.034083, loss_ce: 0.016344
2021-12-11 23:12:43,184 iteration 4583 : loss : 0.059947, loss_ce: 0.018280
2021-12-11 23:12:45,983 iteration 4584 : loss : 0.042034, loss_ce: 0.014749
2021-12-11 23:12:48,659 iteration 4585 : loss : 0.030106, loss_ce: 0.012891
2021-12-11 23:12:51,415 iteration 4586 : loss : 0.029503, loss_ce: 0.016489
2021-12-11 23:12:54,152 iteration 4587 : loss : 0.039205, loss_ce: 0.017879
2021-12-11 23:12:57,108 iteration 4588 : loss : 0.033038, loss_ce: 0.015229
2021-12-11 23:12:59,869 iteration 4589 : loss : 0.032382, loss_ce: 0.018099
2021-12-11 23:12:59,869 Training Data Eval:
2021-12-11 23:13:14,697   Average segmentation loss on training set: 0.0162
2021-12-11 23:13:14,697 Validation Data Eval:
2021-12-11 23:13:19,826   Average segmentation loss on validation set: 0.0907
2021-12-11 23:13:22,615 iteration 4590 : loss : 0.049119, loss_ce: 0.029682
 68%|██████████████████▏        | 270/400 [3:50:20<1:58:42, 54.79s/it]2021-12-11 23:13:25,533 iteration 4591 : loss : 0.043523, loss_ce: 0.016859
2021-12-11 23:13:28,381 iteration 4592 : loss : 0.039623, loss_ce: 0.019873
2021-12-11 23:13:31,185 iteration 4593 : loss : 0.043421, loss_ce: 0.017536
2021-12-11 23:13:34,163 iteration 4594 : loss : 0.029187, loss_ce: 0.013369
2021-12-11 23:13:36,909 iteration 4595 : loss : 0.027659, loss_ce: 0.013627
2021-12-11 23:13:39,774 iteration 4596 : loss : 0.043831, loss_ce: 0.019385
2021-12-11 23:13:42,556 iteration 4597 : loss : 0.034296, loss_ce: 0.013946
2021-12-11 23:13:45,271 iteration 4598 : loss : 0.025795, loss_ce: 0.011000
2021-12-11 23:13:48,103 iteration 4599 : loss : 0.043316, loss_ce: 0.020490
2021-12-11 23:13:50,955 iteration 4600 : loss : 0.026142, loss_ce: 0.012310
2021-12-11 23:13:53,792 iteration 4601 : loss : 0.040934, loss_ce: 0.023370
2021-12-11 23:13:56,716 iteration 4602 : loss : 0.040099, loss_ce: 0.017605
2021-12-11 23:13:59,733 iteration 4603 : loss : 0.042355, loss_ce: 0.017042
2021-12-11 23:14:02,604 iteration 4604 : loss : 0.034282, loss_ce: 0.015725
2021-12-11 23:14:05,426 iteration 4605 : loss : 0.027339, loss_ce: 0.011793
2021-12-11 23:14:08,137 iteration 4606 : loss : 0.034126, loss_ce: 0.014635
2021-12-11 23:14:11,001 iteration 4607 : loss : 0.048185, loss_ce: 0.021236
 68%|██████████████████▎        | 271/400 [3:51:09<1:53:39, 52.87s/it]2021-12-11 23:14:13,835 iteration 4608 : loss : 0.030693, loss_ce: 0.014440
2021-12-11 23:14:16,744 iteration 4609 : loss : 0.029729, loss_ce: 0.012784
2021-12-11 23:14:19,586 iteration 4610 : loss : 0.040184, loss_ce: 0.013665
2021-12-11 23:14:22,399 iteration 4611 : loss : 0.042052, loss_ce: 0.018606
2021-12-11 23:14:25,374 iteration 4612 : loss : 0.043797, loss_ce: 0.022793
2021-12-11 23:14:28,227 iteration 4613 : loss : 0.043348, loss_ce: 0.018962
2021-12-11 23:14:31,119 iteration 4614 : loss : 0.026644, loss_ce: 0.012211
2021-12-11 23:14:34,161 iteration 4615 : loss : 0.065048, loss_ce: 0.029810
2021-12-11 23:14:37,031 iteration 4616 : loss : 0.034623, loss_ce: 0.019434
2021-12-11 23:14:39,867 iteration 4617 : loss : 0.037623, loss_ce: 0.016050
2021-12-11 23:14:42,618 iteration 4618 : loss : 0.019590, loss_ce: 0.010166
2021-12-11 23:14:45,380 iteration 4619 : loss : 0.041179, loss_ce: 0.017353
2021-12-11 23:14:48,251 iteration 4620 : loss : 0.032671, loss_ce: 0.015191
2021-12-11 23:14:50,983 iteration 4621 : loss : 0.032168, loss_ce: 0.013833
2021-12-11 23:14:53,924 iteration 4622 : loss : 0.035794, loss_ce: 0.016163
2021-12-11 23:14:56,652 iteration 4623 : loss : 0.026700, loss_ce: 0.013601
2021-12-11 23:14:59,442 iteration 4624 : loss : 0.031433, loss_ce: 0.012832
 68%|██████████████████▎        | 272/400 [3:51:57<1:49:57, 51.54s/it]2021-12-11 23:15:02,229 iteration 4625 : loss : 0.045651, loss_ce: 0.019835
2021-12-11 23:15:05,164 iteration 4626 : loss : 0.035582, loss_ce: 0.014691
2021-12-11 23:15:07,984 iteration 4627 : loss : 0.032221, loss_ce: 0.018103
2021-12-11 23:15:10,623 iteration 4628 : loss : 0.042703, loss_ce: 0.018906
2021-12-11 23:15:13,278 iteration 4629 : loss : 0.052755, loss_ce: 0.013482
2021-12-11 23:15:16,057 iteration 4630 : loss : 0.033043, loss_ce: 0.019515
2021-12-11 23:15:18,953 iteration 4631 : loss : 0.036691, loss_ce: 0.016553
2021-12-11 23:15:21,670 iteration 4632 : loss : 0.029903, loss_ce: 0.012703
2021-12-11 23:15:24,483 iteration 4633 : loss : 0.031188, loss_ce: 0.015159
2021-12-11 23:15:27,333 iteration 4634 : loss : 0.027761, loss_ce: 0.014679
2021-12-11 23:15:29,965 iteration 4635 : loss : 0.022468, loss_ce: 0.010503
2021-12-11 23:15:32,709 iteration 4636 : loss : 0.021322, loss_ce: 0.010425
2021-12-11 23:15:35,564 iteration 4637 : loss : 0.034748, loss_ce: 0.014563
2021-12-11 23:15:38,359 iteration 4638 : loss : 0.039028, loss_ce: 0.019838
2021-12-11 23:15:41,176 iteration 4639 : loss : 0.047813, loss_ce: 0.019114
2021-12-11 23:15:44,180 iteration 4640 : loss : 0.044993, loss_ce: 0.023876
2021-12-11 23:15:46,931 iteration 4641 : loss : 0.034130, loss_ce: 0.014589
 68%|██████████████████▍        | 273/400 [3:52:45<1:46:31, 50.32s/it]2021-12-11 23:15:49,894 iteration 4642 : loss : 0.034036, loss_ce: 0.016732
2021-12-11 23:15:52,718 iteration 4643 : loss : 0.037464, loss_ce: 0.016430
2021-12-11 23:15:55,448 iteration 4644 : loss : 0.045053, loss_ce: 0.023070
2021-12-11 23:15:58,121 iteration 4645 : loss : 0.033555, loss_ce: 0.013311
2021-12-11 23:16:00,837 iteration 4646 : loss : 0.029056, loss_ce: 0.013823
2021-12-11 23:16:03,463 iteration 4647 : loss : 0.035506, loss_ce: 0.017783
2021-12-11 23:16:06,138 iteration 4648 : loss : 0.025488, loss_ce: 0.014041
2021-12-11 23:16:09,191 iteration 4649 : loss : 0.035732, loss_ce: 0.016343
2021-12-11 23:16:11,859 iteration 4650 : loss : 0.023981, loss_ce: 0.012295
2021-12-11 23:16:14,509 iteration 4651 : loss : 0.026905, loss_ce: 0.012664
2021-12-11 23:16:17,353 iteration 4652 : loss : 0.029454, loss_ce: 0.013655
2021-12-11 23:16:20,076 iteration 4653 : loss : 0.024553, loss_ce: 0.013106
2021-12-11 23:16:22,847 iteration 4654 : loss : 0.032096, loss_ce: 0.012981
2021-12-11 23:16:25,512 iteration 4655 : loss : 0.036002, loss_ce: 0.014356
2021-12-11 23:16:28,414 iteration 4656 : loss : 0.041736, loss_ce: 0.016947
2021-12-11 23:16:31,087 iteration 4657 : loss : 0.035497, loss_ce: 0.018042
2021-12-11 23:16:33,932 iteration 4658 : loss : 0.026626, loss_ce: 0.013743
 68%|██████████████████▍        | 274/400 [3:53:32<1:43:35, 49.33s/it]2021-12-11 23:16:36,789 iteration 4659 : loss : 0.028603, loss_ce: 0.014236
2021-12-11 23:16:39,656 iteration 4660 : loss : 0.029833, loss_ce: 0.014549
2021-12-11 23:16:42,528 iteration 4661 : loss : 0.031836, loss_ce: 0.016510
2021-12-11 23:16:45,256 iteration 4662 : loss : 0.025058, loss_ce: 0.013627
2021-12-11 23:16:48,263 iteration 4663 : loss : 0.031093, loss_ce: 0.014769
2021-12-11 23:16:50,900 iteration 4664 : loss : 0.041941, loss_ce: 0.017960
2021-12-11 23:16:53,811 iteration 4665 : loss : 0.036232, loss_ce: 0.016916
2021-12-11 23:16:56,676 iteration 4666 : loss : 0.028287, loss_ce: 0.014612
2021-12-11 23:16:59,560 iteration 4667 : loss : 0.038325, loss_ce: 0.017430
2021-12-11 23:17:02,235 iteration 4668 : loss : 0.022416, loss_ce: 0.012590
2021-12-11 23:17:05,092 iteration 4669 : loss : 0.028083, loss_ce: 0.013071
2021-12-11 23:17:07,984 iteration 4670 : loss : 0.051354, loss_ce: 0.019218
2021-12-11 23:17:10,822 iteration 4671 : loss : 0.028597, loss_ce: 0.012530
2021-12-11 23:17:13,555 iteration 4672 : loss : 0.022926, loss_ce: 0.011105
2021-12-11 23:17:16,192 iteration 4673 : loss : 0.024910, loss_ce: 0.011267
2021-12-11 23:17:19,020 iteration 4674 : loss : 0.032038, loss_ce: 0.011683
2021-12-11 23:17:19,020 Training Data Eval:
2021-12-11 23:17:34,037   Average segmentation loss on training set: 0.0163
2021-12-11 23:17:34,038 Validation Data Eval:
2021-12-11 23:17:39,326   Average segmentation loss on validation set: 0.0995
2021-12-11 23:17:42,025 iteration 4675 : loss : 0.023109, loss_ce: 0.011760
 69%|██████████████████▌        | 275/400 [3:54:40<1:54:29, 54.96s/it]2021-12-11 23:17:44,794 iteration 4676 : loss : 0.026321, loss_ce: 0.012827
2021-12-11 23:17:47,669 iteration 4677 : loss : 0.021430, loss_ce: 0.012010
2021-12-11 23:17:50,460 iteration 4678 : loss : 0.028873, loss_ce: 0.012800
2021-12-11 23:17:53,247 iteration 4679 : loss : 0.036577, loss_ce: 0.016968
2021-12-11 23:17:56,059 iteration 4680 : loss : 0.037250, loss_ce: 0.015525
2021-12-11 23:17:58,817 iteration 4681 : loss : 0.036608, loss_ce: 0.019902
2021-12-11 23:18:01,637 iteration 4682 : loss : 0.031620, loss_ce: 0.014208
2021-12-11 23:18:04,300 iteration 4683 : loss : 0.034620, loss_ce: 0.014329
2021-12-11 23:18:07,079 iteration 4684 : loss : 0.035483, loss_ce: 0.016241
2021-12-11 23:18:10,046 iteration 4685 : loss : 0.020454, loss_ce: 0.010575
2021-12-11 23:18:12,825 iteration 4686 : loss : 0.038233, loss_ce: 0.015788
2021-12-11 23:18:15,543 iteration 4687 : loss : 0.035863, loss_ce: 0.013470
2021-12-11 23:18:18,208 iteration 4688 : loss : 0.018670, loss_ce: 0.010648
2021-12-11 23:18:21,009 iteration 4689 : loss : 0.039224, loss_ce: 0.016678
2021-12-11 23:18:23,755 iteration 4690 : loss : 0.029686, loss_ce: 0.012723
2021-12-11 23:18:26,442 iteration 4691 : loss : 0.041549, loss_ce: 0.015124
2021-12-11 23:18:29,464 iteration 4692 : loss : 0.049540, loss_ce: 0.024640
 69%|██████████████████▋        | 276/400 [3:55:27<1:48:55, 52.70s/it]2021-12-11 23:18:32,245 iteration 4693 : loss : 0.039034, loss_ce: 0.020907
2021-12-11 23:18:35,021 iteration 4694 : loss : 0.048357, loss_ce: 0.017530
2021-12-11 23:18:37,827 iteration 4695 : loss : 0.038246, loss_ce: 0.015898
2021-12-11 23:18:40,737 iteration 4696 : loss : 0.035178, loss_ce: 0.015141
2021-12-11 23:18:43,497 iteration 4697 : loss : 0.036731, loss_ce: 0.017266
2021-12-11 23:18:46,132 iteration 4698 : loss : 0.033054, loss_ce: 0.015636
2021-12-11 23:18:48,733 iteration 4699 : loss : 0.039237, loss_ce: 0.016373
2021-12-11 23:18:51,565 iteration 4700 : loss : 0.036592, loss_ce: 0.016293
2021-12-11 23:18:54,302 iteration 4701 : loss : 0.029102, loss_ce: 0.011308
2021-12-11 23:18:57,199 iteration 4702 : loss : 0.053346, loss_ce: 0.020741
2021-12-11 23:19:00,010 iteration 4703 : loss : 0.039257, loss_ce: 0.020292
2021-12-11 23:19:02,813 iteration 4704 : loss : 0.042320, loss_ce: 0.013245
2021-12-11 23:19:05,555 iteration 4705 : loss : 0.044556, loss_ce: 0.014103
2021-12-11 23:19:08,260 iteration 4706 : loss : 0.025191, loss_ce: 0.012224
2021-12-11 23:19:10,989 iteration 4707 : loss : 0.035733, loss_ce: 0.014243
2021-12-11 23:19:13,736 iteration 4708 : loss : 0.054976, loss_ce: 0.027097
2021-12-11 23:19:16,353 iteration 4709 : loss : 0.036502, loss_ce: 0.016979
 69%|██████████████████▋        | 277/400 [3:56:14<1:44:27, 50.96s/it]2021-12-11 23:19:19,017 iteration 4710 : loss : 0.026862, loss_ce: 0.012375
2021-12-11 23:19:21,704 iteration 4711 : loss : 0.024858, loss_ce: 0.010833
2021-12-11 23:19:24,404 iteration 4712 : loss : 0.027879, loss_ce: 0.012194
2021-12-11 23:19:27,143 iteration 4713 : loss : 0.037808, loss_ce: 0.014855
2021-12-11 23:19:29,862 iteration 4714 : loss : 0.022297, loss_ce: 0.012259
2021-12-11 23:19:32,518 iteration 4715 : loss : 0.053085, loss_ce: 0.030045
2021-12-11 23:19:35,372 iteration 4716 : loss : 0.039250, loss_ce: 0.019211
2021-12-11 23:19:38,013 iteration 4717 : loss : 0.033589, loss_ce: 0.014907
2021-12-11 23:19:40,930 iteration 4718 : loss : 0.029526, loss_ce: 0.014174
2021-12-11 23:19:43,604 iteration 4719 : loss : 0.019342, loss_ce: 0.009684
2021-12-11 23:19:46,339 iteration 4720 : loss : 0.032689, loss_ce: 0.018160
2021-12-11 23:19:49,124 iteration 4721 : loss : 0.033821, loss_ce: 0.014132
2021-12-11 23:19:51,907 iteration 4722 : loss : 0.038607, loss_ce: 0.019291
2021-12-11 23:19:54,867 iteration 4723 : loss : 0.040634, loss_ce: 0.015816
2021-12-11 23:19:57,717 iteration 4724 : loss : 0.047014, loss_ce: 0.026754
2021-12-11 23:20:00,345 iteration 4725 : loss : 0.038036, loss_ce: 0.015037
2021-12-11 23:20:03,282 iteration 4726 : loss : 0.038626, loss_ce: 0.014308
 70%|██████████████████▊        | 278/400 [3:57:01<1:41:09, 49.75s/it]2021-12-11 23:20:06,092 iteration 4727 : loss : 0.032337, loss_ce: 0.014186
2021-12-11 23:20:09,005 iteration 4728 : loss : 0.028742, loss_ce: 0.013353
2021-12-11 23:20:11,747 iteration 4729 : loss : 0.031993, loss_ce: 0.012573
2021-12-11 23:20:14,480 iteration 4730 : loss : 0.033177, loss_ce: 0.017129
2021-12-11 23:20:17,382 iteration 4731 : loss : 0.052639, loss_ce: 0.017849
2021-12-11 23:20:20,093 iteration 4732 : loss : 0.038911, loss_ce: 0.017162
2021-12-11 23:20:22,830 iteration 4733 : loss : 0.030794, loss_ce: 0.012732
2021-12-11 23:20:25,397 iteration 4734 : loss : 0.026774, loss_ce: 0.013459
2021-12-11 23:20:28,222 iteration 4735 : loss : 0.048481, loss_ce: 0.023735
2021-12-11 23:20:31,149 iteration 4736 : loss : 0.032253, loss_ce: 0.015534
2021-12-11 23:20:34,021 iteration 4737 : loss : 0.049821, loss_ce: 0.017217
2021-12-11 23:20:36,803 iteration 4738 : loss : 0.086763, loss_ce: 0.015755
2021-12-11 23:20:39,549 iteration 4739 : loss : 0.031373, loss_ce: 0.013753
2021-12-11 23:20:42,270 iteration 4740 : loss : 0.024960, loss_ce: 0.013383
2021-12-11 23:20:45,006 iteration 4741 : loss : 0.046142, loss_ce: 0.029333
2021-12-11 23:20:47,629 iteration 4742 : loss : 0.024365, loss_ce: 0.012051
2021-12-11 23:20:50,354 iteration 4743 : loss : 0.028029, loss_ce: 0.012553
 70%|██████████████████▊        | 279/400 [3:57:48<1:38:42, 48.95s/it]2021-12-11 23:20:53,198 iteration 4744 : loss : 0.035439, loss_ce: 0.017411
2021-12-11 23:20:55,909 iteration 4745 : loss : 0.030518, loss_ce: 0.015308
2021-12-11 23:20:58,795 iteration 4746 : loss : 0.034794, loss_ce: 0.016913
2021-12-11 23:21:01,601 iteration 4747 : loss : 0.036824, loss_ce: 0.016853
2021-12-11 23:21:04,567 iteration 4748 : loss : 0.043206, loss_ce: 0.021306
2021-12-11 23:21:07,308 iteration 4749 : loss : 0.039144, loss_ce: 0.014766
2021-12-11 23:21:10,250 iteration 4750 : loss : 0.031909, loss_ce: 0.014772
2021-12-11 23:21:12,851 iteration 4751 : loss : 0.028338, loss_ce: 0.013193
2021-12-11 23:21:15,783 iteration 4752 : loss : 0.035506, loss_ce: 0.012679
2021-12-11 23:21:18,577 iteration 4753 : loss : 0.031864, loss_ce: 0.016379
2021-12-11 23:21:21,291 iteration 4754 : loss : 0.036272, loss_ce: 0.012914
2021-12-11 23:21:24,056 iteration 4755 : loss : 0.033714, loss_ce: 0.015015
2021-12-11 23:21:26,845 iteration 4756 : loss : 0.029389, loss_ce: 0.014909
2021-12-11 23:21:29,679 iteration 4757 : loss : 0.027886, loss_ce: 0.012634
2021-12-11 23:21:32,502 iteration 4758 : loss : 0.035093, loss_ce: 0.014745
2021-12-11 23:21:35,240 iteration 4759 : loss : 0.022805, loss_ce: 0.011589
2021-12-11 23:21:35,241 Training Data Eval:
2021-12-11 23:21:50,049   Average segmentation loss on training set: 0.0189
2021-12-11 23:21:50,050 Validation Data Eval:
2021-12-11 23:21:55,181   Average segmentation loss on validation set: 0.1101
2021-12-11 23:21:57,802 iteration 4760 : loss : 0.029652, loss_ce: 0.012280
 70%|██████████████████▉        | 280/400 [3:58:55<1:48:59, 54.50s/it]2021-12-11 23:22:00,595 iteration 4761 : loss : 0.022081, loss_ce: 0.011829
2021-12-11 23:22:03,283 iteration 4762 : loss : 0.044655, loss_ce: 0.016404
2021-12-11 23:22:06,068 iteration 4763 : loss : 0.034217, loss_ce: 0.016686
2021-12-11 23:22:08,886 iteration 4764 : loss : 0.040914, loss_ce: 0.016760
2021-12-11 23:22:11,634 iteration 4765 : loss : 0.033420, loss_ce: 0.015163
2021-12-11 23:22:14,456 iteration 4766 : loss : 0.047882, loss_ce: 0.014437
2021-12-11 23:22:17,334 iteration 4767 : loss : 0.037777, loss_ce: 0.020492
2021-12-11 23:22:20,082 iteration 4768 : loss : 0.049050, loss_ce: 0.025616
2021-12-11 23:22:22,804 iteration 4769 : loss : 0.026909, loss_ce: 0.012532
2021-12-11 23:22:25,583 iteration 4770 : loss : 0.035458, loss_ce: 0.019101
2021-12-11 23:22:28,269 iteration 4771 : loss : 0.055894, loss_ce: 0.019947
2021-12-11 23:22:31,003 iteration 4772 : loss : 0.024346, loss_ce: 0.012072
2021-12-11 23:22:33,582 iteration 4773 : loss : 0.024151, loss_ce: 0.013429
2021-12-11 23:22:36,408 iteration 4774 : loss : 0.049021, loss_ce: 0.019468
2021-12-11 23:22:39,126 iteration 4775 : loss : 0.030460, loss_ce: 0.015378
2021-12-11 23:22:41,807 iteration 4776 : loss : 0.036183, loss_ce: 0.015384
2021-12-11 23:22:44,554 iteration 4777 : loss : 0.024268, loss_ce: 0.010323
 70%|██████████████████▉        | 281/400 [3:59:42<1:43:28, 52.17s/it]2021-12-11 23:22:47,533 iteration 4778 : loss : 0.034526, loss_ce: 0.020010
2021-12-11 23:22:50,185 iteration 4779 : loss : 0.025008, loss_ce: 0.013945
2021-12-11 23:22:52,956 iteration 4780 : loss : 0.028104, loss_ce: 0.016046
2021-12-11 23:22:55,776 iteration 4781 : loss : 0.044553, loss_ce: 0.018655
2021-12-11 23:22:58,620 iteration 4782 : loss : 0.037553, loss_ce: 0.017780
2021-12-11 23:23:01,471 iteration 4783 : loss : 0.036523, loss_ce: 0.017154
2021-12-11 23:23:04,258 iteration 4784 : loss : 0.040815, loss_ce: 0.015874
2021-12-11 23:23:07,034 iteration 4785 : loss : 0.026910, loss_ce: 0.012541
2021-12-11 23:23:09,775 iteration 4786 : loss : 0.024628, loss_ce: 0.012389
2021-12-11 23:23:12,492 iteration 4787 : loss : 0.069876, loss_ce: 0.021135
2021-12-11 23:23:15,332 iteration 4788 : loss : 0.034390, loss_ce: 0.012997
2021-12-11 23:23:18,198 iteration 4789 : loss : 0.027701, loss_ce: 0.013050
2021-12-11 23:23:20,896 iteration 4790 : loss : 0.024080, loss_ce: 0.012084
2021-12-11 23:23:23,696 iteration 4791 : loss : 0.054496, loss_ce: 0.025904
2021-12-11 23:23:26,446 iteration 4792 : loss : 0.032721, loss_ce: 0.016427
2021-12-11 23:23:29,247 iteration 4793 : loss : 0.033014, loss_ce: 0.013284
2021-12-11 23:23:32,125 iteration 4794 : loss : 0.028334, loss_ce: 0.011894
 70%|███████████████████        | 282/400 [4:00:30<1:39:53, 50.79s/it]2021-12-11 23:23:34,992 iteration 4795 : loss : 0.037330, loss_ce: 0.016591
2021-12-11 23:23:37,867 iteration 4796 : loss : 0.032080, loss_ce: 0.012345
2021-12-11 23:23:40,583 iteration 4797 : loss : 0.024942, loss_ce: 0.011619
2021-12-11 23:23:43,350 iteration 4798 : loss : 0.034067, loss_ce: 0.018051
2021-12-11 23:23:46,068 iteration 4799 : loss : 0.028210, loss_ce: 0.009440
2021-12-11 23:23:48,849 iteration 4800 : loss : 0.041174, loss_ce: 0.016857
2021-12-11 23:23:51,571 iteration 4801 : loss : 0.029154, loss_ce: 0.015781
2021-12-11 23:23:54,514 iteration 4802 : loss : 0.032595, loss_ce: 0.015802
2021-12-11 23:23:57,300 iteration 4803 : loss : 0.027964, loss_ce: 0.012888
2021-12-11 23:24:00,031 iteration 4804 : loss : 0.024975, loss_ce: 0.011949
2021-12-11 23:24:03,009 iteration 4805 : loss : 0.039353, loss_ce: 0.018984
2021-12-11 23:24:05,746 iteration 4806 : loss : 0.037492, loss_ce: 0.011495
2021-12-11 23:24:08,416 iteration 4807 : loss : 0.032106, loss_ce: 0.011688
2021-12-11 23:24:11,316 iteration 4808 : loss : 0.030830, loss_ce: 0.013363
2021-12-11 23:24:13,953 iteration 4809 : loss : 0.031227, loss_ce: 0.018630
2021-12-11 23:24:16,840 iteration 4810 : loss : 0.032532, loss_ce: 0.013682
2021-12-11 23:24:19,626 iteration 4811 : loss : 0.040257, loss_ce: 0.017526
 71%|███████████████████        | 283/400 [4:01:17<1:37:07, 49.81s/it]2021-12-11 23:24:22,583 iteration 4812 : loss : 0.046139, loss_ce: 0.017028
2021-12-11 23:24:25,236 iteration 4813 : loss : 0.023689, loss_ce: 0.012213
2021-12-11 23:24:28,197 iteration 4814 : loss : 0.039702, loss_ce: 0.019133
2021-12-11 23:24:30,866 iteration 4815 : loss : 0.030050, loss_ce: 0.013472
2021-12-11 23:24:33,677 iteration 4816 : loss : 0.037343, loss_ce: 0.016820
2021-12-11 23:24:36,261 iteration 4817 : loss : 0.022177, loss_ce: 0.011308
2021-12-11 23:24:38,934 iteration 4818 : loss : 0.035548, loss_ce: 0.016174
2021-12-11 23:24:41,771 iteration 4819 : loss : 0.026586, loss_ce: 0.010890
2021-12-11 23:24:44,403 iteration 4820 : loss : 0.034556, loss_ce: 0.017547
2021-12-11 23:24:47,350 iteration 4821 : loss : 0.032723, loss_ce: 0.012690
2021-12-11 23:24:50,219 iteration 4822 : loss : 0.037269, loss_ce: 0.015621
2021-12-11 23:24:53,043 iteration 4823 : loss : 0.043558, loss_ce: 0.015044
2021-12-11 23:24:55,662 iteration 4824 : loss : 0.027613, loss_ce: 0.011423
2021-12-11 23:24:58,608 iteration 4825 : loss : 0.028802, loss_ce: 0.015289
2021-12-11 23:25:01,589 iteration 4826 : loss : 0.057212, loss_ce: 0.027526
2021-12-11 23:25:04,153 iteration 4827 : loss : 0.035740, loss_ce: 0.021011
2021-12-11 23:25:06,789 iteration 4828 : loss : 0.022574, loss_ce: 0.010486
 71%|███████████████████▏       | 284/400 [4:02:04<1:34:44, 49.01s/it]2021-12-11 23:25:09,551 iteration 4829 : loss : 0.023393, loss_ce: 0.013526
2021-12-11 23:25:12,499 iteration 4830 : loss : 0.032750, loss_ce: 0.016358
2021-12-11 23:25:15,358 iteration 4831 : loss : 0.034268, loss_ce: 0.020587
2021-12-11 23:25:18,062 iteration 4832 : loss : 0.025549, loss_ce: 0.012459
2021-12-11 23:25:20,742 iteration 4833 : loss : 0.037454, loss_ce: 0.020855
2021-12-11 23:25:23,752 iteration 4834 : loss : 0.033536, loss_ce: 0.015704
2021-12-11 23:25:26,403 iteration 4835 : loss : 0.030231, loss_ce: 0.015787
2021-12-11 23:25:29,190 iteration 4836 : loss : 0.033953, loss_ce: 0.013346
2021-12-11 23:25:31,904 iteration 4837 : loss : 0.026876, loss_ce: 0.010846
2021-12-11 23:25:34,638 iteration 4838 : loss : 0.034801, loss_ce: 0.015954
2021-12-11 23:25:37,313 iteration 4839 : loss : 0.032168, loss_ce: 0.015262
2021-12-11 23:25:40,192 iteration 4840 : loss : 0.035542, loss_ce: 0.015176
2021-12-11 23:25:42,980 iteration 4841 : loss : 0.030181, loss_ce: 0.014947
2021-12-11 23:25:45,727 iteration 4842 : loss : 0.041182, loss_ce: 0.018833
2021-12-11 23:25:48,387 iteration 4843 : loss : 0.024817, loss_ce: 0.011541
2021-12-11 23:25:51,377 iteration 4844 : loss : 0.035058, loss_ce: 0.014584
2021-12-11 23:25:51,378 Training Data Eval:
2021-12-11 23:26:06,193   Average segmentation loss on training set: 0.0160
2021-12-11 23:26:06,193 Validation Data Eval:
2021-12-11 23:26:11,342   Average segmentation loss on validation set: 0.0984
2021-12-11 23:26:14,141 iteration 4845 : loss : 0.033493, loss_ce: 0.014145
 71%|███████████████████▏       | 285/400 [4:03:12<1:44:29, 54.52s/it]2021-12-11 23:26:16,953 iteration 4846 : loss : 0.034844, loss_ce: 0.017021
2021-12-11 23:26:19,741 iteration 4847 : loss : 0.021867, loss_ce: 0.010999
2021-12-11 23:26:22,555 iteration 4848 : loss : 0.042379, loss_ce: 0.021853
2021-12-11 23:26:25,253 iteration 4849 : loss : 0.026219, loss_ce: 0.012161
2021-12-11 23:26:28,031 iteration 4850 : loss : 0.028810, loss_ce: 0.016261
2021-12-11 23:26:30,861 iteration 4851 : loss : 0.036872, loss_ce: 0.014485
2021-12-11 23:26:33,645 iteration 4852 : loss : 0.033979, loss_ce: 0.018539
2021-12-11 23:26:36,538 iteration 4853 : loss : 0.044420, loss_ce: 0.011869
2021-12-11 23:26:39,548 iteration 4854 : loss : 0.048590, loss_ce: 0.021528
2021-12-11 23:26:42,406 iteration 4855 : loss : 0.038461, loss_ce: 0.015762
2021-12-11 23:26:45,014 iteration 4856 : loss : 0.025022, loss_ce: 0.011370
2021-12-11 23:26:47,975 iteration 4857 : loss : 0.026889, loss_ce: 0.014516
2021-12-11 23:26:50,761 iteration 4858 : loss : 0.034864, loss_ce: 0.015220
2021-12-11 23:26:53,610 iteration 4859 : loss : 0.025630, loss_ce: 0.014044
2021-12-11 23:26:56,569 iteration 4860 : loss : 0.047314, loss_ce: 0.020242
2021-12-11 23:26:59,251 iteration 4861 : loss : 0.026367, loss_ce: 0.013071
2021-12-11 23:27:02,056 iteration 4862 : loss : 0.026816, loss_ce: 0.013373
 72%|███████████████████▎       | 286/400 [4:04:00<1:39:49, 52.54s/it]2021-12-11 23:27:04,833 iteration 4863 : loss : 0.025164, loss_ce: 0.011068
2021-12-11 23:27:07,823 iteration 4864 : loss : 0.053322, loss_ce: 0.022091
2021-12-11 23:27:10,632 iteration 4865 : loss : 0.049383, loss_ce: 0.020132
2021-12-11 23:27:13,342 iteration 4866 : loss : 0.018728, loss_ce: 0.010375
2021-12-11 23:27:16,012 iteration 4867 : loss : 0.038680, loss_ce: 0.016497
2021-12-11 23:27:18,720 iteration 4868 : loss : 0.032357, loss_ce: 0.018720
2021-12-11 23:27:21,509 iteration 4869 : loss : 0.033649, loss_ce: 0.016567
2021-12-11 23:27:24,310 iteration 4870 : loss : 0.045972, loss_ce: 0.017111
2021-12-11 23:27:27,119 iteration 4871 : loss : 0.022171, loss_ce: 0.011121
2021-12-11 23:27:29,799 iteration 4872 : loss : 0.036364, loss_ce: 0.017694
2021-12-11 23:27:32,585 iteration 4873 : loss : 0.033055, loss_ce: 0.013874
2021-12-11 23:27:35,208 iteration 4874 : loss : 0.024774, loss_ce: 0.012962
2021-12-11 23:27:37,935 iteration 4875 : loss : 0.021402, loss_ce: 0.010228
2021-12-11 23:27:40,767 iteration 4876 : loss : 0.029221, loss_ce: 0.012862
2021-12-11 23:27:43,633 iteration 4877 : loss : 0.035178, loss_ce: 0.016914
2021-12-11 23:27:46,318 iteration 4878 : loss : 0.023389, loss_ce: 0.010985
2021-12-11 23:27:49,107 iteration 4879 : loss : 0.038699, loss_ce: 0.017197
 72%|███████████████████▎       | 287/400 [4:04:47<1:35:50, 50.89s/it]2021-12-11 23:27:51,821 iteration 4880 : loss : 0.054674, loss_ce: 0.033967
2021-12-11 23:27:54,500 iteration 4881 : loss : 0.042516, loss_ce: 0.015652
2021-12-11 23:27:57,356 iteration 4882 : loss : 0.027466, loss_ce: 0.009924
2021-12-11 23:28:00,055 iteration 4883 : loss : 0.044602, loss_ce: 0.015859
2021-12-11 23:28:02,837 iteration 4884 : loss : 0.028431, loss_ce: 0.019723
2021-12-11 23:28:05,813 iteration 4885 : loss : 0.044176, loss_ce: 0.015634
2021-12-11 23:28:08,692 iteration 4886 : loss : 0.033601, loss_ce: 0.014534
2021-12-11 23:28:11,541 iteration 4887 : loss : 0.037636, loss_ce: 0.017094
2021-12-11 23:28:14,350 iteration 4888 : loss : 0.046999, loss_ce: 0.016642
2021-12-11 23:28:16,988 iteration 4889 : loss : 0.024110, loss_ce: 0.011347
2021-12-11 23:28:19,846 iteration 4890 : loss : 0.035168, loss_ce: 0.014973
2021-12-11 23:28:22,676 iteration 4891 : loss : 0.036280, loss_ce: 0.021422
2021-12-11 23:28:25,424 iteration 4892 : loss : 0.031566, loss_ce: 0.015098
2021-12-11 23:28:28,033 iteration 4893 : loss : 0.028712, loss_ce: 0.016752
2021-12-11 23:28:30,937 iteration 4894 : loss : 0.037237, loss_ce: 0.012909
2021-12-11 23:28:33,482 iteration 4895 : loss : 0.027753, loss_ce: 0.014813
2021-12-11 23:28:36,309 iteration 4896 : loss : 0.041147, loss_ce: 0.020019
 72%|███████████████████▍       | 288/400 [4:05:34<1:32:55, 49.78s/it]2021-12-11 23:28:39,066 iteration 4897 : loss : 0.035532, loss_ce: 0.014497
2021-12-11 23:28:42,018 iteration 4898 : loss : 0.062926, loss_ce: 0.024548
2021-12-11 23:28:44,674 iteration 4899 : loss : 0.029047, loss_ce: 0.014825
2021-12-11 23:28:47,508 iteration 4900 : loss : 0.036382, loss_ce: 0.014410
2021-12-11 23:28:50,408 iteration 4901 : loss : 0.025786, loss_ce: 0.009690
2021-12-11 23:28:53,281 iteration 4902 : loss : 0.036795, loss_ce: 0.016643
2021-12-11 23:28:56,223 iteration 4903 : loss : 0.047424, loss_ce: 0.022089
2021-12-11 23:28:59,002 iteration 4904 : loss : 0.046475, loss_ce: 0.024700
2021-12-11 23:29:01,883 iteration 4905 : loss : 0.040001, loss_ce: 0.016645
2021-12-11 23:29:04,716 iteration 4906 : loss : 0.035836, loss_ce: 0.015389
2021-12-11 23:29:07,486 iteration 4907 : loss : 0.021607, loss_ce: 0.011461
2021-12-11 23:29:10,221 iteration 4908 : loss : 0.033349, loss_ce: 0.016302
2021-12-11 23:29:13,087 iteration 4909 : loss : 0.031508, loss_ce: 0.013405
2021-12-11 23:29:16,055 iteration 4910 : loss : 0.034277, loss_ce: 0.015182
2021-12-11 23:29:18,712 iteration 4911 : loss : 0.026830, loss_ce: 0.012052
2021-12-11 23:29:21,471 iteration 4912 : loss : 0.039568, loss_ce: 0.016386
2021-12-11 23:29:24,450 iteration 4913 : loss : 0.035574, loss_ce: 0.015830
 72%|███████████████████▌       | 289/400 [4:06:22<1:31:11, 49.29s/it]2021-12-11 23:29:27,090 iteration 4914 : loss : 0.028492, loss_ce: 0.014186
2021-12-11 23:29:29,984 iteration 4915 : loss : 0.032098, loss_ce: 0.016388
2021-12-11 23:29:32,866 iteration 4916 : loss : 0.047037, loss_ce: 0.016940
2021-12-11 23:29:35,660 iteration 4917 : loss : 0.023499, loss_ce: 0.011090
2021-12-11 23:29:38,560 iteration 4918 : loss : 0.027993, loss_ce: 0.014039
2021-12-11 23:29:41,435 iteration 4919 : loss : 0.022784, loss_ce: 0.012455
2021-12-11 23:29:44,272 iteration 4920 : loss : 0.035547, loss_ce: 0.016593
2021-12-11 23:29:46,862 iteration 4921 : loss : 0.024818, loss_ce: 0.011811
2021-12-11 23:29:49,667 iteration 4922 : loss : 0.026685, loss_ce: 0.011580
2021-12-11 23:29:52,339 iteration 4923 : loss : 0.032119, loss_ce: 0.014344
2021-12-11 23:29:55,306 iteration 4924 : loss : 0.034864, loss_ce: 0.016917
2021-12-11 23:29:58,040 iteration 4925 : loss : 0.035803, loss_ce: 0.015254
2021-12-11 23:30:00,624 iteration 4926 : loss : 0.031040, loss_ce: 0.016457
2021-12-11 23:30:03,527 iteration 4927 : loss : 0.023367, loss_ce: 0.011902
2021-12-11 23:30:06,235 iteration 4928 : loss : 0.028123, loss_ce: 0.014295
2021-12-11 23:30:08,986 iteration 4929 : loss : 0.040289, loss_ce: 0.016352
2021-12-11 23:30:08,986 Training Data Eval:
2021-12-11 23:30:24,199   Average segmentation loss on training set: 0.0159
2021-12-11 23:30:24,200 Validation Data Eval:
2021-12-11 23:30:29,508   Average segmentation loss on validation set: 0.0901
2021-12-11 23:30:32,164 iteration 4930 : loss : 0.049385, loss_ce: 0.019778
 72%|███████████████████▌       | 290/400 [4:07:30<1:40:30, 54.82s/it]2021-12-11 23:30:35,077 iteration 4931 : loss : 0.041794, loss_ce: 0.023843
2021-12-11 23:30:37,931 iteration 4932 : loss : 0.037139, loss_ce: 0.015073
2021-12-11 23:30:40,545 iteration 4933 : loss : 0.029243, loss_ce: 0.013770
2021-12-11 23:30:43,418 iteration 4934 : loss : 0.030250, loss_ce: 0.014346
2021-12-11 23:30:46,172 iteration 4935 : loss : 0.038619, loss_ce: 0.014299
2021-12-11 23:30:48,944 iteration 4936 : loss : 0.034656, loss_ce: 0.015177
2021-12-11 23:30:51,619 iteration 4937 : loss : 0.027135, loss_ce: 0.011759
2021-12-11 23:30:54,420 iteration 4938 : loss : 0.028966, loss_ce: 0.014017
2021-12-11 23:30:57,419 iteration 4939 : loss : 0.044412, loss_ce: 0.021552
2021-12-11 23:31:00,238 iteration 4940 : loss : 0.029618, loss_ce: 0.014518
2021-12-11 23:31:03,055 iteration 4941 : loss : 0.047479, loss_ce: 0.022126
2021-12-11 23:31:06,143 iteration 4942 : loss : 0.036158, loss_ce: 0.015346
2021-12-11 23:31:08,820 iteration 4943 : loss : 0.033716, loss_ce: 0.016507
2021-12-11 23:31:11,734 iteration 4944 : loss : 0.035131, loss_ce: 0.019911
2021-12-11 23:31:14,577 iteration 4945 : loss : 0.026243, loss_ce: 0.013801
2021-12-11 23:31:17,499 iteration 4946 : loss : 0.025992, loss_ce: 0.012195
2021-12-11 23:31:20,377 iteration 4947 : loss : 0.042171, loss_ce: 0.016974
 73%|███████████████████▋       | 291/400 [4:08:18<1:35:59, 52.84s/it]2021-12-11 23:31:23,358 iteration 4948 : loss : 0.033344, loss_ce: 0.013972
2021-12-11 23:31:26,229 iteration 4949 : loss : 0.038654, loss_ce: 0.018617
2021-12-11 23:31:29,040 iteration 4950 : loss : 0.023752, loss_ce: 0.013502
2021-12-11 23:31:31,776 iteration 4951 : loss : 0.037876, loss_ce: 0.016905
2021-12-11 23:31:34,569 iteration 4952 : loss : 0.034087, loss_ce: 0.016539
2021-12-11 23:31:37,540 iteration 4953 : loss : 0.042349, loss_ce: 0.014003
2021-12-11 23:31:40,157 iteration 4954 : loss : 0.027624, loss_ce: 0.012108
2021-12-11 23:31:43,039 iteration 4955 : loss : 0.025640, loss_ce: 0.014956
2021-12-11 23:31:45,936 iteration 4956 : loss : 0.033789, loss_ce: 0.013708
2021-12-11 23:31:48,828 iteration 4957 : loss : 0.033295, loss_ce: 0.016708
2021-12-11 23:31:51,685 iteration 4958 : loss : 0.036482, loss_ce: 0.017953
2021-12-11 23:31:54,524 iteration 4959 : loss : 0.036158, loss_ce: 0.016441
2021-12-11 23:31:57,549 iteration 4960 : loss : 0.031468, loss_ce: 0.015688
2021-12-11 23:32:00,305 iteration 4961 : loss : 0.053808, loss_ce: 0.019151
2021-12-11 23:32:03,085 iteration 4962 : loss : 0.034208, loss_ce: 0.017673
2021-12-11 23:32:06,022 iteration 4963 : loss : 0.027418, loss_ce: 0.012360
2021-12-11 23:32:08,897 iteration 4964 : loss : 0.025042, loss_ce: 0.010963
 73%|███████████████████▋       | 292/400 [4:09:07<1:32:46, 51.54s/it]2021-12-11 23:32:11,573 iteration 4965 : loss : 0.031031, loss_ce: 0.013121
2021-12-11 23:32:14,405 iteration 4966 : loss : 0.032215, loss_ce: 0.014157
2021-12-11 23:32:17,303 iteration 4967 : loss : 0.040822, loss_ce: 0.014941
2021-12-11 23:32:20,164 iteration 4968 : loss : 0.037979, loss_ce: 0.013204
2021-12-11 23:32:22,851 iteration 4969 : loss : 0.052917, loss_ce: 0.021936
2021-12-11 23:32:25,768 iteration 4970 : loss : 0.053199, loss_ce: 0.027474
2021-12-11 23:32:28,518 iteration 4971 : loss : 0.030325, loss_ce: 0.014348
2021-12-11 23:32:31,220 iteration 4972 : loss : 0.026196, loss_ce: 0.011250
2021-12-11 23:32:34,067 iteration 4973 : loss : 0.023451, loss_ce: 0.010595
2021-12-11 23:32:36,972 iteration 4974 : loss : 0.035061, loss_ce: 0.020318
2021-12-11 23:32:39,834 iteration 4975 : loss : 0.028375, loss_ce: 0.012934
2021-12-11 23:32:42,650 iteration 4976 : loss : 0.024708, loss_ce: 0.011561
2021-12-11 23:32:45,296 iteration 4977 : loss : 0.025994, loss_ce: 0.013199
2021-12-11 23:32:48,207 iteration 4978 : loss : 0.041516, loss_ce: 0.019559
2021-12-11 23:32:50,955 iteration 4979 : loss : 0.024019, loss_ce: 0.011186
2021-12-11 23:32:53,670 iteration 4980 : loss : 0.021023, loss_ce: 0.010497
2021-12-11 23:32:56,403 iteration 4981 : loss : 0.036534, loss_ce: 0.016089
 73%|███████████████████▊       | 293/400 [4:09:54<1:29:45, 50.33s/it]2021-12-11 23:32:59,225 iteration 4982 : loss : 0.060113, loss_ce: 0.019120
2021-12-11 23:33:01,967 iteration 4983 : loss : 0.027258, loss_ce: 0.012382
2021-12-11 23:33:04,738 iteration 4984 : loss : 0.037146, loss_ce: 0.013999
2021-12-11 23:33:07,384 iteration 4985 : loss : 0.023914, loss_ce: 0.011064
2021-12-11 23:33:10,179 iteration 4986 : loss : 0.028606, loss_ce: 0.012214
2021-12-11 23:33:12,986 iteration 4987 : loss : 0.028574, loss_ce: 0.015122
2021-12-11 23:33:15,881 iteration 4988 : loss : 0.031923, loss_ce: 0.017753
2021-12-11 23:33:18,629 iteration 4989 : loss : 0.032258, loss_ce: 0.016866
2021-12-11 23:33:21,547 iteration 4990 : loss : 0.031031, loss_ce: 0.015472
2021-12-11 23:33:24,186 iteration 4991 : loss : 0.028652, loss_ce: 0.012602
2021-12-11 23:33:27,139 iteration 4992 : loss : 0.040849, loss_ce: 0.016312
2021-12-11 23:33:29,808 iteration 4993 : loss : 0.035240, loss_ce: 0.017851
2021-12-11 23:33:32,816 iteration 4994 : loss : 0.039123, loss_ce: 0.017088
2021-12-11 23:33:35,712 iteration 4995 : loss : 0.045654, loss_ce: 0.022951
2021-12-11 23:33:38,469 iteration 4996 : loss : 0.024979, loss_ce: 0.010831
2021-12-11 23:33:41,320 iteration 4997 : loss : 0.030474, loss_ce: 0.014878
2021-12-11 23:33:44,158 iteration 4998 : loss : 0.023560, loss_ce: 0.011723
 74%|███████████████████▊       | 294/400 [4:10:42<1:27:33, 49.56s/it]2021-12-11 23:33:47,042 iteration 4999 : loss : 0.024331, loss_ce: 0.011511
2021-12-11 23:33:49,810 iteration 5000 : loss : 0.036751, loss_ce: 0.016213
2021-12-11 23:33:52,738 iteration 5001 : loss : 0.031593, loss_ce: 0.012141
2021-12-11 23:33:55,672 iteration 5002 : loss : 0.030906, loss_ce: 0.012214
2021-12-11 23:33:58,560 iteration 5003 : loss : 0.033417, loss_ce: 0.017608
2021-12-11 23:34:01,292 iteration 5004 : loss : 0.026629, loss_ce: 0.013457
2021-12-11 23:34:04,145 iteration 5005 : loss : 0.028393, loss_ce: 0.013695
2021-12-11 23:34:07,016 iteration 5006 : loss : 0.030103, loss_ce: 0.013353
2021-12-11 23:34:09,774 iteration 5007 : loss : 0.037229, loss_ce: 0.013806
2021-12-11 23:34:12,578 iteration 5008 : loss : 0.031881, loss_ce: 0.013441
2021-12-11 23:34:15,306 iteration 5009 : loss : 0.029064, loss_ce: 0.017418
2021-12-11 23:34:18,043 iteration 5010 : loss : 0.032508, loss_ce: 0.017549
2021-12-11 23:34:20,742 iteration 5011 : loss : 0.025343, loss_ce: 0.012984
2021-12-11 23:34:23,614 iteration 5012 : loss : 0.045546, loss_ce: 0.016796
2021-12-11 23:34:26,454 iteration 5013 : loss : 0.031461, loss_ce: 0.015596
2021-12-11 23:34:29,009 iteration 5014 : loss : 0.032497, loss_ce: 0.014263
2021-12-11 23:34:29,009 Training Data Eval:
2021-12-11 23:34:44,143   Average segmentation loss on training set: 0.0156
2021-12-11 23:34:44,143 Validation Data Eval:
2021-12-11 23:34:49,312   Average segmentation loss on validation set: 0.1028
2021-12-11 23:34:52,061 iteration 5015 : loss : 0.031058, loss_ce: 0.012255
 74%|███████████████████▉       | 295/400 [4:11:50<1:36:21, 55.06s/it]2021-12-11 23:34:54,814 iteration 5016 : loss : 0.043246, loss_ce: 0.020784
2021-12-11 23:34:57,379 iteration 5017 : loss : 0.020364, loss_ce: 0.010963
2021-12-11 23:35:00,253 iteration 5018 : loss : 0.027046, loss_ce: 0.014451
2021-12-11 23:35:02,898 iteration 5019 : loss : 0.027620, loss_ce: 0.012698
2021-12-11 23:35:05,816 iteration 5020 : loss : 0.066462, loss_ce: 0.020826
2021-12-11 23:35:08,508 iteration 5021 : loss : 0.045507, loss_ce: 0.020708
2021-12-11 23:35:11,387 iteration 5022 : loss : 0.027146, loss_ce: 0.011689
2021-12-11 23:35:14,003 iteration 5023 : loss : 0.032487, loss_ce: 0.017734
2021-12-11 23:35:16,858 iteration 5024 : loss : 0.045934, loss_ce: 0.021115
2021-12-11 23:35:19,779 iteration 5025 : loss : 0.041337, loss_ce: 0.016911
2021-12-11 23:35:22,642 iteration 5026 : loss : 0.037635, loss_ce: 0.013731
2021-12-11 23:35:25,435 iteration 5027 : loss : 0.022880, loss_ce: 0.011402
2021-12-11 23:35:28,303 iteration 5028 : loss : 0.036876, loss_ce: 0.019442
2021-12-11 23:35:31,236 iteration 5029 : loss : 0.041285, loss_ce: 0.019441
2021-12-11 23:35:33,951 iteration 5030 : loss : 0.023372, loss_ce: 0.012158
2021-12-11 23:35:36,775 iteration 5031 : loss : 0.046944, loss_ce: 0.017641
2021-12-11 23:35:39,560 iteration 5032 : loss : 0.029636, loss_ce: 0.013758
 74%|███████████████████▉       | 296/400 [4:12:37<1:31:30, 52.79s/it]2021-12-11 23:35:42,375 iteration 5033 : loss : 0.033969, loss_ce: 0.015349
2021-12-11 23:35:45,163 iteration 5034 : loss : 0.046013, loss_ce: 0.014598
2021-12-11 23:35:47,848 iteration 5035 : loss : 0.030165, loss_ce: 0.012355
2021-12-11 23:35:50,594 iteration 5036 : loss : 0.031724, loss_ce: 0.014830
2021-12-11 23:35:53,278 iteration 5037 : loss : 0.028964, loss_ce: 0.013812
2021-12-11 23:35:56,054 iteration 5038 : loss : 0.035325, loss_ce: 0.013886
2021-12-11 23:35:58,685 iteration 5039 : loss : 0.037481, loss_ce: 0.017153
2021-12-11 23:36:01,502 iteration 5040 : loss : 0.027766, loss_ce: 0.014019
2021-12-11 23:36:04,194 iteration 5041 : loss : 0.030228, loss_ce: 0.015126
2021-12-11 23:36:07,119 iteration 5042 : loss : 0.042947, loss_ce: 0.018377
2021-12-11 23:36:09,740 iteration 5043 : loss : 0.029884, loss_ce: 0.012871
2021-12-11 23:36:12,627 iteration 5044 : loss : 0.025773, loss_ce: 0.012077
2021-12-11 23:36:15,421 iteration 5045 : loss : 0.040696, loss_ce: 0.022543
2021-12-11 23:36:18,317 iteration 5046 : loss : 0.042005, loss_ce: 0.013153
2021-12-11 23:36:21,060 iteration 5047 : loss : 0.021288, loss_ce: 0.009518
2021-12-11 23:36:23,819 iteration 5048 : loss : 0.019453, loss_ce: 0.011785
2021-12-11 23:36:26,686 iteration 5049 : loss : 0.027965, loss_ce: 0.015592
 74%|████████████████████       | 297/400 [4:13:24<1:27:42, 51.09s/it]2021-12-11 23:36:29,432 iteration 5050 : loss : 0.030815, loss_ce: 0.011999
2021-12-11 23:36:32,294 iteration 5051 : loss : 0.048369, loss_ce: 0.019674
2021-12-11 23:36:35,346 iteration 5052 : loss : 0.037982, loss_ce: 0.017464
2021-12-11 23:36:38,322 iteration 5053 : loss : 0.036161, loss_ce: 0.014345
2021-12-11 23:36:41,208 iteration 5054 : loss : 0.039533, loss_ce: 0.013791
2021-12-11 23:36:44,112 iteration 5055 : loss : 0.033996, loss_ce: 0.014869
2021-12-11 23:36:46,880 iteration 5056 : loss : 0.031315, loss_ce: 0.016304
2021-12-11 23:36:49,888 iteration 5057 : loss : 0.035306, loss_ce: 0.016853
2021-12-11 23:36:52,878 iteration 5058 : loss : 0.045151, loss_ce: 0.018770
2021-12-11 23:36:55,650 iteration 5059 : loss : 0.034051, loss_ce: 0.015247
2021-12-11 23:36:58,289 iteration 5060 : loss : 0.034557, loss_ce: 0.019956
2021-12-11 23:37:01,309 iteration 5061 : loss : 0.033641, loss_ce: 0.015875
2021-12-11 23:37:04,188 iteration 5062 : loss : 0.036616, loss_ce: 0.014839
2021-12-11 23:37:07,053 iteration 5063 : loss : 0.043106, loss_ce: 0.021299
2021-12-11 23:37:09,752 iteration 5064 : loss : 0.031869, loss_ce: 0.014266
2021-12-11 23:37:12,649 iteration 5065 : loss : 0.029834, loss_ce: 0.018212
2021-12-11 23:37:15,481 iteration 5066 : loss : 0.030132, loss_ce: 0.014907
 74%|████████████████████       | 298/400 [4:14:13<1:25:40, 50.40s/it]2021-12-11 23:37:18,073 iteration 5067 : loss : 0.025463, loss_ce: 0.013134
2021-12-11 23:37:21,004 iteration 5068 : loss : 0.024563, loss_ce: 0.010467
2021-12-11 23:37:23,656 iteration 5069 : loss : 0.036096, loss_ce: 0.019859
2021-12-11 23:37:26,262 iteration 5070 : loss : 0.034830, loss_ce: 0.014855
2021-12-11 23:37:29,217 iteration 5071 : loss : 0.040194, loss_ce: 0.021438
2021-12-11 23:37:31,854 iteration 5072 : loss : 0.024431, loss_ce: 0.012864
2021-12-11 23:37:34,701 iteration 5073 : loss : 0.031316, loss_ce: 0.015465
2021-12-11 23:37:37,414 iteration 5074 : loss : 0.025176, loss_ce: 0.013205
2021-12-11 23:37:40,178 iteration 5075 : loss : 0.031316, loss_ce: 0.014486
2021-12-11 23:37:43,088 iteration 5076 : loss : 0.058902, loss_ce: 0.015297
2021-12-11 23:37:45,789 iteration 5077 : loss : 0.049629, loss_ce: 0.020229
2021-12-11 23:37:48,557 iteration 5078 : loss : 0.035041, loss_ce: 0.018500
2021-12-11 23:37:51,366 iteration 5079 : loss : 0.024927, loss_ce: 0.012533
2021-12-11 23:37:54,205 iteration 5080 : loss : 0.037592, loss_ce: 0.017013
2021-12-11 23:37:57,045 iteration 5081 : loss : 0.050577, loss_ce: 0.017735
2021-12-11 23:37:59,858 iteration 5082 : loss : 0.026206, loss_ce: 0.013295
2021-12-11 23:38:02,517 iteration 5083 : loss : 0.044761, loss_ce: 0.011880
 75%|████████████████████▏      | 299/400 [4:15:00<1:23:08, 49.39s/it]2021-12-11 23:38:05,350 iteration 5084 : loss : 0.046535, loss_ce: 0.029495
2021-12-11 23:38:08,321 iteration 5085 : loss : 0.036388, loss_ce: 0.015171
2021-12-11 23:38:11,130 iteration 5086 : loss : 0.027280, loss_ce: 0.011274
2021-12-11 23:38:13,817 iteration 5087 : loss : 0.038185, loss_ce: 0.019223
2021-12-11 23:38:16,586 iteration 5088 : loss : 0.039750, loss_ce: 0.019241
2021-12-11 23:38:19,324 iteration 5089 : loss : 0.030057, loss_ce: 0.012148
2021-12-11 23:38:22,223 iteration 5090 : loss : 0.045821, loss_ce: 0.013232
2021-12-11 23:38:25,033 iteration 5091 : loss : 0.032013, loss_ce: 0.014045
2021-12-11 23:38:27,775 iteration 5092 : loss : 0.043917, loss_ce: 0.018068
2021-12-11 23:38:30,446 iteration 5093 : loss : 0.022950, loss_ce: 0.009856
2021-12-11 23:38:33,134 iteration 5094 : loss : 0.030795, loss_ce: 0.014341
2021-12-11 23:38:35,870 iteration 5095 : loss : 0.025291, loss_ce: 0.012740
2021-12-11 23:38:38,610 iteration 5096 : loss : 0.034896, loss_ce: 0.017426
2021-12-11 23:38:41,296 iteration 5097 : loss : 0.027102, loss_ce: 0.015045
2021-12-11 23:38:44,118 iteration 5098 : loss : 0.035350, loss_ce: 0.013466
2021-12-11 23:38:46,840 iteration 5099 : loss : 0.046014, loss_ce: 0.020623
2021-12-11 23:38:46,841 Training Data Eval:
2021-12-11 23:39:01,457   Average segmentation loss on training set: 0.0155
2021-12-11 23:39:01,457 Validation Data Eval:
2021-12-11 23:39:06,558   Average segmentation loss on validation set: 0.0873
2021-12-11 23:39:09,380 iteration 5100 : loss : 0.048687, loss_ce: 0.017427
2021-12-11 23:39:11,495 save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_seed2epoch_299.pth
 75%|████████████████████▎      | 300/400 [4:16:09<1:32:05, 55.25s/it]2021-12-11 23:39:13,892 iteration 5101 : loss : 0.035312, loss_ce: 0.016085
2021-12-11 23:39:16,698 iteration 5102 : loss : 0.025659, loss_ce: 0.011689
2021-12-11 23:39:19,487 iteration 5103 : loss : 0.032478, loss_ce: 0.019263
2021-12-11 23:39:22,298 iteration 5104 : loss : 0.029169, loss_ce: 0.012121
2021-12-11 23:39:24,939 iteration 5105 : loss : 0.025964, loss_ce: 0.013043
2021-12-11 23:39:27,550 iteration 5106 : loss : 0.027797, loss_ce: 0.012583
2021-12-11 23:39:30,526 iteration 5107 : loss : 0.047804, loss_ce: 0.020192
2021-12-11 23:39:33,279 iteration 5108 : loss : 0.035522, loss_ce: 0.018270
2021-12-11 23:39:35,993 iteration 5109 : loss : 0.045568, loss_ce: 0.020205
2021-12-11 23:39:38,675 iteration 5110 : loss : 0.025103, loss_ce: 0.011272
2021-12-11 23:39:41,478 iteration 5111 : loss : 0.031222, loss_ce: 0.013134
2021-12-11 23:39:44,318 iteration 5112 : loss : 0.044716, loss_ce: 0.018333
2021-12-11 23:39:47,112 iteration 5113 : loss : 0.027662, loss_ce: 0.011245
2021-12-11 23:39:50,042 iteration 5114 : loss : 0.034604, loss_ce: 0.014400
2021-12-11 23:39:52,907 iteration 5115 : loss : 0.039163, loss_ce: 0.014765
2021-12-11 23:39:55,645 iteration 5116 : loss : 0.028096, loss_ce: 0.013941
2021-12-11 23:39:58,509 iteration 5117 : loss : 0.037478, loss_ce: 0.017664
 75%|████████████████████▎      | 301/400 [4:16:56<1:27:07, 52.80s/it]2021-12-11 23:40:01,272 iteration 5118 : loss : 0.041513, loss_ce: 0.014839
2021-12-11 23:40:03,960 iteration 5119 : loss : 0.035593, loss_ce: 0.017476
2021-12-11 23:40:06,616 iteration 5120 : loss : 0.023380, loss_ce: 0.012022
2021-12-11 23:40:09,549 iteration 5121 : loss : 0.030065, loss_ce: 0.011619
2021-12-11 23:40:12,368 iteration 5122 : loss : 0.033137, loss_ce: 0.017075
2021-12-11 23:40:15,291 iteration 5123 : loss : 0.051990, loss_ce: 0.020620
2021-12-11 23:40:17,979 iteration 5124 : loss : 0.024907, loss_ce: 0.014294
2021-12-11 23:40:20,803 iteration 5125 : loss : 0.024843, loss_ce: 0.011429
2021-12-11 23:40:23,653 iteration 5126 : loss : 0.033886, loss_ce: 0.014410
2021-12-11 23:40:26,378 iteration 5127 : loss : 0.043030, loss_ce: 0.021818
2021-12-11 23:40:29,227 iteration 5128 : loss : 0.031746, loss_ce: 0.012705
2021-12-11 23:40:32,063 iteration 5129 : loss : 0.030928, loss_ce: 0.015645
2021-12-11 23:40:34,970 iteration 5130 : loss : 0.036942, loss_ce: 0.016923
2021-12-11 23:40:37,644 iteration 5131 : loss : 0.025220, loss_ce: 0.012145
2021-12-11 23:40:40,500 iteration 5132 : loss : 0.032730, loss_ce: 0.014082
2021-12-11 23:40:43,297 iteration 5133 : loss : 0.067243, loss_ce: 0.029826
2021-12-11 23:40:46,159 iteration 5134 : loss : 0.055647, loss_ce: 0.018452
 76%|████████████████████▍      | 302/400 [4:17:44<1:23:42, 51.25s/it]2021-12-11 23:40:49,079 iteration 5135 : loss : 0.039091, loss_ce: 0.020191
2021-12-11 23:40:51,923 iteration 5136 : loss : 0.026590, loss_ce: 0.011854
2021-12-11 23:40:54,540 iteration 5137 : loss : 0.024117, loss_ce: 0.011555
2021-12-11 23:40:57,469 iteration 5138 : loss : 0.027045, loss_ce: 0.014255
2021-12-11 23:41:00,220 iteration 5139 : loss : 0.032106, loss_ce: 0.015217
2021-12-11 23:41:03,012 iteration 5140 : loss : 0.035961, loss_ce: 0.016525
2021-12-11 23:41:05,858 iteration 5141 : loss : 0.030402, loss_ce: 0.013962
2021-12-11 23:41:08,587 iteration 5142 : loss : 0.026831, loss_ce: 0.012127
2021-12-11 23:41:11,383 iteration 5143 : loss : 0.026935, loss_ce: 0.015976
2021-12-11 23:41:14,215 iteration 5144 : loss : 0.027834, loss_ce: 0.014406
2021-12-11 23:41:17,099 iteration 5145 : loss : 0.033026, loss_ce: 0.011793
2021-12-11 23:41:20,040 iteration 5146 : loss : 0.025452, loss_ce: 0.010956
2021-12-11 23:41:22,823 iteration 5147 : loss : 0.038031, loss_ce: 0.013018
2021-12-11 23:41:25,729 iteration 5148 : loss : 0.025843, loss_ce: 0.011330
2021-12-11 23:41:28,458 iteration 5149 : loss : 0.035663, loss_ce: 0.015745
2021-12-11 23:41:31,256 iteration 5150 : loss : 0.034601, loss_ce: 0.019913
2021-12-11 23:41:34,105 iteration 5151 : loss : 0.053142, loss_ce: 0.025437
 76%|████████████████████▍      | 303/400 [4:18:32<1:21:15, 50.26s/it]2021-12-11 23:41:36,886 iteration 5152 : loss : 0.039917, loss_ce: 0.020059
2021-12-11 23:41:39,728 iteration 5153 : loss : 0.030872, loss_ce: 0.014325
2021-12-11 23:41:42,526 iteration 5154 : loss : 0.028235, loss_ce: 0.012380
2021-12-11 23:41:45,232 iteration 5155 : loss : 0.022644, loss_ce: 0.011724
2021-12-11 23:41:48,026 iteration 5156 : loss : 0.045566, loss_ce: 0.016975
2021-12-11 23:41:50,706 iteration 5157 : loss : 0.019540, loss_ce: 0.009753
2021-12-11 23:41:53,418 iteration 5158 : loss : 0.032521, loss_ce: 0.014241
2021-12-11 23:41:56,243 iteration 5159 : loss : 0.033500, loss_ce: 0.014993
2021-12-11 23:41:59,027 iteration 5160 : loss : 0.030869, loss_ce: 0.017133
2021-12-11 23:42:02,001 iteration 5161 : loss : 0.029346, loss_ce: 0.013683
2021-12-11 23:42:04,891 iteration 5162 : loss : 0.035236, loss_ce: 0.014579
2021-12-11 23:42:07,668 iteration 5163 : loss : 0.038246, loss_ce: 0.021992
2021-12-11 23:42:10,363 iteration 5164 : loss : 0.030932, loss_ce: 0.012994
2021-12-11 23:42:13,277 iteration 5165 : loss : 0.034600, loss_ce: 0.014461
2021-12-11 23:42:16,149 iteration 5166 : loss : 0.031772, loss_ce: 0.013542
2021-12-11 23:42:18,930 iteration 5167 : loss : 0.046373, loss_ce: 0.015677
2021-12-11 23:42:21,691 iteration 5168 : loss : 0.032994, loss_ce: 0.017212
 76%|████████████████████▌      | 304/400 [4:19:19<1:19:08, 49.46s/it]2021-12-11 23:42:24,741 iteration 5169 : loss : 0.030878, loss_ce: 0.013071
2021-12-11 23:42:27,484 iteration 5170 : loss : 0.038581, loss_ce: 0.021687
2021-12-11 23:42:30,227 iteration 5171 : loss : 0.036814, loss_ce: 0.016899
2021-12-11 23:42:33,040 iteration 5172 : loss : 0.032614, loss_ce: 0.014911
2021-12-11 23:42:35,948 iteration 5173 : loss : 0.037220, loss_ce: 0.017105
2021-12-11 23:42:38,664 iteration 5174 : loss : 0.019062, loss_ce: 0.008722
2021-12-11 23:42:41,502 iteration 5175 : loss : 0.025308, loss_ce: 0.011996
2021-12-11 23:42:44,519 iteration 5176 : loss : 0.040160, loss_ce: 0.016760
2021-12-11 23:42:47,258 iteration 5177 : loss : 0.031745, loss_ce: 0.013448
2021-12-11 23:42:50,022 iteration 5178 : loss : 0.023635, loss_ce: 0.013352
2021-12-11 23:42:52,831 iteration 5179 : loss : 0.029255, loss_ce: 0.015324
2021-12-11 23:42:55,471 iteration 5180 : loss : 0.033931, loss_ce: 0.018056
2021-12-11 23:42:58,124 iteration 5181 : loss : 0.030745, loss_ce: 0.012225
2021-12-11 23:43:00,825 iteration 5182 : loss : 0.030360, loss_ce: 0.015072
2021-12-11 23:43:03,603 iteration 5183 : loss : 0.031239, loss_ce: 0.013691
2021-12-11 23:43:06,378 iteration 5184 : loss : 0.031298, loss_ce: 0.016916
2021-12-11 23:43:06,378 Training Data Eval:
2021-12-11 23:43:21,610   Average segmentation loss on training set: 0.0147
2021-12-11 23:43:21,611 Validation Data Eval:
2021-12-11 23:43:26,830   Average segmentation loss on validation set: 0.0862
2021-12-11 23:43:29,650 iteration 5185 : loss : 0.027557, loss_ce: 0.013034
 76%|████████████████████▌      | 305/400 [4:20:27<1:27:05, 55.01s/it]2021-12-11 23:43:32,474 iteration 5186 : loss : 0.046132, loss_ce: 0.017578
2021-12-11 23:43:35,150 iteration 5187 : loss : 0.029242, loss_ce: 0.014624
2021-12-11 23:43:37,943 iteration 5188 : loss : 0.024221, loss_ce: 0.010931
2021-12-11 23:43:40,682 iteration 5189 : loss : 0.042916, loss_ce: 0.016278
2021-12-11 23:43:43,585 iteration 5190 : loss : 0.039975, loss_ce: 0.016876
2021-12-11 23:43:46,419 iteration 5191 : loss : 0.025854, loss_ce: 0.012375
2021-12-11 23:43:49,113 iteration 5192 : loss : 0.027539, loss_ce: 0.014113
2021-12-11 23:43:51,996 iteration 5193 : loss : 0.031010, loss_ce: 0.016870
2021-12-11 23:43:54,838 iteration 5194 : loss : 0.029798, loss_ce: 0.011577
2021-12-11 23:43:57,540 iteration 5195 : loss : 0.033517, loss_ce: 0.014810
2021-12-11 23:44:00,582 iteration 5196 : loss : 0.036736, loss_ce: 0.015376
2021-12-11 23:44:03,227 iteration 5197 : loss : 0.034039, loss_ce: 0.013151
2021-12-11 23:44:05,894 iteration 5198 : loss : 0.034492, loss_ce: 0.022961
2021-12-11 23:44:08,744 iteration 5199 : loss : 0.024398, loss_ce: 0.011754
2021-12-11 23:44:11,357 iteration 5200 : loss : 0.040683, loss_ce: 0.019414
2021-12-11 23:44:14,147 iteration 5201 : loss : 0.031514, loss_ce: 0.015028
2021-12-11 23:44:16,853 iteration 5202 : loss : 0.029570, loss_ce: 0.014503
 76%|████████████████████▋      | 306/400 [4:21:15<1:22:30, 52.67s/it]2021-12-11 23:44:19,612 iteration 5203 : loss : 0.031480, loss_ce: 0.015419
2021-12-11 23:44:22,329 iteration 5204 : loss : 0.045888, loss_ce: 0.021421
2021-12-11 23:44:25,193 iteration 5205 : loss : 0.056384, loss_ce: 0.017575
2021-12-11 23:44:28,053 iteration 5206 : loss : 0.039107, loss_ce: 0.016318
2021-12-11 23:44:31,038 iteration 5207 : loss : 0.083112, loss_ce: 0.021415
2021-12-11 23:44:33,936 iteration 5208 : loss : 0.020427, loss_ce: 0.011790
2021-12-11 23:44:36,793 iteration 5209 : loss : 0.037311, loss_ce: 0.016351
2021-12-11 23:44:39,472 iteration 5210 : loss : 0.035727, loss_ce: 0.019215
2021-12-11 23:44:42,374 iteration 5211 : loss : 0.039422, loss_ce: 0.015643
2021-12-11 23:44:45,259 iteration 5212 : loss : 0.038488, loss_ce: 0.014290
2021-12-11 23:44:48,016 iteration 5213 : loss : 0.029965, loss_ce: 0.013536
2021-12-11 23:44:50,731 iteration 5214 : loss : 0.031575, loss_ce: 0.017030
2021-12-11 23:44:53,513 iteration 5215 : loss : 0.020959, loss_ce: 0.009852
2021-12-11 23:44:56,302 iteration 5216 : loss : 0.031444, loss_ce: 0.012530
2021-12-11 23:44:59,100 iteration 5217 : loss : 0.041988, loss_ce: 0.020646
2021-12-11 23:45:01,877 iteration 5218 : loss : 0.036094, loss_ce: 0.015558
2021-12-11 23:45:04,628 iteration 5219 : loss : 0.031772, loss_ce: 0.014210
 77%|████████████████████▋      | 307/400 [4:22:02<1:19:21, 51.20s/it]2021-12-11 23:45:07,392 iteration 5220 : loss : 0.021871, loss_ce: 0.011449
2021-12-11 23:45:10,013 iteration 5221 : loss : 0.050397, loss_ce: 0.016353
2021-12-11 23:45:12,826 iteration 5222 : loss : 0.033359, loss_ce: 0.013365
2021-12-11 23:45:15,688 iteration 5223 : loss : 0.030060, loss_ce: 0.013954
2021-12-11 23:45:18,545 iteration 5224 : loss : 0.077214, loss_ce: 0.020601
2021-12-11 23:45:21,395 iteration 5225 : loss : 0.024357, loss_ce: 0.013429
2021-12-11 23:45:24,143 iteration 5226 : loss : 0.026668, loss_ce: 0.013189
2021-12-11 23:45:26,838 iteration 5227 : loss : 0.043485, loss_ce: 0.015234
2021-12-11 23:45:29,678 iteration 5228 : loss : 0.023684, loss_ce: 0.010507
2021-12-11 23:45:32,552 iteration 5229 : loss : 0.033726, loss_ce: 0.015605
2021-12-11 23:45:35,340 iteration 5230 : loss : 0.029035, loss_ce: 0.012556
2021-12-11 23:45:38,241 iteration 5231 : loss : 0.022744, loss_ce: 0.010246
2021-12-11 23:45:41,088 iteration 5232 : loss : 0.040030, loss_ce: 0.017135
2021-12-11 23:45:43,952 iteration 5233 : loss : 0.026027, loss_ce: 0.013338
2021-12-11 23:45:46,790 iteration 5234 : loss : 0.029225, loss_ce: 0.013551
2021-12-11 23:45:49,529 iteration 5235 : loss : 0.046736, loss_ce: 0.022970
2021-12-11 23:45:52,609 iteration 5236 : loss : 0.041870, loss_ce: 0.020131
 77%|████████████████████▊      | 308/400 [4:22:50<1:17:01, 50.23s/it]2021-12-11 23:45:55,425 iteration 5237 : loss : 0.023005, loss_ce: 0.013548
2021-12-11 23:45:58,157 iteration 5238 : loss : 0.027545, loss_ce: 0.012979
2021-12-11 23:46:00,828 iteration 5239 : loss : 0.022543, loss_ce: 0.010505
2021-12-11 23:46:03,877 iteration 5240 : loss : 0.046999, loss_ce: 0.012778
2021-12-11 23:46:06,751 iteration 5241 : loss : 0.035795, loss_ce: 0.014171
2021-12-11 23:46:09,531 iteration 5242 : loss : 0.029102, loss_ce: 0.015128
2021-12-11 23:46:12,456 iteration 5243 : loss : 0.054897, loss_ce: 0.031718
2021-12-11 23:46:15,351 iteration 5244 : loss : 0.032668, loss_ce: 0.014593
2021-12-11 23:46:18,252 iteration 5245 : loss : 0.038230, loss_ce: 0.015186
2021-12-11 23:46:21,285 iteration 5246 : loss : 0.037520, loss_ce: 0.016388
2021-12-11 23:46:24,071 iteration 5247 : loss : 0.031428, loss_ce: 0.013388
2021-12-11 23:46:26,907 iteration 5248 : loss : 0.043699, loss_ce: 0.023086
2021-12-11 23:46:29,690 iteration 5249 : loss : 0.022487, loss_ce: 0.010851
2021-12-11 23:46:32,549 iteration 5250 : loss : 0.028211, loss_ce: 0.012693
2021-12-11 23:46:35,558 iteration 5251 : loss : 0.032169, loss_ce: 0.015218
2021-12-11 23:46:38,211 iteration 5252 : loss : 0.032869, loss_ce: 0.018075
2021-12-11 23:46:41,032 iteration 5253 : loss : 0.027386, loss_ce: 0.012507
 77%|████████████████████▊      | 309/400 [4:23:39<1:15:21, 49.69s/it]2021-12-11 23:46:43,991 iteration 5254 : loss : 0.026394, loss_ce: 0.014073
2021-12-11 23:46:46,872 iteration 5255 : loss : 0.034696, loss_ce: 0.013936
2021-12-11 23:46:49,637 iteration 5256 : loss : 0.035440, loss_ce: 0.016503
2021-12-11 23:46:52,373 iteration 5257 : loss : 0.042280, loss_ce: 0.017486
2021-12-11 23:46:55,096 iteration 5258 : loss : 0.029741, loss_ce: 0.014784
2021-12-11 23:46:57,959 iteration 5259 : loss : 0.035887, loss_ce: 0.016295
2021-12-11 23:47:00,735 iteration 5260 : loss : 0.028363, loss_ce: 0.011870
2021-12-11 23:47:03,496 iteration 5261 : loss : 0.027438, loss_ce: 0.013792
2021-12-11 23:47:06,143 iteration 5262 : loss : 0.035800, loss_ce: 0.017793
2021-12-11 23:47:08,929 iteration 5263 : loss : 0.032584, loss_ce: 0.015039
2021-12-11 23:47:11,798 iteration 5264 : loss : 0.041264, loss_ce: 0.018226
2021-12-11 23:47:14,675 iteration 5265 : loss : 0.047687, loss_ce: 0.020721
2021-12-11 23:47:17,416 iteration 5266 : loss : 0.030647, loss_ce: 0.013403
2021-12-11 23:47:20,429 iteration 5267 : loss : 0.048228, loss_ce: 0.024495
2021-12-11 23:47:23,068 iteration 5268 : loss : 0.022696, loss_ce: 0.009628
2021-12-11 23:47:25,808 iteration 5269 : loss : 0.025802, loss_ce: 0.012256
2021-12-11 23:47:25,809 Training Data Eval:
2021-12-11 23:47:40,922   Average segmentation loss on training set: 0.0155
2021-12-11 23:47:40,923 Validation Data Eval:
2021-12-11 23:47:46,123   Average segmentation loss on validation set: 0.0791
2021-12-11 23:47:48,862 iteration 5270 : loss : 0.044825, loss_ce: 0.011716
 78%|████████████████████▉      | 310/400 [4:24:47<1:22:41, 55.13s/it]2021-12-11 23:47:51,701 iteration 5271 : loss : 0.046980, loss_ce: 0.017197
2021-12-11 23:47:54,630 iteration 5272 : loss : 0.048800, loss_ce: 0.022180
2021-12-11 23:47:57,286 iteration 5273 : loss : 0.035090, loss_ce: 0.017199
2021-12-11 23:48:00,075 iteration 5274 : loss : 0.030264, loss_ce: 0.014875
2021-12-11 23:48:02,828 iteration 5275 : loss : 0.028357, loss_ce: 0.013901
2021-12-11 23:48:05,685 iteration 5276 : loss : 0.040783, loss_ce: 0.016816
2021-12-11 23:48:08,705 iteration 5277 : loss : 0.043996, loss_ce: 0.021305
2021-12-11 23:48:11,480 iteration 5278 : loss : 0.034718, loss_ce: 0.012327
2021-12-11 23:48:14,421 iteration 5279 : loss : 0.034461, loss_ce: 0.015079
2021-12-11 23:48:17,127 iteration 5280 : loss : 0.028574, loss_ce: 0.011964
2021-12-11 23:48:19,883 iteration 5281 : loss : 0.030795, loss_ce: 0.016450
2021-12-11 23:48:22,740 iteration 5282 : loss : 0.028752, loss_ce: 0.013786
2021-12-11 23:48:25,437 iteration 5283 : loss : 0.030101, loss_ce: 0.015468
2021-12-11 23:48:28,464 iteration 5284 : loss : 0.022616, loss_ce: 0.011368
2021-12-11 23:48:31,345 iteration 5285 : loss : 0.030946, loss_ce: 0.013865
2021-12-11 23:48:34,145 iteration 5286 : loss : 0.031545, loss_ce: 0.017218
2021-12-11 23:48:36,890 iteration 5287 : loss : 0.025288, loss_ce: 0.011966
 78%|████████████████████▉      | 311/400 [4:25:35<1:18:37, 53.00s/it]2021-12-11 23:48:39,734 iteration 5288 : loss : 0.031386, loss_ce: 0.016576
2021-12-11 23:48:42,605 iteration 5289 : loss : 0.038133, loss_ce: 0.016533
2021-12-11 23:48:45,444 iteration 5290 : loss : 0.032931, loss_ce: 0.012610
2021-12-11 23:48:48,216 iteration 5291 : loss : 0.033626, loss_ce: 0.014276
2021-12-11 23:48:51,122 iteration 5292 : loss : 0.030454, loss_ce: 0.010709
2021-12-11 23:48:53,880 iteration 5293 : loss : 0.032012, loss_ce: 0.014364
2021-12-11 23:48:56,728 iteration 5294 : loss : 0.051241, loss_ce: 0.017720
2021-12-11 23:48:59,479 iteration 5295 : loss : 0.031063, loss_ce: 0.012431
2021-12-11 23:49:02,176 iteration 5296 : loss : 0.037023, loss_ce: 0.012380
2021-12-11 23:49:05,011 iteration 5297 : loss : 0.033160, loss_ce: 0.012978
2021-12-11 23:49:07,822 iteration 5298 : loss : 0.026918, loss_ce: 0.014693
2021-12-11 23:49:10,583 iteration 5299 : loss : 0.036811, loss_ce: 0.012062
2021-12-11 23:49:13,453 iteration 5300 : loss : 0.023403, loss_ce: 0.011377
2021-12-11 23:49:16,200 iteration 5301 : loss : 0.029109, loss_ce: 0.010390
2021-12-11 23:49:19,023 iteration 5302 : loss : 0.031887, loss_ce: 0.015167
2021-12-11 23:49:21,853 iteration 5303 : loss : 0.028092, loss_ce: 0.015144
2021-12-11 23:49:24,592 iteration 5304 : loss : 0.038535, loss_ce: 0.022222
 78%|█████████████████████      | 312/400 [4:26:22<1:15:24, 51.41s/it]2021-12-11 23:49:27,366 iteration 5305 : loss : 0.019309, loss_ce: 0.009870
2021-12-11 23:49:30,113 iteration 5306 : loss : 0.034852, loss_ce: 0.016498
2021-12-11 23:49:32,995 iteration 5307 : loss : 0.032614, loss_ce: 0.014589
2021-12-11 23:49:35,620 iteration 5308 : loss : 0.022593, loss_ce: 0.010526
2021-12-11 23:49:38,513 iteration 5309 : loss : 0.033646, loss_ce: 0.018607
2021-12-11 23:49:41,520 iteration 5310 : loss : 0.031234, loss_ce: 0.013236
2021-12-11 23:49:44,438 iteration 5311 : loss : 0.033258, loss_ce: 0.013885
2021-12-11 23:49:47,280 iteration 5312 : loss : 0.038162, loss_ce: 0.017503
2021-12-11 23:49:49,953 iteration 5313 : loss : 0.029930, loss_ce: 0.016941
2021-12-11 23:49:52,670 iteration 5314 : loss : 0.034535, loss_ce: 0.014075
2021-12-11 23:49:55,334 iteration 5315 : loss : 0.026074, loss_ce: 0.011782
2021-12-11 23:49:58,099 iteration 5316 : loss : 0.026898, loss_ce: 0.010302
2021-12-11 23:50:01,050 iteration 5317 : loss : 0.025766, loss_ce: 0.010912
2021-12-11 23:50:03,863 iteration 5318 : loss : 0.027099, loss_ce: 0.012973
2021-12-11 23:50:06,733 iteration 5319 : loss : 0.056307, loss_ce: 0.020795
2021-12-11 23:50:09,388 iteration 5320 : loss : 0.025320, loss_ce: 0.011003
2021-12-11 23:50:12,134 iteration 5321 : loss : 0.030611, loss_ce: 0.014507
 78%|█████████████████████▏     | 313/400 [4:27:10<1:12:51, 50.25s/it]2021-12-11 23:50:14,851 iteration 5322 : loss : 0.026693, loss_ce: 0.011848
2021-12-11 23:50:17,785 iteration 5323 : loss : 0.037406, loss_ce: 0.013464
2021-12-11 23:50:20,418 iteration 5324 : loss : 0.032283, loss_ce: 0.013121
2021-12-11 23:50:23,375 iteration 5325 : loss : 0.029847, loss_ce: 0.014350
2021-12-11 23:50:26,201 iteration 5326 : loss : 0.034099, loss_ce: 0.016153
2021-12-11 23:50:28,897 iteration 5327 : loss : 0.032423, loss_ce: 0.014936
2021-12-11 23:50:31,686 iteration 5328 : loss : 0.026364, loss_ce: 0.011982
2021-12-11 23:50:34,622 iteration 5329 : loss : 0.033928, loss_ce: 0.018418
2021-12-11 23:50:37,439 iteration 5330 : loss : 0.027143, loss_ce: 0.012583
2021-12-11 23:50:40,063 iteration 5331 : loss : 0.035088, loss_ce: 0.014836
2021-12-11 23:50:42,767 iteration 5332 : loss : 0.031646, loss_ce: 0.014571
2021-12-11 23:50:45,496 iteration 5333 : loss : 0.037428, loss_ce: 0.014541
2021-12-11 23:50:48,313 iteration 5334 : loss : 0.029659, loss_ce: 0.012160
2021-12-11 23:50:51,041 iteration 5335 : loss : 0.067689, loss_ce: 0.031030
2021-12-11 23:50:53,776 iteration 5336 : loss : 0.029572, loss_ce: 0.013696
2021-12-11 23:50:56,541 iteration 5337 : loss : 0.030186, loss_ce: 0.015024
2021-12-11 23:50:59,261 iteration 5338 : loss : 0.031945, loss_ce: 0.013222
 78%|█████████████████████▏     | 314/400 [4:27:57<1:10:40, 49.31s/it]2021-12-11 23:51:02,046 iteration 5339 : loss : 0.029032, loss_ce: 0.012751
2021-12-11 23:51:04,840 iteration 5340 : loss : 0.030983, loss_ce: 0.019861
2021-12-11 23:51:07,762 iteration 5341 : loss : 0.030756, loss_ce: 0.014778
2021-12-11 23:51:10,670 iteration 5342 : loss : 0.038311, loss_ce: 0.014251
2021-12-11 23:51:13,339 iteration 5343 : loss : 0.042722, loss_ce: 0.017282
2021-12-11 23:51:15,982 iteration 5344 : loss : 0.028993, loss_ce: 0.015449
2021-12-11 23:51:18,822 iteration 5345 : loss : 0.029436, loss_ce: 0.011616
2021-12-11 23:51:21,542 iteration 5346 : loss : 0.034941, loss_ce: 0.014417
2021-12-11 23:51:24,401 iteration 5347 : loss : 0.033120, loss_ce: 0.016457
2021-12-11 23:51:27,188 iteration 5348 : loss : 0.028137, loss_ce: 0.013753
2021-12-11 23:51:29,914 iteration 5349 : loss : 0.024895, loss_ce: 0.013875
2021-12-11 23:51:32,550 iteration 5350 : loss : 0.022251, loss_ce: 0.011093
2021-12-11 23:51:35,395 iteration 5351 : loss : 0.048076, loss_ce: 0.013259
2021-12-11 23:51:38,249 iteration 5352 : loss : 0.029245, loss_ce: 0.012276
2021-12-11 23:51:41,136 iteration 5353 : loss : 0.034786, loss_ce: 0.016204
2021-12-11 23:51:43,798 iteration 5354 : loss : 0.026376, loss_ce: 0.013760
2021-12-11 23:51:43,799 Training Data Eval:
2021-12-11 23:51:58,645   Average segmentation loss on training set: 0.0150
2021-12-11 23:51:58,645 Validation Data Eval:
2021-12-11 23:52:03,781   Average segmentation loss on validation set: 0.0920
2021-12-11 23:52:06,564 iteration 5355 : loss : 0.026285, loss_ce: 0.012155
 79%|█████████████████████▎     | 315/400 [4:29:04<1:17:30, 54.71s/it]2021-12-11 23:52:09,585 iteration 5356 : loss : 0.038891, loss_ce: 0.017530
2021-12-11 23:52:12,228 iteration 5357 : loss : 0.020576, loss_ce: 0.009758
2021-12-11 23:52:14,939 iteration 5358 : loss : 0.031372, loss_ce: 0.010746
2021-12-11 23:52:17,566 iteration 5359 : loss : 0.037421, loss_ce: 0.021969
2021-12-11 23:52:20,320 iteration 5360 : loss : 0.025979, loss_ce: 0.012775
2021-12-11 23:52:23,170 iteration 5361 : loss : 0.046475, loss_ce: 0.016669
2021-12-11 23:52:25,972 iteration 5362 : loss : 0.053363, loss_ce: 0.032807
2021-12-11 23:52:28,825 iteration 5363 : loss : 0.027296, loss_ce: 0.013727
2021-12-11 23:52:31,602 iteration 5364 : loss : 0.040874, loss_ce: 0.011018
2021-12-11 23:52:34,290 iteration 5365 : loss : 0.031888, loss_ce: 0.013833
2021-12-11 23:52:37,070 iteration 5366 : loss : 0.036252, loss_ce: 0.015016
2021-12-11 23:52:39,999 iteration 5367 : loss : 0.050697, loss_ce: 0.015309
2021-12-11 23:52:42,764 iteration 5368 : loss : 0.032119, loss_ce: 0.014612
2021-12-11 23:52:45,669 iteration 5369 : loss : 0.038953, loss_ce: 0.018984
2021-12-11 23:52:48,468 iteration 5370 : loss : 0.044211, loss_ce: 0.015637
2021-12-11 23:52:51,376 iteration 5371 : loss : 0.027146, loss_ce: 0.010968
2021-12-11 23:52:54,035 iteration 5372 : loss : 0.033883, loss_ce: 0.015767
 79%|█████████████████████▎     | 316/400 [4:29:52<1:13:33, 52.54s/it]2021-12-11 23:52:56,820 iteration 5373 : loss : 0.023184, loss_ce: 0.012790
2021-12-11 23:52:59,782 iteration 5374 : loss : 0.036904, loss_ce: 0.013050
2021-12-11 23:53:02,405 iteration 5375 : loss : 0.028402, loss_ce: 0.012265
2021-12-11 23:53:05,066 iteration 5376 : loss : 0.026252, loss_ce: 0.011875
2021-12-11 23:53:07,831 iteration 5377 : loss : 0.044130, loss_ce: 0.013047
2021-12-11 23:53:10,503 iteration 5378 : loss : 0.021293, loss_ce: 0.009956
2021-12-11 23:53:13,175 iteration 5379 : loss : 0.035439, loss_ce: 0.014396
2021-12-11 23:53:15,853 iteration 5380 : loss : 0.024187, loss_ce: 0.012212
2021-12-11 23:53:18,499 iteration 5381 : loss : 0.026912, loss_ce: 0.014231
2021-12-11 23:53:21,436 iteration 5382 : loss : 0.037569, loss_ce: 0.014685
2021-12-11 23:53:24,402 iteration 5383 : loss : 0.027590, loss_ce: 0.013773
2021-12-11 23:53:27,176 iteration 5384 : loss : 0.033638, loss_ce: 0.017641
2021-12-11 23:53:29,865 iteration 5385 : loss : 0.040036, loss_ce: 0.017996
2021-12-11 23:53:32,819 iteration 5386 : loss : 0.024999, loss_ce: 0.011813
2021-12-11 23:53:35,577 iteration 5387 : loss : 0.031650, loss_ce: 0.015814
2021-12-11 23:53:38,254 iteration 5388 : loss : 0.023003, loss_ce: 0.011329
2021-12-11 23:53:40,954 iteration 5389 : loss : 0.026330, loss_ce: 0.011857
 79%|█████████████████████▍     | 317/400 [4:30:39<1:10:20, 50.85s/it]2021-12-11 23:53:44,015 iteration 5390 : loss : 0.041926, loss_ce: 0.015359
2021-12-11 23:53:46,628 iteration 5391 : loss : 0.034564, loss_ce: 0.014263
2021-12-11 23:53:49,324 iteration 5392 : loss : 0.028604, loss_ce: 0.015101
2021-12-11 23:53:52,246 iteration 5393 : loss : 0.053796, loss_ce: 0.018802
2021-12-11 23:53:54,886 iteration 5394 : loss : 0.024186, loss_ce: 0.012540
2021-12-11 23:53:57,470 iteration 5395 : loss : 0.026415, loss_ce: 0.013352
2021-12-11 23:54:00,434 iteration 5396 : loss : 0.039539, loss_ce: 0.013170
2021-12-11 23:54:03,285 iteration 5397 : loss : 0.047800, loss_ce: 0.020718
2021-12-11 23:54:05,989 iteration 5398 : loss : 0.024159, loss_ce: 0.014287
2021-12-11 23:54:08,751 iteration 5399 : loss : 0.021362, loss_ce: 0.011564
2021-12-11 23:54:11,723 iteration 5400 : loss : 0.043441, loss_ce: 0.015977
2021-12-11 23:54:14,557 iteration 5401 : loss : 0.032894, loss_ce: 0.013075
2021-12-11 23:54:17,336 iteration 5402 : loss : 0.026135, loss_ce: 0.012062
2021-12-11 23:54:20,214 iteration 5403 : loss : 0.032583, loss_ce: 0.012186
2021-12-11 23:54:23,011 iteration 5404 : loss : 0.030027, loss_ce: 0.015523
2021-12-11 23:54:25,750 iteration 5405 : loss : 0.023913, loss_ce: 0.012150
2021-12-11 23:54:28,583 iteration 5406 : loss : 0.032035, loss_ce: 0.010949
 80%|█████████████████████▍     | 318/400 [4:31:26<1:08:10, 49.89s/it]2021-12-11 23:54:31,384 iteration 5407 : loss : 0.044145, loss_ce: 0.014261
2021-12-11 23:54:34,221 iteration 5408 : loss : 0.033640, loss_ce: 0.013804
2021-12-11 23:54:36,926 iteration 5409 : loss : 0.024319, loss_ce: 0.014545
2021-12-11 23:54:39,777 iteration 5410 : loss : 0.028690, loss_ce: 0.012346
2021-12-11 23:54:42,473 iteration 5411 : loss : 0.027873, loss_ce: 0.012317
2021-12-11 23:54:45,317 iteration 5412 : loss : 0.042018, loss_ce: 0.021249
2021-12-11 23:54:48,169 iteration 5413 : loss : 0.027830, loss_ce: 0.013152
2021-12-11 23:54:50,963 iteration 5414 : loss : 0.043367, loss_ce: 0.018207
2021-12-11 23:54:53,828 iteration 5415 : loss : 0.029169, loss_ce: 0.012874
2021-12-11 23:54:56,662 iteration 5416 : loss : 0.035141, loss_ce: 0.018382
2021-12-11 23:54:59,289 iteration 5417 : loss : 0.027756, loss_ce: 0.013174
2021-12-11 23:55:02,111 iteration 5418 : loss : 0.031081, loss_ce: 0.013182
2021-12-11 23:55:04,976 iteration 5419 : loss : 0.025594, loss_ce: 0.013774
2021-12-11 23:55:07,657 iteration 5420 : loss : 0.026576, loss_ce: 0.014247
2021-12-11 23:55:10,594 iteration 5421 : loss : 0.034009, loss_ce: 0.013251
2021-12-11 23:55:13,337 iteration 5422 : loss : 0.035658, loss_ce: 0.014975
2021-12-11 23:55:16,175 iteration 5423 : loss : 0.037435, loss_ce: 0.013201
 80%|█████████████████████▌     | 319/400 [4:32:14<1:06:24, 49.20s/it]2021-12-11 23:55:19,001 iteration 5424 : loss : 0.036985, loss_ce: 0.015632
2021-12-11 23:55:21,677 iteration 5425 : loss : 0.036658, loss_ce: 0.016774
2021-12-11 23:55:24,388 iteration 5426 : loss : 0.023946, loss_ce: 0.010800
2021-12-11 23:55:27,460 iteration 5427 : loss : 0.043108, loss_ce: 0.014860
2021-12-11 23:55:30,293 iteration 5428 : loss : 0.043492, loss_ce: 0.019902
2021-12-11 23:55:32,986 iteration 5429 : loss : 0.028724, loss_ce: 0.013026
2021-12-11 23:55:35,674 iteration 5430 : loss : 0.028751, loss_ce: 0.014102
2021-12-11 23:55:38,483 iteration 5431 : loss : 0.039560, loss_ce: 0.015413
2021-12-11 23:55:41,180 iteration 5432 : loss : 0.027886, loss_ce: 0.013063
2021-12-11 23:55:44,087 iteration 5433 : loss : 0.046800, loss_ce: 0.020843
2021-12-11 23:55:46,962 iteration 5434 : loss : 0.033789, loss_ce: 0.014413
2021-12-11 23:55:49,746 iteration 5435 : loss : 0.033222, loss_ce: 0.015880
2021-12-11 23:55:52,696 iteration 5436 : loss : 0.032795, loss_ce: 0.017522
2021-12-11 23:55:55,507 iteration 5437 : loss : 0.048439, loss_ce: 0.015699
2021-12-11 23:55:58,198 iteration 5438 : loss : 0.046504, loss_ce: 0.019539
2021-12-11 23:56:01,052 iteration 5439 : loss : 0.042240, loss_ce: 0.015951
2021-12-11 23:56:01,052 Training Data Eval:
2021-12-11 23:56:16,049   Average segmentation loss on training set: 0.0159
2021-12-11 23:56:16,049 Validation Data Eval:
2021-12-11 23:56:21,192   Average segmentation loss on validation set: 0.0901
2021-12-11 23:56:23,947 iteration 5440 : loss : 0.032318, loss_ce: 0.014617
 80%|█████████████████████▌     | 320/400 [4:33:22<1:13:01, 54.77s/it]2021-12-11 23:56:26,853 iteration 5441 : loss : 0.032169, loss_ce: 0.015491
2021-12-11 23:56:29,749 iteration 5442 : loss : 0.044046, loss_ce: 0.020255
2021-12-11 23:56:32,580 iteration 5443 : loss : 0.045338, loss_ce: 0.019690
2021-12-11 23:56:35,271 iteration 5444 : loss : 0.027995, loss_ce: 0.015473
2021-12-11 23:56:38,060 iteration 5445 : loss : 0.033746, loss_ce: 0.019997
2021-12-11 23:56:40,739 iteration 5446 : loss : 0.038814, loss_ce: 0.015993
2021-12-11 23:56:43,388 iteration 5447 : loss : 0.047640, loss_ce: 0.012241
2021-12-11 23:56:46,296 iteration 5448 : loss : 0.030632, loss_ce: 0.013275
2021-12-11 23:56:48,939 iteration 5449 : loss : 0.034477, loss_ce: 0.011561
2021-12-11 23:56:51,632 iteration 5450 : loss : 0.029287, loss_ce: 0.011774
2021-12-11 23:56:54,437 iteration 5451 : loss : 0.026204, loss_ce: 0.010952
2021-12-11 23:56:57,099 iteration 5452 : loss : 0.026758, loss_ce: 0.013289
2021-12-11 23:57:00,040 iteration 5453 : loss : 0.028372, loss_ce: 0.012971
2021-12-11 23:57:02,851 iteration 5454 : loss : 0.049360, loss_ce: 0.025052
2021-12-11 23:57:05,602 iteration 5455 : loss : 0.046111, loss_ce: 0.026217
2021-12-11 23:57:08,354 iteration 5456 : loss : 0.030761, loss_ce: 0.011873
2021-12-11 23:57:11,149 iteration 5457 : loss : 0.028197, loss_ce: 0.013136
 80%|█████████████████████▋     | 321/400 [4:34:09<1:09:07, 52.50s/it]2021-12-11 23:57:14,179 iteration 5458 : loss : 0.044931, loss_ce: 0.014952
2021-12-11 23:57:16,785 iteration 5459 : loss : 0.036731, loss_ce: 0.017914
2021-12-11 23:57:19,689 iteration 5460 : loss : 0.033431, loss_ce: 0.016222
2021-12-11 23:57:22,543 iteration 5461 : loss : 0.032751, loss_ce: 0.014174
2021-12-11 23:57:25,344 iteration 5462 : loss : 0.028568, loss_ce: 0.014148
2021-12-11 23:57:28,138 iteration 5463 : loss : 0.032508, loss_ce: 0.012537
2021-12-11 23:57:31,053 iteration 5464 : loss : 0.046336, loss_ce: 0.018321
2021-12-11 23:57:33,792 iteration 5465 : loss : 0.028995, loss_ce: 0.013585
2021-12-11 23:57:36,524 iteration 5466 : loss : 0.024823, loss_ce: 0.012561
2021-12-11 23:57:39,533 iteration 5467 : loss : 0.030168, loss_ce: 0.012592
2021-12-11 23:57:42,256 iteration 5468 : loss : 0.024544, loss_ce: 0.013277
2021-12-11 23:57:45,129 iteration 5469 : loss : 0.046157, loss_ce: 0.021316
2021-12-11 23:57:47,871 iteration 5470 : loss : 0.027057, loss_ce: 0.015420
2021-12-11 23:57:50,628 iteration 5471 : loss : 0.029866, loss_ce: 0.016396
2021-12-11 23:57:53,543 iteration 5472 : loss : 0.039429, loss_ce: 0.018377
2021-12-11 23:57:56,294 iteration 5473 : loss : 0.034330, loss_ce: 0.012299
2021-12-11 23:57:59,058 iteration 5474 : loss : 0.024903, loss_ce: 0.010525
 80%|█████████████████████▋     | 322/400 [4:34:57<1:06:27, 51.12s/it]2021-12-11 23:58:01,895 iteration 5475 : loss : 0.027241, loss_ce: 0.012797
2021-12-11 23:58:04,544 iteration 5476 : loss : 0.032422, loss_ce: 0.016441
2021-12-11 23:58:07,260 iteration 5477 : loss : 0.025663, loss_ce: 0.011340
2021-12-11 23:58:10,130 iteration 5478 : loss : 0.033215, loss_ce: 0.016767
2021-12-11 23:58:12,842 iteration 5479 : loss : 0.050590, loss_ce: 0.016575
2021-12-11 23:58:15,449 iteration 5480 : loss : 0.043475, loss_ce: 0.014197
2021-12-11 23:58:18,245 iteration 5481 : loss : 0.038628, loss_ce: 0.018128
2021-12-11 23:58:21,175 iteration 5482 : loss : 0.029190, loss_ce: 0.013020
2021-12-11 23:58:23,957 iteration 5483 : loss : 0.028555, loss_ce: 0.013227
2021-12-11 23:58:26,627 iteration 5484 : loss : 0.027938, loss_ce: 0.013987
2021-12-11 23:58:29,308 iteration 5485 : loss : 0.033479, loss_ce: 0.017725
2021-12-11 23:58:32,181 iteration 5486 : loss : 0.039945, loss_ce: 0.014415
2021-12-11 23:58:34,768 iteration 5487 : loss : 0.037133, loss_ce: 0.023465
2021-12-11 23:58:37,717 iteration 5488 : loss : 0.042678, loss_ce: 0.015383
2021-12-11 23:58:40,574 iteration 5489 : loss : 0.033165, loss_ce: 0.011774
2021-12-11 23:58:43,269 iteration 5490 : loss : 0.038883, loss_ce: 0.018434
2021-12-11 23:58:46,070 iteration 5491 : loss : 0.030499, loss_ce: 0.013144
 81%|█████████████████████▊     | 323/400 [4:35:44<1:04:01, 49.89s/it]2021-12-11 23:58:48,932 iteration 5492 : loss : 0.041224, loss_ce: 0.021691
2021-12-11 23:58:51,497 iteration 5493 : loss : 0.027180, loss_ce: 0.011822
2021-12-11 23:58:54,110 iteration 5494 : loss : 0.025878, loss_ce: 0.014485
2021-12-11 23:58:56,953 iteration 5495 : loss : 0.044926, loss_ce: 0.014924
2021-12-11 23:58:59,683 iteration 5496 : loss : 0.022721, loss_ce: 0.010869
2021-12-11 23:59:02,505 iteration 5497 : loss : 0.039262, loss_ce: 0.019715
2021-12-11 23:59:05,298 iteration 5498 : loss : 0.027371, loss_ce: 0.014097
2021-12-11 23:59:08,174 iteration 5499 : loss : 0.039225, loss_ce: 0.013235
2021-12-11 23:59:10,965 iteration 5500 : loss : 0.022087, loss_ce: 0.010047
2021-12-11 23:59:13,576 iteration 5501 : loss : 0.026066, loss_ce: 0.012216
2021-12-11 23:59:16,276 iteration 5502 : loss : 0.025050, loss_ce: 0.014231
2021-12-11 23:59:19,323 iteration 5503 : loss : 0.029186, loss_ce: 0.011320
2021-12-11 23:59:22,162 iteration 5504 : loss : 0.028462, loss_ce: 0.013808
2021-12-11 23:59:24,839 iteration 5505 : loss : 0.027801, loss_ce: 0.012984
2021-12-11 23:59:27,701 iteration 5506 : loss : 0.024609, loss_ce: 0.012591
2021-12-11 23:59:30,532 iteration 5507 : loss : 0.033189, loss_ce: 0.013587
2021-12-11 23:59:33,464 iteration 5508 : loss : 0.041314, loss_ce: 0.019185
 81%|█████████████████████▊     | 324/400 [4:36:31<1:02:14, 49.14s/it]2021-12-11 23:59:36,292 iteration 5509 : loss : 0.047589, loss_ce: 0.022911
2021-12-11 23:59:38,919 iteration 5510 : loss : 0.025472, loss_ce: 0.012315
2021-12-11 23:59:41,850 iteration 5511 : loss : 0.038410, loss_ce: 0.016501
2021-12-11 23:59:44,729 iteration 5512 : loss : 0.035282, loss_ce: 0.016721
2021-12-11 23:59:47,361 iteration 5513 : loss : 0.027687, loss_ce: 0.018133
2021-12-11 23:59:50,235 iteration 5514 : loss : 0.036243, loss_ce: 0.016547
2021-12-11 23:59:53,005 iteration 5515 : loss : 0.026120, loss_ce: 0.012479
2021-12-11 23:59:55,872 iteration 5516 : loss : 0.024580, loss_ce: 0.014629
2021-12-11 23:59:58,695 iteration 5517 : loss : 0.037603, loss_ce: 0.015901
2021-12-12 00:00:01,489 iteration 5518 : loss : 0.025651, loss_ce: 0.012724
2021-12-12 00:00:04,378 iteration 5519 : loss : 0.066627, loss_ce: 0.022463
2021-12-12 00:00:07,264 iteration 5520 : loss : 0.041380, loss_ce: 0.012345
2021-12-12 00:00:10,100 iteration 5521 : loss : 0.031014, loss_ce: 0.013060
2021-12-12 00:00:12,844 iteration 5522 : loss : 0.025045, loss_ce: 0.011843
2021-12-12 00:00:15,608 iteration 5523 : loss : 0.032363, loss_ce: 0.016207
2021-12-12 00:00:18,439 iteration 5524 : loss : 0.035978, loss_ce: 0.016960
2021-12-12 00:00:18,439 Training Data Eval:
2021-12-12 00:00:33,093   Average segmentation loss on training set: 0.0158
2021-12-12 00:00:33,093 Validation Data Eval:
2021-12-12 00:00:38,234   Average segmentation loss on validation set: 0.0996
2021-12-12 00:00:41,035 iteration 5525 : loss : 0.053828, loss_ce: 0.028410
 81%|█████████████████████▉     | 325/400 [4:37:39<1:08:20, 54.67s/it]2021-12-12 00:00:43,893 iteration 5526 : loss : 0.035920, loss_ce: 0.014341
2021-12-12 00:00:46,620 iteration 5527 : loss : 0.026746, loss_ce: 0.013234
2021-12-12 00:00:49,529 iteration 5528 : loss : 0.044663, loss_ce: 0.016643
2021-12-12 00:00:52,283 iteration 5529 : loss : 0.028976, loss_ce: 0.013515
2021-12-12 00:00:55,018 iteration 5530 : loss : 0.027378, loss_ce: 0.014239
2021-12-12 00:00:57,635 iteration 5531 : loss : 0.020556, loss_ce: 0.009568
2021-12-12 00:01:00,313 iteration 5532 : loss : 0.028394, loss_ce: 0.012953
2021-12-12 00:01:03,073 iteration 5533 : loss : 0.028640, loss_ce: 0.015494
2021-12-12 00:01:05,758 iteration 5534 : loss : 0.020882, loss_ce: 0.010328
2021-12-12 00:01:08,635 iteration 5535 : loss : 0.025366, loss_ce: 0.012729
2021-12-12 00:01:11,306 iteration 5536 : loss : 0.029036, loss_ce: 0.011553
2021-12-12 00:01:14,012 iteration 5537 : loss : 0.034888, loss_ce: 0.017233
2021-12-12 00:01:17,023 iteration 5538 : loss : 0.047490, loss_ce: 0.017034
2021-12-12 00:01:19,818 iteration 5539 : loss : 0.026869, loss_ce: 0.012007
2021-12-12 00:01:22,570 iteration 5540 : loss : 0.027476, loss_ce: 0.014091
2021-12-12 00:01:25,433 iteration 5541 : loss : 0.039831, loss_ce: 0.020337
2021-12-12 00:01:28,152 iteration 5542 : loss : 0.027747, loss_ce: 0.014256
 82%|██████████████████████     | 326/400 [4:38:26<1:04:37, 52.40s/it]2021-12-12 00:01:31,163 iteration 5543 : loss : 0.027040, loss_ce: 0.013837
2021-12-12 00:01:33,769 iteration 5544 : loss : 0.026076, loss_ce: 0.011686
2021-12-12 00:01:36,709 iteration 5545 : loss : 0.024160, loss_ce: 0.013835
2021-12-12 00:01:39,581 iteration 5546 : loss : 0.064083, loss_ce: 0.019525
2021-12-12 00:01:42,377 iteration 5547 : loss : 0.023329, loss_ce: 0.011642
2021-12-12 00:01:45,085 iteration 5548 : loss : 0.033123, loss_ce: 0.011897
2021-12-12 00:01:47,978 iteration 5549 : loss : 0.023977, loss_ce: 0.008806
2021-12-12 00:01:50,735 iteration 5550 : loss : 0.031348, loss_ce: 0.014930
2021-12-12 00:01:53,488 iteration 5551 : loss : 0.034433, loss_ce: 0.016926
2021-12-12 00:01:56,224 iteration 5552 : loss : 0.028912, loss_ce: 0.010689
2021-12-12 00:01:59,034 iteration 5553 : loss : 0.033373, loss_ce: 0.016142
2021-12-12 00:02:01,750 iteration 5554 : loss : 0.029306, loss_ce: 0.014803
2021-12-12 00:02:04,556 iteration 5555 : loss : 0.025680, loss_ce: 0.013258
2021-12-12 00:02:07,342 iteration 5556 : loss : 0.036602, loss_ce: 0.015284
2021-12-12 00:02:10,117 iteration 5557 : loss : 0.024456, loss_ce: 0.012598
2021-12-12 00:02:12,808 iteration 5558 : loss : 0.039972, loss_ce: 0.016519
2021-12-12 00:02:15,478 iteration 5559 : loss : 0.035951, loss_ce: 0.012815
 82%|██████████████████████     | 327/400 [4:39:13<1:01:54, 50.88s/it]2021-12-12 00:02:18,282 iteration 5560 : loss : 0.035946, loss_ce: 0.015603
2021-12-12 00:02:21,144 iteration 5561 : loss : 0.033185, loss_ce: 0.015585
2021-12-12 00:02:23,837 iteration 5562 : loss : 0.030557, loss_ce: 0.015546
2021-12-12 00:02:26,700 iteration 5563 : loss : 0.030149, loss_ce: 0.014482
2021-12-12 00:02:29,366 iteration 5564 : loss : 0.044096, loss_ce: 0.021655
2021-12-12 00:02:32,186 iteration 5565 : loss : 0.043285, loss_ce: 0.019219
2021-12-12 00:02:35,120 iteration 5566 : loss : 0.026382, loss_ce: 0.014039
2021-12-12 00:02:37,970 iteration 5567 : loss : 0.036209, loss_ce: 0.018571
2021-12-12 00:02:40,812 iteration 5568 : loss : 0.042487, loss_ce: 0.016787
2021-12-12 00:02:43,515 iteration 5569 : loss : 0.031889, loss_ce: 0.013127
2021-12-12 00:02:46,264 iteration 5570 : loss : 0.026022, loss_ce: 0.010566
2021-12-12 00:02:48,976 iteration 5571 : loss : 0.021011, loss_ce: 0.010885
2021-12-12 00:02:51,776 iteration 5572 : loss : 0.042249, loss_ce: 0.014726
2021-12-12 00:02:54,556 iteration 5573 : loss : 0.021261, loss_ce: 0.009281
2021-12-12 00:02:57,242 iteration 5574 : loss : 0.026770, loss_ce: 0.011380
2021-12-12 00:03:00,166 iteration 5575 : loss : 0.041343, loss_ce: 0.016818
2021-12-12 00:03:02,980 iteration 5576 : loss : 0.034858, loss_ce: 0.014073
 82%|███████████████████████▊     | 328/400 [4:40:01<59:50, 49.86s/it]2021-12-12 00:03:05,784 iteration 5577 : loss : 0.028830, loss_ce: 0.014684
2021-12-12 00:03:08,683 iteration 5578 : loss : 0.032161, loss_ce: 0.014373
2021-12-12 00:03:11,480 iteration 5579 : loss : 0.037512, loss_ce: 0.017831
2021-12-12 00:03:14,156 iteration 5580 : loss : 0.039949, loss_ce: 0.019146
2021-12-12 00:03:16,934 iteration 5581 : loss : 0.021820, loss_ce: 0.011200
2021-12-12 00:03:19,616 iteration 5582 : loss : 0.022026, loss_ce: 0.009100
2021-12-12 00:03:22,403 iteration 5583 : loss : 0.016901, loss_ce: 0.008880
2021-12-12 00:03:25,066 iteration 5584 : loss : 0.043776, loss_ce: 0.023076
2021-12-12 00:03:27,870 iteration 5585 : loss : 0.026659, loss_ce: 0.010744
2021-12-12 00:03:30,684 iteration 5586 : loss : 0.045130, loss_ce: 0.015691
2021-12-12 00:03:33,266 iteration 5587 : loss : 0.034324, loss_ce: 0.015517
2021-12-12 00:03:36,023 iteration 5588 : loss : 0.026776, loss_ce: 0.013117
2021-12-12 00:03:38,806 iteration 5589 : loss : 0.028946, loss_ce: 0.012180
2021-12-12 00:03:41,636 iteration 5590 : loss : 0.032837, loss_ce: 0.012512
2021-12-12 00:03:44,383 iteration 5591 : loss : 0.026560, loss_ce: 0.013918
2021-12-12 00:03:47,124 iteration 5592 : loss : 0.035902, loss_ce: 0.014574
2021-12-12 00:03:49,887 iteration 5593 : loss : 0.036170, loss_ce: 0.017346
 82%|███████████████████████▊     | 329/400 [4:40:48<57:57, 48.98s/it]2021-12-12 00:03:52,806 iteration 5594 : loss : 0.032535, loss_ce: 0.012303
2021-12-12 00:03:55,764 iteration 5595 : loss : 0.035192, loss_ce: 0.017330
2021-12-12 00:03:58,458 iteration 5596 : loss : 0.031929, loss_ce: 0.015039
2021-12-12 00:04:01,280 iteration 5597 : loss : 0.025259, loss_ce: 0.012615
2021-12-12 00:04:03,874 iteration 5598 : loss : 0.022611, loss_ce: 0.011815
2021-12-12 00:04:06,625 iteration 5599 : loss : 0.044576, loss_ce: 0.019343
2021-12-12 00:04:09,287 iteration 5600 : loss : 0.025180, loss_ce: 0.012728
2021-12-12 00:04:11,959 iteration 5601 : loss : 0.021075, loss_ce: 0.011199
2021-12-12 00:04:14,726 iteration 5602 : loss : 0.030078, loss_ce: 0.014725
2021-12-12 00:04:17,533 iteration 5603 : loss : 0.021071, loss_ce: 0.010894
2021-12-12 00:04:20,305 iteration 5604 : loss : 0.041939, loss_ce: 0.014425
2021-12-12 00:04:23,148 iteration 5605 : loss : 0.035462, loss_ce: 0.017515
2021-12-12 00:04:26,094 iteration 5606 : loss : 0.034746, loss_ce: 0.014224
2021-12-12 00:04:28,899 iteration 5607 : loss : 0.029443, loss_ce: 0.012063
2021-12-12 00:04:31,630 iteration 5608 : loss : 0.034087, loss_ce: 0.014124
2021-12-12 00:04:34,405 iteration 5609 : loss : 0.024615, loss_ce: 0.013648
2021-12-12 00:04:34,405 Training Data Eval:
2021-12-12 00:04:49,445   Average segmentation loss on training set: 0.0151
2021-12-12 00:04:49,445 Validation Data Eval:
2021-12-12 00:04:54,592   Average segmentation loss on validation set: 0.0873
2021-12-12 00:04:57,410 iteration 5610 : loss : 0.024425, loss_ce: 0.011425
 82%|██████████████████████▎    | 330/400 [4:41:55<1:03:37, 54.54s/it]2021-12-12 00:05:00,175 iteration 5611 : loss : 0.033088, loss_ce: 0.013232
2021-12-12 00:05:02,999 iteration 5612 : loss : 0.046566, loss_ce: 0.011550
2021-12-12 00:05:05,735 iteration 5613 : loss : 0.025572, loss_ce: 0.012648
2021-12-12 00:05:08,515 iteration 5614 : loss : 0.031212, loss_ce: 0.014180
2021-12-12 00:05:11,092 iteration 5615 : loss : 0.021575, loss_ce: 0.012413
2021-12-12 00:05:14,058 iteration 5616 : loss : 0.030416, loss_ce: 0.014914
2021-12-12 00:05:16,857 iteration 5617 : loss : 0.027915, loss_ce: 0.014623
2021-12-12 00:05:19,784 iteration 5618 : loss : 0.027609, loss_ce: 0.013966
2021-12-12 00:05:22,558 iteration 5619 : loss : 0.030856, loss_ce: 0.012364
2021-12-12 00:05:25,360 iteration 5620 : loss : 0.039036, loss_ce: 0.017079
2021-12-12 00:05:28,034 iteration 5621 : loss : 0.026124, loss_ce: 0.011472
2021-12-12 00:05:30,989 iteration 5622 : loss : 0.027924, loss_ce: 0.012306
2021-12-12 00:05:33,825 iteration 5623 : loss : 0.040639, loss_ce: 0.017601
2021-12-12 00:05:36,645 iteration 5624 : loss : 0.034495, loss_ce: 0.013729
2021-12-12 00:05:39,410 iteration 5625 : loss : 0.027286, loss_ce: 0.014602
2021-12-12 00:05:42,227 iteration 5626 : loss : 0.030341, loss_ce: 0.011738
2021-12-12 00:05:45,164 iteration 5627 : loss : 0.028024, loss_ce: 0.012919
 83%|██████████████████████▎    | 331/400 [4:42:43<1:00:22, 52.51s/it]2021-12-12 00:05:48,225 iteration 5628 : loss : 0.043595, loss_ce: 0.022280
2021-12-12 00:05:51,112 iteration 5629 : loss : 0.026421, loss_ce: 0.013788
2021-12-12 00:05:53,901 iteration 5630 : loss : 0.027243, loss_ce: 0.014438
2021-12-12 00:05:56,814 iteration 5631 : loss : 0.028904, loss_ce: 0.013364
2021-12-12 00:05:59,732 iteration 5632 : loss : 0.029054, loss_ce: 0.015261
2021-12-12 00:06:02,511 iteration 5633 : loss : 0.043834, loss_ce: 0.016970
2021-12-12 00:06:05,418 iteration 5634 : loss : 0.020645, loss_ce: 0.011731
2021-12-12 00:06:08,203 iteration 5635 : loss : 0.036635, loss_ce: 0.015976
2021-12-12 00:06:10,992 iteration 5636 : loss : 0.041553, loss_ce: 0.012701
2021-12-12 00:06:13,659 iteration 5637 : loss : 0.026989, loss_ce: 0.011110
2021-12-12 00:06:16,301 iteration 5638 : loss : 0.034836, loss_ce: 0.016968
2021-12-12 00:06:18,961 iteration 5639 : loss : 0.024078, loss_ce: 0.011578
2021-12-12 00:06:21,622 iteration 5640 : loss : 0.037636, loss_ce: 0.017326
2021-12-12 00:06:24,304 iteration 5641 : loss : 0.031457, loss_ce: 0.012595
2021-12-12 00:06:27,142 iteration 5642 : loss : 0.029002, loss_ce: 0.013431
2021-12-12 00:06:30,035 iteration 5643 : loss : 0.036933, loss_ce: 0.016255
2021-12-12 00:06:32,853 iteration 5644 : loss : 0.034452, loss_ce: 0.019304
 83%|████████████████████████     | 332/400 [4:43:31<57:52, 51.06s/it]2021-12-12 00:06:35,771 iteration 5645 : loss : 0.030303, loss_ce: 0.012606
2021-12-12 00:06:38,640 iteration 5646 : loss : 0.032500, loss_ce: 0.013344
2021-12-12 00:06:41,480 iteration 5647 : loss : 0.031425, loss_ce: 0.013267
2021-12-12 00:06:44,190 iteration 5648 : loss : 0.042138, loss_ce: 0.017629
2021-12-12 00:06:47,010 iteration 5649 : loss : 0.022061, loss_ce: 0.011104
2021-12-12 00:06:49,919 iteration 5650 : loss : 0.036158, loss_ce: 0.014974
2021-12-12 00:06:52,649 iteration 5651 : loss : 0.030624, loss_ce: 0.015868
2021-12-12 00:06:55,317 iteration 5652 : loss : 0.023634, loss_ce: 0.012147
2021-12-12 00:06:58,045 iteration 5653 : loss : 0.027145, loss_ce: 0.012388
2021-12-12 00:07:00,905 iteration 5654 : loss : 0.032193, loss_ce: 0.014038
2021-12-12 00:07:03,673 iteration 5655 : loss : 0.039010, loss_ce: 0.015208
2021-12-12 00:07:06,574 iteration 5656 : loss : 0.045521, loss_ce: 0.022768
2021-12-12 00:07:09,384 iteration 5657 : loss : 0.043258, loss_ce: 0.023001
2021-12-12 00:07:12,134 iteration 5658 : loss : 0.046279, loss_ce: 0.016766
2021-12-12 00:07:14,988 iteration 5659 : loss : 0.034278, loss_ce: 0.016597
2021-12-12 00:07:17,566 iteration 5660 : loss : 0.019729, loss_ce: 0.010422
2021-12-12 00:07:20,404 iteration 5661 : loss : 0.040054, loss_ce: 0.018424
 83%|████████████████████████▏    | 333/400 [4:44:18<55:50, 50.01s/it]2021-12-12 00:07:23,294 iteration 5662 : loss : 0.023916, loss_ce: 0.012936
2021-12-12 00:07:25,986 iteration 5663 : loss : 0.029651, loss_ce: 0.015536
2021-12-12 00:07:28,838 iteration 5664 : loss : 0.031300, loss_ce: 0.014270
2021-12-12 00:07:31,494 iteration 5665 : loss : 0.021995, loss_ce: 0.008358
2021-12-12 00:07:34,442 iteration 5666 : loss : 0.025231, loss_ce: 0.012649
2021-12-12 00:07:37,320 iteration 5667 : loss : 0.040764, loss_ce: 0.017361
2021-12-12 00:07:40,267 iteration 5668 : loss : 0.031667, loss_ce: 0.013882
2021-12-12 00:07:43,111 iteration 5669 : loss : 0.032985, loss_ce: 0.019794
2021-12-12 00:07:45,958 iteration 5670 : loss : 0.026806, loss_ce: 0.011752
2021-12-12 00:07:48,707 iteration 5671 : loss : 0.053427, loss_ce: 0.015088
2021-12-12 00:07:51,491 iteration 5672 : loss : 0.024996, loss_ce: 0.009852
2021-12-12 00:07:54,280 iteration 5673 : loss : 0.031669, loss_ce: 0.018325
2021-12-12 00:07:57,001 iteration 5674 : loss : 0.039556, loss_ce: 0.016482
2021-12-12 00:07:59,947 iteration 5675 : loss : 0.036864, loss_ce: 0.016856
2021-12-12 00:08:02,707 iteration 5676 : loss : 0.034371, loss_ce: 0.019916
2021-12-12 00:08:05,421 iteration 5677 : loss : 0.026511, loss_ce: 0.012678
2021-12-12 00:08:08,239 iteration 5678 : loss : 0.034286, loss_ce: 0.017168
 84%|████████████████████████▏    | 334/400 [4:45:06<54:17, 49.36s/it]2021-12-12 00:08:11,098 iteration 5679 : loss : 0.034836, loss_ce: 0.015314
2021-12-12 00:08:13,857 iteration 5680 : loss : 0.027508, loss_ce: 0.013988
2021-12-12 00:08:16,728 iteration 5681 : loss : 0.045039, loss_ce: 0.016482
2021-12-12 00:08:19,471 iteration 5682 : loss : 0.023958, loss_ce: 0.011373
2021-12-12 00:08:22,068 iteration 5683 : loss : 0.021052, loss_ce: 0.011834
2021-12-12 00:08:24,869 iteration 5684 : loss : 0.035615, loss_ce: 0.021667
2021-12-12 00:08:27,693 iteration 5685 : loss : 0.037650, loss_ce: 0.014235
2021-12-12 00:08:30,569 iteration 5686 : loss : 0.033312, loss_ce: 0.015267
2021-12-12 00:08:33,522 iteration 5687 : loss : 0.031779, loss_ce: 0.012240
2021-12-12 00:08:36,385 iteration 5688 : loss : 0.031758, loss_ce: 0.014631
2021-12-12 00:08:39,253 iteration 5689 : loss : 0.028397, loss_ce: 0.013953
2021-12-12 00:08:41,989 iteration 5690 : loss : 0.025838, loss_ce: 0.012427
2021-12-12 00:08:44,810 iteration 5691 : loss : 0.029272, loss_ce: 0.014911
2021-12-12 00:08:47,533 iteration 5692 : loss : 0.026367, loss_ce: 0.011570
2021-12-12 00:08:50,123 iteration 5693 : loss : 0.023476, loss_ce: 0.012535
2021-12-12 00:08:52,791 iteration 5694 : loss : 0.028985, loss_ce: 0.014493
2021-12-12 00:08:52,792 Training Data Eval:
2021-12-12 00:09:07,786   Average segmentation loss on training set: 0.0146
2021-12-12 00:09:07,786 Validation Data Eval:
2021-12-12 00:09:13,050   Average segmentation loss on validation set: 0.0997
2021-12-12 00:09:15,871 iteration 5695 : loss : 0.032438, loss_ce: 0.016266
 84%|████████████████████████▎    | 335/400 [4:46:14<59:24, 54.84s/it]2021-12-12 00:09:18,703 iteration 5696 : loss : 0.027555, loss_ce: 0.011649
2021-12-12 00:09:21,413 iteration 5697 : loss : 0.026949, loss_ce: 0.011760
2021-12-12 00:09:24,172 iteration 5698 : loss : 0.034664, loss_ce: 0.014458
2021-12-12 00:09:27,145 iteration 5699 : loss : 0.033415, loss_ce: 0.012079
2021-12-12 00:09:29,741 iteration 5700 : loss : 0.023054, loss_ce: 0.012177
2021-12-12 00:09:32,641 iteration 5701 : loss : 0.071535, loss_ce: 0.021386
2021-12-12 00:09:35,494 iteration 5702 : loss : 0.029869, loss_ce: 0.014027
2021-12-12 00:09:38,254 iteration 5703 : loss : 0.028561, loss_ce: 0.015340
2021-12-12 00:09:40,999 iteration 5704 : loss : 0.029790, loss_ce: 0.014348
2021-12-12 00:09:43,801 iteration 5705 : loss : 0.022121, loss_ce: 0.011266
2021-12-12 00:09:46,568 iteration 5706 : loss : 0.024113, loss_ce: 0.011794
2021-12-12 00:09:49,519 iteration 5707 : loss : 0.038347, loss_ce: 0.012662
2021-12-12 00:09:52,321 iteration 5708 : loss : 0.028687, loss_ce: 0.017072
2021-12-12 00:09:54,946 iteration 5709 : loss : 0.025371, loss_ce: 0.010712
2021-12-12 00:09:57,853 iteration 5710 : loss : 0.031289, loss_ce: 0.018208
2021-12-12 00:10:00,552 iteration 5711 : loss : 0.023474, loss_ce: 0.012798
2021-12-12 00:10:03,191 iteration 5712 : loss : 0.033438, loss_ce: 0.016307
 84%|████████████████████████▎    | 336/400 [4:47:01<56:05, 52.59s/it]2021-12-12 00:10:06,020 iteration 5713 : loss : 0.028267, loss_ce: 0.014330
2021-12-12 00:10:08,957 iteration 5714 : loss : 0.061200, loss_ce: 0.018776
2021-12-12 00:10:11,645 iteration 5715 : loss : 0.026206, loss_ce: 0.012959
2021-12-12 00:10:14,381 iteration 5716 : loss : 0.039035, loss_ce: 0.017776
2021-12-12 00:10:17,166 iteration 5717 : loss : 0.019504, loss_ce: 0.009462
2021-12-12 00:10:19,945 iteration 5718 : loss : 0.036726, loss_ce: 0.018508
2021-12-12 00:10:22,767 iteration 5719 : loss : 0.034848, loss_ce: 0.013374
2021-12-12 00:10:25,722 iteration 5720 : loss : 0.056188, loss_ce: 0.026181
2021-12-12 00:10:28,339 iteration 5721 : loss : 0.029889, loss_ce: 0.011605
2021-12-12 00:10:31,286 iteration 5722 : loss : 0.033350, loss_ce: 0.015459
2021-12-12 00:10:34,121 iteration 5723 : loss : 0.038668, loss_ce: 0.016051
2021-12-12 00:10:36,931 iteration 5724 : loss : 0.030178, loss_ce: 0.013905
2021-12-12 00:10:39,604 iteration 5725 : loss : 0.029606, loss_ce: 0.014113
2021-12-12 00:10:42,277 iteration 5726 : loss : 0.020887, loss_ce: 0.010464
2021-12-12 00:10:45,142 iteration 5727 : loss : 0.043535, loss_ce: 0.023142
2021-12-12 00:10:48,014 iteration 5728 : loss : 0.026353, loss_ce: 0.010568
2021-12-12 00:10:50,766 iteration 5729 : loss : 0.032974, loss_ce: 0.011256
 84%|████████████████████████▍    | 337/400 [4:47:48<53:38, 51.08s/it]2021-12-12 00:10:53,616 iteration 5730 : loss : 0.038351, loss_ce: 0.016910
2021-12-12 00:10:56,483 iteration 5731 : loss : 0.039488, loss_ce: 0.017111
2021-12-12 00:10:59,126 iteration 5732 : loss : 0.057087, loss_ce: 0.015961
2021-12-12 00:11:01,844 iteration 5733 : loss : 0.032007, loss_ce: 0.014473
2021-12-12 00:11:04,593 iteration 5734 : loss : 0.021352, loss_ce: 0.010261
2021-12-12 00:11:07,255 iteration 5735 : loss : 0.057939, loss_ce: 0.022219
2021-12-12 00:11:09,981 iteration 5736 : loss : 0.031241, loss_ce: 0.015375
2021-12-12 00:11:12,633 iteration 5737 : loss : 0.032560, loss_ce: 0.014625
2021-12-12 00:11:15,376 iteration 5738 : loss : 0.031083, loss_ce: 0.018079
2021-12-12 00:11:18,339 iteration 5739 : loss : 0.029085, loss_ce: 0.012717
2021-12-12 00:11:21,230 iteration 5740 : loss : 0.034441, loss_ce: 0.018164
2021-12-12 00:11:24,025 iteration 5741 : loss : 0.021839, loss_ce: 0.012160
2021-12-12 00:11:26,577 iteration 5742 : loss : 0.058337, loss_ce: 0.011409
2021-12-12 00:11:29,409 iteration 5743 : loss : 0.041497, loss_ce: 0.017120
2021-12-12 00:11:32,175 iteration 5744 : loss : 0.025896, loss_ce: 0.013929
2021-12-12 00:11:35,127 iteration 5745 : loss : 0.024383, loss_ce: 0.011314
2021-12-12 00:11:37,839 iteration 5746 : loss : 0.037781, loss_ce: 0.019638
 84%|████████████████████████▌    | 338/400 [4:48:36<51:32, 49.88s/it]2021-12-12 00:11:40,584 iteration 5747 : loss : 0.052634, loss_ce: 0.023505
2021-12-12 00:11:43,167 iteration 5748 : loss : 0.026988, loss_ce: 0.013418
2021-12-12 00:11:46,049 iteration 5749 : loss : 0.041615, loss_ce: 0.020271
2021-12-12 00:11:48,804 iteration 5750 : loss : 0.037715, loss_ce: 0.015783
2021-12-12 00:11:51,698 iteration 5751 : loss : 0.051385, loss_ce: 0.020433
2021-12-12 00:11:54,602 iteration 5752 : loss : 0.030493, loss_ce: 0.011114
2021-12-12 00:11:57,468 iteration 5753 : loss : 0.037457, loss_ce: 0.021376
2021-12-12 00:12:00,164 iteration 5754 : loss : 0.029937, loss_ce: 0.012937
2021-12-12 00:12:02,915 iteration 5755 : loss : 0.032027, loss_ce: 0.012054
2021-12-12 00:12:05,605 iteration 5756 : loss : 0.037604, loss_ce: 0.021217
2021-12-12 00:12:08,382 iteration 5757 : loss : 0.030352, loss_ce: 0.012744
2021-12-12 00:12:11,121 iteration 5758 : loss : 0.019049, loss_ce: 0.008851
2021-12-12 00:12:14,031 iteration 5759 : loss : 0.036625, loss_ce: 0.013363
2021-12-12 00:12:16,668 iteration 5760 : loss : 0.025826, loss_ce: 0.011811
2021-12-12 00:12:19,623 iteration 5761 : loss : 0.033197, loss_ce: 0.014576
2021-12-12 00:12:22,460 iteration 5762 : loss : 0.036408, loss_ce: 0.019777
2021-12-12 00:12:25,174 iteration 5763 : loss : 0.035366, loss_ce: 0.016916
 85%|████████████████████████▌    | 339/400 [4:49:23<49:55, 49.11s/it]2021-12-12 00:12:28,200 iteration 5764 : loss : 0.031330, loss_ce: 0.016144
2021-12-12 00:12:30,909 iteration 5765 : loss : 0.031982, loss_ce: 0.012597
2021-12-12 00:12:33,590 iteration 5766 : loss : 0.029503, loss_ce: 0.013138
2021-12-12 00:12:36,437 iteration 5767 : loss : 0.030270, loss_ce: 0.015142
2021-12-12 00:12:39,232 iteration 5768 : loss : 0.050990, loss_ce: 0.018405
2021-12-12 00:12:41,933 iteration 5769 : loss : 0.025467, loss_ce: 0.010043
2021-12-12 00:12:44,723 iteration 5770 : loss : 0.037354, loss_ce: 0.017381
2021-12-12 00:12:47,412 iteration 5771 : loss : 0.024179, loss_ce: 0.012632
2021-12-12 00:12:50,310 iteration 5772 : loss : 0.024897, loss_ce: 0.011762
2021-12-12 00:12:52,974 iteration 5773 : loss : 0.021145, loss_ce: 0.010141
2021-12-12 00:12:55,642 iteration 5774 : loss : 0.024036, loss_ce: 0.012531
2021-12-12 00:12:58,548 iteration 5775 : loss : 0.040589, loss_ce: 0.016635
2021-12-12 00:13:01,284 iteration 5776 : loss : 0.027699, loss_ce: 0.012855
2021-12-12 00:13:04,101 iteration 5777 : loss : 0.024238, loss_ce: 0.011041
2021-12-12 00:13:06,935 iteration 5778 : loss : 0.024742, loss_ce: 0.012295
2021-12-12 00:13:09,727 iteration 5779 : loss : 0.024880, loss_ce: 0.012495
2021-12-12 00:13:09,727 Training Data Eval:
2021-12-12 00:13:24,610   Average segmentation loss on training set: 0.0140
2021-12-12 00:13:24,610 Validation Data Eval:
2021-12-12 00:13:29,887   Average segmentation loss on validation set: 0.0913
2021-12-12 00:13:32,511 iteration 5780 : loss : 0.032637, loss_ce: 0.015961
 85%|████████████████████████▋    | 340/400 [4:50:30<54:35, 54.58s/it]2021-12-12 00:13:35,352 iteration 5781 : loss : 0.040082, loss_ce: 0.016019
2021-12-12 00:13:38,039 iteration 5782 : loss : 0.018841, loss_ce: 0.009258
2021-12-12 00:13:40,895 iteration 5783 : loss : 0.034614, loss_ce: 0.014199
2021-12-12 00:13:43,606 iteration 5784 : loss : 0.025015, loss_ce: 0.010459
2021-12-12 00:13:46,368 iteration 5785 : loss : 0.024944, loss_ce: 0.013846
2021-12-12 00:13:49,314 iteration 5786 : loss : 0.033843, loss_ce: 0.015316
2021-12-12 00:13:52,201 iteration 5787 : loss : 0.035975, loss_ce: 0.015285
2021-12-12 00:13:55,024 iteration 5788 : loss : 0.034773, loss_ce: 0.015488
2021-12-12 00:13:57,740 iteration 5789 : loss : 0.032355, loss_ce: 0.013115
2021-12-12 00:14:00,719 iteration 5790 : loss : 0.042358, loss_ce: 0.020527
2021-12-12 00:14:03,641 iteration 5791 : loss : 0.026318, loss_ce: 0.013178
2021-12-12 00:14:06,471 iteration 5792 : loss : 0.032852, loss_ce: 0.013296
2021-12-12 00:14:09,158 iteration 5793 : loss : 0.023971, loss_ce: 0.011813
2021-12-12 00:14:11,785 iteration 5794 : loss : 0.024222, loss_ce: 0.010990
2021-12-12 00:14:14,633 iteration 5795 : loss : 0.031618, loss_ce: 0.016313
2021-12-12 00:14:17,524 iteration 5796 : loss : 0.021714, loss_ce: 0.011414
2021-12-12 00:14:20,402 iteration 5797 : loss : 0.025580, loss_ce: 0.011592
 85%|████████████████████████▋    | 341/400 [4:51:18<51:41, 52.57s/it]2021-12-12 00:14:23,054 iteration 5798 : loss : 0.034714, loss_ce: 0.013567
2021-12-12 00:14:25,786 iteration 5799 : loss : 0.024705, loss_ce: 0.012126
2021-12-12 00:14:28,374 iteration 5800 : loss : 0.018366, loss_ce: 0.009976
2021-12-12 00:14:31,228 iteration 5801 : loss : 0.029910, loss_ce: 0.016016
2021-12-12 00:14:33,792 iteration 5802 : loss : 0.023132, loss_ce: 0.012242
2021-12-12 00:14:36,544 iteration 5803 : loss : 0.037649, loss_ce: 0.014376
2021-12-12 00:14:39,225 iteration 5804 : loss : 0.036128, loss_ce: 0.015461
2021-12-12 00:14:41,919 iteration 5805 : loss : 0.024986, loss_ce: 0.011648
2021-12-12 00:14:44,822 iteration 5806 : loss : 0.023450, loss_ce: 0.012760
2021-12-12 00:14:47,598 iteration 5807 : loss : 0.034179, loss_ce: 0.014221
2021-12-12 00:14:50,547 iteration 5808 : loss : 0.051378, loss_ce: 0.019748
2021-12-12 00:14:53,345 iteration 5809 : loss : 0.032081, loss_ce: 0.013704
2021-12-12 00:14:56,330 iteration 5810 : loss : 0.040194, loss_ce: 0.016598
2021-12-12 00:14:59,093 iteration 5811 : loss : 0.045017, loss_ce: 0.018872
2021-12-12 00:15:01,916 iteration 5812 : loss : 0.037055, loss_ce: 0.018837
2021-12-12 00:15:04,491 iteration 5813 : loss : 0.027824, loss_ce: 0.010812
2021-12-12 00:15:07,304 iteration 5814 : loss : 0.039787, loss_ce: 0.016249
 86%|████████████████████████▊    | 342/400 [4:52:05<49:10, 50.88s/it]2021-12-12 00:15:10,046 iteration 5815 : loss : 0.038331, loss_ce: 0.015454
2021-12-12 00:15:12,783 iteration 5816 : loss : 0.028209, loss_ce: 0.012966
2021-12-12 00:15:15,772 iteration 5817 : loss : 0.039454, loss_ce: 0.019224
2021-12-12 00:15:18,552 iteration 5818 : loss : 0.033478, loss_ce: 0.016449
2021-12-12 00:15:21,256 iteration 5819 : loss : 0.027422, loss_ce: 0.010911
2021-12-12 00:15:24,044 iteration 5820 : loss : 0.036188, loss_ce: 0.015313
2021-12-12 00:15:26,791 iteration 5821 : loss : 0.027449, loss_ce: 0.013104
2021-12-12 00:15:29,718 iteration 5822 : loss : 0.031190, loss_ce: 0.011927
2021-12-12 00:15:32,468 iteration 5823 : loss : 0.023129, loss_ce: 0.013579
2021-12-12 00:15:35,410 iteration 5824 : loss : 0.047195, loss_ce: 0.014081
2021-12-12 00:15:38,209 iteration 5825 : loss : 0.024633, loss_ce: 0.011573
2021-12-12 00:15:40,997 iteration 5826 : loss : 0.030430, loss_ce: 0.015335
2021-12-12 00:15:43,719 iteration 5827 : loss : 0.030232, loss_ce: 0.014096
2021-12-12 00:15:46,695 iteration 5828 : loss : 0.047814, loss_ce: 0.014979
2021-12-12 00:15:49,448 iteration 5829 : loss : 0.024971, loss_ce: 0.011415
2021-12-12 00:15:52,106 iteration 5830 : loss : 0.029276, loss_ce: 0.013100
2021-12-12 00:15:55,073 iteration 5831 : loss : 0.036999, loss_ce: 0.016768
 86%|████████████████████████▊    | 343/400 [4:52:53<47:26, 49.94s/it]2021-12-12 00:15:57,927 iteration 5832 : loss : 0.031283, loss_ce: 0.016618
2021-12-12 00:16:00,743 iteration 5833 : loss : 0.043338, loss_ce: 0.015525
2021-12-12 00:16:03,653 iteration 5834 : loss : 0.035450, loss_ce: 0.019902
2021-12-12 00:16:06,337 iteration 5835 : loss : 0.031658, loss_ce: 0.014259
2021-12-12 00:16:09,170 iteration 5836 : loss : 0.051392, loss_ce: 0.021134
2021-12-12 00:16:12,057 iteration 5837 : loss : 0.034332, loss_ce: 0.014172
2021-12-12 00:16:14,944 iteration 5838 : loss : 0.032367, loss_ce: 0.012806
2021-12-12 00:16:17,833 iteration 5839 : loss : 0.042197, loss_ce: 0.020292
2021-12-12 00:16:20,507 iteration 5840 : loss : 0.030147, loss_ce: 0.013881
2021-12-12 00:16:23,549 iteration 5841 : loss : 0.052187, loss_ce: 0.020007
2021-12-12 00:16:26,347 iteration 5842 : loss : 0.030872, loss_ce: 0.012550
2021-12-12 00:16:29,045 iteration 5843 : loss : 0.030984, loss_ce: 0.014332
2021-12-12 00:16:31,833 iteration 5844 : loss : 0.034939, loss_ce: 0.018968
2021-12-12 00:16:34,640 iteration 5845 : loss : 0.023633, loss_ce: 0.011265
2021-12-12 00:16:37,412 iteration 5846 : loss : 0.020373, loss_ce: 0.010825
2021-12-12 00:16:40,153 iteration 5847 : loss : 0.025634, loss_ce: 0.012775
2021-12-12 00:16:42,812 iteration 5848 : loss : 0.021813, loss_ce: 0.010058
 86%|████████████████████████▉    | 344/400 [4:53:40<45:59, 49.28s/it]2021-12-12 00:16:45,678 iteration 5849 : loss : 0.041139, loss_ce: 0.015208
2021-12-12 00:16:48,571 iteration 5850 : loss : 0.031717, loss_ce: 0.016702
2021-12-12 00:16:51,302 iteration 5851 : loss : 0.027961, loss_ce: 0.010392
2021-12-12 00:16:54,017 iteration 5852 : loss : 0.036119, loss_ce: 0.019404
2021-12-12 00:16:56,738 iteration 5853 : loss : 0.027448, loss_ce: 0.011702
2021-12-12 00:16:59,527 iteration 5854 : loss : 0.029060, loss_ce: 0.013299
2021-12-12 00:17:02,271 iteration 5855 : loss : 0.033188, loss_ce: 0.017699
2021-12-12 00:17:04,908 iteration 5856 : loss : 0.029539, loss_ce: 0.013793
2021-12-12 00:17:07,712 iteration 5857 : loss : 0.027537, loss_ce: 0.012677
2021-12-12 00:17:10,632 iteration 5858 : loss : 0.026533, loss_ce: 0.011278
2021-12-12 00:17:13,421 iteration 5859 : loss : 0.040385, loss_ce: 0.014609
2021-12-12 00:17:16,362 iteration 5860 : loss : 0.025173, loss_ce: 0.011329
2021-12-12 00:17:19,046 iteration 5861 : loss : 0.033891, loss_ce: 0.016617
2021-12-12 00:17:21,867 iteration 5862 : loss : 0.035318, loss_ce: 0.018337
2021-12-12 00:17:24,766 iteration 5863 : loss : 0.045575, loss_ce: 0.016796
2021-12-12 00:17:27,437 iteration 5864 : loss : 0.034794, loss_ce: 0.017771
2021-12-12 00:17:27,438 Training Data Eval:
2021-12-12 00:17:42,384   Average segmentation loss on training set: 0.0141
2021-12-12 00:17:42,385 Validation Data Eval:
2021-12-12 00:17:47,701   Average segmentation loss on validation set: 0.0890
2021-12-12 00:17:50,682 iteration 5865 : loss : 0.065164, loss_ce: 0.021877
 86%|█████████████████████████    | 345/400 [4:54:48<50:16, 54.85s/it]2021-12-12 00:17:53,357 iteration 5866 : loss : 0.028859, loss_ce: 0.011180
2021-12-12 00:17:56,075 iteration 5867 : loss : 0.024334, loss_ce: 0.012775
2021-12-12 00:17:58,942 iteration 5868 : loss : 0.027418, loss_ce: 0.012910
2021-12-12 00:18:01,599 iteration 5869 : loss : 0.027653, loss_ce: 0.012022
2021-12-12 00:18:04,416 iteration 5870 : loss : 0.046021, loss_ce: 0.022113
2021-12-12 00:18:07,329 iteration 5871 : loss : 0.031440, loss_ce: 0.013419
2021-12-12 00:18:09,954 iteration 5872 : loss : 0.036416, loss_ce: 0.014648
2021-12-12 00:18:12,676 iteration 5873 : loss : 0.041188, loss_ce: 0.026611
2021-12-12 00:18:15,345 iteration 5874 : loss : 0.029375, loss_ce: 0.013019
2021-12-12 00:18:18,194 iteration 5875 : loss : 0.025725, loss_ce: 0.013896
2021-12-12 00:18:21,041 iteration 5876 : loss : 0.025220, loss_ce: 0.013161
2021-12-12 00:18:23,838 iteration 5877 : loss : 0.040394, loss_ce: 0.018035
2021-12-12 00:18:26,555 iteration 5878 : loss : 0.028412, loss_ce: 0.011667
2021-12-12 00:18:29,306 iteration 5879 : loss : 0.036744, loss_ce: 0.015725
2021-12-12 00:18:32,245 iteration 5880 : loss : 0.041516, loss_ce: 0.014350
2021-12-12 00:18:35,040 iteration 5881 : loss : 0.032552, loss_ce: 0.014492
2021-12-12 00:18:37,844 iteration 5882 : loss : 0.029424, loss_ce: 0.013516
 86%|█████████████████████████    | 346/400 [4:55:36<47:17, 52.55s/it]2021-12-12 00:18:40,734 iteration 5883 : loss : 0.027709, loss_ce: 0.012712
2021-12-12 00:18:43,458 iteration 5884 : loss : 0.021999, loss_ce: 0.010693
2021-12-12 00:18:46,423 iteration 5885 : loss : 0.035341, loss_ce: 0.013263
2021-12-12 00:18:49,296 iteration 5886 : loss : 0.035854, loss_ce: 0.014844
2021-12-12 00:18:51,966 iteration 5887 : loss : 0.028334, loss_ce: 0.013867
2021-12-12 00:18:54,870 iteration 5888 : loss : 0.019875, loss_ce: 0.010811
2021-12-12 00:18:57,771 iteration 5889 : loss : 0.025972, loss_ce: 0.011935
2021-12-12 00:19:00,410 iteration 5890 : loss : 0.031028, loss_ce: 0.014974
2021-12-12 00:19:03,050 iteration 5891 : loss : 0.043503, loss_ce: 0.014350
2021-12-12 00:19:05,726 iteration 5892 : loss : 0.035901, loss_ce: 0.013985
2021-12-12 00:19:08,476 iteration 5893 : loss : 0.034552, loss_ce: 0.018117
2021-12-12 00:19:11,171 iteration 5894 : loss : 0.030577, loss_ce: 0.015873
2021-12-12 00:19:13,911 iteration 5895 : loss : 0.024259, loss_ce: 0.012454
2021-12-12 00:19:16,519 iteration 5896 : loss : 0.030687, loss_ce: 0.015057
2021-12-12 00:19:19,388 iteration 5897 : loss : 0.034340, loss_ce: 0.018927
2021-12-12 00:19:22,047 iteration 5898 : loss : 0.047992, loss_ce: 0.017601
2021-12-12 00:19:25,011 iteration 5899 : loss : 0.036779, loss_ce: 0.017529
 87%|█████████████████████████▏   | 347/400 [4:56:23<44:59, 50.93s/it]2021-12-12 00:19:27,734 iteration 5900 : loss : 0.028044, loss_ce: 0.012035
2021-12-12 00:19:30,728 iteration 5901 : loss : 0.041409, loss_ce: 0.020349
2021-12-12 00:19:33,550 iteration 5902 : loss : 0.035461, loss_ce: 0.018515
2021-12-12 00:19:36,427 iteration 5903 : loss : 0.036269, loss_ce: 0.014698
2021-12-12 00:19:39,230 iteration 5904 : loss : 0.029914, loss_ce: 0.014748
2021-12-12 00:19:41,898 iteration 5905 : loss : 0.029766, loss_ce: 0.012730
2021-12-12 00:19:44,867 iteration 5906 : loss : 0.026591, loss_ce: 0.012660
2021-12-12 00:19:47,743 iteration 5907 : loss : 0.030132, loss_ce: 0.014273
2021-12-12 00:19:50,364 iteration 5908 : loss : 0.031041, loss_ce: 0.013862
2021-12-12 00:19:52,995 iteration 5909 : loss : 0.019706, loss_ce: 0.010858
2021-12-12 00:19:55,692 iteration 5910 : loss : 0.030976, loss_ce: 0.016723
2021-12-12 00:19:58,384 iteration 5911 : loss : 0.026818, loss_ce: 0.012960
2021-12-12 00:20:01,264 iteration 5912 : loss : 0.075948, loss_ce: 0.020264
2021-12-12 00:20:03,845 iteration 5913 : loss : 0.029718, loss_ce: 0.013971
2021-12-12 00:20:06,574 iteration 5914 : loss : 0.024754, loss_ce: 0.011301
2021-12-12 00:20:09,473 iteration 5915 : loss : 0.038211, loss_ce: 0.018579
2021-12-12 00:20:12,211 iteration 5916 : loss : 0.030394, loss_ce: 0.013589
 87%|█████████████████████████▏   | 348/400 [4:57:10<43:10, 49.81s/it]2021-12-12 00:20:14,933 iteration 5917 : loss : 0.044542, loss_ce: 0.012677
2021-12-12 00:20:17,700 iteration 5918 : loss : 0.036652, loss_ce: 0.016539
2021-12-12 00:20:20,385 iteration 5919 : loss : 0.041485, loss_ce: 0.029177
2021-12-12 00:20:23,321 iteration 5920 : loss : 0.027706, loss_ce: 0.014790
2021-12-12 00:20:26,009 iteration 5921 : loss : 0.032110, loss_ce: 0.011649
2021-12-12 00:20:28,676 iteration 5922 : loss : 0.037328, loss_ce: 0.017437
2021-12-12 00:20:31,422 iteration 5923 : loss : 0.035426, loss_ce: 0.013019
2021-12-12 00:20:34,216 iteration 5924 : loss : 0.022948, loss_ce: 0.011726
2021-12-12 00:20:37,044 iteration 5925 : loss : 0.022758, loss_ce: 0.010133
2021-12-12 00:20:39,834 iteration 5926 : loss : 0.020127, loss_ce: 0.009666
2021-12-12 00:20:42,657 iteration 5927 : loss : 0.037190, loss_ce: 0.017265
2021-12-12 00:20:45,337 iteration 5928 : loss : 0.022701, loss_ce: 0.011965
2021-12-12 00:20:48,111 iteration 5929 : loss : 0.031260, loss_ce: 0.011622
2021-12-12 00:20:50,927 iteration 5930 : loss : 0.044205, loss_ce: 0.016986
2021-12-12 00:20:53,686 iteration 5931 : loss : 0.026661, loss_ce: 0.013202
2021-12-12 00:20:56,495 iteration 5932 : loss : 0.043066, loss_ce: 0.017067
2021-12-12 00:20:59,128 iteration 5933 : loss : 0.022149, loss_ce: 0.010548
 87%|█████████████████████████▎   | 349/400 [4:57:57<41:36, 48.95s/it]2021-12-12 00:21:01,914 iteration 5934 : loss : 0.025100, loss_ce: 0.012545
2021-12-12 00:21:04,710 iteration 5935 : loss : 0.036289, loss_ce: 0.019387
2021-12-12 00:21:07,306 iteration 5936 : loss : 0.024747, loss_ce: 0.012162
2021-12-12 00:21:10,231 iteration 5937 : loss : 0.031690, loss_ce: 0.015964
2021-12-12 00:21:12,932 iteration 5938 : loss : 0.029002, loss_ce: 0.016919
2021-12-12 00:21:15,795 iteration 5939 : loss : 0.058074, loss_ce: 0.023393
2021-12-12 00:21:18,472 iteration 5940 : loss : 0.017875, loss_ce: 0.009905
2021-12-12 00:21:21,310 iteration 5941 : loss : 0.028867, loss_ce: 0.013098
2021-12-12 00:21:23,999 iteration 5942 : loss : 0.023374, loss_ce: 0.011815
2021-12-12 00:21:26,836 iteration 5943 : loss : 0.028335, loss_ce: 0.013581
2021-12-12 00:21:29,473 iteration 5944 : loss : 0.028293, loss_ce: 0.014059
2021-12-12 00:21:32,198 iteration 5945 : loss : 0.040010, loss_ce: 0.014507
2021-12-12 00:21:34,952 iteration 5946 : loss : 0.023435, loss_ce: 0.011547
2021-12-12 00:21:37,905 iteration 5947 : loss : 0.046838, loss_ce: 0.016843
2021-12-12 00:21:40,697 iteration 5948 : loss : 0.043785, loss_ce: 0.018341
2021-12-12 00:21:43,368 iteration 5949 : loss : 0.053709, loss_ce: 0.015424
2021-12-12 00:21:43,369 Training Data Eval:
2021-12-12 00:21:57,998   Average segmentation loss on training set: 0.0143
2021-12-12 00:21:57,998 Validation Data Eval:
2021-12-12 00:22:02,992   Average segmentation loss on validation set: 0.0888
2021-12-12 00:22:05,556 iteration 5950 : loss : 0.023416, loss_ce: 0.009945
2021-12-12 00:22:07,583 save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_seed2epoch_349.pth
 88%|█████████████████████████▍   | 350/400 [4:59:05<45:39, 54.78s/it]2021-12-12 00:22:09,851 iteration 5951 : loss : 0.034246, loss_ce: 0.013509
2021-12-12 00:22:12,746 iteration 5952 : loss : 0.034762, loss_ce: 0.019171
2021-12-12 00:22:15,540 iteration 5953 : loss : 0.051905, loss_ce: 0.025356
2021-12-12 00:22:18,146 iteration 5954 : loss : 0.036267, loss_ce: 0.012320
2021-12-12 00:22:20,943 iteration 5955 : loss : 0.046774, loss_ce: 0.017728
2021-12-12 00:22:23,664 iteration 5956 : loss : 0.049069, loss_ce: 0.017018
2021-12-12 00:22:26,469 iteration 5957 : loss : 0.036388, loss_ce: 0.017027
2021-12-12 00:22:29,254 iteration 5958 : loss : 0.038420, loss_ce: 0.019490
2021-12-12 00:22:32,196 iteration 5959 : loss : 0.027391, loss_ce: 0.009936
2021-12-12 00:22:34,851 iteration 5960 : loss : 0.028313, loss_ce: 0.010132
2021-12-12 00:22:37,662 iteration 5961 : loss : 0.042904, loss_ce: 0.024038
2021-12-12 00:22:40,591 iteration 5962 : loss : 0.026932, loss_ce: 0.012328
2021-12-12 00:22:43,343 iteration 5963 : loss : 0.029001, loss_ce: 0.013702
2021-12-12 00:22:46,213 iteration 5964 : loss : 0.037354, loss_ce: 0.017336
2021-12-12 00:22:49,015 iteration 5965 : loss : 0.029147, loss_ce: 0.018501
2021-12-12 00:22:51,732 iteration 5966 : loss : 0.021366, loss_ce: 0.009947
2021-12-12 00:22:54,644 iteration 5967 : loss : 0.025570, loss_ce: 0.011055
 88%|█████████████████████████▍   | 351/400 [4:59:52<42:51, 52.48s/it]2021-12-12 00:22:57,380 iteration 5968 : loss : 0.024976, loss_ce: 0.011880
2021-12-12 00:23:00,171 iteration 5969 : loss : 0.036217, loss_ce: 0.021017
2021-12-12 00:23:02,923 iteration 5970 : loss : 0.036797, loss_ce: 0.016364
2021-12-12 00:23:05,561 iteration 5971 : loss : 0.020756, loss_ce: 0.011442
2021-12-12 00:23:08,250 iteration 5972 : loss : 0.033728, loss_ce: 0.012243
2021-12-12 00:23:10,898 iteration 5973 : loss : 0.031669, loss_ce: 0.015005
2021-12-12 00:23:13,689 iteration 5974 : loss : 0.054602, loss_ce: 0.018800
2021-12-12 00:23:16,597 iteration 5975 : loss : 0.042928, loss_ce: 0.027044
2021-12-12 00:23:19,424 iteration 5976 : loss : 0.025547, loss_ce: 0.011736
2021-12-12 00:23:22,203 iteration 5977 : loss : 0.030412, loss_ce: 0.013387
2021-12-12 00:23:24,928 iteration 5978 : loss : 0.041455, loss_ce: 0.015638
2021-12-12 00:23:27,647 iteration 5979 : loss : 0.035958, loss_ce: 0.013901
2021-12-12 00:23:30,503 iteration 5980 : loss : 0.048297, loss_ce: 0.015221
2021-12-12 00:23:33,407 iteration 5981 : loss : 0.033925, loss_ce: 0.018710
2021-12-12 00:23:36,231 iteration 5982 : loss : 0.038757, loss_ce: 0.021098
2021-12-12 00:23:38,949 iteration 5983 : loss : 0.038032, loss_ce: 0.014594
2021-12-12 00:23:41,750 iteration 5984 : loss : 0.033651, loss_ce: 0.013586
 88%|█████████████████████████▌   | 352/400 [5:00:39<40:41, 50.87s/it]2021-12-12 00:23:44,664 iteration 5985 : loss : 0.039983, loss_ce: 0.018903
2021-12-12 00:23:47,251 iteration 5986 : loss : 0.023898, loss_ce: 0.013344
2021-12-12 00:23:50,205 iteration 5987 : loss : 0.039994, loss_ce: 0.014300
2021-12-12 00:23:52,864 iteration 5988 : loss : 0.035964, loss_ce: 0.016743
2021-12-12 00:23:55,575 iteration 5989 : loss : 0.030427, loss_ce: 0.013993
2021-12-12 00:23:58,361 iteration 5990 : loss : 0.032119, loss_ce: 0.012669
2021-12-12 00:24:01,232 iteration 5991 : loss : 0.034487, loss_ce: 0.013820
2021-12-12 00:24:04,099 iteration 5992 : loss : 0.054680, loss_ce: 0.018599
2021-12-12 00:24:06,740 iteration 5993 : loss : 0.027783, loss_ce: 0.014043
2021-12-12 00:24:09,423 iteration 5994 : loss : 0.020268, loss_ce: 0.011284
2021-12-12 00:24:12,329 iteration 5995 : loss : 0.032751, loss_ce: 0.013772
2021-12-12 00:24:15,111 iteration 5996 : loss : 0.023402, loss_ce: 0.014410
2021-12-12 00:24:17,900 iteration 5997 : loss : 0.024678, loss_ce: 0.010076
2021-12-12 00:24:20,763 iteration 5998 : loss : 0.042101, loss_ce: 0.016356
2021-12-12 00:24:23,383 iteration 5999 : loss : 0.041634, loss_ce: 0.017664
2021-12-12 00:24:26,219 iteration 6000 : loss : 0.035482, loss_ce: 0.015620
2021-12-12 00:24:28,949 iteration 6001 : loss : 0.022294, loss_ce: 0.011029
 88%|█████████████████████████▌   | 353/400 [5:01:27<38:59, 49.77s/it]2021-12-12 00:24:31,756 iteration 6002 : loss : 0.030314, loss_ce: 0.013842
2021-12-12 00:24:34,503 iteration 6003 : loss : 0.027178, loss_ce: 0.014621
2021-12-12 00:24:37,287 iteration 6004 : loss : 0.027099, loss_ce: 0.012541
2021-12-12 00:24:39,903 iteration 6005 : loss : 0.028270, loss_ce: 0.013652
2021-12-12 00:24:42,851 iteration 6006 : loss : 0.046501, loss_ce: 0.018263
2021-12-12 00:24:45,494 iteration 6007 : loss : 0.028603, loss_ce: 0.013570
2021-12-12 00:24:48,344 iteration 6008 : loss : 0.030275, loss_ce: 0.014030
2021-12-12 00:24:51,175 iteration 6009 : loss : 0.041687, loss_ce: 0.014425
2021-12-12 00:24:53,875 iteration 6010 : loss : 0.032199, loss_ce: 0.010308
2021-12-12 00:24:56,784 iteration 6011 : loss : 0.044030, loss_ce: 0.015933
2021-12-12 00:24:59,570 iteration 6012 : loss : 0.032368, loss_ce: 0.015937
2021-12-12 00:25:02,264 iteration 6013 : loss : 0.029107, loss_ce: 0.012652
2021-12-12 00:25:04,980 iteration 6014 : loss : 0.025646, loss_ce: 0.012001
2021-12-12 00:25:07,646 iteration 6015 : loss : 0.020825, loss_ce: 0.012293
2021-12-12 00:25:10,509 iteration 6016 : loss : 0.026369, loss_ce: 0.010829
2021-12-12 00:25:13,377 iteration 6017 : loss : 0.029292, loss_ce: 0.012783
2021-12-12 00:25:16,104 iteration 6018 : loss : 0.029888, loss_ce: 0.015157
 88%|█████████████████████████▋   | 354/400 [5:02:14<37:33, 48.98s/it]2021-12-12 00:25:19,092 iteration 6019 : loss : 0.031428, loss_ce: 0.014893
2021-12-12 00:25:21,877 iteration 6020 : loss : 0.036574, loss_ce: 0.014149
2021-12-12 00:25:24,615 iteration 6021 : loss : 0.021846, loss_ce: 0.010612
2021-12-12 00:25:27,444 iteration 6022 : loss : 0.026623, loss_ce: 0.012973
2021-12-12 00:25:30,202 iteration 6023 : loss : 0.029424, loss_ce: 0.014519
2021-12-12 00:25:33,039 iteration 6024 : loss : 0.027441, loss_ce: 0.013060
2021-12-12 00:25:35,637 iteration 6025 : loss : 0.045540, loss_ce: 0.014682
2021-12-12 00:25:38,419 iteration 6026 : loss : 0.034478, loss_ce: 0.015582
2021-12-12 00:25:41,329 iteration 6027 : loss : 0.021347, loss_ce: 0.011556
2021-12-12 00:25:44,035 iteration 6028 : loss : 0.027380, loss_ce: 0.014938
2021-12-12 00:25:46,726 iteration 6029 : loss : 0.031845, loss_ce: 0.016367
2021-12-12 00:25:49,604 iteration 6030 : loss : 0.034645, loss_ce: 0.015644
2021-12-12 00:25:52,410 iteration 6031 : loss : 0.041850, loss_ce: 0.016230
2021-12-12 00:25:55,162 iteration 6032 : loss : 0.032704, loss_ce: 0.012279
2021-12-12 00:25:57,924 iteration 6033 : loss : 0.042269, loss_ce: 0.018146
2021-12-12 00:26:00,635 iteration 6034 : loss : 0.021863, loss_ce: 0.011298
2021-12-12 00:26:00,635 Training Data Eval:
2021-12-12 00:26:15,682   Average segmentation loss on training set: 0.0146
2021-12-12 00:26:15,683 Validation Data Eval:
2021-12-12 00:26:20,916   Average segmentation loss on validation set: 0.0896
2021-12-12 00:26:23,528 iteration 6035 : loss : 0.025712, loss_ce: 0.013321
 89%|█████████████████████████▋   | 355/400 [5:03:21<40:53, 54.52s/it]2021-12-12 00:26:26,481 iteration 6036 : loss : 0.025455, loss_ce: 0.013310
2021-12-12 00:26:29,232 iteration 6037 : loss : 0.019654, loss_ce: 0.009641
2021-12-12 00:26:31,793 iteration 6038 : loss : 0.022707, loss_ce: 0.011141
2021-12-12 00:26:34,486 iteration 6039 : loss : 0.022188, loss_ce: 0.009948
2021-12-12 00:26:37,231 iteration 6040 : loss : 0.027758, loss_ce: 0.011070
2021-12-12 00:26:40,134 iteration 6041 : loss : 0.030357, loss_ce: 0.014603
2021-12-12 00:26:42,786 iteration 6042 : loss : 0.028443, loss_ce: 0.010817
2021-12-12 00:26:45,475 iteration 6043 : loss : 0.024450, loss_ce: 0.009202
2021-12-12 00:26:48,267 iteration 6044 : loss : 0.044276, loss_ce: 0.014760
2021-12-12 00:26:50,858 iteration 6045 : loss : 0.023001, loss_ce: 0.012069
2021-12-12 00:26:53,628 iteration 6046 : loss : 0.037440, loss_ce: 0.018709
2021-12-12 00:26:56,350 iteration 6047 : loss : 0.024697, loss_ce: 0.011717
2021-12-12 00:26:59,205 iteration 6048 : loss : 0.031312, loss_ce: 0.016909
2021-12-12 00:27:01,980 iteration 6049 : loss : 0.023008, loss_ce: 0.013467
2021-12-12 00:27:04,730 iteration 6050 : loss : 0.026267, loss_ce: 0.012173
2021-12-12 00:27:07,673 iteration 6051 : loss : 0.027452, loss_ce: 0.012085
2021-12-12 00:27:10,470 iteration 6052 : loss : 0.024786, loss_ce: 0.012236
 89%|█████████████████████████▊   | 356/400 [5:04:08<38:18, 52.24s/it]2021-12-12 00:27:13,162 iteration 6053 : loss : 0.039212, loss_ce: 0.014953
2021-12-12 00:27:15,854 iteration 6054 : loss : 0.030531, loss_ce: 0.017569
2021-12-12 00:27:18,638 iteration 6055 : loss : 0.029663, loss_ce: 0.012954
2021-12-12 00:27:21,372 iteration 6056 : loss : 0.027444, loss_ce: 0.012528
2021-12-12 00:27:24,058 iteration 6057 : loss : 0.023675, loss_ce: 0.009829
2021-12-12 00:27:26,724 iteration 6058 : loss : 0.039400, loss_ce: 0.014482
2021-12-12 00:27:29,664 iteration 6059 : loss : 0.037509, loss_ce: 0.018603
2021-12-12 00:27:32,544 iteration 6060 : loss : 0.085359, loss_ce: 0.030397
2021-12-12 00:27:35,330 iteration 6061 : loss : 0.029616, loss_ce: 0.014396
2021-12-12 00:27:38,195 iteration 6062 : loss : 0.047086, loss_ce: 0.018382
2021-12-12 00:27:40,827 iteration 6063 : loss : 0.033401, loss_ce: 0.013528
2021-12-12 00:27:43,836 iteration 6064 : loss : 0.028382, loss_ce: 0.014282
2021-12-12 00:27:46,575 iteration 6065 : loss : 0.021011, loss_ce: 0.010249
2021-12-12 00:27:49,310 iteration 6066 : loss : 0.025309, loss_ce: 0.012350
2021-12-12 00:27:52,096 iteration 6067 : loss : 0.023791, loss_ce: 0.012398
2021-12-12 00:27:54,901 iteration 6068 : loss : 0.022280, loss_ce: 0.009412
2021-12-12 00:27:57,595 iteration 6069 : loss : 0.040364, loss_ce: 0.019326
 89%|█████████████████████████▉   | 357/400 [5:04:55<36:20, 50.71s/it]2021-12-12 00:28:00,465 iteration 6070 : loss : 0.021600, loss_ce: 0.013495
2021-12-12 00:28:03,234 iteration 6071 : loss : 0.023287, loss_ce: 0.011318
2021-12-12 00:28:06,009 iteration 6072 : loss : 0.026738, loss_ce: 0.016069
2021-12-12 00:28:08,760 iteration 6073 : loss : 0.025098, loss_ce: 0.012298
2021-12-12 00:28:11,552 iteration 6074 : loss : 0.023785, loss_ce: 0.011541
2021-12-12 00:28:14,274 iteration 6075 : loss : 0.026064, loss_ce: 0.012938
2021-12-12 00:28:16,980 iteration 6076 : loss : 0.027583, loss_ce: 0.010258
2021-12-12 00:28:19,860 iteration 6077 : loss : 0.024901, loss_ce: 0.012269
2021-12-12 00:28:22,795 iteration 6078 : loss : 0.038664, loss_ce: 0.016527
2021-12-12 00:28:25,672 iteration 6079 : loss : 0.036721, loss_ce: 0.016024
2021-12-12 00:28:28,551 iteration 6080 : loss : 0.026581, loss_ce: 0.012226
2021-12-12 00:28:31,392 iteration 6081 : loss : 0.039251, loss_ce: 0.017740
2021-12-12 00:28:34,116 iteration 6082 : loss : 0.041900, loss_ce: 0.014712
2021-12-12 00:28:36,840 iteration 6083 : loss : 0.035478, loss_ce: 0.014046
2021-12-12 00:28:39,805 iteration 6084 : loss : 0.041093, loss_ce: 0.012987
2021-12-12 00:28:42,493 iteration 6085 : loss : 0.032121, loss_ce: 0.012903
2021-12-12 00:28:45,335 iteration 6086 : loss : 0.029706, loss_ce: 0.012671
 90%|█████████████████████████▉   | 358/400 [5:05:43<34:52, 49.82s/it]2021-12-12 00:28:48,007 iteration 6087 : loss : 0.031798, loss_ce: 0.013875
2021-12-12 00:28:50,886 iteration 6088 : loss : 0.026305, loss_ce: 0.012999
2021-12-12 00:28:53,739 iteration 6089 : loss : 0.026870, loss_ce: 0.011508
2021-12-12 00:28:56,372 iteration 6090 : loss : 0.019770, loss_ce: 0.010733
2021-12-12 00:28:59,141 iteration 6091 : loss : 0.025580, loss_ce: 0.011906
2021-12-12 00:29:02,133 iteration 6092 : loss : 0.034993, loss_ce: 0.012879
2021-12-12 00:29:04,804 iteration 6093 : loss : 0.039574, loss_ce: 0.020438
2021-12-12 00:29:07,622 iteration 6094 : loss : 0.031442, loss_ce: 0.013488
2021-12-12 00:29:10,374 iteration 6095 : loss : 0.027668, loss_ce: 0.011446
2021-12-12 00:29:13,309 iteration 6096 : loss : 0.029509, loss_ce: 0.011855
2021-12-12 00:29:16,091 iteration 6097 : loss : 0.018514, loss_ce: 0.009316
2021-12-12 00:29:18,973 iteration 6098 : loss : 0.032422, loss_ce: 0.015014
2021-12-12 00:29:21,931 iteration 6099 : loss : 0.027887, loss_ce: 0.013914
2021-12-12 00:29:24,840 iteration 6100 : loss : 0.031743, loss_ce: 0.019315
2021-12-12 00:29:27,545 iteration 6101 : loss : 0.043241, loss_ce: 0.018237
2021-12-12 00:29:30,330 iteration 6102 : loss : 0.051835, loss_ce: 0.029369
2021-12-12 00:29:33,277 iteration 6103 : loss : 0.060099, loss_ce: 0.020451
 90%|██████████████████████████   | 359/400 [5:06:31<33:39, 49.26s/it]2021-12-12 00:29:36,106 iteration 6104 : loss : 0.041005, loss_ce: 0.018716
2021-12-12 00:29:38,798 iteration 6105 : loss : 0.032761, loss_ce: 0.016108
2021-12-12 00:29:41,437 iteration 6106 : loss : 0.044655, loss_ce: 0.023538
2021-12-12 00:29:44,109 iteration 6107 : loss : 0.029239, loss_ce: 0.013034
2021-12-12 00:29:46,957 iteration 6108 : loss : 0.027652, loss_ce: 0.013356
2021-12-12 00:29:49,641 iteration 6109 : loss : 0.030746, loss_ce: 0.011184
2021-12-12 00:29:52,288 iteration 6110 : loss : 0.027099, loss_ce: 0.014113
2021-12-12 00:29:55,149 iteration 6111 : loss : 0.029479, loss_ce: 0.012326
2021-12-12 00:29:58,021 iteration 6112 : loss : 0.042651, loss_ce: 0.015356
2021-12-12 00:30:00,908 iteration 6113 : loss : 0.029229, loss_ce: 0.014275
2021-12-12 00:30:03,596 iteration 6114 : loss : 0.041876, loss_ce: 0.015307
2021-12-12 00:30:06,366 iteration 6115 : loss : 0.029158, loss_ce: 0.012814
2021-12-12 00:30:09,329 iteration 6116 : loss : 0.038832, loss_ce: 0.014680
2021-12-12 00:30:12,073 iteration 6117 : loss : 0.029538, loss_ce: 0.013532
2021-12-12 00:30:14,751 iteration 6118 : loss : 0.020960, loss_ce: 0.009544
2021-12-12 00:30:17,736 iteration 6119 : loss : 0.037792, loss_ce: 0.019754
2021-12-12 00:30:17,737 Training Data Eval:
2021-12-12 00:30:32,690   Average segmentation loss on training set: 0.0140
2021-12-12 00:30:32,691 Validation Data Eval:
2021-12-12 00:30:37,964   Average segmentation loss on validation set: 0.0892
2021-12-12 00:30:40,809 iteration 6120 : loss : 0.034174, loss_ce: 0.013848
 90%|██████████████████████████   | 360/400 [5:07:38<36:29, 54.74s/it]2021-12-12 00:30:43,676 iteration 6121 : loss : 0.041709, loss_ce: 0.014775
2021-12-12 00:30:46,446 iteration 6122 : loss : 0.028708, loss_ce: 0.014905
2021-12-12 00:30:49,189 iteration 6123 : loss : 0.029188, loss_ce: 0.015144
2021-12-12 00:30:52,066 iteration 6124 : loss : 0.020540, loss_ce: 0.009616
2021-12-12 00:30:54,863 iteration 6125 : loss : 0.047330, loss_ce: 0.021158
2021-12-12 00:30:57,595 iteration 6126 : loss : 0.027623, loss_ce: 0.017439
2021-12-12 00:31:00,514 iteration 6127 : loss : 0.025741, loss_ce: 0.011534
2021-12-12 00:31:03,202 iteration 6128 : loss : 0.036113, loss_ce: 0.014551
2021-12-12 00:31:06,211 iteration 6129 : loss : 0.029041, loss_ce: 0.012982
2021-12-12 00:31:08,896 iteration 6130 : loss : 0.026987, loss_ce: 0.010666
2021-12-12 00:31:11,815 iteration 6131 : loss : 0.035349, loss_ce: 0.020318
2021-12-12 00:31:14,600 iteration 6132 : loss : 0.032531, loss_ce: 0.019429
2021-12-12 00:31:17,368 iteration 6133 : loss : 0.033966, loss_ce: 0.012295
2021-12-12 00:31:20,183 iteration 6134 : loss : 0.029488, loss_ce: 0.011948
2021-12-12 00:31:23,048 iteration 6135 : loss : 0.047659, loss_ce: 0.019727
2021-12-12 00:31:25,859 iteration 6136 : loss : 0.035111, loss_ce: 0.013416
2021-12-12 00:31:28,485 iteration 6137 : loss : 0.030454, loss_ce: 0.014234
 90%|██████████████████████████▏  | 361/400 [5:08:26<34:12, 52.62s/it]2021-12-12 00:31:31,404 iteration 6138 : loss : 0.029408, loss_ce: 0.011912
2021-12-12 00:31:34,199 iteration 6139 : loss : 0.024533, loss_ce: 0.012372
2021-12-12 00:31:36,772 iteration 6140 : loss : 0.032382, loss_ce: 0.018767
2021-12-12 00:31:39,434 iteration 6141 : loss : 0.022899, loss_ce: 0.011062
2021-12-12 00:31:42,148 iteration 6142 : loss : 0.019622, loss_ce: 0.011193
2021-12-12 00:31:45,101 iteration 6143 : loss : 0.026720, loss_ce: 0.010778
2021-12-12 00:31:47,839 iteration 6144 : loss : 0.024985, loss_ce: 0.011773
2021-12-12 00:31:50,646 iteration 6145 : loss : 0.027392, loss_ce: 0.014047
2021-12-12 00:31:53,334 iteration 6146 : loss : 0.036863, loss_ce: 0.014045
2021-12-12 00:31:56,023 iteration 6147 : loss : 0.026530, loss_ce: 0.013081
2021-12-12 00:31:58,795 iteration 6148 : loss : 0.034200, loss_ce: 0.016129
2021-12-12 00:32:01,666 iteration 6149 : loss : 0.032825, loss_ce: 0.015410
2021-12-12 00:32:04,643 iteration 6150 : loss : 0.032480, loss_ce: 0.014062
2021-12-12 00:32:07,620 iteration 6151 : loss : 0.037832, loss_ce: 0.012941
2021-12-12 00:32:10,261 iteration 6152 : loss : 0.024527, loss_ce: 0.012659
2021-12-12 00:32:13,193 iteration 6153 : loss : 0.046776, loss_ce: 0.011837
2021-12-12 00:32:16,049 iteration 6154 : loss : 0.034986, loss_ce: 0.015889
 90%|██████████████████████████▏  | 362/400 [5:09:14<32:21, 51.10s/it]2021-12-12 00:32:18,882 iteration 6155 : loss : 0.035700, loss_ce: 0.015073
2021-12-12 00:32:21,808 iteration 6156 : loss : 0.035779, loss_ce: 0.018014
2021-12-12 00:32:24,638 iteration 6157 : loss : 0.028082, loss_ce: 0.011951
2021-12-12 00:32:27,488 iteration 6158 : loss : 0.041142, loss_ce: 0.014614
2021-12-12 00:32:30,243 iteration 6159 : loss : 0.024794, loss_ce: 0.011933
2021-12-12 00:32:33,006 iteration 6160 : loss : 0.026069, loss_ce: 0.012935
2021-12-12 00:32:35,950 iteration 6161 : loss : 0.048914, loss_ce: 0.018091
2021-12-12 00:32:38,834 iteration 6162 : loss : 0.042607, loss_ce: 0.013070
2021-12-12 00:32:41,595 iteration 6163 : loss : 0.035568, loss_ce: 0.016537
2021-12-12 00:32:44,136 iteration 6164 : loss : 0.021527, loss_ce: 0.009116
2021-12-12 00:32:46,992 iteration 6165 : loss : 0.033696, loss_ce: 0.014399
2021-12-12 00:32:49,627 iteration 6166 : loss : 0.023407, loss_ce: 0.011183
2021-12-12 00:32:52,511 iteration 6167 : loss : 0.031660, loss_ce: 0.017985
2021-12-12 00:32:55,283 iteration 6168 : loss : 0.041450, loss_ce: 0.018261
2021-12-12 00:32:58,085 iteration 6169 : loss : 0.032013, loss_ce: 0.011228
2021-12-12 00:33:00,933 iteration 6170 : loss : 0.030482, loss_ce: 0.016732
2021-12-12 00:33:03,670 iteration 6171 : loss : 0.038643, loss_ce: 0.017609
 91%|██████████████████████████▎  | 363/400 [5:10:01<30:52, 50.06s/it]2021-12-12 00:33:06,462 iteration 6172 : loss : 0.032126, loss_ce: 0.015261
2021-12-12 00:33:09,310 iteration 6173 : loss : 0.064408, loss_ce: 0.021504
2021-12-12 00:33:12,155 iteration 6174 : loss : 0.026625, loss_ce: 0.012924
2021-12-12 00:33:15,130 iteration 6175 : loss : 0.030552, loss_ce: 0.014731
2021-12-12 00:33:17,879 iteration 6176 : loss : 0.020630, loss_ce: 0.010135
2021-12-12 00:33:20,507 iteration 6177 : loss : 0.022324, loss_ce: 0.012907
2021-12-12 00:33:23,331 iteration 6178 : loss : 0.035208, loss_ce: 0.014300
2021-12-12 00:33:25,938 iteration 6179 : loss : 0.028072, loss_ce: 0.011214
2021-12-12 00:33:28,801 iteration 6180 : loss : 0.028332, loss_ce: 0.014620
2021-12-12 00:33:31,579 iteration 6181 : loss : 0.023852, loss_ce: 0.009974
2021-12-12 00:33:34,354 iteration 6182 : loss : 0.027309, loss_ce: 0.010794
2021-12-12 00:33:37,091 iteration 6183 : loss : 0.035658, loss_ce: 0.016929
2021-12-12 00:33:40,046 iteration 6184 : loss : 0.028032, loss_ce: 0.014318
2021-12-12 00:33:42,819 iteration 6185 : loss : 0.038174, loss_ce: 0.013659
2021-12-12 00:33:45,536 iteration 6186 : loss : 0.023899, loss_ce: 0.011738
2021-12-12 00:33:48,405 iteration 6187 : loss : 0.034274, loss_ce: 0.018402
2021-12-12 00:33:51,238 iteration 6188 : loss : 0.044113, loss_ce: 0.022074
 91%|██████████████████████████▍  | 364/400 [5:10:49<29:35, 49.31s/it]2021-12-12 00:33:54,170 iteration 6189 : loss : 0.031366, loss_ce: 0.010017
2021-12-12 00:33:57,029 iteration 6190 : loss : 0.044062, loss_ce: 0.012724
2021-12-12 00:33:59,868 iteration 6191 : loss : 0.030934, loss_ce: 0.017658
2021-12-12 00:34:02,494 iteration 6192 : loss : 0.026491, loss_ce: 0.013553
2021-12-12 00:34:05,348 iteration 6193 : loss : 0.034999, loss_ce: 0.013452
2021-12-12 00:34:08,009 iteration 6194 : loss : 0.032175, loss_ce: 0.014413
2021-12-12 00:34:10,973 iteration 6195 : loss : 0.028516, loss_ce: 0.012380
2021-12-12 00:34:13,589 iteration 6196 : loss : 0.040453, loss_ce: 0.015702
2021-12-12 00:34:16,193 iteration 6197 : loss : 0.032450, loss_ce: 0.020120
2021-12-12 00:34:18,966 iteration 6198 : loss : 0.050828, loss_ce: 0.016676
2021-12-12 00:34:21,828 iteration 6199 : loss : 0.038109, loss_ce: 0.018017
2021-12-12 00:34:24,627 iteration 6200 : loss : 0.025832, loss_ce: 0.013973
2021-12-12 00:34:27,534 iteration 6201 : loss : 0.030673, loss_ce: 0.014921
2021-12-12 00:34:30,140 iteration 6202 : loss : 0.024352, loss_ce: 0.011194
2021-12-12 00:34:33,028 iteration 6203 : loss : 0.038763, loss_ce: 0.017902
2021-12-12 00:34:35,722 iteration 6204 : loss : 0.023656, loss_ce: 0.013026
2021-12-12 00:34:35,722 Training Data Eval:
2021-12-12 00:34:50,558   Average segmentation loss on training set: 0.0142
2021-12-12 00:34:50,559 Validation Data Eval:
2021-12-12 00:34:55,846   Average segmentation loss on validation set: 0.0923
2021-12-12 00:34:58,711 iteration 6205 : loss : 0.037053, loss_ce: 0.015784
 91%|██████████████████████████▍  | 365/400 [5:11:56<31:56, 54.76s/it]2021-12-12 00:35:01,515 iteration 6206 : loss : 0.049556, loss_ce: 0.022511
2021-12-12 00:35:04,322 iteration 6207 : loss : 0.027160, loss_ce: 0.014588
2021-12-12 00:35:07,292 iteration 6208 : loss : 0.039888, loss_ce: 0.016037
2021-12-12 00:35:10,095 iteration 6209 : loss : 0.034704, loss_ce: 0.015568
2021-12-12 00:35:12,750 iteration 6210 : loss : 0.020268, loss_ce: 0.010237
2021-12-12 00:35:15,639 iteration 6211 : loss : 0.030751, loss_ce: 0.016008
2021-12-12 00:35:18,330 iteration 6212 : loss : 0.028286, loss_ce: 0.013577
2021-12-12 00:35:21,104 iteration 6213 : loss : 0.022451, loss_ce: 0.009899
2021-12-12 00:35:24,070 iteration 6214 : loss : 0.042345, loss_ce: 0.019341
2021-12-12 00:35:26,738 iteration 6215 : loss : 0.034953, loss_ce: 0.014325
2021-12-12 00:35:29,424 iteration 6216 : loss : 0.024682, loss_ce: 0.010409
2021-12-12 00:35:32,282 iteration 6217 : loss : 0.037003, loss_ce: 0.016904
2021-12-12 00:35:34,971 iteration 6218 : loss : 0.026695, loss_ce: 0.014244
2021-12-12 00:35:37,872 iteration 6219 : loss : 0.038315, loss_ce: 0.016648
2021-12-12 00:35:40,541 iteration 6220 : loss : 0.026309, loss_ce: 0.010153
2021-12-12 00:35:43,334 iteration 6221 : loss : 0.034754, loss_ce: 0.014257
2021-12-12 00:35:46,111 iteration 6222 : loss : 0.027385, loss_ce: 0.014555
 92%|██████████████████████████▌  | 366/400 [5:12:44<29:46, 52.55s/it]2021-12-12 00:35:49,076 iteration 6223 : loss : 0.034911, loss_ce: 0.018927
2021-12-12 00:35:51,744 iteration 6224 : loss : 0.027765, loss_ce: 0.014754
2021-12-12 00:35:54,376 iteration 6225 : loss : 0.026892, loss_ce: 0.015771
2021-12-12 00:35:57,198 iteration 6226 : loss : 0.029890, loss_ce: 0.013822
2021-12-12 00:35:59,835 iteration 6227 : loss : 0.025071, loss_ce: 0.011110
2021-12-12 00:36:02,742 iteration 6228 : loss : 0.033818, loss_ce: 0.014503
2021-12-12 00:36:05,519 iteration 6229 : loss : 0.034100, loss_ce: 0.013335
2021-12-12 00:36:08,282 iteration 6230 : loss : 0.043528, loss_ce: 0.011582
2021-12-12 00:36:11,060 iteration 6231 : loss : 0.031790, loss_ce: 0.016031
2021-12-12 00:36:13,818 iteration 6232 : loss : 0.029615, loss_ce: 0.013274
2021-12-12 00:36:16,596 iteration 6233 : loss : 0.044130, loss_ce: 0.017321
2021-12-12 00:36:19,387 iteration 6234 : loss : 0.028603, loss_ce: 0.013639
2021-12-12 00:36:22,089 iteration 6235 : loss : 0.027442, loss_ce: 0.011661
2021-12-12 00:36:24,878 iteration 6236 : loss : 0.041848, loss_ce: 0.018924
2021-12-12 00:36:27,743 iteration 6237 : loss : 0.028625, loss_ce: 0.012878
2021-12-12 00:36:30,592 iteration 6238 : loss : 0.029075, loss_ce: 0.014135
2021-12-12 00:36:33,475 iteration 6239 : loss : 0.027836, loss_ce: 0.014610
 92%|██████████████████████████▌  | 367/400 [5:13:31<28:02, 51.00s/it]2021-12-12 00:36:36,137 iteration 6240 : loss : 0.032405, loss_ce: 0.012312
2021-12-12 00:36:38,814 iteration 6241 : loss : 0.026949, loss_ce: 0.012946
2021-12-12 00:36:41,610 iteration 6242 : loss : 0.022940, loss_ce: 0.013141
2021-12-12 00:36:44,386 iteration 6243 : loss : 0.038178, loss_ce: 0.017458
2021-12-12 00:36:47,132 iteration 6244 : loss : 0.051811, loss_ce: 0.017366
2021-12-12 00:36:49,910 iteration 6245 : loss : 0.029026, loss_ce: 0.014896
2021-12-12 00:36:52,774 iteration 6246 : loss : 0.026153, loss_ce: 0.012670
2021-12-12 00:36:55,394 iteration 6247 : loss : 0.025289, loss_ce: 0.011231
2021-12-12 00:36:58,202 iteration 6248 : loss : 0.053741, loss_ce: 0.017632
2021-12-12 00:37:00,992 iteration 6249 : loss : 0.036624, loss_ce: 0.015847
2021-12-12 00:37:03,849 iteration 6250 : loss : 0.028108, loss_ce: 0.009382
2021-12-12 00:37:06,820 iteration 6251 : loss : 0.051645, loss_ce: 0.034580
2021-12-12 00:37:09,584 iteration 6252 : loss : 0.021689, loss_ce: 0.009660
2021-12-12 00:37:12,432 iteration 6253 : loss : 0.035932, loss_ce: 0.016099
2021-12-12 00:37:15,046 iteration 6254 : loss : 0.017007, loss_ce: 0.009834
2021-12-12 00:37:18,053 iteration 6255 : loss : 0.047696, loss_ce: 0.019868
2021-12-12 00:37:20,771 iteration 6256 : loss : 0.034585, loss_ce: 0.016209
 92%|██████████████████████████▋  | 368/400 [5:14:18<26:36, 49.88s/it]2021-12-12 00:37:23,675 iteration 6257 : loss : 0.029110, loss_ce: 0.013821
2021-12-12 00:37:26,497 iteration 6258 : loss : 0.022858, loss_ce: 0.013476
2021-12-12 00:37:29,282 iteration 6259 : loss : 0.036868, loss_ce: 0.017740
2021-12-12 00:37:31,995 iteration 6260 : loss : 0.043949, loss_ce: 0.016012
2021-12-12 00:37:34,706 iteration 6261 : loss : 0.026267, loss_ce: 0.013642
2021-12-12 00:37:37,443 iteration 6262 : loss : 0.026207, loss_ce: 0.012864
2021-12-12 00:37:40,364 iteration 6263 : loss : 0.032702, loss_ce: 0.011840
2021-12-12 00:37:43,110 iteration 6264 : loss : 0.029378, loss_ce: 0.015875
2021-12-12 00:37:45,770 iteration 6265 : loss : 0.025675, loss_ce: 0.013217
2021-12-12 00:37:48,684 iteration 6266 : loss : 0.023507, loss_ce: 0.011971
2021-12-12 00:37:51,431 iteration 6267 : loss : 0.034753, loss_ce: 0.016749
2021-12-12 00:37:54,127 iteration 6268 : loss : 0.036207, loss_ce: 0.016715
2021-12-12 00:37:56,880 iteration 6269 : loss : 0.032898, loss_ce: 0.013582
2021-12-12 00:37:59,656 iteration 6270 : loss : 0.035549, loss_ce: 0.013494
2021-12-12 00:38:02,539 iteration 6271 : loss : 0.036271, loss_ce: 0.011225
2021-12-12 00:38:05,188 iteration 6272 : loss : 0.021253, loss_ce: 0.008777
2021-12-12 00:38:07,899 iteration 6273 : loss : 0.018667, loss_ce: 0.010109
 92%|██████████████████████████▊  | 369/400 [5:15:06<25:20, 49.06s/it]2021-12-12 00:38:10,594 iteration 6274 : loss : 0.033586, loss_ce: 0.013816
2021-12-12 00:38:13,381 iteration 6275 : loss : 0.033819, loss_ce: 0.017327
2021-12-12 00:38:16,233 iteration 6276 : loss : 0.039398, loss_ce: 0.016960
2021-12-12 00:38:19,196 iteration 6277 : loss : 0.051545, loss_ce: 0.016609
2021-12-12 00:38:21,931 iteration 6278 : loss : 0.037125, loss_ce: 0.016783
2021-12-12 00:38:24,936 iteration 6279 : loss : 0.038639, loss_ce: 0.013938
2021-12-12 00:38:27,860 iteration 6280 : loss : 0.028677, loss_ce: 0.015300
2021-12-12 00:38:30,590 iteration 6281 : loss : 0.023657, loss_ce: 0.010550
2021-12-12 00:38:33,409 iteration 6282 : loss : 0.023109, loss_ce: 0.012240
2021-12-12 00:38:36,361 iteration 6283 : loss : 0.042598, loss_ce: 0.015689
2021-12-12 00:38:39,085 iteration 6284 : loss : 0.029860, loss_ce: 0.016405
2021-12-12 00:38:42,051 iteration 6285 : loss : 0.034747, loss_ce: 0.016055
2021-12-12 00:38:44,822 iteration 6286 : loss : 0.027647, loss_ce: 0.015156
2021-12-12 00:38:47,546 iteration 6287 : loss : 0.024316, loss_ce: 0.011108
2021-12-12 00:38:50,530 iteration 6288 : loss : 0.043619, loss_ce: 0.016578
2021-12-12 00:38:53,340 iteration 6289 : loss : 0.029936, loss_ce: 0.012287
2021-12-12 00:38:53,340 Training Data Eval:
2021-12-12 00:39:08,224   Average segmentation loss on training set: 0.0145
2021-12-12 00:39:08,224 Validation Data Eval:
2021-12-12 00:39:13,465   Average segmentation loss on validation set: 0.1005
2021-12-12 00:39:16,134 iteration 6290 : loss : 0.023640, loss_ce: 0.011334
 92%|██████████████████████████▊  | 370/400 [5:16:14<27:24, 54.81s/it]2021-12-12 00:39:18,951 iteration 6291 : loss : 0.029487, loss_ce: 0.010006
2021-12-12 00:39:21,793 iteration 6292 : loss : 0.025922, loss_ce: 0.011649
2021-12-12 00:39:24,498 iteration 6293 : loss : 0.026246, loss_ce: 0.013244
2021-12-12 00:39:27,356 iteration 6294 : loss : 0.030391, loss_ce: 0.013846
2021-12-12 00:39:30,266 iteration 6295 : loss : 0.039223, loss_ce: 0.016574
2021-12-12 00:39:33,025 iteration 6296 : loss : 0.022915, loss_ce: 0.010468
2021-12-12 00:39:35,811 iteration 6297 : loss : 0.051087, loss_ce: 0.016015
2021-12-12 00:39:38,476 iteration 6298 : loss : 0.024844, loss_ce: 0.010707
2021-12-12 00:39:41,418 iteration 6299 : loss : 0.039375, loss_ce: 0.018074
2021-12-12 00:39:44,196 iteration 6300 : loss : 0.029167, loss_ce: 0.014628
2021-12-12 00:39:47,004 iteration 6301 : loss : 0.028163, loss_ce: 0.012417
2021-12-12 00:39:49,776 iteration 6302 : loss : 0.025133, loss_ce: 0.012460
2021-12-12 00:39:52,602 iteration 6303 : loss : 0.032896, loss_ce: 0.016359
2021-12-12 00:39:55,333 iteration 6304 : loss : 0.029236, loss_ce: 0.015198
2021-12-12 00:39:58,125 iteration 6305 : loss : 0.020322, loss_ce: 0.011064
2021-12-12 00:40:00,875 iteration 6306 : loss : 0.025189, loss_ce: 0.011699
2021-12-12 00:40:03,836 iteration 6307 : loss : 0.031187, loss_ce: 0.014240
 93%|██████████████████████████▉  | 371/400 [5:17:02<25:27, 52.68s/it]2021-12-12 00:40:06,721 iteration 6308 : loss : 0.020697, loss_ce: 0.010375
2021-12-12 00:40:09,565 iteration 6309 : loss : 0.036465, loss_ce: 0.019251
2021-12-12 00:40:12,358 iteration 6310 : loss : 0.030478, loss_ce: 0.014849
2021-12-12 00:40:15,129 iteration 6311 : loss : 0.030607, loss_ce: 0.011744
2021-12-12 00:40:18,071 iteration 6312 : loss : 0.025892, loss_ce: 0.012849
2021-12-12 00:40:20,882 iteration 6313 : loss : 0.033013, loss_ce: 0.011798
2021-12-12 00:40:23,661 iteration 6314 : loss : 0.030563, loss_ce: 0.014650
2021-12-12 00:40:26,407 iteration 6315 : loss : 0.031120, loss_ce: 0.014060
2021-12-12 00:40:29,173 iteration 6316 : loss : 0.029034, loss_ce: 0.016880
2021-12-12 00:40:32,106 iteration 6317 : loss : 0.023264, loss_ce: 0.010841
2021-12-12 00:40:34,956 iteration 6318 : loss : 0.030338, loss_ce: 0.015403
2021-12-12 00:40:37,814 iteration 6319 : loss : 0.026906, loss_ce: 0.011427
2021-12-12 00:40:40,654 iteration 6320 : loss : 0.027652, loss_ce: 0.013685
2021-12-12 00:40:43,437 iteration 6321 : loss : 0.033697, loss_ce: 0.016187
2021-12-12 00:40:46,416 iteration 6322 : loss : 0.029776, loss_ce: 0.013361
2021-12-12 00:40:49,050 iteration 6323 : loss : 0.024341, loss_ce: 0.012101
2021-12-12 00:40:51,770 iteration 6324 : loss : 0.031331, loss_ce: 0.012080
 93%|██████████████████████████▉  | 372/400 [5:17:49<23:55, 51.26s/it]2021-12-12 00:40:54,444 iteration 6325 : loss : 0.045558, loss_ce: 0.017819
2021-12-12 00:40:57,080 iteration 6326 : loss : 0.029217, loss_ce: 0.015363
2021-12-12 00:40:59,766 iteration 6327 : loss : 0.021075, loss_ce: 0.010237
2021-12-12 00:41:02,628 iteration 6328 : loss : 0.028496, loss_ce: 0.013695
2021-12-12 00:41:05,649 iteration 6329 : loss : 0.027820, loss_ce: 0.011631
2021-12-12 00:41:08,437 iteration 6330 : loss : 0.023225, loss_ce: 0.011608
2021-12-12 00:41:11,222 iteration 6331 : loss : 0.028537, loss_ce: 0.015159
2021-12-12 00:41:14,036 iteration 6332 : loss : 0.029073, loss_ce: 0.012985
2021-12-12 00:41:16,926 iteration 6333 : loss : 0.031048, loss_ce: 0.014205
2021-12-12 00:41:19,841 iteration 6334 : loss : 0.023353, loss_ce: 0.011637
2021-12-12 00:41:22,817 iteration 6335 : loss : 0.042847, loss_ce: 0.021555
2021-12-12 00:41:25,758 iteration 6336 : loss : 0.026544, loss_ce: 0.012015
2021-12-12 00:41:28,444 iteration 6337 : loss : 0.026147, loss_ce: 0.012334
2021-12-12 00:41:31,235 iteration 6338 : loss : 0.028157, loss_ce: 0.011550
2021-12-12 00:41:34,066 iteration 6339 : loss : 0.033444, loss_ce: 0.013374
2021-12-12 00:41:36,841 iteration 6340 : loss : 0.030673, loss_ce: 0.017222
2021-12-12 00:41:39,622 iteration 6341 : loss : 0.034915, loss_ce: 0.012385
 93%|███████████████████████████  | 373/400 [5:18:37<22:36, 50.23s/it]2021-12-12 00:41:42,535 iteration 6342 : loss : 0.036486, loss_ce: 0.016045
2021-12-12 00:41:45,231 iteration 6343 : loss : 0.029980, loss_ce: 0.012673
2021-12-12 00:41:47,917 iteration 6344 : loss : 0.031572, loss_ce: 0.016229
2021-12-12 00:41:50,737 iteration 6345 : loss : 0.038286, loss_ce: 0.021704
2021-12-12 00:41:53,511 iteration 6346 : loss : 0.027096, loss_ce: 0.010792
2021-12-12 00:41:56,260 iteration 6347 : loss : 0.034074, loss_ce: 0.020732
2021-12-12 00:41:59,246 iteration 6348 : loss : 0.035093, loss_ce: 0.015298
2021-12-12 00:42:01,847 iteration 6349 : loss : 0.023220, loss_ce: 0.011947
2021-12-12 00:42:04,607 iteration 6350 : loss : 0.036653, loss_ce: 0.013160
2021-12-12 00:42:07,479 iteration 6351 : loss : 0.022854, loss_ce: 0.010635
2021-12-12 00:42:10,297 iteration 6352 : loss : 0.026937, loss_ce: 0.012522
2021-12-12 00:42:13,247 iteration 6353 : loss : 0.028444, loss_ce: 0.015220
2021-12-12 00:42:16,062 iteration 6354 : loss : 0.021808, loss_ce: 0.010625
2021-12-12 00:42:18,838 iteration 6355 : loss : 0.026587, loss_ce: 0.014092
2021-12-12 00:42:21,639 iteration 6356 : loss : 0.029179, loss_ce: 0.015365
2021-12-12 00:42:24,366 iteration 6357 : loss : 0.039439, loss_ce: 0.016710
2021-12-12 00:42:27,237 iteration 6358 : loss : 0.037104, loss_ce: 0.016318
 94%|███████████████████████████  | 374/400 [5:19:25<21:25, 49.45s/it]2021-12-12 00:42:30,188 iteration 6359 : loss : 0.033412, loss_ce: 0.013961
2021-12-12 00:42:32,909 iteration 6360 : loss : 0.041632, loss_ce: 0.013769
2021-12-12 00:42:35,611 iteration 6361 : loss : 0.040623, loss_ce: 0.014883
2021-12-12 00:42:38,390 iteration 6362 : loss : 0.041606, loss_ce: 0.021043
2021-12-12 00:42:41,270 iteration 6363 : loss : 0.038098, loss_ce: 0.017869
2021-12-12 00:42:43,913 iteration 6364 : loss : 0.019439, loss_ce: 0.009866
2021-12-12 00:42:46,688 iteration 6365 : loss : 0.028504, loss_ce: 0.013033
2021-12-12 00:42:49,476 iteration 6366 : loss : 0.028787, loss_ce: 0.014565
2021-12-12 00:42:52,358 iteration 6367 : loss : 0.028338, loss_ce: 0.013503
2021-12-12 00:42:55,035 iteration 6368 : loss : 0.028957, loss_ce: 0.014439
2021-12-12 00:42:57,937 iteration 6369 : loss : 0.035591, loss_ce: 0.018512
2021-12-12 00:43:00,726 iteration 6370 : loss : 0.052020, loss_ce: 0.015272
2021-12-12 00:43:03,657 iteration 6371 : loss : 0.037243, loss_ce: 0.016759
2021-12-12 00:43:06,535 iteration 6372 : loss : 0.032677, loss_ce: 0.018676
2021-12-12 00:43:09,152 iteration 6373 : loss : 0.022189, loss_ce: 0.008813
2021-12-12 00:43:12,044 iteration 6374 : loss : 0.025317, loss_ce: 0.012970
2021-12-12 00:43:12,044 Training Data Eval:
2021-12-12 00:43:27,293   Average segmentation loss on training set: 0.0141
2021-12-12 00:43:27,293 Validation Data Eval:
2021-12-12 00:43:32,564   Average segmentation loss on validation set: 0.0849
2021-12-12 00:43:35,229 iteration 6375 : loss : 0.029449, loss_ce: 0.012665
 94%|███████████████████████████▏ | 375/400 [5:20:33<22:55, 55.01s/it]2021-12-12 00:43:38,287 iteration 6376 : loss : 0.042451, loss_ce: 0.018022
2021-12-12 00:43:41,108 iteration 6377 : loss : 0.023850, loss_ce: 0.011456
2021-12-12 00:43:43,793 iteration 6378 : loss : 0.030890, loss_ce: 0.014398
2021-12-12 00:43:46,608 iteration 6379 : loss : 0.045610, loss_ce: 0.016424
2021-12-12 00:43:49,291 iteration 6380 : loss : 0.022747, loss_ce: 0.012259
2021-12-12 00:43:52,033 iteration 6381 : loss : 0.029738, loss_ce: 0.012813
2021-12-12 00:43:54,814 iteration 6382 : loss : 0.039519, loss_ce: 0.019069
2021-12-12 00:43:57,607 iteration 6383 : loss : 0.028193, loss_ce: 0.013572
2021-12-12 00:44:00,339 iteration 6384 : loss : 0.027691, loss_ce: 0.013864
2021-12-12 00:44:03,188 iteration 6385 : loss : 0.031759, loss_ce: 0.015410
2021-12-12 00:44:06,031 iteration 6386 : loss : 0.031078, loss_ce: 0.014373
2021-12-12 00:44:08,783 iteration 6387 : loss : 0.040295, loss_ce: 0.020296
2021-12-12 00:44:11,558 iteration 6388 : loss : 0.020098, loss_ce: 0.009660
2021-12-12 00:44:14,131 iteration 6389 : loss : 0.017882, loss_ce: 0.008276
2021-12-12 00:44:16,999 iteration 6390 : loss : 0.024182, loss_ce: 0.011932
2021-12-12 00:44:19,817 iteration 6391 : loss : 0.061079, loss_ce: 0.016238
2021-12-12 00:44:22,660 iteration 6392 : loss : 0.022722, loss_ce: 0.011656
 94%|███████████████████████████▎ | 376/400 [5:21:20<21:05, 52.74s/it]2021-12-12 00:44:25,406 iteration 6393 : loss : 0.025464, loss_ce: 0.011122
2021-12-12 00:44:28,242 iteration 6394 : loss : 0.037280, loss_ce: 0.014879
2021-12-12 00:44:30,932 iteration 6395 : loss : 0.032670, loss_ce: 0.018639
2021-12-12 00:44:33,878 iteration 6396 : loss : 0.036058, loss_ce: 0.019571
2021-12-12 00:44:36,595 iteration 6397 : loss : 0.028462, loss_ce: 0.014473
2021-12-12 00:44:39,334 iteration 6398 : loss : 0.030487, loss_ce: 0.012619
2021-12-12 00:44:42,061 iteration 6399 : loss : 0.030477, loss_ce: 0.012956
2021-12-12 00:44:45,042 iteration 6400 : loss : 0.031415, loss_ce: 0.016507
2021-12-12 00:44:47,682 iteration 6401 : loss : 0.023282, loss_ce: 0.012001
2021-12-12 00:44:50,314 iteration 6402 : loss : 0.038717, loss_ce: 0.017068
2021-12-12 00:44:53,438 iteration 6403 : loss : 0.042830, loss_ce: 0.018821
2021-12-12 00:44:56,126 iteration 6404 : loss : 0.030506, loss_ce: 0.011940
2021-12-12 00:44:58,799 iteration 6405 : loss : 0.031539, loss_ce: 0.013758
2021-12-12 00:45:01,617 iteration 6406 : loss : 0.028156, loss_ce: 0.014740
2021-12-12 00:45:04,362 iteration 6407 : loss : 0.025766, loss_ce: 0.010916
2021-12-12 00:45:07,116 iteration 6408 : loss : 0.034217, loss_ce: 0.012070
2021-12-12 00:45:09,952 iteration 6409 : loss : 0.026914, loss_ce: 0.013664
 94%|███████████████████████████▎ | 377/400 [5:22:08<19:35, 51.10s/it]2021-12-12 00:45:12,803 iteration 6410 : loss : 0.046992, loss_ce: 0.020149
2021-12-12 00:45:15,676 iteration 6411 : loss : 0.035003, loss_ce: 0.015368
2021-12-12 00:45:18,551 iteration 6412 : loss : 0.024315, loss_ce: 0.012952
2021-12-12 00:45:21,315 iteration 6413 : loss : 0.022389, loss_ce: 0.010376
2021-12-12 00:45:24,066 iteration 6414 : loss : 0.027854, loss_ce: 0.012667
2021-12-12 00:45:26,772 iteration 6415 : loss : 0.030062, loss_ce: 0.012179
2021-12-12 00:45:29,649 iteration 6416 : loss : 0.043635, loss_ce: 0.014463
2021-12-12 00:45:32,490 iteration 6417 : loss : 0.032465, loss_ce: 0.017463
2021-12-12 00:45:35,302 iteration 6418 : loss : 0.029614, loss_ce: 0.013151
2021-12-12 00:45:38,160 iteration 6419 : loss : 0.029639, loss_ce: 0.012192
2021-12-12 00:45:40,913 iteration 6420 : loss : 0.049377, loss_ce: 0.017715
2021-12-12 00:45:43,603 iteration 6421 : loss : 0.028017, loss_ce: 0.014216
2021-12-12 00:45:46,285 iteration 6422 : loss : 0.023636, loss_ce: 0.012903
2021-12-12 00:45:49,348 iteration 6423 : loss : 0.036191, loss_ce: 0.014529
2021-12-12 00:45:52,192 iteration 6424 : loss : 0.036723, loss_ce: 0.018735
2021-12-12 00:45:54,794 iteration 6425 : loss : 0.026759, loss_ce: 0.012658
2021-12-12 00:45:57,754 iteration 6426 : loss : 0.031330, loss_ce: 0.014643
 94%|███████████████████████████▍ | 378/400 [5:22:55<18:22, 50.11s/it]2021-12-12 00:46:00,724 iteration 6427 : loss : 0.030280, loss_ce: 0.014894
2021-12-12 00:46:03,370 iteration 6428 : loss : 0.025796, loss_ce: 0.014197
2021-12-12 00:46:06,064 iteration 6429 : loss : 0.037195, loss_ce: 0.014576
2021-12-12 00:46:08,725 iteration 6430 : loss : 0.033231, loss_ce: 0.012316
2021-12-12 00:46:11,387 iteration 6431 : loss : 0.031526, loss_ce: 0.013895
2021-12-12 00:46:14,340 iteration 6432 : loss : 0.025228, loss_ce: 0.011568
2021-12-12 00:46:17,114 iteration 6433 : loss : 0.032761, loss_ce: 0.014294
2021-12-12 00:46:19,875 iteration 6434 : loss : 0.020822, loss_ce: 0.009842
2021-12-12 00:46:22,541 iteration 6435 : loss : 0.025769, loss_ce: 0.014721
2021-12-12 00:46:25,208 iteration 6436 : loss : 0.032473, loss_ce: 0.015398
2021-12-12 00:46:28,129 iteration 6437 : loss : 0.050377, loss_ce: 0.015548
2021-12-12 00:46:31,091 iteration 6438 : loss : 0.028468, loss_ce: 0.014747
2021-12-12 00:46:34,001 iteration 6439 : loss : 0.035288, loss_ce: 0.015695
2021-12-12 00:46:36,846 iteration 6440 : loss : 0.036259, loss_ce: 0.017485
2021-12-12 00:46:39,538 iteration 6441 : loss : 0.029604, loss_ce: 0.011997
2021-12-12 00:46:42,412 iteration 6442 : loss : 0.028640, loss_ce: 0.014695
2021-12-12 00:46:45,187 iteration 6443 : loss : 0.040854, loss_ce: 0.019835
 95%|███████████████████████████▍ | 379/400 [5:23:43<17:15, 49.31s/it]2021-12-12 00:46:48,162 iteration 6444 : loss : 0.027397, loss_ce: 0.015360
2021-12-12 00:46:50,922 iteration 6445 : loss : 0.028190, loss_ce: 0.013142
2021-12-12 00:46:53,809 iteration 6446 : loss : 0.028570, loss_ce: 0.011072
2021-12-12 00:46:56,478 iteration 6447 : loss : 0.039356, loss_ce: 0.024089
2021-12-12 00:46:59,354 iteration 6448 : loss : 0.026981, loss_ce: 0.014472
2021-12-12 00:47:02,070 iteration 6449 : loss : 0.026521, loss_ce: 0.011836
2021-12-12 00:47:04,864 iteration 6450 : loss : 0.021557, loss_ce: 0.011740
2021-12-12 00:47:07,711 iteration 6451 : loss : 0.032181, loss_ce: 0.013045
2021-12-12 00:47:10,533 iteration 6452 : loss : 0.029858, loss_ce: 0.016157
2021-12-12 00:47:13,327 iteration 6453 : loss : 0.027785, loss_ce: 0.013836
2021-12-12 00:47:16,014 iteration 6454 : loss : 0.026889, loss_ce: 0.011243
2021-12-12 00:47:18,842 iteration 6455 : loss : 0.026442, loss_ce: 0.010763
2021-12-12 00:47:21,808 iteration 6456 : loss : 0.035427, loss_ce: 0.017299
2021-12-12 00:47:24,793 iteration 6457 : loss : 0.023608, loss_ce: 0.010152
2021-12-12 00:47:27,416 iteration 6458 : loss : 0.027378, loss_ce: 0.010308
2021-12-12 00:47:30,296 iteration 6459 : loss : 0.028688, loss_ce: 0.012705
2021-12-12 00:47:30,296 Training Data Eval:
2021-12-12 00:47:45,302   Average segmentation loss on training set: 0.0140
2021-12-12 00:47:45,302 Validation Data Eval:
2021-12-12 00:47:50,584   Average segmentation loss on validation set: 0.0951
2021-12-12 00:47:53,375 iteration 6460 : loss : 0.051509, loss_ce: 0.017221
 95%|███████████████████████████▌ | 380/400 [5:24:51<18:19, 54.97s/it]2021-12-12 00:47:56,226 iteration 6461 : loss : 0.039676, loss_ce: 0.013475
2021-12-12 00:47:59,176 iteration 6462 : loss : 0.027041, loss_ce: 0.011217
2021-12-12 00:48:01,938 iteration 6463 : loss : 0.025066, loss_ce: 0.010893
2021-12-12 00:48:04,729 iteration 6464 : loss : 0.023745, loss_ce: 0.011487
2021-12-12 00:48:07,594 iteration 6465 : loss : 0.035345, loss_ce: 0.014736
2021-12-12 00:48:10,463 iteration 6466 : loss : 0.031596, loss_ce: 0.013885
2021-12-12 00:48:13,269 iteration 6467 : loss : 0.030179, loss_ce: 0.015146
2021-12-12 00:48:16,209 iteration 6468 : loss : 0.032439, loss_ce: 0.017171
2021-12-12 00:48:19,057 iteration 6469 : loss : 0.037985, loss_ce: 0.019121
2021-12-12 00:48:21,825 iteration 6470 : loss : 0.037093, loss_ce: 0.013992
2021-12-12 00:48:24,535 iteration 6471 : loss : 0.026522, loss_ce: 0.011886
2021-12-12 00:48:27,228 iteration 6472 : loss : 0.028244, loss_ce: 0.011432
2021-12-12 00:48:29,863 iteration 6473 : loss : 0.028108, loss_ce: 0.015905
2021-12-12 00:48:32,681 iteration 6474 : loss : 0.038628, loss_ce: 0.023779
2021-12-12 00:48:35,516 iteration 6475 : loss : 0.018590, loss_ce: 0.009226
2021-12-12 00:48:38,254 iteration 6476 : loss : 0.022419, loss_ce: 0.010622
2021-12-12 00:48:40,960 iteration 6477 : loss : 0.020847, loss_ce: 0.010019
 95%|███████████████████████████▌ | 381/400 [5:25:39<16:42, 52.76s/it]2021-12-12 00:48:43,941 iteration 6478 : loss : 0.040885, loss_ce: 0.020038
2021-12-12 00:48:46,926 iteration 6479 : loss : 0.035349, loss_ce: 0.022484
2021-12-12 00:48:49,974 iteration 6480 : loss : 0.041708, loss_ce: 0.014217
2021-12-12 00:48:52,753 iteration 6481 : loss : 0.035420, loss_ce: 0.016277
2021-12-12 00:48:55,525 iteration 6482 : loss : 0.036038, loss_ce: 0.016806
2021-12-12 00:48:58,387 iteration 6483 : loss : 0.029433, loss_ce: 0.013168
2021-12-12 00:49:01,130 iteration 6484 : loss : 0.037248, loss_ce: 0.015041
2021-12-12 00:49:03,922 iteration 6485 : loss : 0.040647, loss_ce: 0.019703
2021-12-12 00:49:06,984 iteration 6486 : loss : 0.025592, loss_ce: 0.011470
2021-12-12 00:49:09,852 iteration 6487 : loss : 0.030475, loss_ce: 0.012961
2021-12-12 00:49:12,572 iteration 6488 : loss : 0.021491, loss_ce: 0.010931
2021-12-12 00:49:15,551 iteration 6489 : loss : 0.027957, loss_ce: 0.010348
2021-12-12 00:49:18,157 iteration 6490 : loss : 0.029526, loss_ce: 0.014917
2021-12-12 00:49:20,980 iteration 6491 : loss : 0.036416, loss_ce: 0.014024
2021-12-12 00:49:23,696 iteration 6492 : loss : 0.024720, loss_ce: 0.011868
2021-12-12 00:49:26,455 iteration 6493 : loss : 0.039425, loss_ce: 0.015934
2021-12-12 00:49:29,397 iteration 6494 : loss : 0.030259, loss_ce: 0.012611
 96%|███████████████████████████▋ | 382/400 [5:26:27<15:26, 51.46s/it]2021-12-12 00:49:32,387 iteration 6495 : loss : 0.026007, loss_ce: 0.009791
2021-12-12 00:49:35,245 iteration 6496 : loss : 0.023431, loss_ce: 0.011426
2021-12-12 00:49:38,037 iteration 6497 : loss : 0.024541, loss_ce: 0.011898
2021-12-12 00:49:40,878 iteration 6498 : loss : 0.050533, loss_ce: 0.030045
2021-12-12 00:49:43,766 iteration 6499 : loss : 0.035369, loss_ce: 0.014545
2021-12-12 00:49:46,722 iteration 6500 : loss : 0.037370, loss_ce: 0.016307
2021-12-12 00:49:49,604 iteration 6501 : loss : 0.030622, loss_ce: 0.011610
2021-12-12 00:49:52,420 iteration 6502 : loss : 0.030046, loss_ce: 0.014120
2021-12-12 00:49:55,227 iteration 6503 : loss : 0.031121, loss_ce: 0.014595
2021-12-12 00:49:57,976 iteration 6504 : loss : 0.026261, loss_ce: 0.015773
2021-12-12 00:50:00,667 iteration 6505 : loss : 0.025488, loss_ce: 0.010830
2021-12-12 00:50:03,371 iteration 6506 : loss : 0.026579, loss_ce: 0.011950
2021-12-12 00:50:06,113 iteration 6507 : loss : 0.027272, loss_ce: 0.013075
2021-12-12 00:50:09,003 iteration 6508 : loss : 0.029875, loss_ce: 0.012118
2021-12-12 00:50:11,870 iteration 6509 : loss : 0.027220, loss_ce: 0.014282
2021-12-12 00:50:14,536 iteration 6510 : loss : 0.027614, loss_ce: 0.012695
2021-12-12 00:50:17,273 iteration 6511 : loss : 0.031379, loss_ce: 0.015777
 96%|███████████████████████████▊ | 383/400 [5:27:15<14:16, 50.38s/it]2021-12-12 00:50:20,110 iteration 6512 : loss : 0.026870, loss_ce: 0.012696
2021-12-12 00:50:22,782 iteration 6513 : loss : 0.029345, loss_ce: 0.012971
2021-12-12 00:50:25,442 iteration 6514 : loss : 0.029558, loss_ce: 0.010156
2021-12-12 00:50:28,214 iteration 6515 : loss : 0.046085, loss_ce: 0.018095
2021-12-12 00:50:31,019 iteration 6516 : loss : 0.031846, loss_ce: 0.012618
2021-12-12 00:50:34,020 iteration 6517 : loss : 0.046008, loss_ce: 0.021883
2021-12-12 00:50:36,752 iteration 6518 : loss : 0.043228, loss_ce: 0.020367
2021-12-12 00:50:39,646 iteration 6519 : loss : 0.051691, loss_ce: 0.020432
2021-12-12 00:50:42,397 iteration 6520 : loss : 0.023101, loss_ce: 0.012649
2021-12-12 00:50:45,253 iteration 6521 : loss : 0.022549, loss_ce: 0.011755
2021-12-12 00:50:48,074 iteration 6522 : loss : 0.030263, loss_ce: 0.014506
2021-12-12 00:50:50,831 iteration 6523 : loss : 0.034190, loss_ce: 0.015242
2021-12-12 00:50:53,602 iteration 6524 : loss : 0.032689, loss_ce: 0.014634
2021-12-12 00:50:56,356 iteration 6525 : loss : 0.025201, loss_ce: 0.011637
2021-12-12 00:50:59,102 iteration 6526 : loss : 0.026347, loss_ce: 0.012915
2021-12-12 00:51:02,072 iteration 6527 : loss : 0.025806, loss_ce: 0.011092
2021-12-12 00:51:04,751 iteration 6528 : loss : 0.028048, loss_ce: 0.014652
 96%|███████████████████████████▊ | 384/400 [5:28:02<13:12, 49.51s/it]2021-12-12 00:51:07,534 iteration 6529 : loss : 0.022816, loss_ce: 0.011283
2021-12-12 00:51:10,252 iteration 6530 : loss : 0.021905, loss_ce: 0.011989
2021-12-12 00:51:12,913 iteration 6531 : loss : 0.035750, loss_ce: 0.010758
2021-12-12 00:51:15,747 iteration 6532 : loss : 0.025791, loss_ce: 0.010787
2021-12-12 00:51:18,628 iteration 6533 : loss : 0.028469, loss_ce: 0.013810
2021-12-12 00:51:21,324 iteration 6534 : loss : 0.038313, loss_ce: 0.014284
2021-12-12 00:51:24,156 iteration 6535 : loss : 0.030801, loss_ce: 0.014859
2021-12-12 00:51:27,067 iteration 6536 : loss : 0.035300, loss_ce: 0.015941
2021-12-12 00:51:29,945 iteration 6537 : loss : 0.037154, loss_ce: 0.014632
2021-12-12 00:51:32,618 iteration 6538 : loss : 0.022372, loss_ce: 0.010136
2021-12-12 00:51:35,663 iteration 6539 : loss : 0.038733, loss_ce: 0.018710
2021-12-12 00:51:38,439 iteration 6540 : loss : 0.026474, loss_ce: 0.014246
2021-12-12 00:51:41,191 iteration 6541 : loss : 0.023170, loss_ce: 0.011715
2021-12-12 00:51:43,974 iteration 6542 : loss : 0.034899, loss_ce: 0.016030
2021-12-12 00:51:46,750 iteration 6543 : loss : 0.032633, loss_ce: 0.013044
2021-12-12 00:51:49,593 iteration 6544 : loss : 0.036504, loss_ce: 0.017065
2021-12-12 00:51:49,593 Training Data Eval:
2021-12-12 00:52:04,563   Average segmentation loss on training set: 0.0139
2021-12-12 00:52:04,563 Validation Data Eval:
2021-12-12 00:52:09,705   Average segmentation loss on validation set: 0.0960
2021-12-12 00:52:12,476 iteration 6545 : loss : 0.022156, loss_ce: 0.008939
 96%|███████████████████████████▉ | 385/400 [5:29:10<13:44, 54.98s/it]2021-12-12 00:52:15,384 iteration 6546 : loss : 0.037439, loss_ce: 0.014986
2021-12-12 00:52:18,127 iteration 6547 : loss : 0.024553, loss_ce: 0.009487
2021-12-12 00:52:20,947 iteration 6548 : loss : 0.048411, loss_ce: 0.016675
2021-12-12 00:52:23,964 iteration 6549 : loss : 0.041621, loss_ce: 0.014876
2021-12-12 00:52:26,755 iteration 6550 : loss : 0.030040, loss_ce: 0.016201
2021-12-12 00:52:29,537 iteration 6551 : loss : 0.037505, loss_ce: 0.019186
2021-12-12 00:52:32,554 iteration 6552 : loss : 0.043489, loss_ce: 0.019248
2021-12-12 00:52:35,352 iteration 6553 : loss : 0.028234, loss_ce: 0.013307
2021-12-12 00:52:38,265 iteration 6554 : loss : 0.040947, loss_ce: 0.015189
2021-12-12 00:52:41,010 iteration 6555 : loss : 0.029664, loss_ce: 0.013922
2021-12-12 00:52:43,794 iteration 6556 : loss : 0.037189, loss_ce: 0.018097
2021-12-12 00:52:46,494 iteration 6557 : loss : 0.020561, loss_ce: 0.010703
2021-12-12 00:52:49,343 iteration 6558 : loss : 0.035140, loss_ce: 0.016057
2021-12-12 00:52:52,028 iteration 6559 : loss : 0.021836, loss_ce: 0.011132
2021-12-12 00:52:54,919 iteration 6560 : loss : 0.027553, loss_ce: 0.015026
2021-12-12 00:52:57,698 iteration 6561 : loss : 0.030207, loss_ce: 0.013504
2021-12-12 00:53:00,627 iteration 6562 : loss : 0.039548, loss_ce: 0.019280
 96%|███████████████████████████▉ | 386/400 [5:29:58<12:20, 52.93s/it]2021-12-12 00:53:03,490 iteration 6563 : loss : 0.030049, loss_ce: 0.016865
2021-12-12 00:53:06,280 iteration 6564 : loss : 0.034719, loss_ce: 0.013306
2021-12-12 00:53:09,049 iteration 6565 : loss : 0.029852, loss_ce: 0.011089
2021-12-12 00:53:11,892 iteration 6566 : loss : 0.048533, loss_ce: 0.020533
2021-12-12 00:53:14,520 iteration 6567 : loss : 0.037572, loss_ce: 0.014215
2021-12-12 00:53:17,258 iteration 6568 : loss : 0.027120, loss_ce: 0.013582
2021-12-12 00:53:20,032 iteration 6569 : loss : 0.034523, loss_ce: 0.012989
2021-12-12 00:53:22,761 iteration 6570 : loss : 0.031363, loss_ce: 0.015188
2021-12-12 00:53:25,595 iteration 6571 : loss : 0.022960, loss_ce: 0.011769
2021-12-12 00:53:28,363 iteration 6572 : loss : 0.027830, loss_ce: 0.012419
2021-12-12 00:53:31,097 iteration 6573 : loss : 0.024026, loss_ce: 0.010595
2021-12-12 00:53:34,041 iteration 6574 : loss : 0.033390, loss_ce: 0.017115
2021-12-12 00:53:36,689 iteration 6575 : loss : 0.019726, loss_ce: 0.010934
2021-12-12 00:53:39,536 iteration 6576 : loss : 0.034979, loss_ce: 0.016696
2021-12-12 00:53:42,241 iteration 6577 : loss : 0.028825, loss_ce: 0.012712
2021-12-12 00:53:45,074 iteration 6578 : loss : 0.031747, loss_ce: 0.015376
2021-12-12 00:53:47,874 iteration 6579 : loss : 0.028871, loss_ce: 0.011098
 97%|████████████████████████████ | 387/400 [5:30:46<11:05, 51.23s/it]2021-12-12 00:53:50,635 iteration 6580 : loss : 0.029359, loss_ce: 0.013031
2021-12-12 00:53:53,525 iteration 6581 : loss : 0.036849, loss_ce: 0.013878
2021-12-12 00:53:56,426 iteration 6582 : loss : 0.034541, loss_ce: 0.017017
2021-12-12 00:53:59,204 iteration 6583 : loss : 0.026009, loss_ce: 0.012852
2021-12-12 00:54:01,872 iteration 6584 : loss : 0.041416, loss_ce: 0.017144
2021-12-12 00:54:04,652 iteration 6585 : loss : 0.027513, loss_ce: 0.011285
2021-12-12 00:54:07,589 iteration 6586 : loss : 0.029701, loss_ce: 0.014947
2021-12-12 00:54:10,203 iteration 6587 : loss : 0.028366, loss_ce: 0.013819
2021-12-12 00:54:12,969 iteration 6588 : loss : 0.032120, loss_ce: 0.015147
2021-12-12 00:54:15,750 iteration 6589 : loss : 0.026091, loss_ce: 0.011074
2021-12-12 00:54:18,624 iteration 6590 : loss : 0.026309, loss_ce: 0.013214
2021-12-12 00:54:21,330 iteration 6591 : loss : 0.035383, loss_ce: 0.018546
2021-12-12 00:54:24,222 iteration 6592 : loss : 0.024246, loss_ce: 0.010280
2021-12-12 00:54:27,029 iteration 6593 : loss : 0.041529, loss_ce: 0.020291
2021-12-12 00:54:29,715 iteration 6594 : loss : 0.021647, loss_ce: 0.011006
2021-12-12 00:54:32,405 iteration 6595 : loss : 0.024651, loss_ce: 0.010207
2021-12-12 00:54:35,238 iteration 6596 : loss : 0.016861, loss_ce: 0.008998
 97%|████████████████████████████▏| 388/400 [5:31:33<10:00, 50.07s/it]2021-12-12 00:54:38,163 iteration 6597 : loss : 0.042651, loss_ce: 0.022077
2021-12-12 00:54:41,047 iteration 6598 : loss : 0.038448, loss_ce: 0.014067
2021-12-12 00:54:43,656 iteration 6599 : loss : 0.025467, loss_ce: 0.011643
2021-12-12 00:54:46,465 iteration 6600 : loss : 0.032135, loss_ce: 0.014027
2021-12-12 00:54:49,210 iteration 6601 : loss : 0.021973, loss_ce: 0.010943
2021-12-12 00:54:51,847 iteration 6602 : loss : 0.020774, loss_ce: 0.009686
2021-12-12 00:54:54,692 iteration 6603 : loss : 0.043558, loss_ce: 0.017514
2021-12-12 00:54:57,416 iteration 6604 : loss : 0.028438, loss_ce: 0.013135
2021-12-12 00:55:00,309 iteration 6605 : loss : 0.049128, loss_ce: 0.019279
2021-12-12 00:55:03,102 iteration 6606 : loss : 0.030096, loss_ce: 0.016833
2021-12-12 00:55:05,810 iteration 6607 : loss : 0.024950, loss_ce: 0.010506
2021-12-12 00:55:08,727 iteration 6608 : loss : 0.023944, loss_ce: 0.011418
2021-12-12 00:55:11,716 iteration 6609 : loss : 0.026480, loss_ce: 0.013614
2021-12-12 00:55:14,414 iteration 6610 : loss : 0.040660, loss_ce: 0.015489
2021-12-12 00:55:17,182 iteration 6611 : loss : 0.025021, loss_ce: 0.013364
2021-12-12 00:55:20,072 iteration 6612 : loss : 0.027294, loss_ce: 0.013059
2021-12-12 00:55:22,883 iteration 6613 : loss : 0.038667, loss_ce: 0.013142
 97%|████████████████████████████▏| 389/400 [5:32:21<09:02, 49.34s/it]2021-12-12 00:55:25,616 iteration 6614 : loss : 0.024986, loss_ce: 0.011470
2021-12-12 00:55:28,427 iteration 6615 : loss : 0.046857, loss_ce: 0.021080
2021-12-12 00:55:31,364 iteration 6616 : loss : 0.027127, loss_ce: 0.013540
2021-12-12 00:55:34,076 iteration 6617 : loss : 0.033183, loss_ce: 0.010818
2021-12-12 00:55:36,950 iteration 6618 : loss : 0.026041, loss_ce: 0.012142
2021-12-12 00:55:39,600 iteration 6619 : loss : 0.030017, loss_ce: 0.011975
2021-12-12 00:55:42,244 iteration 6620 : loss : 0.040421, loss_ce: 0.015403
2021-12-12 00:55:44,982 iteration 6621 : loss : 0.034142, loss_ce: 0.017704
2021-12-12 00:55:47,561 iteration 6622 : loss : 0.018508, loss_ce: 0.009139
2021-12-12 00:55:50,445 iteration 6623 : loss : 0.028042, loss_ce: 0.016043
2021-12-12 00:55:53,064 iteration 6624 : loss : 0.019340, loss_ce: 0.009722
2021-12-12 00:55:55,943 iteration 6625 : loss : 0.019798, loss_ce: 0.010514
2021-12-12 00:55:58,726 iteration 6626 : loss : 0.029872, loss_ce: 0.014163
2021-12-12 00:56:01,523 iteration 6627 : loss : 0.021143, loss_ce: 0.010388
2021-12-12 00:56:04,370 iteration 6628 : loss : 0.030870, loss_ce: 0.013594
2021-12-12 00:56:07,184 iteration 6629 : loss : 0.023956, loss_ce: 0.012016
2021-12-12 00:56:07,184 Training Data Eval:
2021-12-12 00:56:22,185   Average segmentation loss on training set: 0.0138
2021-12-12 00:56:22,185 Validation Data Eval:
2021-12-12 00:56:27,363   Average segmentation loss on validation set: 0.0910
2021-12-12 00:56:30,122 iteration 6630 : loss : 0.037989, loss_ce: 0.016567
 98%|████████████████████████████▎| 390/400 [5:33:28<09:07, 54.71s/it]2021-12-12 00:56:32,726 iteration 6631 : loss : 0.020559, loss_ce: 0.010571
2021-12-12 00:56:35,673 iteration 6632 : loss : 0.035209, loss_ce: 0.011662
2021-12-12 00:56:38,421 iteration 6633 : loss : 0.025155, loss_ce: 0.011284
2021-12-12 00:56:41,157 iteration 6634 : loss : 0.022642, loss_ce: 0.011195
2021-12-12 00:56:44,086 iteration 6635 : loss : 0.022298, loss_ce: 0.010799
2021-12-12 00:56:46,948 iteration 6636 : loss : 0.024743, loss_ce: 0.014678
2021-12-12 00:56:49,785 iteration 6637 : loss : 0.037201, loss_ce: 0.015809
2021-12-12 00:56:52,698 iteration 6638 : loss : 0.021686, loss_ce: 0.009740
2021-12-12 00:56:55,345 iteration 6639 : loss : 0.048849, loss_ce: 0.022111
2021-12-12 00:56:58,052 iteration 6640 : loss : 0.043630, loss_ce: 0.015332
2021-12-12 00:57:00,857 iteration 6641 : loss : 0.028244, loss_ce: 0.013199
2021-12-12 00:57:03,713 iteration 6642 : loss : 0.025159, loss_ce: 0.010841
2021-12-12 00:57:06,636 iteration 6643 : loss : 0.043657, loss_ce: 0.022344
2021-12-12 00:57:09,425 iteration 6644 : loss : 0.037816, loss_ce: 0.018807
2021-12-12 00:57:12,158 iteration 6645 : loss : 0.032043, loss_ce: 0.016059
2021-12-12 00:57:14,839 iteration 6646 : loss : 0.032382, loss_ce: 0.016722
2021-12-12 00:57:17,498 iteration 6647 : loss : 0.043265, loss_ce: 0.012701
 98%|████████████████████████████▎| 391/400 [5:34:15<07:52, 52.51s/it]2021-12-12 00:57:20,500 iteration 6648 : loss : 0.033549, loss_ce: 0.013905
2021-12-12 00:57:23,367 iteration 6649 : loss : 0.032538, loss_ce: 0.012064
2021-12-12 00:57:26,052 iteration 6650 : loss : 0.024581, loss_ce: 0.011564
2021-12-12 00:57:28,725 iteration 6651 : loss : 0.034473, loss_ce: 0.016527
2021-12-12 00:57:31,393 iteration 6652 : loss : 0.034149, loss_ce: 0.014519
2021-12-12 00:57:34,344 iteration 6653 : loss : 0.037647, loss_ce: 0.014438
2021-12-12 00:57:37,246 iteration 6654 : loss : 0.036886, loss_ce: 0.014924
2021-12-12 00:57:39,997 iteration 6655 : loss : 0.028327, loss_ce: 0.014030
2021-12-12 00:57:42,824 iteration 6656 : loss : 0.039451, loss_ce: 0.016994
2021-12-12 00:57:45,738 iteration 6657 : loss : 0.033074, loss_ce: 0.012806
2021-12-12 00:57:48,562 iteration 6658 : loss : 0.030882, loss_ce: 0.013705
2021-12-12 00:57:51,270 iteration 6659 : loss : 0.034580, loss_ce: 0.013231
2021-12-12 00:57:54,099 iteration 6660 : loss : 0.028746, loss_ce: 0.013486
2021-12-12 00:57:56,827 iteration 6661 : loss : 0.023989, loss_ce: 0.011289
2021-12-12 00:57:59,571 iteration 6662 : loss : 0.032012, loss_ce: 0.013879
2021-12-12 00:58:02,353 iteration 6663 : loss : 0.021640, loss_ce: 0.011144
2021-12-12 00:58:05,209 iteration 6664 : loss : 0.024556, loss_ce: 0.013373
 98%|████████████████████████████▍| 392/400 [5:35:03<06:48, 51.07s/it]2021-12-12 00:58:08,042 iteration 6665 : loss : 0.057622, loss_ce: 0.020741
2021-12-12 00:58:10,895 iteration 6666 : loss : 0.019900, loss_ce: 0.008837
2021-12-12 00:58:13,790 iteration 6667 : loss : 0.029877, loss_ce: 0.013422
2021-12-12 00:58:16,658 iteration 6668 : loss : 0.039042, loss_ce: 0.021198
2021-12-12 00:58:19,428 iteration 6669 : loss : 0.030627, loss_ce: 0.012857
2021-12-12 00:58:22,105 iteration 6670 : loss : 0.023941, loss_ce: 0.012363
2021-12-12 00:58:24,816 iteration 6671 : loss : 0.031912, loss_ce: 0.014021
2021-12-12 00:58:27,500 iteration 6672 : loss : 0.021759, loss_ce: 0.009721
2021-12-12 00:58:30,355 iteration 6673 : loss : 0.033673, loss_ce: 0.014633
2021-12-12 00:58:33,122 iteration 6674 : loss : 0.026078, loss_ce: 0.013194
2021-12-12 00:58:35,968 iteration 6675 : loss : 0.029404, loss_ce: 0.012161
2021-12-12 00:58:38,588 iteration 6676 : loss : 0.024398, loss_ce: 0.010337
2021-12-12 00:58:41,409 iteration 6677 : loss : 0.034068, loss_ce: 0.012076
2021-12-12 00:58:44,055 iteration 6678 : loss : 0.020496, loss_ce: 0.010850
2021-12-12 00:58:46,983 iteration 6679 : loss : 0.032582, loss_ce: 0.016925
2021-12-12 00:58:49,752 iteration 6680 : loss : 0.034354, loss_ce: 0.014488
2021-12-12 00:58:52,603 iteration 6681 : loss : 0.034743, loss_ce: 0.018348
 98%|████████████████████████████▍| 393/400 [5:35:50<05:49, 49.97s/it]2021-12-12 00:58:55,393 iteration 6682 : loss : 0.021065, loss_ce: 0.011556
2021-12-12 00:58:58,121 iteration 6683 : loss : 0.022981, loss_ce: 0.010374
2021-12-12 00:59:00,701 iteration 6684 : loss : 0.023411, loss_ce: 0.011001
2021-12-12 00:59:03,562 iteration 6685 : loss : 0.042347, loss_ce: 0.022033
2021-12-12 00:59:06,253 iteration 6686 : loss : 0.027048, loss_ce: 0.016292
2021-12-12 00:59:08,996 iteration 6687 : loss : 0.066387, loss_ce: 0.022309
2021-12-12 00:59:11,961 iteration 6688 : loss : 0.029933, loss_ce: 0.012975
2021-12-12 00:59:14,629 iteration 6689 : loss : 0.028510, loss_ce: 0.012833
2021-12-12 00:59:17,253 iteration 6690 : loss : 0.039191, loss_ce: 0.017958
2021-12-12 00:59:20,160 iteration 6691 : loss : 0.033652, loss_ce: 0.014450
2021-12-12 00:59:22,900 iteration 6692 : loss : 0.033897, loss_ce: 0.014614
2021-12-12 00:59:25,696 iteration 6693 : loss : 0.026825, loss_ce: 0.011101
2021-12-12 00:59:28,472 iteration 6694 : loss : 0.033581, loss_ce: 0.015507
2021-12-12 00:59:31,368 iteration 6695 : loss : 0.036176, loss_ce: 0.018234
2021-12-12 00:59:34,012 iteration 6696 : loss : 0.027829, loss_ce: 0.014763
2021-12-12 00:59:36,686 iteration 6697 : loss : 0.030378, loss_ce: 0.013865
2021-12-12 00:59:39,593 iteration 6698 : loss : 0.033790, loss_ce: 0.014558
 98%|████████████████████████████▌| 394/400 [5:36:37<04:54, 49.07s/it]2021-12-12 00:59:42,403 iteration 6699 : loss : 0.031456, loss_ce: 0.018768
2021-12-12 00:59:45,159 iteration 6700 : loss : 0.032089, loss_ce: 0.017092
2021-12-12 00:59:47,977 iteration 6701 : loss : 0.030019, loss_ce: 0.014664
2021-12-12 00:59:50,689 iteration 6702 : loss : 0.053005, loss_ce: 0.021557
2021-12-12 00:59:53,460 iteration 6703 : loss : 0.025830, loss_ce: 0.010573
2021-12-12 00:59:56,426 iteration 6704 : loss : 0.034964, loss_ce: 0.014550
2021-12-12 00:59:59,223 iteration 6705 : loss : 0.041956, loss_ce: 0.015776
2021-12-12 01:00:01,974 iteration 6706 : loss : 0.023636, loss_ce: 0.013491
2021-12-12 01:00:04,683 iteration 6707 : loss : 0.035877, loss_ce: 0.013142
2021-12-12 01:00:07,276 iteration 6708 : loss : 0.022710, loss_ce: 0.010650
2021-12-12 01:00:10,084 iteration 6709 : loss : 0.026742, loss_ce: 0.010406
2021-12-12 01:00:12,711 iteration 6710 : loss : 0.031726, loss_ce: 0.013089
2021-12-12 01:00:15,486 iteration 6711 : loss : 0.040457, loss_ce: 0.012762
2021-12-12 01:00:18,316 iteration 6712 : loss : 0.040737, loss_ce: 0.019343
2021-12-12 01:00:21,131 iteration 6713 : loss : 0.029787, loss_ce: 0.012719
2021-12-12 01:00:24,038 iteration 6714 : loss : 0.027278, loss_ce: 0.014866
2021-12-12 01:00:24,038 Training Data Eval:
2021-12-12 01:00:38,825   Average segmentation loss on training set: 0.0134
2021-12-12 01:00:38,825 Validation Data Eval:
2021-12-12 01:00:43,996   Average segmentation loss on validation set: 0.0934
2021-12-12 01:00:46,962 iteration 6715 : loss : 0.027385, loss_ce: 0.012081
 99%|████████████████████████████▋| 395/400 [5:37:45<04:32, 54.56s/it]2021-12-12 01:00:49,813 iteration 6716 : loss : 0.016155, loss_ce: 0.008616
2021-12-12 01:00:52,504 iteration 6717 : loss : 0.031903, loss_ce: 0.013359
2021-12-12 01:00:55,102 iteration 6718 : loss : 0.025300, loss_ce: 0.011075
2021-12-12 01:00:57,809 iteration 6719 : loss : 0.026162, loss_ce: 0.012175
2021-12-12 01:01:00,465 iteration 6720 : loss : 0.022566, loss_ce: 0.011220
2021-12-12 01:01:03,277 iteration 6721 : loss : 0.031825, loss_ce: 0.013929
2021-12-12 01:01:06,244 iteration 6722 : loss : 0.042910, loss_ce: 0.014487
2021-12-12 01:01:08,879 iteration 6723 : loss : 0.034172, loss_ce: 0.013887
2021-12-12 01:01:11,558 iteration 6724 : loss : 0.032312, loss_ce: 0.016923
2021-12-12 01:01:14,215 iteration 6725 : loss : 0.027148, loss_ce: 0.013881
2021-12-12 01:01:16,875 iteration 6726 : loss : 0.040746, loss_ce: 0.015935
2021-12-12 01:01:19,552 iteration 6727 : loss : 0.032498, loss_ce: 0.016364
2021-12-12 01:01:22,333 iteration 6728 : loss : 0.030870, loss_ce: 0.016669
2021-12-12 01:01:25,007 iteration 6729 : loss : 0.026172, loss_ce: 0.011720
2021-12-12 01:01:27,813 iteration 6730 : loss : 0.024801, loss_ce: 0.011770
2021-12-12 01:01:30,589 iteration 6731 : loss : 0.029347, loss_ce: 0.016942
2021-12-12 01:01:33,331 iteration 6732 : loss : 0.038733, loss_ce: 0.021615
 99%|████████████████████████████▋| 396/400 [5:38:31<03:28, 52.10s/it]2021-12-12 01:01:36,323 iteration 6733 : loss : 0.035406, loss_ce: 0.019593
2021-12-12 01:01:39,043 iteration 6734 : loss : 0.016153, loss_ce: 0.008540
2021-12-12 01:01:41,880 iteration 6735 : loss : 0.030197, loss_ce: 0.016304
2021-12-12 01:01:44,629 iteration 6736 : loss : 0.032574, loss_ce: 0.013957
2021-12-12 01:01:47,217 iteration 6737 : loss : 0.022371, loss_ce: 0.012091
2021-12-12 01:01:50,036 iteration 6738 : loss : 0.019721, loss_ce: 0.009440
2021-12-12 01:01:52,944 iteration 6739 : loss : 0.038113, loss_ce: 0.017298
2021-12-12 01:01:55,740 iteration 6740 : loss : 0.033026, loss_ce: 0.016037
2021-12-12 01:01:58,387 iteration 6741 : loss : 0.019446, loss_ce: 0.010863
2021-12-12 01:02:01,336 iteration 6742 : loss : 0.027413, loss_ce: 0.013542
2021-12-12 01:02:04,136 iteration 6743 : loss : 0.031707, loss_ce: 0.013839
2021-12-12 01:02:06,889 iteration 6744 : loss : 0.028931, loss_ce: 0.013808
2021-12-12 01:02:09,541 iteration 6745 : loss : 0.038827, loss_ce: 0.015059
2021-12-12 01:02:12,203 iteration 6746 : loss : 0.036841, loss_ce: 0.017385
2021-12-12 01:02:14,941 iteration 6747 : loss : 0.027999, loss_ce: 0.012096
2021-12-12 01:02:17,691 iteration 6748 : loss : 0.029253, loss_ce: 0.014485
2021-12-12 01:02:20,487 iteration 6749 : loss : 0.041370, loss_ce: 0.017146
 99%|████████████████████████████▊| 397/400 [5:39:18<02:31, 50.62s/it]2021-12-12 01:02:23,456 iteration 6750 : loss : 0.026137, loss_ce: 0.012102
2021-12-12 01:02:26,303 iteration 6751 : loss : 0.038542, loss_ce: 0.019913
2021-12-12 01:02:29,169 iteration 6752 : loss : 0.066016, loss_ce: 0.035757
2021-12-12 01:02:31,918 iteration 6753 : loss : 0.031849, loss_ce: 0.015231
2021-12-12 01:02:34,754 iteration 6754 : loss : 0.041828, loss_ce: 0.014862
2021-12-12 01:02:37,490 iteration 6755 : loss : 0.017953, loss_ce: 0.008434
2021-12-12 01:02:40,173 iteration 6756 : loss : 0.027746, loss_ce: 0.013614
2021-12-12 01:02:43,110 iteration 6757 : loss : 0.038142, loss_ce: 0.019164
2021-12-12 01:02:45,844 iteration 6758 : loss : 0.016975, loss_ce: 0.008953
2021-12-12 01:02:48,654 iteration 6759 : loss : 0.028594, loss_ce: 0.012604
2021-12-12 01:02:51,358 iteration 6760 : loss : 0.044835, loss_ce: 0.020192
2021-12-12 01:02:54,121 iteration 6761 : loss : 0.026251, loss_ce: 0.013783
2021-12-12 01:02:56,626 iteration 6762 : loss : 0.020476, loss_ce: 0.010200
2021-12-12 01:02:59,387 iteration 6763 : loss : 0.034801, loss_ce: 0.012144
2021-12-12 01:03:02,109 iteration 6764 : loss : 0.028336, loss_ce: 0.012972
2021-12-12 01:03:04,725 iteration 6765 : loss : 0.025275, loss_ce: 0.011211
2021-12-12 01:03:07,437 iteration 6766 : loss : 0.033321, loss_ce: 0.015795
100%|████████████████████████████▊| 398/400 [5:40:05<01:39, 49.52s/it]2021-12-12 01:03:10,535 iteration 6767 : loss : 0.033800, loss_ce: 0.014620
2021-12-12 01:03:13,141 iteration 6768 : loss : 0.034147, loss_ce: 0.018790
2021-12-12 01:03:15,929 iteration 6769 : loss : 0.025243, loss_ce: 0.010569
2021-12-12 01:03:18,659 iteration 6770 : loss : 0.020906, loss_ce: 0.010901
2021-12-12 01:03:21,426 iteration 6771 : loss : 0.023558, loss_ce: 0.010196
2021-12-12 01:03:24,224 iteration 6772 : loss : 0.031609, loss_ce: 0.011830
2021-12-12 01:03:27,034 iteration 6773 : loss : 0.021675, loss_ce: 0.011048
2021-12-12 01:03:29,757 iteration 6774 : loss : 0.029360, loss_ce: 0.014903
2021-12-12 01:03:32,644 iteration 6775 : loss : 0.045564, loss_ce: 0.017814
2021-12-12 01:03:35,370 iteration 6776 : loss : 0.048032, loss_ce: 0.023268
2021-12-12 01:03:37,999 iteration 6777 : loss : 0.024760, loss_ce: 0.011649
2021-12-12 01:03:40,826 iteration 6778 : loss : 0.029309, loss_ce: 0.012333
2021-12-12 01:03:43,586 iteration 6779 : loss : 0.023242, loss_ce: 0.009642
2021-12-12 01:03:46,365 iteration 6780 : loss : 0.020222, loss_ce: 0.009896
2021-12-12 01:03:49,037 iteration 6781 : loss : 0.030226, loss_ce: 0.016634
2021-12-12 01:03:51,686 iteration 6782 : loss : 0.021666, loss_ce: 0.012157
2021-12-12 01:03:54,339 iteration 6783 : loss : 0.025415, loss_ce: 0.013053
100%|████████████████████████████▉| 399/400 [5:40:52<00:48, 48.73s/it]2021-12-12 01:03:57,159 iteration 6784 : loss : 0.031436, loss_ce: 0.013459
2021-12-12 01:03:59,856 iteration 6785 : loss : 0.037162, loss_ce: 0.019194
2021-12-12 01:04:02,625 iteration 6786 : loss : 0.025959, loss_ce: 0.013466
2021-12-12 01:04:05,303 iteration 6787 : loss : 0.034051, loss_ce: 0.013494
2021-12-12 01:04:08,024 iteration 6788 : loss : 0.042944, loss_ce: 0.014301
2021-12-12 01:04:10,687 iteration 6789 : loss : 0.023769, loss_ce: 0.012816
2021-12-12 01:04:13,414 iteration 6790 : loss : 0.021814, loss_ce: 0.010208
2021-12-12 01:04:16,294 iteration 6791 : loss : 0.042607, loss_ce: 0.017637
2021-12-12 01:04:19,031 iteration 6792 : loss : 0.023174, loss_ce: 0.011997
2021-12-12 01:04:21,740 iteration 6793 : loss : 0.037063, loss_ce: 0.015279
2021-12-12 01:04:24,477 iteration 6794 : loss : 0.025074, loss_ce: 0.011725
2021-12-12 01:04:27,111 iteration 6795 : loss : 0.022983, loss_ce: 0.011405
2021-12-12 01:04:29,828 iteration 6796 : loss : 0.029106, loss_ce: 0.012893
2021-12-12 01:04:32,571 iteration 6797 : loss : 0.030564, loss_ce: 0.012888
2021-12-12 01:04:35,121 iteration 6798 : loss : 0.023182, loss_ce: 0.010989
2021-12-12 01:04:37,832 iteration 6799 : loss : 0.036691, loss_ce: 0.019803
2021-12-12 01:04:37,832 Training Data Eval:
2021-12-12 01:04:52,823   Average segmentation loss on training set: 0.0136
2021-12-12 01:04:52,823 Validation Data Eval:
2021-12-12 01:04:58,056   Average segmentation loss on validation set: 0.0903
2021-12-12 01:05:00,830 iteration 6800 : loss : 0.034737, loss_ce: 0.017478
2021-12-12 01:05:02,805 save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_seed2epoch_399.pth
2021-12-12 01:05:04,724 save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_seed2epoch_399.pth
100%|████████████████████████████▉| 399/400 [5:42:02<00:51, 51.44s/it]
