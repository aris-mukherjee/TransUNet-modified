2021-11-30 11:41:57,649 Logging directory: /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/TransUNet/log_dir/trRUNMC_cv1_r3/i2i2l/
2021-11-30 11:41:57,650 Tensorboard directory: /scratch_net/biwidl217_second/arismu/Tensorboard/trRUNMC_cv1_r3/i2i2l/
2021-11-30 11:41:57,650 ============================================================
2021-11-30 11:41:57,650 EXPERIMENT NAME: trRUNMC_cv1_r3/i2i2l/
2021-11-30 11:41:57,650 ============================================================
2021-11-30 11:41:57,650 Loading data...
2021-11-30 11:41:57,650 Reading NCI - RUNMC images...
2021-11-30 11:41:57,650 Data root directory: /itet-stor/arismu/bmicdatasets-originals/Originals/Challenge_Datasets/NCI_Prostate/
2021-11-30 11:41:57,652 Already preprocessed this configuration. Loading now!
2021-11-30 11:41:57,670 Training Images: (256, 256, 286)
2021-11-30 11:41:57,670 Training Labels: (256, 256, 286)
2021-11-30 11:41:57,670 Validation Images: (256, 256, 98)
2021-11-30 11:41:57,670 Validation Labels: (256, 256, 98)
2021-11-30 11:41:57,670 ============================================================
2021-11-30 11:41:57,708 17 iterations per epoch. 6800 max iterations 
  0%|                                         | 0/400 [00:00<?, ?it/s]2021-11-30 11:42:00,580 iteration 1 : loss : 1.062061, loss_ce: 1.354724
2021-11-30 11:42:01,980 iteration 2 : loss : 1.062172, loss_ce: 1.350572
2021-11-30 11:42:03,496 iteration 3 : loss : 1.056104, loss_ce: 1.340701
2021-11-30 11:42:04,913 iteration 4 : loss : 1.051252, loss_ce: 1.327936
2021-11-30 11:42:06,345 iteration 5 : loss : 1.035475, loss_ce: 1.308294
2021-11-30 11:42:07,797 iteration 6 : loss : 1.033766, loss_ce: 1.294529
2021-11-30 11:42:09,311 iteration 7 : loss : 1.013418, loss_ce: 1.269985
2021-11-30 11:42:10,756 iteration 8 : loss : 1.015295, loss_ce: 1.248044
2021-11-30 11:42:12,217 iteration 9 : loss : 0.970471, loss_ce: 1.217165
2021-11-30 11:42:13,706 iteration 10 : loss : 0.981561, loss_ce: 1.189394
2021-11-30 11:42:15,269 iteration 11 : loss : 0.948010, loss_ce: 1.159641
2021-11-30 11:42:16,701 iteration 12 : loss : 0.938064, loss_ce: 1.131880
2021-11-30 11:42:18,115 iteration 13 : loss : 0.919016, loss_ce: 1.096470
2021-11-30 11:42:19,482 iteration 14 : loss : 0.899348, loss_ce: 1.066566
2021-11-30 11:42:20,953 iteration 15 : loss : 0.878607, loss_ce: 1.034574
2021-11-30 11:42:22,385 iteration 16 : loss : 0.867602, loss_ce: 1.007002
2021-11-30 11:42:23,815 iteration 17 : loss : 0.838788, loss_ce: 0.978157
  0%|                               | 1/400 [00:26<2:54:09, 26.19s/it]2021-11-30 11:42:25,378 iteration 18 : loss : 0.838842, loss_ce: 0.941215
2021-11-30 11:42:26,729 iteration 19 : loss : 0.813159, loss_ce: 0.917609
2021-11-30 11:42:28,282 iteration 20 : loss : 0.795534, loss_ce: 0.888508
2021-11-30 11:42:29,696 iteration 21 : loss : 0.785910, loss_ce: 0.852208
2021-11-30 11:42:31,139 iteration 22 : loss : 0.756321, loss_ce: 0.836655
2021-11-30 11:42:32,682 iteration 23 : loss : 0.751949, loss_ce: 0.797563
2021-11-30 11:42:34,122 iteration 24 : loss : 0.729149, loss_ce: 0.777579
2021-11-30 11:42:35,626 iteration 25 : loss : 0.708664, loss_ce: 0.765120
2021-11-30 11:42:37,043 iteration 26 : loss : 0.697169, loss_ce: 0.731292
2021-11-30 11:42:38,394 iteration 27 : loss : 0.684609, loss_ce: 0.714001
2021-11-30 11:42:39,770 iteration 28 : loss : 0.671540, loss_ce: 0.682682
2021-11-30 11:42:41,261 iteration 29 : loss : 0.661205, loss_ce: 0.659941
2021-11-30 11:42:42,726 iteration 30 : loss : 0.643499, loss_ce: 0.642933
2021-11-30 11:42:44,098 iteration 31 : loss : 0.632437, loss_ce: 0.630845
2021-11-30 11:42:45,594 iteration 32 : loss : 0.621188, loss_ce: 0.619951
2021-11-30 11:42:47,082 iteration 33 : loss : 0.609551, loss_ce: 0.595190
2021-11-30 11:42:48,569 iteration 34 : loss : 0.595744, loss_ce: 0.581839
  0%|▏                              | 2/400 [00:50<2:48:04, 25.34s/it]2021-11-30 11:42:50,122 iteration 35 : loss : 0.590339, loss_ce: 0.546466
2021-11-30 11:42:51,627 iteration 36 : loss : 0.581213, loss_ce: 0.538642
2021-11-30 11:42:53,137 iteration 37 : loss : 0.575058, loss_ce: 0.512555
2021-11-30 11:42:54,550 iteration 38 : loss : 0.558487, loss_ce: 0.510965
2021-11-30 11:42:55,979 iteration 39 : loss : 0.552740, loss_ce: 0.504948
2021-11-30 11:42:57,498 iteration 40 : loss : 0.546017, loss_ce: 0.489719
2021-11-30 11:42:59,011 iteration 41 : loss : 0.543594, loss_ce: 0.479312
2021-11-30 11:43:00,487 iteration 42 : loss : 0.537202, loss_ce: 0.466568
2021-11-30 11:43:01,856 iteration 43 : loss : 0.530261, loss_ce: 0.445763
2021-11-30 11:43:03,403 iteration 44 : loss : 0.516589, loss_ce: 0.442141
2021-11-30 11:43:04,849 iteration 45 : loss : 0.513168, loss_ce: 0.437452
2021-11-30 11:43:06,314 iteration 46 : loss : 0.507051, loss_ce: 0.409468
2021-11-30 11:43:07,824 iteration 47 : loss : 0.496770, loss_ce: 0.398728
2021-11-30 11:43:09,299 iteration 48 : loss : 0.494061, loss_ce: 0.397594
2021-11-30 11:43:10,834 iteration 49 : loss : 0.495351, loss_ce: 0.395120
2021-11-30 11:43:12,241 iteration 50 : loss : 0.491561, loss_ce: 0.379446
2021-11-30 11:43:13,624 iteration 51 : loss : 0.481260, loss_ce: 0.379361
  1%|▏                              | 3/400 [01:15<2:46:46, 25.20s/it]2021-11-30 11:43:15,227 iteration 52 : loss : 0.480462, loss_ce: 0.385578
2021-11-30 11:43:16,729 iteration 53 : loss : 0.476911, loss_ce: 0.371737
2021-11-30 11:43:18,192 iteration 54 : loss : 0.469832, loss_ce: 0.344292
2021-11-30 11:43:19,663 iteration 55 : loss : 0.475943, loss_ce: 0.370217
2021-11-30 11:43:21,156 iteration 56 : loss : 0.466646, loss_ce: 0.342109
2021-11-30 11:43:22,630 iteration 57 : loss : 0.456097, loss_ce: 0.330647
2021-11-30 11:43:24,121 iteration 58 : loss : 0.455680, loss_ce: 0.324994
2021-11-30 11:43:25,574 iteration 59 : loss : 0.455834, loss_ce: 0.339263
2021-11-30 11:43:27,064 iteration 60 : loss : 0.457449, loss_ce: 0.329477
2021-11-30 11:43:28,559 iteration 61 : loss : 0.453988, loss_ce: 0.333390
2021-11-30 11:43:30,015 iteration 62 : loss : 0.452530, loss_ce: 0.288985
2021-11-30 11:43:31,386 iteration 63 : loss : 0.446353, loss_ce: 0.317636
2021-11-30 11:43:32,822 iteration 64 : loss : 0.456511, loss_ce: 0.302091
2021-11-30 11:43:34,198 iteration 65 : loss : 0.442136, loss_ce: 0.289690
2021-11-30 11:43:35,639 iteration 66 : loss : 0.433639, loss_ce: 0.303227
2021-11-30 11:43:37,193 iteration 67 : loss : 0.435071, loss_ce: 0.279903
2021-11-30 11:43:38,674 iteration 68 : loss : 0.427868, loss_ce: 0.302941
  1%|▎                              | 4/400 [01:41<2:45:56, 25.14s/it]2021-11-30 11:43:40,261 iteration 69 : loss : 0.423775, loss_ce: 0.284401
2021-11-30 11:43:41,846 iteration 70 : loss : 0.432262, loss_ce: 0.285677
2021-11-30 11:43:43,299 iteration 71 : loss : 0.423302, loss_ce: 0.277115
2021-11-30 11:43:44,809 iteration 72 : loss : 0.420411, loss_ce: 0.272141
2021-11-30 11:43:46,256 iteration 73 : loss : 0.429127, loss_ce: 0.294790
2021-11-30 11:43:47,660 iteration 74 : loss : 0.416467, loss_ce: 0.271166
2021-11-30 11:43:49,112 iteration 75 : loss : 0.427125, loss_ce: 0.274848
2021-11-30 11:43:50,551 iteration 76 : loss : 0.417346, loss_ce: 0.268486
2021-11-30 11:43:51,907 iteration 77 : loss : 0.420000, loss_ce: 0.267259
2021-11-30 11:43:53,392 iteration 78 : loss : 0.421726, loss_ce: 0.268588
2021-11-30 11:43:54,844 iteration 79 : loss : 0.416611, loss_ce: 0.245151
2021-11-30 11:43:56,271 iteration 80 : loss : 0.424853, loss_ce: 0.282887
2021-11-30 11:43:57,689 iteration 81 : loss : 0.409733, loss_ce: 0.251755
2021-11-30 11:43:59,130 iteration 82 : loss : 0.414055, loss_ce: 0.244701
2021-11-30 11:44:00,562 iteration 83 : loss : 0.408184, loss_ce: 0.237891
2021-11-30 11:44:02,090 iteration 84 : loss : 0.425763, loss_ce: 0.288620
2021-11-30 11:44:02,090 Training Data Eval:
2021-11-30 11:44:09,436   Average segmentation loss on training set: 0.4062
2021-11-30 11:44:09,437 Validation Data Eval:
2021-11-30 11:44:12,116   Average segmentation loss on validation set: 0.4296
2021-11-30 11:44:14,018 Found new lowest validation loss at iteration 84! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/UNET_RUNMC/best_val_loss.pth
2021-11-30 11:44:15,441 iteration 85 : loss : 0.408447, loss_ce: 0.242658
  1%|▍                              | 5/400 [02:17<3:13:04, 29.33s/it]2021-11-30 11:44:16,936 iteration 86 : loss : 0.396060, loss_ce: 0.218201
2021-11-30 11:44:18,525 iteration 87 : loss : 0.405133, loss_ce: 0.249672
2021-11-30 11:44:19,911 iteration 88 : loss : 0.412531, loss_ce: 0.258724
2021-11-30 11:44:21,389 iteration 89 : loss : 0.403903, loss_ce: 0.247072
2021-11-30 11:44:22,837 iteration 90 : loss : 0.405664, loss_ce: 0.246723
2021-11-30 11:44:24,383 iteration 91 : loss : 0.414532, loss_ce: 0.270981
2021-11-30 11:44:25,745 iteration 92 : loss : 0.403239, loss_ce: 0.238373
2021-11-30 11:44:27,171 iteration 93 : loss : 0.389293, loss_ce: 0.214014
2021-11-30 11:44:28,619 iteration 94 : loss : 0.400099, loss_ce: 0.231239
2021-11-30 11:44:30,212 iteration 95 : loss : 0.396825, loss_ce: 0.243826
2021-11-30 11:44:31,623 iteration 96 : loss : 0.399267, loss_ce: 0.240936
2021-11-30 11:44:33,086 iteration 97 : loss : 0.394303, loss_ce: 0.233141
2021-11-30 11:44:34,553 iteration 98 : loss : 0.400223, loss_ce: 0.230120
2021-11-30 11:44:36,112 iteration 99 : loss : 0.391313, loss_ce: 0.236948
2021-11-30 11:44:37,621 iteration 100 : loss : 0.394009, loss_ce: 0.240412
2021-11-30 11:44:39,115 iteration 101 : loss : 0.384199, loss_ce: 0.222201
2021-11-30 11:44:40,505 iteration 102 : loss : 0.382726, loss_ce: 0.219152
  2%|▍                              | 6/400 [02:42<3:03:05, 27.88s/it]2021-11-30 11:44:42,148 iteration 103 : loss : 0.396119, loss_ce: 0.235727
2021-11-30 11:44:43,722 iteration 104 : loss : 0.398609, loss_ce: 0.220487
2021-11-30 11:44:45,337 iteration 105 : loss : 0.392800, loss_ce: 0.211230
2021-11-30 11:44:46,764 iteration 106 : loss : 0.393648, loss_ce: 0.212605
2021-11-30 11:44:48,406 iteration 107 : loss : 0.386240, loss_ce: 0.231708
2021-11-30 11:44:49,821 iteration 108 : loss : 0.398535, loss_ce: 0.218362
2021-11-30 11:44:51,268 iteration 109 : loss : 0.386796, loss_ce: 0.231230
2021-11-30 11:44:52,671 iteration 110 : loss : 0.380531, loss_ce: 0.219112
2021-11-30 11:44:54,185 iteration 111 : loss : 0.396895, loss_ce: 0.225939
2021-11-30 11:44:55,583 iteration 112 : loss : 0.388901, loss_ce: 0.216373
2021-11-30 11:44:57,072 iteration 113 : loss : 0.398703, loss_ce: 0.249111
2021-11-30 11:44:58,521 iteration 114 : loss : 0.367808, loss_ce: 0.190396
2021-11-30 11:44:59,986 iteration 115 : loss : 0.374453, loss_ce: 0.214028
2021-11-30 11:45:01,510 iteration 116 : loss : 0.397982, loss_ce: 0.222421
2021-11-30 11:45:03,025 iteration 117 : loss : 0.385680, loss_ce: 0.224540
2021-11-30 11:45:04,491 iteration 118 : loss : 0.404037, loss_ce: 0.231703
2021-11-30 11:45:05,990 iteration 119 : loss : 0.380201, loss_ce: 0.197919
  2%|▌                              | 7/400 [03:08<2:57:29, 27.10s/it]2021-11-30 11:45:07,487 iteration 120 : loss : 0.403361, loss_ce: 0.231583
2021-11-30 11:45:08,904 iteration 121 : loss : 0.381765, loss_ce: 0.213226
2021-11-30 11:45:10,451 iteration 122 : loss : 0.377805, loss_ce: 0.211648
2021-11-30 11:45:11,947 iteration 123 : loss : 0.369190, loss_ce: 0.198549
2021-11-30 11:45:13,406 iteration 124 : loss : 0.379419, loss_ce: 0.198469
2021-11-30 11:45:14,829 iteration 125 : loss : 0.388921, loss_ce: 0.231797
2021-11-30 11:45:16,330 iteration 126 : loss : 0.369345, loss_ce: 0.185308
2021-11-30 11:45:17,751 iteration 127 : loss : 0.377531, loss_ce: 0.204269
2021-11-30 11:45:19,200 iteration 128 : loss : 0.366231, loss_ce: 0.206289
2021-11-30 11:45:20,744 iteration 129 : loss : 0.377251, loss_ce: 0.192363
2021-11-30 11:45:22,204 iteration 130 : loss : 0.375511, loss_ce: 0.195509
2021-11-30 11:45:23,764 iteration 131 : loss : 0.399030, loss_ce: 0.228121
2021-11-30 11:45:25,338 iteration 132 : loss : 0.363647, loss_ce: 0.169924
2021-11-30 11:45:26,765 iteration 133 : loss : 0.378224, loss_ce: 0.200201
2021-11-30 11:45:28,326 iteration 134 : loss : 0.385800, loss_ce: 0.216243
2021-11-30 11:45:29,902 iteration 135 : loss : 0.387488, loss_ce: 0.227502
2021-11-30 11:45:31,363 iteration 136 : loss : 0.390515, loss_ce: 0.220104
  2%|▌                              | 8/400 [03:33<2:53:27, 26.55s/it]2021-11-30 11:45:32,981 iteration 137 : loss : 0.371195, loss_ce: 0.184013
2021-11-30 11:45:34,413 iteration 138 : loss : 0.390953, loss_ce: 0.231996
2021-11-30 11:45:35,929 iteration 139 : loss : 0.387659, loss_ce: 0.207343
2021-11-30 11:45:37,421 iteration 140 : loss : 0.378171, loss_ce: 0.183310
2021-11-30 11:45:38,964 iteration 141 : loss : 0.391582, loss_ce: 0.226540
2021-11-30 11:45:40,498 iteration 142 : loss : 0.367794, loss_ce: 0.199369
2021-11-30 11:45:42,070 iteration 143 : loss : 0.379243, loss_ce: 0.197838
2021-11-30 11:45:43,605 iteration 144 : loss : 0.365588, loss_ce: 0.183533
2021-11-30 11:45:45,065 iteration 145 : loss : 0.363951, loss_ce: 0.185631
2021-11-30 11:45:46,582 iteration 146 : loss : 0.374613, loss_ce: 0.209355
2021-11-30 11:45:48,106 iteration 147 : loss : 0.367512, loss_ce: 0.192060
2021-11-30 11:45:49,479 iteration 148 : loss : 0.382902, loss_ce: 0.208553
2021-11-30 11:45:50,952 iteration 149 : loss : 0.377246, loss_ce: 0.196135
2021-11-30 11:45:52,391 iteration 150 : loss : 0.366889, loss_ce: 0.172669
2021-11-30 11:45:53,853 iteration 151 : loss : 0.388604, loss_ce: 0.217056
2021-11-30 11:45:55,317 iteration 152 : loss : 0.365670, loss_ce: 0.176991
2021-11-30 11:45:56,829 iteration 153 : loss : 0.367805, loss_ce: 0.197904
  2%|▋                              | 9/400 [03:59<2:50:49, 26.21s/it]2021-11-30 11:45:58,299 iteration 154 : loss : 0.387895, loss_ce: 0.211163
2021-11-30 11:45:59,802 iteration 155 : loss : 0.355118, loss_ce: 0.171293
2021-11-30 11:46:01,392 iteration 156 : loss : 0.374801, loss_ce: 0.193542
2021-11-30 11:46:02,875 iteration 157 : loss : 0.373367, loss_ce: 0.183324
2021-11-30 11:46:04,493 iteration 158 : loss : 0.367361, loss_ce: 0.197192
2021-11-30 11:46:05,840 iteration 159 : loss : 0.376022, loss_ce: 0.190515
2021-11-30 11:46:07,432 iteration 160 : loss : 0.363860, loss_ce: 0.173897
2021-11-30 11:46:08,980 iteration 161 : loss : 0.366205, loss_ce: 0.176555
2021-11-30 11:46:10,536 iteration 162 : loss : 0.380754, loss_ce: 0.215729
2021-11-30 11:46:11,993 iteration 163 : loss : 0.374294, loss_ce: 0.189059
2021-11-30 11:46:13,493 iteration 164 : loss : 0.359007, loss_ce: 0.169792
2021-11-30 11:46:14,957 iteration 165 : loss : 0.373360, loss_ce: 0.196977
2021-11-30 11:46:16,415 iteration 166 : loss : 0.366148, loss_ce: 0.182131
2021-11-30 11:46:17,902 iteration 167 : loss : 0.355820, loss_ce: 0.178325
2021-11-30 11:46:19,423 iteration 168 : loss : 0.386337, loss_ce: 0.223381
2021-11-30 11:46:20,920 iteration 169 : loss : 0.371024, loss_ce: 0.217831
2021-11-30 11:46:20,921 Training Data Eval:
2021-11-30 11:46:28,287   Average segmentation loss on training set: 0.3580
2021-11-30 11:46:28,288 Validation Data Eval:
2021-11-30 11:46:30,830   Average segmentation loss on validation set: 0.3827
2021-11-30 11:46:32,814 Found new lowest validation loss at iteration 169! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/UNET_RUNMC/best_val_loss.pth
2021-11-30 11:46:34,330 iteration 170 : loss : 0.360689, loss_ce: 0.188860
  2%|▊                             | 10/400 [04:36<3:12:59, 29.69s/it]2021-11-30 11:46:35,845 iteration 171 : loss : 0.366203, loss_ce: 0.181075
2021-11-30 11:46:37,328 iteration 172 : loss : 0.359156, loss_ce: 0.190989
2021-11-30 11:46:38,818 iteration 173 : loss : 0.361825, loss_ce: 0.180916
2021-11-30 11:46:40,209 iteration 174 : loss : 0.376642, loss_ce: 0.200093
2021-11-30 11:46:41,590 iteration 175 : loss : 0.365692, loss_ce: 0.179401
2021-11-30 11:46:43,053 iteration 176 : loss : 0.379116, loss_ce: 0.213273
2021-11-30 11:46:44,494 iteration 177 : loss : 0.362607, loss_ce: 0.165386
2021-11-30 11:46:45,928 iteration 178 : loss : 0.355485, loss_ce: 0.198325
2021-11-30 11:46:47,378 iteration 179 : loss : 0.367721, loss_ce: 0.183232
2021-11-30 11:46:48,797 iteration 180 : loss : 0.359472, loss_ce: 0.163494
2021-11-30 11:46:50,292 iteration 181 : loss : 0.370139, loss_ce: 0.196399
2021-11-30 11:46:51,752 iteration 182 : loss : 0.367087, loss_ce: 0.192123
2021-11-30 11:46:53,200 iteration 183 : loss : 0.359324, loss_ce: 0.167777
2021-11-30 11:46:54,630 iteration 184 : loss : 0.358962, loss_ce: 0.176349
2021-11-30 11:46:56,159 iteration 185 : loss : 0.358097, loss_ce: 0.168303
2021-11-30 11:46:57,666 iteration 186 : loss : 0.380731, loss_ce: 0.209860
2021-11-30 11:46:59,218 iteration 187 : loss : 0.365773, loss_ce: 0.183950
  3%|▊                             | 11/400 [05:01<3:02:59, 28.22s/it]2021-11-30 11:47:00,786 iteration 188 : loss : 0.371145, loss_ce: 0.170162
2021-11-30 11:47:02,310 iteration 189 : loss : 0.363633, loss_ce: 0.194668
2021-11-30 11:47:03,831 iteration 190 : loss : 0.354658, loss_ce: 0.173914
2021-11-30 11:47:05,300 iteration 191 : loss : 0.355902, loss_ce: 0.179265
2021-11-30 11:47:06,709 iteration 192 : loss : 0.355498, loss_ce: 0.185591
2021-11-30 11:47:08,270 iteration 193 : loss : 0.379599, loss_ce: 0.191254
2021-11-30 11:47:09,739 iteration 194 : loss : 0.353162, loss_ce: 0.146474
2021-11-30 11:47:11,182 iteration 195 : loss : 0.380789, loss_ce: 0.192503
2021-11-30 11:47:12,577 iteration 196 : loss : 0.347548, loss_ce: 0.152007
2021-11-30 11:47:14,087 iteration 197 : loss : 0.367193, loss_ce: 0.176478
2021-11-30 11:47:15,624 iteration 198 : loss : 0.367318, loss_ce: 0.186769
2021-11-30 11:47:17,075 iteration 199 : loss : 0.363219, loss_ce: 0.176419
2021-11-30 11:47:18,549 iteration 200 : loss : 0.369634, loss_ce: 0.186984
2021-11-30 11:47:20,051 iteration 201 : loss : 0.345337, loss_ce: 0.174907
2021-11-30 11:47:21,567 iteration 202 : loss : 0.359524, loss_ce: 0.195080
2021-11-30 11:47:23,008 iteration 203 : loss : 0.359205, loss_ce: 0.179301
2021-11-30 11:47:24,616 iteration 204 : loss : 0.356955, loss_ce: 0.160150
  3%|▉                             | 12/400 [05:26<2:56:56, 27.36s/it]2021-11-30 11:47:26,108 iteration 205 : loss : 0.347374, loss_ce: 0.162832
2021-11-30 11:47:27,568 iteration 206 : loss : 0.337452, loss_ce: 0.149004
2021-11-30 11:47:29,065 iteration 207 : loss : 0.363554, loss_ce: 0.192033
2021-11-30 11:47:30,526 iteration 208 : loss : 0.348937, loss_ce: 0.147520
2021-11-30 11:47:32,086 iteration 209 : loss : 0.391833, loss_ce: 0.212501
2021-11-30 11:47:33,483 iteration 210 : loss : 0.345954, loss_ce: 0.155444
2021-11-30 11:47:35,039 iteration 211 : loss : 0.374805, loss_ce: 0.202112
2021-11-30 11:47:36,545 iteration 212 : loss : 0.364908, loss_ce: 0.163735
2021-11-30 11:47:38,141 iteration 213 : loss : 0.353843, loss_ce: 0.193779
2021-11-30 11:47:39,609 iteration 214 : loss : 0.351708, loss_ce: 0.153306
2021-11-30 11:47:41,102 iteration 215 : loss : 0.352990, loss_ce: 0.165308
2021-11-30 11:47:42,769 iteration 216 : loss : 0.365619, loss_ce: 0.175816
2021-11-30 11:47:44,347 iteration 217 : loss : 0.360221, loss_ce: 0.176398
2021-11-30 11:47:45,843 iteration 218 : loss : 0.349158, loss_ce: 0.182231
2021-11-30 11:47:47,213 iteration 219 : loss : 0.369647, loss_ce: 0.189911
2021-11-30 11:47:48,712 iteration 220 : loss : 0.363048, loss_ce: 0.194294
2021-11-30 11:47:50,203 iteration 221 : loss : 0.355901, loss_ce: 0.159149
  3%|▉                             | 13/400 [05:52<2:53:00, 26.82s/it]2021-11-30 11:47:51,739 iteration 222 : loss : 0.384204, loss_ce: 0.209609
2021-11-30 11:47:53,255 iteration 223 : loss : 0.351488, loss_ce: 0.173600
2021-11-30 11:47:54,722 iteration 224 : loss : 0.392361, loss_ce: 0.192631
2021-11-30 11:47:56,310 iteration 225 : loss : 0.365681, loss_ce: 0.146586
2021-11-30 11:47:57,776 iteration 226 : loss : 0.343945, loss_ce: 0.154607
2021-11-30 11:47:59,307 iteration 227 : loss : 0.366223, loss_ce: 0.191891
2021-11-30 11:48:00,781 iteration 228 : loss : 0.357274, loss_ce: 0.184799
2021-11-30 11:48:02,214 iteration 229 : loss : 0.327992, loss_ce: 0.154077
2021-11-30 11:48:03,716 iteration 230 : loss : 0.361874, loss_ce: 0.179756
2021-11-30 11:48:05,255 iteration 231 : loss : 0.366972, loss_ce: 0.161956
2021-11-30 11:48:06,777 iteration 232 : loss : 0.346751, loss_ce: 0.169994
2021-11-30 11:48:08,209 iteration 233 : loss : 0.362465, loss_ce: 0.173514
2021-11-30 11:48:09,774 iteration 234 : loss : 0.355846, loss_ce: 0.170272
2021-11-30 11:48:11,241 iteration 235 : loss : 0.367958, loss_ce: 0.190951
2021-11-30 11:48:12,754 iteration 236 : loss : 0.366479, loss_ce: 0.176154
2021-11-30 11:48:14,220 iteration 237 : loss : 0.345067, loss_ce: 0.172763
2021-11-30 11:48:15,709 iteration 238 : loss : 0.368347, loss_ce: 0.186855
  4%|█                             | 14/400 [06:18<2:50:00, 26.43s/it]2021-11-30 11:48:17,178 iteration 239 : loss : 0.357418, loss_ce: 0.153740
2021-11-30 11:48:18,689 iteration 240 : loss : 0.370816, loss_ce: 0.201589
2021-11-30 11:48:20,219 iteration 241 : loss : 0.355582, loss_ce: 0.155944
2021-11-30 11:48:21,740 iteration 242 : loss : 0.375477, loss_ce: 0.195447
2021-11-30 11:48:23,401 iteration 243 : loss : 0.360387, loss_ce: 0.180534
2021-11-30 11:48:24,812 iteration 244 : loss : 0.343303, loss_ce: 0.133846
2021-11-30 11:48:26,307 iteration 245 : loss : 0.365619, loss_ce: 0.192503
2021-11-30 11:48:27,852 iteration 246 : loss : 0.367604, loss_ce: 0.180572
2021-11-30 11:48:29,323 iteration 247 : loss : 0.345001, loss_ce: 0.159345
2021-11-30 11:48:30,795 iteration 248 : loss : 0.358111, loss_ce: 0.146067
2021-11-30 11:48:32,251 iteration 249 : loss : 0.392011, loss_ce: 0.220976
2021-11-30 11:48:33,725 iteration 250 : loss : 0.348183, loss_ce: 0.163067
2021-11-30 11:48:35,154 iteration 251 : loss : 0.333697, loss_ce: 0.148884
2021-11-30 11:48:36,644 iteration 252 : loss : 0.337914, loss_ce: 0.160866
2021-11-30 11:48:38,167 iteration 253 : loss : 0.352009, loss_ce: 0.168246
2021-11-30 11:48:39,740 iteration 254 : loss : 0.369960, loss_ce: 0.169212
2021-11-30 11:48:39,740 Training Data Eval:
2021-11-30 11:48:47,126   Average segmentation loss on training set: 0.3427
2021-11-30 11:48:47,127 Validation Data Eval:
2021-11-30 11:48:49,692   Average segmentation loss on validation set: 0.3654
2021-11-30 11:48:51,634 Found new lowest validation loss at iteration 254! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/UNET_RUNMC/best_val_loss.pth
2021-11-30 11:48:53,061 iteration 255 : loss : 0.357489, loss_ce: 0.166768
  4%|█▏                            | 15/400 [06:55<3:10:44, 29.73s/it]2021-11-30 11:48:54,594 iteration 256 : loss : 0.354977, loss_ce: 0.173595
2021-11-30 11:48:56,115 iteration 257 : loss : 0.347667, loss_ce: 0.165572
2021-11-30 11:48:57,686 iteration 258 : loss : 0.351210, loss_ce: 0.174537
2021-11-30 11:48:59,040 iteration 259 : loss : 0.362022, loss_ce: 0.185921
2021-11-30 11:49:00,553 iteration 260 : loss : 0.350234, loss_ce: 0.175914
2021-11-30 11:49:02,037 iteration 261 : loss : 0.343256, loss_ce: 0.152077
2021-11-30 11:49:03,601 iteration 262 : loss : 0.343486, loss_ce: 0.167217
2021-11-30 11:49:05,067 iteration 263 : loss : 0.344807, loss_ce: 0.145116
2021-11-30 11:49:06,567 iteration 264 : loss : 0.376798, loss_ce: 0.197572
2021-11-30 11:49:08,009 iteration 265 : loss : 0.336870, loss_ce: 0.163365
2021-11-30 11:49:09,549 iteration 266 : loss : 0.355726, loss_ce: 0.174860
2021-11-30 11:49:11,050 iteration 267 : loss : 0.338525, loss_ce: 0.136746
2021-11-30 11:49:12,547 iteration 268 : loss : 0.343869, loss_ce: 0.167114
2021-11-30 11:49:14,267 iteration 269 : loss : 0.350703, loss_ce: 0.163377
2021-11-30 11:49:15,878 iteration 270 : loss : 0.341861, loss_ce: 0.168727
2021-11-30 11:49:17,226 iteration 271 : loss : 0.332016, loss_ce: 0.137742
2021-11-30 11:49:18,723 iteration 272 : loss : 0.369631, loss_ce: 0.195150
  4%|█▏                            | 16/400 [07:21<3:02:23, 28.50s/it]2021-11-30 11:49:20,194 iteration 273 : loss : 0.345134, loss_ce: 0.161353
2021-11-30 11:49:21,623 iteration 274 : loss : 0.341945, loss_ce: 0.166790
2021-11-30 11:49:23,066 iteration 275 : loss : 0.327793, loss_ce: 0.157684
2021-11-30 11:49:24,571 iteration 276 : loss : 0.357916, loss_ce: 0.185855
2021-11-30 11:49:26,096 iteration 277 : loss : 0.335664, loss_ce: 0.147774
2021-11-30 11:49:27,459 iteration 278 : loss : 0.345494, loss_ce: 0.153533
2021-11-30 11:49:28,905 iteration 279 : loss : 0.349199, loss_ce: 0.150122
2021-11-30 11:49:30,334 iteration 280 : loss : 0.343956, loss_ce: 0.164904
2021-11-30 11:49:31,840 iteration 281 : loss : 0.360465, loss_ce: 0.166663
2021-11-30 11:49:33,307 iteration 282 : loss : 0.365217, loss_ce: 0.167953
2021-11-30 11:49:34,747 iteration 283 : loss : 0.349768, loss_ce: 0.171380
2021-11-30 11:49:36,296 iteration 284 : loss : 0.364081, loss_ce: 0.194287
2021-11-30 11:49:37,712 iteration 285 : loss : 0.340343, loss_ce: 0.156818
2021-11-30 11:49:39,166 iteration 286 : loss : 0.336781, loss_ce: 0.162845
2021-11-30 11:49:40,756 iteration 287 : loss : 0.364852, loss_ce: 0.179850
2021-11-30 11:49:42,307 iteration 288 : loss : 0.350086, loss_ce: 0.165004
2021-11-30 11:49:43,763 iteration 289 : loss : 0.326343, loss_ce: 0.143792
  4%|█▎                            | 17/400 [07:46<2:55:15, 27.46s/it]2021-11-30 11:49:45,300 iteration 290 : loss : 0.324790, loss_ce: 0.146170
2021-11-30 11:49:46,731 iteration 291 : loss : 0.353887, loss_ce: 0.153245
2021-11-30 11:49:48,169 iteration 292 : loss : 0.359111, loss_ce: 0.166971
2021-11-30 11:49:49,612 iteration 293 : loss : 0.372768, loss_ce: 0.177678
2021-11-30 11:49:51,096 iteration 294 : loss : 0.345573, loss_ce: 0.165217
2021-11-30 11:49:52,722 iteration 295 : loss : 0.319982, loss_ce: 0.141361
2021-11-30 11:49:54,178 iteration 296 : loss : 0.337111, loss_ce: 0.149087
2021-11-30 11:49:55,678 iteration 297 : loss : 0.325518, loss_ce: 0.137267
2021-11-30 11:49:57,132 iteration 298 : loss : 0.326609, loss_ce: 0.147949
2021-11-30 11:49:58,609 iteration 299 : loss : 0.332101, loss_ce: 0.153698
2021-11-30 11:50:00,090 iteration 300 : loss : 0.339948, loss_ce: 0.154359
2021-11-30 11:50:01,595 iteration 301 : loss : 0.392784, loss_ce: 0.213822
2021-11-30 11:50:03,154 iteration 302 : loss : 0.332563, loss_ce: 0.155686
2021-11-30 11:50:04,688 iteration 303 : loss : 0.347270, loss_ce: 0.183968
2021-11-30 11:50:06,220 iteration 304 : loss : 0.374526, loss_ce: 0.184449
2021-11-30 11:50:07,778 iteration 305 : loss : 0.349193, loss_ce: 0.164538
2021-11-30 11:50:09,325 iteration 306 : loss : 0.340061, loss_ce: 0.170381
  4%|█▎                            | 18/400 [08:11<2:51:11, 26.89s/it]2021-11-30 11:50:10,881 iteration 307 : loss : 0.337392, loss_ce: 0.143482
2021-11-30 11:50:12,328 iteration 308 : loss : 0.364795, loss_ce: 0.188320
2021-11-30 11:50:13,838 iteration 309 : loss : 0.366016, loss_ce: 0.162387
2021-11-30 11:50:15,376 iteration 310 : loss : 0.351005, loss_ce: 0.159795
2021-11-30 11:50:16,803 iteration 311 : loss : 0.346512, loss_ce: 0.149336
2021-11-30 11:50:18,276 iteration 312 : loss : 0.354302, loss_ce: 0.189587
2021-11-30 11:50:19,760 iteration 313 : loss : 0.375642, loss_ce: 0.212841
2021-11-30 11:50:21,159 iteration 314 : loss : 0.359070, loss_ce: 0.175461
2021-11-30 11:50:22,632 iteration 315 : loss : 0.332926, loss_ce: 0.140600
2021-11-30 11:50:24,150 iteration 316 : loss : 0.328968, loss_ce: 0.149025
2021-11-30 11:50:25,652 iteration 317 : loss : 0.371263, loss_ce: 0.171319
2021-11-30 11:50:27,152 iteration 318 : loss : 0.347143, loss_ce: 0.182929
2021-11-30 11:50:28,548 iteration 319 : loss : 0.344667, loss_ce: 0.155559
2021-11-30 11:50:30,122 iteration 320 : loss : 0.354947, loss_ce: 0.138408
2021-11-30 11:50:31,652 iteration 321 : loss : 0.332445, loss_ce: 0.140902
2021-11-30 11:50:33,139 iteration 322 : loss : 0.354488, loss_ce: 0.180192
2021-11-30 11:50:34,695 iteration 323 : loss : 0.327704, loss_ce: 0.138804
  5%|█▍                            | 19/400 [08:37<2:47:50, 26.43s/it]2021-11-30 11:50:36,203 iteration 324 : loss : 0.340092, loss_ce: 0.164246
2021-11-30 11:50:37,684 iteration 325 : loss : 0.337594, loss_ce: 0.148197
2021-11-30 11:50:39,225 iteration 326 : loss : 0.339826, loss_ce: 0.169168
2021-11-30 11:50:40,824 iteration 327 : loss : 0.351420, loss_ce: 0.168283
2021-11-30 11:50:42,308 iteration 328 : loss : 0.338813, loss_ce: 0.151746
2021-11-30 11:50:43,827 iteration 329 : loss : 0.350038, loss_ce: 0.178687
2021-11-30 11:50:45,374 iteration 330 : loss : 0.338041, loss_ce: 0.154743
2021-11-30 11:50:46,810 iteration 331 : loss : 0.360869, loss_ce: 0.177423
2021-11-30 11:50:48,286 iteration 332 : loss : 0.322196, loss_ce: 0.140664
2021-11-30 11:50:49,805 iteration 333 : loss : 0.333308, loss_ce: 0.172291
2021-11-30 11:50:51,288 iteration 334 : loss : 0.362192, loss_ce: 0.164426
2021-11-30 11:50:52,778 iteration 335 : loss : 0.345949, loss_ce: 0.158549
2021-11-30 11:50:54,330 iteration 336 : loss : 0.333829, loss_ce: 0.144276
2021-11-30 11:50:55,808 iteration 337 : loss : 0.325898, loss_ce: 0.127887
2021-11-30 11:50:57,247 iteration 338 : loss : 0.373123, loss_ce: 0.204356
2021-11-30 11:50:58,696 iteration 339 : loss : 0.331490, loss_ce: 0.145426
2021-11-30 11:50:58,697 Training Data Eval:
2021-11-30 11:51:06,097   Average segmentation loss on training set: 0.3266
2021-11-30 11:51:06,097 Validation Data Eval:
2021-11-30 11:51:08,649   Average segmentation loss on validation set: 0.3433
2021-11-30 11:51:10,578 Found new lowest validation loss at iteration 339! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/UNET_RUNMC/best_val_loss.pth
2021-11-30 11:51:12,104 iteration 340 : loss : 0.346895, loss_ce: 0.169643
  5%|█▌                            | 20/400 [09:14<3:08:17, 29.73s/it]2021-11-30 11:51:13,675 iteration 341 : loss : 0.349897, loss_ce: 0.168560
2021-11-30 11:51:15,161 iteration 342 : loss : 0.313791, loss_ce: 0.124091
2021-11-30 11:51:16,606 iteration 343 : loss : 0.316695, loss_ce: 0.141226
2021-11-30 11:51:18,119 iteration 344 : loss : 0.357645, loss_ce: 0.186150
2021-11-30 11:51:19,566 iteration 345 : loss : 0.316526, loss_ce: 0.140167
2021-11-30 11:51:21,102 iteration 346 : loss : 0.350976, loss_ce: 0.165477
2021-11-30 11:51:22,486 iteration 347 : loss : 0.361949, loss_ce: 0.192723
2021-11-30 11:51:23,956 iteration 348 : loss : 0.343605, loss_ce: 0.154064
2021-11-30 11:51:25,571 iteration 349 : loss : 0.368899, loss_ce: 0.193151
2021-11-30 11:51:27,019 iteration 350 : loss : 0.335389, loss_ce: 0.153618
2021-11-30 11:51:28,548 iteration 351 : loss : 0.349714, loss_ce: 0.180711
2021-11-30 11:51:30,067 iteration 352 : loss : 0.336451, loss_ce: 0.150401
2021-11-30 11:51:31,518 iteration 353 : loss : 0.321578, loss_ce: 0.134222
2021-11-30 11:51:33,048 iteration 354 : loss : 0.346434, loss_ce: 0.145305
2021-11-30 11:51:34,600 iteration 355 : loss : 0.326160, loss_ce: 0.155989
2021-11-30 11:51:36,256 iteration 356 : loss : 0.344298, loss_ce: 0.156609
2021-11-30 11:51:37,677 iteration 357 : loss : 0.317111, loss_ce: 0.140635
  5%|█▌                            | 21/400 [09:40<2:59:54, 28.48s/it]2021-11-30 11:51:39,268 iteration 358 : loss : 0.331312, loss_ce: 0.172138
2021-11-30 11:51:40,832 iteration 359 : loss : 0.346274, loss_ce: 0.164175
2021-11-30 11:51:42,245 iteration 360 : loss : 0.321981, loss_ce: 0.128622
2021-11-30 11:51:43,776 iteration 361 : loss : 0.350774, loss_ce: 0.146665
2021-11-30 11:51:45,363 iteration 362 : loss : 0.330379, loss_ce: 0.136485
2021-11-30 11:51:46,939 iteration 363 : loss : 0.323801, loss_ce: 0.145122
2021-11-30 11:51:48,467 iteration 364 : loss : 0.318415, loss_ce: 0.151128
2021-11-30 11:51:49,861 iteration 365 : loss : 0.358594, loss_ce: 0.190088
2021-11-30 11:51:51,409 iteration 366 : loss : 0.352130, loss_ce: 0.176909
2021-11-30 11:51:52,830 iteration 367 : loss : 0.335887, loss_ce: 0.163701
2021-11-30 11:51:54,391 iteration 368 : loss : 0.331747, loss_ce: 0.127689
2021-11-30 11:51:55,949 iteration 369 : loss : 0.327190, loss_ce: 0.128257
2021-11-30 11:51:57,495 iteration 370 : loss : 0.324096, loss_ce: 0.174604
2021-11-30 11:51:59,005 iteration 371 : loss : 0.317845, loss_ce: 0.118207
2021-11-30 11:52:00,525 iteration 372 : loss : 0.366461, loss_ce: 0.190287
2021-11-30 11:52:01,965 iteration 373 : loss : 0.306883, loss_ce: 0.118964
2021-11-30 11:52:03,453 iteration 374 : loss : 0.331481, loss_ce: 0.157430
  6%|█▋                            | 22/400 [10:05<2:54:20, 27.67s/it]2021-11-30 11:52:05,072 iteration 375 : loss : 0.375200, loss_ce: 0.177201
2021-11-30 11:52:06,510 iteration 376 : loss : 0.311497, loss_ce: 0.133734
2021-11-30 11:52:07,970 iteration 377 : loss : 0.339322, loss_ce: 0.144527
2021-11-30 11:52:09,544 iteration 378 : loss : 0.325597, loss_ce: 0.146895
2021-11-30 11:52:10,927 iteration 379 : loss : 0.337311, loss_ce: 0.167213
2021-11-30 11:52:12,450 iteration 380 : loss : 0.325663, loss_ce: 0.139501
2021-11-30 11:52:13,924 iteration 381 : loss : 0.321504, loss_ce: 0.126973
2021-11-30 11:52:15,329 iteration 382 : loss : 0.320896, loss_ce: 0.148541
2021-11-30 11:52:16,828 iteration 383 : loss : 0.343208, loss_ce: 0.159074
2021-11-30 11:52:18,347 iteration 384 : loss : 0.345593, loss_ce: 0.158818
2021-11-30 11:52:19,854 iteration 385 : loss : 0.326270, loss_ce: 0.156874
2021-11-30 11:52:21,357 iteration 386 : loss : 0.328925, loss_ce: 0.131765
2021-11-30 11:52:22,868 iteration 387 : loss : 0.321899, loss_ce: 0.135177
2021-11-30 11:52:24,388 iteration 388 : loss : 0.311788, loss_ce: 0.145148
2021-11-30 11:52:25,913 iteration 389 : loss : 0.358181, loss_ce: 0.184352
2021-11-30 11:52:27,397 iteration 390 : loss : 0.309855, loss_ce: 0.150508
2021-11-30 11:52:28,878 iteration 391 : loss : 0.375300, loss_ce: 0.216545
  6%|█▋                            | 23/400 [10:31<2:49:35, 26.99s/it]2021-11-30 11:52:30,437 iteration 392 : loss : 0.353047, loss_ce: 0.174457
2021-11-30 11:52:31,877 iteration 393 : loss : 0.311861, loss_ce: 0.137992
2021-11-30 11:52:33,429 iteration 394 : loss : 0.300800, loss_ce: 0.123096
2021-11-30 11:52:34,949 iteration 395 : loss : 0.310491, loss_ce: 0.150312
2021-11-30 11:52:36,409 iteration 396 : loss : 0.312185, loss_ce: 0.129075
2021-11-30 11:52:37,904 iteration 397 : loss : 0.339522, loss_ce: 0.168348
2021-11-30 11:52:39,418 iteration 398 : loss : 0.311136, loss_ce: 0.136447
2021-11-30 11:52:40,810 iteration 399 : loss : 0.317237, loss_ce: 0.153416
2021-11-30 11:52:42,314 iteration 400 : loss : 0.328525, loss_ce: 0.143799
2021-11-30 11:52:43,865 iteration 401 : loss : 0.325361, loss_ce: 0.155033
2021-11-30 11:52:45,322 iteration 402 : loss : 0.322028, loss_ce: 0.155165
2021-11-30 11:52:46,790 iteration 403 : loss : 0.310386, loss_ce: 0.139858
2021-11-30 11:52:48,316 iteration 404 : loss : 0.342024, loss_ce: 0.167978
2021-11-30 11:52:49,882 iteration 405 : loss : 0.341268, loss_ce: 0.147463
2021-11-30 11:52:51,340 iteration 406 : loss : 0.333661, loss_ce: 0.152292
2021-11-30 11:52:52,765 iteration 407 : loss : 0.329010, loss_ce: 0.147174
2021-11-30 11:52:54,248 iteration 408 : loss : 0.315868, loss_ce: 0.143524
  6%|█▊                            | 24/400 [10:56<2:46:06, 26.51s/it]2021-11-30 11:52:55,821 iteration 409 : loss : 0.333321, loss_ce: 0.177212
2021-11-30 11:52:57,358 iteration 410 : loss : 0.346255, loss_ce: 0.161339
2021-11-30 11:52:58,832 iteration 411 : loss : 0.334099, loss_ce: 0.131850
2021-11-30 11:53:00,214 iteration 412 : loss : 0.310507, loss_ce: 0.134333
2021-11-30 11:53:01,718 iteration 413 : loss : 0.321868, loss_ce: 0.137029
2021-11-30 11:53:03,154 iteration 414 : loss : 0.336464, loss_ce: 0.175138
2021-11-30 11:53:04,540 iteration 415 : loss : 0.309952, loss_ce: 0.137996
2021-11-30 11:53:06,048 iteration 416 : loss : 0.359784, loss_ce: 0.190586
2021-11-30 11:53:07,493 iteration 417 : loss : 0.345771, loss_ce: 0.140833
2021-11-30 11:53:09,054 iteration 418 : loss : 0.320720, loss_ce: 0.157685
2021-11-30 11:53:10,510 iteration 419 : loss : 0.354655, loss_ce: 0.192789
2021-11-30 11:53:11,994 iteration 420 : loss : 0.336501, loss_ce: 0.148141
2021-11-30 11:53:13,425 iteration 421 : loss : 0.328593, loss_ce: 0.143592
2021-11-30 11:53:14,863 iteration 422 : loss : 0.330450, loss_ce: 0.163534
2021-11-30 11:53:16,301 iteration 423 : loss : 0.318827, loss_ce: 0.140669
2021-11-30 11:53:17,818 iteration 424 : loss : 0.343049, loss_ce: 0.143182
2021-11-30 11:53:17,818 Training Data Eval:
2021-11-30 11:53:25,186   Average segmentation loss on training set: 0.3088
2021-11-30 11:53:25,187 Validation Data Eval:
2021-11-30 11:53:27,748   Average segmentation loss on validation set: 0.3247
2021-11-30 11:53:29,665 Found new lowest validation loss at iteration 424! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/UNET_RUNMC/best_val_loss.pth
2021-11-30 11:53:31,112 iteration 425 : loss : 0.333225, loss_ce: 0.147563
  6%|█▉                            | 25/400 [11:33<3:05:05, 29.61s/it]2021-11-30 11:53:32,547 iteration 426 : loss : 0.316029, loss_ce: 0.133304
2021-11-30 11:53:34,043 iteration 427 : loss : 0.297646, loss_ce: 0.123765
2021-11-30 11:53:35,482 iteration 428 : loss : 0.351382, loss_ce: 0.152695
2021-11-30 11:53:36,875 iteration 429 : loss : 0.374918, loss_ce: 0.203756
2021-11-30 11:53:38,258 iteration 430 : loss : 0.315299, loss_ce: 0.142129
2021-11-30 11:53:39,666 iteration 431 : loss : 0.372406, loss_ce: 0.162168
2021-11-30 11:53:41,181 iteration 432 : loss : 0.304189, loss_ce: 0.143118
2021-11-30 11:53:42,749 iteration 433 : loss : 0.305940, loss_ce: 0.138314
2021-11-30 11:53:44,246 iteration 434 : loss : 0.304789, loss_ce: 0.138544
2021-11-30 11:53:45,682 iteration 435 : loss : 0.341223, loss_ce: 0.158915
2021-11-30 11:53:47,166 iteration 436 : loss : 0.331938, loss_ce: 0.159499
2021-11-30 11:53:48,637 iteration 437 : loss : 0.350786, loss_ce: 0.186724
2021-11-30 11:53:50,064 iteration 438 : loss : 0.341521, loss_ce: 0.145214
2021-11-30 11:53:51,567 iteration 439 : loss : 0.319784, loss_ce: 0.128490
2021-11-30 11:53:53,042 iteration 440 : loss : 0.346394, loss_ce: 0.152082
2021-11-30 11:53:54,581 iteration 441 : loss : 0.325772, loss_ce: 0.151020
2021-11-30 11:53:56,098 iteration 442 : loss : 0.332819, loss_ce: 0.178612
  6%|█▉                            | 26/400 [11:58<2:55:56, 28.23s/it]2021-11-30 11:53:57,608 iteration 443 : loss : 0.349291, loss_ce: 0.179253
2021-11-30 11:53:59,038 iteration 444 : loss : 0.338483, loss_ce: 0.155149
2021-11-30 11:54:00,528 iteration 445 : loss : 0.319314, loss_ce: 0.126839
2021-11-30 11:54:02,173 iteration 446 : loss : 0.318883, loss_ce: 0.150097
2021-11-30 11:54:03,720 iteration 447 : loss : 0.338365, loss_ce: 0.189773
2021-11-30 11:54:05,265 iteration 448 : loss : 0.324972, loss_ce: 0.127051
2021-11-30 11:54:06,750 iteration 449 : loss : 0.323540, loss_ce: 0.165498
2021-11-30 11:54:08,219 iteration 450 : loss : 0.348200, loss_ce: 0.155736
2021-11-30 11:54:09,797 iteration 451 : loss : 0.305012, loss_ce: 0.136154
2021-11-30 11:54:11,321 iteration 452 : loss : 0.306067, loss_ce: 0.140494
2021-11-30 11:54:12,829 iteration 453 : loss : 0.307656, loss_ce: 0.127835
2021-11-30 11:54:14,340 iteration 454 : loss : 0.299572, loss_ce: 0.112870
2021-11-30 11:54:15,901 iteration 455 : loss : 0.301901, loss_ce: 0.143729
2021-11-30 11:54:17,398 iteration 456 : loss : 0.292595, loss_ce: 0.127769
2021-11-30 11:54:18,889 iteration 457 : loss : 0.321147, loss_ce: 0.153644
2021-11-30 11:54:20,444 iteration 458 : loss : 0.331391, loss_ce: 0.164134
2021-11-30 11:54:21,848 iteration 459 : loss : 0.308855, loss_ce: 0.132507
  7%|██                            | 27/400 [12:24<2:50:51, 27.48s/it]2021-11-30 11:54:23,415 iteration 460 : loss : 0.327263, loss_ce: 0.166184
2021-11-30 11:54:24,928 iteration 461 : loss : 0.328726, loss_ce: 0.161410
2021-11-30 11:54:26,407 iteration 462 : loss : 0.305681, loss_ce: 0.138416
2021-11-30 11:54:27,964 iteration 463 : loss : 0.306629, loss_ce: 0.129679
2021-11-30 11:54:29,454 iteration 464 : loss : 0.311513, loss_ce: 0.148562
2021-11-30 11:54:30,895 iteration 465 : loss : 0.295456, loss_ce: 0.110232
2021-11-30 11:54:32,371 iteration 466 : loss : 0.312820, loss_ce: 0.128524
2021-11-30 11:54:33,820 iteration 467 : loss : 0.308039, loss_ce: 0.126859
2021-11-30 11:54:35,419 iteration 468 : loss : 0.333513, loss_ce: 0.145699
2021-11-30 11:54:36,890 iteration 469 : loss : 0.306031, loss_ce: 0.127231
2021-11-30 11:54:38,387 iteration 470 : loss : 0.306124, loss_ce: 0.142474
2021-11-30 11:54:39,850 iteration 471 : loss : 0.291491, loss_ce: 0.124032
2021-11-30 11:54:41,386 iteration 472 : loss : 0.303395, loss_ce: 0.149106
2021-11-30 11:54:43,026 iteration 473 : loss : 0.336676, loss_ce: 0.157841
2021-11-30 11:54:44,628 iteration 474 : loss : 0.329179, loss_ce: 0.163827
2021-11-30 11:54:46,074 iteration 475 : loss : 0.288362, loss_ce: 0.121296
2021-11-30 11:54:47,637 iteration 476 : loss : 0.311883, loss_ce: 0.163292
  7%|██                            | 28/400 [12:49<2:47:14, 26.97s/it]2021-11-30 11:54:49,175 iteration 477 : loss : 0.281914, loss_ce: 0.114601
2021-11-30 11:54:50,710 iteration 478 : loss : 0.295140, loss_ce: 0.120260
2021-11-30 11:54:52,181 iteration 479 : loss : 0.309210, loss_ce: 0.134193
2021-11-30 11:54:53,693 iteration 480 : loss : 0.314057, loss_ce: 0.156533
2021-11-30 11:54:55,092 iteration 481 : loss : 0.326505, loss_ce: 0.146524
2021-11-30 11:54:56,611 iteration 482 : loss : 0.294346, loss_ce: 0.102766
2021-11-30 11:54:58,133 iteration 483 : loss : 0.306349, loss_ce: 0.133610
2021-11-30 11:54:59,669 iteration 484 : loss : 0.347957, loss_ce: 0.176028
2021-11-30 11:55:01,133 iteration 485 : loss : 0.286547, loss_ce: 0.138923
2021-11-30 11:55:02,590 iteration 486 : loss : 0.287954, loss_ce: 0.126066
2021-11-30 11:55:04,140 iteration 487 : loss : 0.319405, loss_ce: 0.144518
2021-11-30 11:55:05,550 iteration 488 : loss : 0.297054, loss_ce: 0.138066
2021-11-30 11:55:07,052 iteration 489 : loss : 0.315454, loss_ce: 0.147933
2021-11-30 11:55:08,434 iteration 490 : loss : 0.310189, loss_ce: 0.133695
2021-11-30 11:55:09,947 iteration 491 : loss : 0.330565, loss_ce: 0.156605
2021-11-30 11:55:11,433 iteration 492 : loss : 0.299234, loss_ce: 0.138930
2021-11-30 11:55:12,947 iteration 493 : loss : 0.334450, loss_ce: 0.175904
  7%|██▏                           | 29/400 [13:15<2:43:41, 26.47s/it]2021-11-30 11:55:14,470 iteration 494 : loss : 0.308368, loss_ce: 0.149255
2021-11-30 11:55:16,002 iteration 495 : loss : 0.302136, loss_ce: 0.126202
2021-11-30 11:55:17,484 iteration 496 : loss : 0.370921, loss_ce: 0.206423
2021-11-30 11:55:19,024 iteration 497 : loss : 0.338462, loss_ce: 0.164885
2021-11-30 11:55:20,597 iteration 498 : loss : 0.308659, loss_ce: 0.161143
2021-11-30 11:55:21,997 iteration 499 : loss : 0.289587, loss_ce: 0.139280
2021-11-30 11:55:23,420 iteration 500 : loss : 0.308509, loss_ce: 0.135048
2021-11-30 11:55:24,868 iteration 501 : loss : 0.336455, loss_ce: 0.141723
2021-11-30 11:55:26,427 iteration 502 : loss : 0.326174, loss_ce: 0.150418
2021-11-30 11:55:27,884 iteration 503 : loss : 0.270056, loss_ce: 0.101674
2021-11-30 11:55:29,408 iteration 504 : loss : 0.303200, loss_ce: 0.120497
2021-11-30 11:55:30,840 iteration 505 : loss : 0.270876, loss_ce: 0.119122
2021-11-30 11:55:32,440 iteration 506 : loss : 0.274043, loss_ce: 0.117385
2021-11-30 11:55:33,962 iteration 507 : loss : 0.338339, loss_ce: 0.154880
2021-11-30 11:55:35,441 iteration 508 : loss : 0.340387, loss_ce: 0.157230
2021-11-30 11:55:36,963 iteration 509 : loss : 0.297993, loss_ce: 0.120228
2021-11-30 11:55:36,980 Training Data Eval:
2021-11-30 11:55:44,346   Average segmentation loss on training set: 0.2726
2021-11-30 11:55:44,346 Validation Data Eval:
2021-11-30 11:55:46,922   Average segmentation loss on validation set: 0.2818
2021-11-30 11:55:48,840 Found new lowest validation loss at iteration 509! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/UNET_RUNMC/best_val_loss.pth
2021-11-30 11:55:50,383 iteration 510 : loss : 0.299323, loss_ce: 0.135249
  8%|██▎                           | 30/400 [13:52<3:03:32, 29.76s/it]2021-11-30 11:55:51,907 iteration 511 : loss : 0.337339, loss_ce: 0.154603
2021-11-30 11:55:53,313 iteration 512 : loss : 0.322599, loss_ce: 0.140558
2021-11-30 11:55:54,745 iteration 513 : loss : 0.275964, loss_ce: 0.108226
2021-11-30 11:55:56,179 iteration 514 : loss : 0.304752, loss_ce: 0.134037
2021-11-30 11:55:57,598 iteration 515 : loss : 0.298588, loss_ce: 0.132833
2021-11-30 11:55:59,068 iteration 516 : loss : 0.305097, loss_ce: 0.114501
2021-11-30 11:56:00,586 iteration 517 : loss : 0.301071, loss_ce: 0.146316
2021-11-30 11:56:01,981 iteration 518 : loss : 0.304655, loss_ce: 0.118363
2021-11-30 11:56:03,452 iteration 519 : loss : 0.294395, loss_ce: 0.143473
2021-11-30 11:56:04,882 iteration 520 : loss : 0.288483, loss_ce: 0.127269
2021-11-30 11:56:06,383 iteration 521 : loss : 0.312935, loss_ce: 0.127793
2021-11-30 11:56:07,935 iteration 522 : loss : 0.278227, loss_ce: 0.119103
2021-11-30 11:56:09,503 iteration 523 : loss : 0.290241, loss_ce: 0.138035
2021-11-30 11:56:11,098 iteration 524 : loss : 0.273659, loss_ce: 0.123238
2021-11-30 11:56:12,590 iteration 525 : loss : 0.297690, loss_ce: 0.153633
2021-11-30 11:56:14,083 iteration 526 : loss : 0.325202, loss_ce: 0.159729
2021-11-30 11:56:15,572 iteration 527 : loss : 0.280678, loss_ce: 0.109100
  8%|██▎                           | 31/400 [14:17<2:54:35, 28.39s/it]2021-11-30 11:56:17,084 iteration 528 : loss : 0.283699, loss_ce: 0.140658
2021-11-30 11:56:18,570 iteration 529 : loss : 0.294680, loss_ce: 0.128044
2021-11-30 11:56:20,018 iteration 530 : loss : 0.300749, loss_ce: 0.117068
2021-11-30 11:56:21,460 iteration 531 : loss : 0.305279, loss_ce: 0.126288
2021-11-30 11:56:22,914 iteration 532 : loss : 0.317318, loss_ce: 0.162555
2021-11-30 11:56:24,504 iteration 533 : loss : 0.299430, loss_ce: 0.129232
2021-11-30 11:56:25,930 iteration 534 : loss : 0.263205, loss_ce: 0.132466
2021-11-30 11:56:27,453 iteration 535 : loss : 0.295874, loss_ce: 0.135883
2021-11-30 11:56:29,010 iteration 536 : loss : 0.330753, loss_ce: 0.168704
2021-11-30 11:56:30,599 iteration 537 : loss : 0.277262, loss_ce: 0.113298
2021-11-30 11:56:32,119 iteration 538 : loss : 0.290387, loss_ce: 0.134232
2021-11-30 11:56:33,583 iteration 539 : loss : 0.288319, loss_ce: 0.133725
2021-11-30 11:56:35,105 iteration 540 : loss : 0.292611, loss_ce: 0.119928
2021-11-30 11:56:36,576 iteration 541 : loss : 0.255740, loss_ce: 0.117484
2021-11-30 11:56:38,070 iteration 542 : loss : 0.338387, loss_ce: 0.172565
2021-11-30 11:56:39,488 iteration 543 : loss : 0.268315, loss_ce: 0.114470
2021-11-30 11:56:40,942 iteration 544 : loss : 0.289684, loss_ce: 0.120900
  8%|██▍                           | 32/400 [14:43<2:48:34, 27.48s/it]2021-11-30 11:56:42,489 iteration 545 : loss : 0.283498, loss_ce: 0.144947
2021-11-30 11:56:43,969 iteration 546 : loss : 0.262231, loss_ce: 0.112265
2021-11-30 11:56:45,468 iteration 547 : loss : 0.285046, loss_ce: 0.134093
2021-11-30 11:56:46,935 iteration 548 : loss : 0.303407, loss_ce: 0.129801
2021-11-30 11:56:48,454 iteration 549 : loss : 0.309107, loss_ce: 0.148804
2021-11-30 11:56:49,902 iteration 550 : loss : 0.298684, loss_ce: 0.119268
2021-11-30 11:56:51,387 iteration 551 : loss : 0.309964, loss_ce: 0.142202
2021-11-30 11:56:52,846 iteration 552 : loss : 0.293833, loss_ce: 0.130227
2021-11-30 11:56:54,374 iteration 553 : loss : 0.256350, loss_ce: 0.104616
2021-11-30 11:56:55,793 iteration 554 : loss : 0.272079, loss_ce: 0.139164
2021-11-30 11:56:57,275 iteration 555 : loss : 0.280277, loss_ce: 0.099285
2021-11-30 11:56:58,810 iteration 556 : loss : 0.247549, loss_ce: 0.115771
2021-11-30 11:57:00,221 iteration 557 : loss : 0.280953, loss_ce: 0.116285
2021-11-30 11:57:01,756 iteration 558 : loss : 0.265452, loss_ce: 0.100801
2021-11-30 11:57:03,223 iteration 559 : loss : 0.264504, loss_ce: 0.120948
2021-11-30 11:57:04,731 iteration 560 : loss : 0.272004, loss_ce: 0.095960
2021-11-30 11:57:06,258 iteration 561 : loss : 0.270321, loss_ce: 0.140204
  8%|██▍                           | 33/400 [15:08<2:44:08, 26.84s/it]2021-11-30 11:57:07,919 iteration 562 : loss : 0.300077, loss_ce: 0.148502
2021-11-30 11:57:09,284 iteration 563 : loss : 0.275263, loss_ce: 0.131825
2021-11-30 11:57:10,791 iteration 564 : loss : 0.276081, loss_ce: 0.137646
2021-11-30 11:57:12,324 iteration 565 : loss : 0.281836, loss_ce: 0.140409
2021-11-30 11:57:13,881 iteration 566 : loss : 0.270170, loss_ce: 0.124400
2021-11-30 11:57:15,333 iteration 567 : loss : 0.263958, loss_ce: 0.102974
2021-11-30 11:57:16,786 iteration 568 : loss : 0.285625, loss_ce: 0.104167
2021-11-30 11:57:18,270 iteration 569 : loss : 0.333583, loss_ce: 0.143741
2021-11-30 11:57:19,826 iteration 570 : loss : 0.264507, loss_ce: 0.121123
2021-11-30 11:57:21,284 iteration 571 : loss : 0.332205, loss_ce: 0.123469
2021-11-30 11:57:22,791 iteration 572 : loss : 0.257722, loss_ce: 0.107380
2021-11-30 11:57:24,239 iteration 573 : loss : 0.266655, loss_ce: 0.109930
2021-11-30 11:57:25,711 iteration 574 : loss : 0.282405, loss_ce: 0.108557
2021-11-30 11:57:27,261 iteration 575 : loss : 0.263465, loss_ce: 0.103870
2021-11-30 11:57:28,778 iteration 576 : loss : 0.284959, loss_ce: 0.126488
2021-11-30 11:57:30,279 iteration 577 : loss : 0.266694, loss_ce: 0.122151
2021-11-30 11:57:31,713 iteration 578 : loss : 0.312239, loss_ce: 0.145631
  8%|██▌                           | 34/400 [15:34<2:41:10, 26.42s/it]2021-11-30 11:57:33,440 iteration 579 : loss : 0.279664, loss_ce: 0.124530
2021-11-30 11:57:34,948 iteration 580 : loss : 0.318817, loss_ce: 0.147005
2021-11-30 11:57:36,489 iteration 581 : loss : 0.285111, loss_ce: 0.103686
2021-11-30 11:57:37,949 iteration 582 : loss : 0.252519, loss_ce: 0.114490
2021-11-30 11:57:39,448 iteration 583 : loss : 0.297527, loss_ce: 0.115550
2021-11-30 11:57:40,893 iteration 584 : loss : 0.247855, loss_ce: 0.112239
2021-11-30 11:57:42,449 iteration 585 : loss : 0.247161, loss_ce: 0.099708
2021-11-30 11:57:43,872 iteration 586 : loss : 0.265941, loss_ce: 0.119534
2021-11-30 11:57:45,311 iteration 587 : loss : 0.291518, loss_ce: 0.132010
2021-11-30 11:57:46,827 iteration 588 : loss : 0.263120, loss_ce: 0.108658
2021-11-30 11:57:48,296 iteration 589 : loss : 0.235914, loss_ce: 0.096390
2021-11-30 11:57:49,787 iteration 590 : loss : 0.273868, loss_ce: 0.118754
2021-11-30 11:57:51,284 iteration 591 : loss : 0.235071, loss_ce: 0.103191
2021-11-30 11:57:52,778 iteration 592 : loss : 0.263202, loss_ce: 0.115269
2021-11-30 11:57:54,313 iteration 593 : loss : 0.202129, loss_ce: 0.083524
2021-11-30 11:57:55,821 iteration 594 : loss : 0.235555, loss_ce: 0.110380
2021-11-30 11:57:55,821 Training Data Eval:
2021-11-30 11:58:03,197   Average segmentation loss on training set: 0.2286
2021-11-30 11:58:03,198 Validation Data Eval:
2021-11-30 11:58:05,749   Average segmentation loss on validation set: 0.2289
2021-11-30 11:58:07,686 Found new lowest validation loss at iteration 594! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/UNET_RUNMC/best_val_loss.pth
2021-11-30 11:58:09,251 iteration 595 : loss : 0.339552, loss_ce: 0.191252
  9%|██▋                           | 35/400 [16:11<3:01:01, 29.76s/it]2021-11-30 11:58:10,770 iteration 596 : loss : 0.248518, loss_ce: 0.117824
2021-11-30 11:58:12,186 iteration 597 : loss : 0.242250, loss_ce: 0.097888
2021-11-30 11:58:13,587 iteration 598 : loss : 0.249153, loss_ce: 0.102323
2021-11-30 11:58:15,012 iteration 599 : loss : 0.272593, loss_ce: 0.109036
2021-11-30 11:58:16,445 iteration 600 : loss : 0.299658, loss_ce: 0.142704
2021-11-30 11:58:17,926 iteration 601 : loss : 0.238318, loss_ce: 0.101264
2021-11-30 11:58:19,388 iteration 602 : loss : 0.244023, loss_ce: 0.109640
2021-11-30 11:58:20,763 iteration 603 : loss : 0.272623, loss_ce: 0.104252
2021-11-30 11:58:22,167 iteration 604 : loss : 0.264122, loss_ce: 0.104832
2021-11-30 11:58:23,593 iteration 605 : loss : 0.265667, loss_ce: 0.109716
2021-11-30 11:58:25,253 iteration 606 : loss : 0.256828, loss_ce: 0.119351
2021-11-30 11:58:26,728 iteration 607 : loss : 0.277656, loss_ce: 0.117798
2021-11-30 11:58:28,191 iteration 608 : loss : 0.265790, loss_ce: 0.126924
2021-11-30 11:58:29,583 iteration 609 : loss : 0.270432, loss_ce: 0.122754
2021-11-30 11:58:31,125 iteration 610 : loss : 0.238790, loss_ce: 0.105791
2021-11-30 11:58:32,724 iteration 611 : loss : 0.275101, loss_ce: 0.136619
2021-11-30 11:58:34,225 iteration 612 : loss : 0.219753, loss_ce: 0.092477
  9%|██▋                           | 36/400 [16:36<2:51:50, 28.33s/it]2021-11-30 11:58:35,812 iteration 613 : loss : 0.259852, loss_ce: 0.116853
2021-11-30 11:58:37,258 iteration 614 : loss : 0.274284, loss_ce: 0.111396
2021-11-30 11:58:38,703 iteration 615 : loss : 0.304734, loss_ce: 0.155544
2021-11-30 11:58:40,273 iteration 616 : loss : 0.270482, loss_ce: 0.133729
2021-11-30 11:58:41,707 iteration 617 : loss : 0.250467, loss_ce: 0.121610
2021-11-30 11:58:43,205 iteration 618 : loss : 0.257272, loss_ce: 0.097229
2021-11-30 11:58:44,710 iteration 619 : loss : 0.299542, loss_ce: 0.152034
2021-11-30 11:58:46,305 iteration 620 : loss : 0.280155, loss_ce: 0.098121
2021-11-30 11:58:47,782 iteration 621 : loss : 0.222072, loss_ce: 0.089236
2021-11-30 11:58:49,409 iteration 622 : loss : 0.267065, loss_ce: 0.120931
2021-11-30 11:58:50,999 iteration 623 : loss : 0.303484, loss_ce: 0.162422
2021-11-30 11:58:52,469 iteration 624 : loss : 0.238429, loss_ce: 0.094166
2021-11-30 11:58:53,903 iteration 625 : loss : 0.281919, loss_ce: 0.132502
2021-11-30 11:58:55,455 iteration 626 : loss : 0.282206, loss_ce: 0.129452
2021-11-30 11:58:57,008 iteration 627 : loss : 0.272219, loss_ce: 0.109295
2021-11-30 11:58:58,606 iteration 628 : loss : 0.294721, loss_ce: 0.126129
2021-11-30 11:59:00,081 iteration 629 : loss : 0.230696, loss_ce: 0.094609
  9%|██▊                           | 37/400 [17:02<2:46:51, 27.58s/it]2021-11-30 11:59:01,660 iteration 630 : loss : 0.254807, loss_ce: 0.115462
2021-11-30 11:59:03,114 iteration 631 : loss : 0.247964, loss_ce: 0.109564
2021-11-30 11:59:04,584 iteration 632 : loss : 0.297197, loss_ce: 0.142319
2021-11-30 11:59:06,161 iteration 633 : loss : 0.235117, loss_ce: 0.108358
2021-11-30 11:59:07,657 iteration 634 : loss : 0.270376, loss_ce: 0.111514
2021-11-30 11:59:09,255 iteration 635 : loss : 0.247258, loss_ce: 0.113526
2021-11-30 11:59:10,768 iteration 636 : loss : 0.296443, loss_ce: 0.126009
2021-11-30 11:59:12,168 iteration 637 : loss : 0.288988, loss_ce: 0.139351
2021-11-30 11:59:13,720 iteration 638 : loss : 0.256257, loss_ce: 0.097460
2021-11-30 11:59:15,166 iteration 639 : loss : 0.247301, loss_ce: 0.107929
2021-11-30 11:59:16,595 iteration 640 : loss : 0.276219, loss_ce: 0.094250
2021-11-30 11:59:18,017 iteration 641 : loss : 0.239604, loss_ce: 0.114781
2021-11-30 11:59:19,456 iteration 642 : loss : 0.242125, loss_ce: 0.092543
2021-11-30 11:59:20,934 iteration 643 : loss : 0.240927, loss_ce: 0.106474
2021-11-30 11:59:22,449 iteration 644 : loss : 0.220617, loss_ce: 0.101263
2021-11-30 11:59:23,898 iteration 645 : loss : 0.252417, loss_ce: 0.100663
2021-11-30 11:59:25,476 iteration 646 : loss : 0.282138, loss_ce: 0.134957
 10%|██▊                           | 38/400 [17:27<2:42:26, 26.92s/it]2021-11-30 11:59:26,990 iteration 647 : loss : 0.277539, loss_ce: 0.132634
2021-11-30 11:59:28,486 iteration 648 : loss : 0.263863, loss_ce: 0.119972
2021-11-30 11:59:29,992 iteration 649 : loss : 0.252665, loss_ce: 0.114516
2021-11-30 11:59:31,559 iteration 650 : loss : 0.255220, loss_ce: 0.097829
2021-11-30 11:59:33,042 iteration 651 : loss : 0.225310, loss_ce: 0.091872
2021-11-30 11:59:34,417 iteration 652 : loss : 0.194012, loss_ce: 0.072794
2021-11-30 11:59:35,983 iteration 653 : loss : 0.260417, loss_ce: 0.109396
2021-11-30 11:59:37,424 iteration 654 : loss : 0.239213, loss_ce: 0.090948
2021-11-30 11:59:38,917 iteration 655 : loss : 0.265857, loss_ce: 0.107515
2021-11-30 11:59:40,425 iteration 656 : loss : 0.247770, loss_ce: 0.109409
2021-11-30 11:59:42,007 iteration 657 : loss : 0.217674, loss_ce: 0.086082
2021-11-30 11:59:43,518 iteration 658 : loss : 0.251928, loss_ce: 0.099169
2021-11-30 11:59:45,027 iteration 659 : loss : 0.296935, loss_ce: 0.139107
2021-11-30 11:59:46,489 iteration 660 : loss : 0.264299, loss_ce: 0.130286
2021-11-30 11:59:48,020 iteration 661 : loss : 0.250658, loss_ce: 0.113616
2021-11-30 11:59:49,618 iteration 662 : loss : 0.265481, loss_ce: 0.092121
2021-11-30 11:59:51,111 iteration 663 : loss : 0.225002, loss_ce: 0.099443
 10%|██▉                           | 39/400 [17:53<2:39:38, 26.53s/it]2021-11-30 11:59:52,671 iteration 664 : loss : 0.228936, loss_ce: 0.103610
2021-11-30 11:59:54,105 iteration 665 : loss : 0.242387, loss_ce: 0.117053
2021-11-30 11:59:55,590 iteration 666 : loss : 0.257830, loss_ce: 0.102025
2021-11-30 11:59:57,128 iteration 667 : loss : 0.213622, loss_ce: 0.094226
2021-11-30 11:59:58,638 iteration 668 : loss : 0.237807, loss_ce: 0.102888
2021-11-30 12:00:00,106 iteration 669 : loss : 0.270236, loss_ce: 0.103342
2021-11-30 12:00:01,746 iteration 670 : loss : 0.207745, loss_ce: 0.085446
2021-11-30 12:00:03,193 iteration 671 : loss : 0.287894, loss_ce: 0.117619
2021-11-30 12:00:04,746 iteration 672 : loss : 0.229909, loss_ce: 0.093539
2021-11-30 12:00:06,268 iteration 673 : loss : 0.236267, loss_ce: 0.105032
2021-11-30 12:00:07,785 iteration 674 : loss : 0.239858, loss_ce: 0.118600
2021-11-30 12:00:09,333 iteration 675 : loss : 0.293663, loss_ce: 0.127514
2021-11-30 12:00:10,809 iteration 676 : loss : 0.245196, loss_ce: 0.093659
2021-11-30 12:00:12,319 iteration 677 : loss : 0.266970, loss_ce: 0.131902
2021-11-30 12:00:13,884 iteration 678 : loss : 0.240822, loss_ce: 0.094180
2021-11-30 12:00:15,386 iteration 679 : loss : 0.289179, loss_ce: 0.138854
2021-11-30 12:00:15,386 Training Data Eval:
2021-11-30 12:00:22,714   Average segmentation loss on training set: 0.2087
2021-11-30 12:00:22,714 Validation Data Eval:
2021-11-30 12:00:25,281   Average segmentation loss on validation set: 0.2137
2021-11-30 12:00:27,205 Found new lowest validation loss at iteration 679! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/UNET_RUNMC/best_val_loss.pth
2021-11-30 12:00:28,699 iteration 680 : loss : 0.249091, loss_ce: 0.112949
 10%|███                           | 40/400 [18:31<2:59:07, 29.85s/it]2021-11-30 12:00:30,224 iteration 681 : loss : 0.201368, loss_ce: 0.080637
2021-11-30 12:00:31,591 iteration 682 : loss : 0.246869, loss_ce: 0.089478
2021-11-30 12:00:33,040 iteration 683 : loss : 0.210639, loss_ce: 0.097436
2021-11-30 12:00:34,443 iteration 684 : loss : 0.202953, loss_ce: 0.085000
2021-11-30 12:00:35,972 iteration 685 : loss : 0.250051, loss_ce: 0.109643
2021-11-30 12:00:37,411 iteration 686 : loss : 0.245344, loss_ce: 0.105684
2021-11-30 12:00:38,858 iteration 687 : loss : 0.233138, loss_ce: 0.105428
2021-11-30 12:00:40,289 iteration 688 : loss : 0.236049, loss_ce: 0.086635
2021-11-30 12:00:41,776 iteration 689 : loss : 0.221970, loss_ce: 0.089356
2021-11-30 12:00:43,258 iteration 690 : loss : 0.242080, loss_ce: 0.097389
2021-11-30 12:00:44,767 iteration 691 : loss : 0.268665, loss_ce: 0.122969
2021-11-30 12:00:46,316 iteration 692 : loss : 0.236713, loss_ce: 0.094816
2021-11-30 12:00:47,753 iteration 693 : loss : 0.224832, loss_ce: 0.101660
2021-11-30 12:00:49,266 iteration 694 : loss : 0.232299, loss_ce: 0.102667
2021-11-30 12:00:50,718 iteration 695 : loss : 0.267723, loss_ce: 0.124011
2021-11-30 12:00:52,288 iteration 696 : loss : 0.255295, loss_ce: 0.106870
2021-11-30 12:00:53,784 iteration 697 : loss : 0.225759, loss_ce: 0.100731
 10%|███                           | 41/400 [18:56<2:50:04, 28.42s/it]2021-11-30 12:00:55,280 iteration 698 : loss : 0.226892, loss_ce: 0.084371
2021-11-30 12:00:56,827 iteration 699 : loss : 0.255824, loss_ce: 0.119454
2021-11-30 12:00:58,295 iteration 700 : loss : 0.258052, loss_ce: 0.118459
2021-11-30 12:00:59,747 iteration 701 : loss : 0.233772, loss_ce: 0.117165
2021-11-30 12:01:01,276 iteration 702 : loss : 0.227566, loss_ce: 0.089486
2021-11-30 12:01:02,864 iteration 703 : loss : 0.207243, loss_ce: 0.090532
2021-11-30 12:01:04,274 iteration 704 : loss : 0.198263, loss_ce: 0.080077
2021-11-30 12:01:05,774 iteration 705 : loss : 0.276751, loss_ce: 0.106749
2021-11-30 12:01:07,289 iteration 706 : loss : 0.193975, loss_ce: 0.080050
2021-11-30 12:01:08,704 iteration 707 : loss : 0.225626, loss_ce: 0.101813
2021-11-30 12:01:10,180 iteration 708 : loss : 0.223623, loss_ce: 0.092831
2021-11-30 12:01:11,671 iteration 709 : loss : 0.223042, loss_ce: 0.101332
2021-11-30 12:01:13,212 iteration 710 : loss : 0.225370, loss_ce: 0.092876
2021-11-30 12:01:14,736 iteration 711 : loss : 0.231554, loss_ce: 0.106569
2021-11-30 12:01:16,333 iteration 712 : loss : 0.221092, loss_ce: 0.088538
2021-11-30 12:01:17,831 iteration 713 : loss : 0.229863, loss_ce: 0.102374
2021-11-30 12:01:19,400 iteration 714 : loss : 0.245233, loss_ce: 0.098767
 10%|███▏                          | 42/400 [19:21<2:44:35, 27.59s/it]2021-11-30 12:01:20,987 iteration 715 : loss : 0.218298, loss_ce: 0.085701
2021-11-30 12:01:22,521 iteration 716 : loss : 0.214055, loss_ce: 0.093550
2021-11-30 12:01:24,003 iteration 717 : loss : 0.294398, loss_ce: 0.118170
2021-11-30 12:01:25,495 iteration 718 : loss : 0.251844, loss_ce: 0.118121
2021-11-30 12:01:27,049 iteration 719 : loss : 0.219738, loss_ce: 0.093233
2021-11-30 12:01:28,529 iteration 720 : loss : 0.228000, loss_ce: 0.094901
2021-11-30 12:01:29,999 iteration 721 : loss : 0.221737, loss_ce: 0.086394
2021-11-30 12:01:31,480 iteration 722 : loss : 0.233917, loss_ce: 0.100007
2021-11-30 12:01:33,034 iteration 723 : loss : 0.198470, loss_ce: 0.092373
2021-11-30 12:01:34,502 iteration 724 : loss : 0.260349, loss_ce: 0.131797
2021-11-30 12:01:36,065 iteration 725 : loss : 0.204491, loss_ce: 0.083490
2021-11-30 12:01:37,550 iteration 726 : loss : 0.230623, loss_ce: 0.105880
2021-11-30 12:01:38,975 iteration 727 : loss : 0.187885, loss_ce: 0.073945
2021-11-30 12:01:40,515 iteration 728 : loss : 0.226075, loss_ce: 0.104049
2021-11-30 12:01:42,075 iteration 729 : loss : 0.176245, loss_ce: 0.073440
2021-11-30 12:01:43,578 iteration 730 : loss : 0.202899, loss_ce: 0.085029
2021-11-30 12:01:44,958 iteration 731 : loss : 0.182835, loss_ce: 0.072565
 11%|███▏                          | 43/400 [19:47<2:40:30, 26.98s/it]2021-11-30 12:01:46,453 iteration 732 : loss : 0.227711, loss_ce: 0.106510
2021-11-30 12:01:47,856 iteration 733 : loss : 0.215156, loss_ce: 0.099081
2021-11-30 12:01:49,390 iteration 734 : loss : 0.172897, loss_ce: 0.069436
2021-11-30 12:01:50,872 iteration 735 : loss : 0.214644, loss_ce: 0.083242
2021-11-30 12:01:52,343 iteration 736 : loss : 0.257157, loss_ce: 0.104234
2021-11-30 12:01:53,959 iteration 737 : loss : 0.256510, loss_ce: 0.123615
2021-11-30 12:01:55,357 iteration 738 : loss : 0.219927, loss_ce: 0.107260
2021-11-30 12:01:56,901 iteration 739 : loss : 0.223742, loss_ce: 0.085892
2021-11-30 12:01:58,372 iteration 740 : loss : 0.224051, loss_ce: 0.094919
2021-11-30 12:01:59,841 iteration 741 : loss : 0.225305, loss_ce: 0.094820
2021-11-30 12:02:01,241 iteration 742 : loss : 0.269801, loss_ce: 0.137626
2021-11-30 12:02:02,715 iteration 743 : loss : 0.195711, loss_ce: 0.079268
2021-11-30 12:02:04,247 iteration 744 : loss : 0.214153, loss_ce: 0.093921
2021-11-30 12:02:05,689 iteration 745 : loss : 0.214566, loss_ce: 0.068526
2021-11-30 12:02:07,148 iteration 746 : loss : 0.223844, loss_ce: 0.095286
2021-11-30 12:02:08,602 iteration 747 : loss : 0.284013, loss_ce: 0.129767
2021-11-30 12:02:10,048 iteration 748 : loss : 0.214934, loss_ce: 0.092683
 11%|███▎                          | 44/400 [20:12<2:36:42, 26.41s/it]2021-11-30 12:02:11,542 iteration 749 : loss : 0.325875, loss_ce: 0.084902
2021-11-30 12:02:13,012 iteration 750 : loss : 0.195413, loss_ce: 0.077776
2021-11-30 12:02:14,509 iteration 751 : loss : 0.201389, loss_ce: 0.070438
2021-11-30 12:02:15,981 iteration 752 : loss : 0.215277, loss_ce: 0.100867
2021-11-30 12:02:17,414 iteration 753 : loss : 0.223463, loss_ce: 0.086903
2021-11-30 12:02:18,856 iteration 754 : loss : 0.207763, loss_ce: 0.080932
2021-11-30 12:02:20,261 iteration 755 : loss : 0.254160, loss_ce: 0.123600
2021-11-30 12:02:21,674 iteration 756 : loss : 0.197263, loss_ce: 0.075119
2021-11-30 12:02:23,142 iteration 757 : loss : 0.232502, loss_ce: 0.108683
2021-11-30 12:02:24,715 iteration 758 : loss : 0.200295, loss_ce: 0.081665
2021-11-30 12:02:26,351 iteration 759 : loss : 0.217049, loss_ce: 0.093310
2021-11-30 12:02:27,801 iteration 760 : loss : 0.270052, loss_ce: 0.143727
2021-11-30 12:02:29,299 iteration 761 : loss : 0.240052, loss_ce: 0.112840
2021-11-30 12:02:30,817 iteration 762 : loss : 0.224803, loss_ce: 0.102280
2021-11-30 12:02:32,376 iteration 763 : loss : 0.232882, loss_ce: 0.101903
2021-11-30 12:02:33,830 iteration 764 : loss : 0.245446, loss_ce: 0.109540
2021-11-30 12:02:33,830 Training Data Eval:
2021-11-30 12:02:41,169   Average segmentation loss on training set: 0.1776
2021-11-30 12:02:41,170 Validation Data Eval:
2021-11-30 12:02:43,724   Average segmentation loss on validation set: 0.1875
2021-11-30 12:02:45,645 Found new lowest validation loss at iteration 764! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/UNET_RUNMC/best_val_loss.pth
2021-11-30 12:02:47,335 iteration 765 : loss : 0.182742, loss_ce: 0.075736
 11%|███▍                          | 45/400 [20:49<2:55:32, 29.67s/it]2021-11-30 12:02:48,903 iteration 766 : loss : 0.245186, loss_ce: 0.117061
2021-11-30 12:02:50,391 iteration 767 : loss : 0.230439, loss_ce: 0.103368
2021-11-30 12:02:51,877 iteration 768 : loss : 0.233906, loss_ce: 0.095957
2021-11-30 12:02:53,390 iteration 769 : loss : 0.211165, loss_ce: 0.094126
2021-11-30 12:02:54,749 iteration 770 : loss : 0.186039, loss_ce: 0.075066
2021-11-30 12:02:56,195 iteration 771 : loss : 0.245819, loss_ce: 0.073122
2021-11-30 12:02:57,672 iteration 772 : loss : 0.228328, loss_ce: 0.094650
2021-11-30 12:02:59,152 iteration 773 : loss : 0.193414, loss_ce: 0.060694
2021-11-30 12:03:00,726 iteration 774 : loss : 0.215994, loss_ce: 0.092683
2021-11-30 12:03:02,149 iteration 775 : loss : 0.206640, loss_ce: 0.089960
2021-11-30 12:03:03,555 iteration 776 : loss : 0.222819, loss_ce: 0.096106
2021-11-30 12:03:05,223 iteration 777 : loss : 0.215810, loss_ce: 0.096777
2021-11-30 12:03:06,605 iteration 778 : loss : 0.216134, loss_ce: 0.082513
2021-11-30 12:03:08,160 iteration 779 : loss : 0.228714, loss_ce: 0.104491
2021-11-30 12:03:09,714 iteration 780 : loss : 0.230041, loss_ce: 0.096478
2021-11-30 12:03:11,161 iteration 781 : loss : 0.197102, loss_ce: 0.091080
2021-11-30 12:03:12,572 iteration 782 : loss : 0.197575, loss_ce: 0.092569
 12%|███▍                          | 46/400 [21:14<2:47:13, 28.34s/it]2021-11-30 12:03:14,124 iteration 783 : loss : 0.226836, loss_ce: 0.107916
2021-11-30 12:03:15,577 iteration 784 : loss : 0.227997, loss_ce: 0.097240
2021-11-30 12:03:17,070 iteration 785 : loss : 0.234283, loss_ce: 0.117262
2021-11-30 12:03:18,537 iteration 786 : loss : 0.257088, loss_ce: 0.110303
2021-11-30 12:03:20,018 iteration 787 : loss : 0.230412, loss_ce: 0.104460
2021-11-30 12:03:21,582 iteration 788 : loss : 0.228998, loss_ce: 0.105593
2021-11-30 12:03:22,981 iteration 789 : loss : 0.194394, loss_ce: 0.076235
2021-11-30 12:03:24,508 iteration 790 : loss : 0.231652, loss_ce: 0.092217
2021-11-30 12:03:26,003 iteration 791 : loss : 0.161471, loss_ce: 0.066538
2021-11-30 12:03:27,413 iteration 792 : loss : 0.223266, loss_ce: 0.105526
2021-11-30 12:03:28,967 iteration 793 : loss : 0.212321, loss_ce: 0.103586
2021-11-30 12:03:30,366 iteration 794 : loss : 0.178332, loss_ce: 0.077998
2021-11-30 12:03:31,811 iteration 795 : loss : 0.243178, loss_ce: 0.099804
2021-11-30 12:03:33,228 iteration 796 : loss : 0.217743, loss_ce: 0.098696
2021-11-30 12:03:34,751 iteration 797 : loss : 0.256072, loss_ce: 0.118698
2021-11-30 12:03:36,205 iteration 798 : loss : 0.241183, loss_ce: 0.086806
2021-11-30 12:03:37,603 iteration 799 : loss : 0.190045, loss_ce: 0.078838
 12%|███▌                          | 47/400 [21:39<2:40:54, 27.35s/it]2021-11-30 12:03:39,270 iteration 800 : loss : 0.185573, loss_ce: 0.072701
2021-11-30 12:03:40,785 iteration 801 : loss : 0.188340, loss_ce: 0.080032
2021-11-30 12:03:42,283 iteration 802 : loss : 0.228597, loss_ce: 0.088021
2021-11-30 12:03:43,778 iteration 803 : loss : 0.219178, loss_ce: 0.105703
2021-11-30 12:03:45,361 iteration 804 : loss : 0.283945, loss_ce: 0.133147
2021-11-30 12:03:47,003 iteration 805 : loss : 0.170533, loss_ce: 0.065154
2021-11-30 12:03:48,447 iteration 806 : loss : 0.197009, loss_ce: 0.081022
2021-11-30 12:03:49,864 iteration 807 : loss : 0.196472, loss_ce: 0.074515
2021-11-30 12:03:51,414 iteration 808 : loss : 0.240134, loss_ce: 0.120779
2021-11-30 12:03:52,903 iteration 809 : loss : 0.171730, loss_ce: 0.067475
2021-11-30 12:03:54,352 iteration 810 : loss : 0.211108, loss_ce: 0.086622
2021-11-30 12:03:55,783 iteration 811 : loss : 0.229100, loss_ce: 0.122135
2021-11-30 12:03:57,235 iteration 812 : loss : 0.208325, loss_ce: 0.093457
2021-11-30 12:03:58,732 iteration 813 : loss : 0.201728, loss_ce: 0.076529
2021-11-30 12:04:00,373 iteration 814 : loss : 0.189805, loss_ce: 0.068345
2021-11-30 12:04:01,924 iteration 815 : loss : 0.210421, loss_ce: 0.087384
2021-11-30 12:04:03,415 iteration 816 : loss : 0.223295, loss_ce: 0.087024
 12%|███▌                          | 48/400 [22:05<2:37:44, 26.89s/it]2021-11-30 12:04:04,936 iteration 817 : loss : 0.205938, loss_ce: 0.086311
2021-11-30 12:04:06,365 iteration 818 : loss : 0.178451, loss_ce: 0.074762
2021-11-30 12:04:07,824 iteration 819 : loss : 0.209155, loss_ce: 0.081361
2021-11-30 12:04:09,239 iteration 820 : loss : 0.209774, loss_ce: 0.082244
2021-11-30 12:04:10,801 iteration 821 : loss : 0.181613, loss_ce: 0.070886
2021-11-30 12:04:12,318 iteration 822 : loss : 0.183398, loss_ce: 0.077452
2021-11-30 12:04:13,745 iteration 823 : loss : 0.174584, loss_ce: 0.073875
2021-11-30 12:04:15,175 iteration 824 : loss : 0.238805, loss_ce: 0.106905
2021-11-30 12:04:16,664 iteration 825 : loss : 0.184298, loss_ce: 0.074757
2021-11-30 12:04:18,153 iteration 826 : loss : 0.200212, loss_ce: 0.070531
2021-11-30 12:04:19,626 iteration 827 : loss : 0.244811, loss_ce: 0.089808
2021-11-30 12:04:21,120 iteration 828 : loss : 0.215225, loss_ce: 0.090667
2021-11-30 12:04:22,618 iteration 829 : loss : 0.208986, loss_ce: 0.086465
2021-11-30 12:04:24,159 iteration 830 : loss : 0.192660, loss_ce: 0.075553
2021-11-30 12:04:25,600 iteration 831 : loss : 0.220784, loss_ce: 0.103666
2021-11-30 12:04:27,039 iteration 832 : loss : 0.147183, loss_ce: 0.061523
2021-11-30 12:04:28,580 iteration 833 : loss : 0.214520, loss_ce: 0.098306
 12%|███▋                          | 49/400 [22:30<2:34:16, 26.37s/it]2021-11-30 12:04:30,115 iteration 834 : loss : 0.215545, loss_ce: 0.089810
2021-11-30 12:04:31,488 iteration 835 : loss : 0.180150, loss_ce: 0.065458
2021-11-30 12:04:32,982 iteration 836 : loss : 0.151171, loss_ce: 0.064413
2021-11-30 12:04:34,412 iteration 837 : loss : 0.182909, loss_ce: 0.086030
2021-11-30 12:04:35,814 iteration 838 : loss : 0.187914, loss_ce: 0.076395
2021-11-30 12:04:37,325 iteration 839 : loss : 0.201329, loss_ce: 0.087188
2021-11-30 12:04:38,785 iteration 840 : loss : 0.220298, loss_ce: 0.085015
2021-11-30 12:04:40,216 iteration 841 : loss : 0.225768, loss_ce: 0.098267
2021-11-30 12:04:41,681 iteration 842 : loss : 0.186808, loss_ce: 0.083168
2021-11-30 12:04:43,290 iteration 843 : loss : 0.182450, loss_ce: 0.077446
2021-11-30 12:04:44,741 iteration 844 : loss : 0.184097, loss_ce: 0.072887
2021-11-30 12:04:46,234 iteration 845 : loss : 0.188179, loss_ce: 0.073130
2021-11-30 12:04:47,856 iteration 846 : loss : 0.206433, loss_ce: 0.101773
2021-11-30 12:04:49,344 iteration 847 : loss : 0.164421, loss_ce: 0.068821
2021-11-30 12:04:50,709 iteration 848 : loss : 0.173302, loss_ce: 0.074289
2021-11-30 12:04:52,209 iteration 849 : loss : 0.181156, loss_ce: 0.072465
2021-11-30 12:04:52,209 Training Data Eval:
2021-11-30 12:04:59,588   Average segmentation loss on training set: 0.2110
2021-11-30 12:04:59,588 Validation Data Eval:
2021-11-30 12:05:02,123   Average segmentation loss on validation set: 0.2868
2021-11-30 12:05:03,668 iteration 850 : loss : 0.199256, loss_ce: 0.084600
 12%|███▊                          | 50/400 [23:06<2:49:05, 28.99s/it]2021-11-30 12:05:05,240 iteration 851 : loss : 0.183964, loss_ce: 0.069135
2021-11-30 12:05:06,707 iteration 852 : loss : 0.202409, loss_ce: 0.083575
2021-11-30 12:05:08,267 iteration 853 : loss : 0.172165, loss_ce: 0.078524
2021-11-30 12:05:09,774 iteration 854 : loss : 0.167900, loss_ce: 0.059879
2021-11-30 12:05:11,260 iteration 855 : loss : 0.215192, loss_ce: 0.093886
2021-11-30 12:05:12,780 iteration 856 : loss : 0.196642, loss_ce: 0.082601
2021-11-30 12:05:14,250 iteration 857 : loss : 0.182234, loss_ce: 0.077729
2021-11-30 12:05:15,641 iteration 858 : loss : 0.189734, loss_ce: 0.083689
2021-11-30 12:05:17,109 iteration 859 : loss : 0.217325, loss_ce: 0.080339
2021-11-30 12:05:18,525 iteration 860 : loss : 0.227849, loss_ce: 0.094925
2021-11-30 12:05:20,064 iteration 861 : loss : 0.248827, loss_ce: 0.117578
2021-11-30 12:05:21,465 iteration 862 : loss : 0.211456, loss_ce: 0.082117
2021-11-30 12:05:23,030 iteration 863 : loss : 0.229734, loss_ce: 0.103920
2021-11-30 12:05:24,559 iteration 864 : loss : 0.187999, loss_ce: 0.081777
2021-11-30 12:05:25,945 iteration 865 : loss : 0.173756, loss_ce: 0.074778
2021-11-30 12:05:27,495 iteration 866 : loss : 0.214293, loss_ce: 0.096845
2021-11-30 12:05:28,916 iteration 867 : loss : 0.137892, loss_ce: 0.056184
 13%|███▊                          | 51/400 [23:31<2:42:04, 27.86s/it]2021-11-30 12:05:30,483 iteration 868 : loss : 0.242115, loss_ce: 0.090949
2021-11-30 12:05:31,965 iteration 869 : loss : 0.196673, loss_ce: 0.077157
2021-11-30 12:05:33,345 iteration 870 : loss : 0.154618, loss_ce: 0.058769
2021-11-30 12:05:34,760 iteration 871 : loss : 0.160485, loss_ce: 0.066658
2021-11-30 12:05:36,199 iteration 872 : loss : 0.158599, loss_ce: 0.071610
2021-11-30 12:05:37,766 iteration 873 : loss : 0.208820, loss_ce: 0.079416
2021-11-30 12:05:39,158 iteration 874 : loss : 0.189980, loss_ce: 0.081967
2021-11-30 12:05:40,813 iteration 875 : loss : 0.179829, loss_ce: 0.072542
2021-11-30 12:05:42,297 iteration 876 : loss : 0.155434, loss_ce: 0.067998
2021-11-30 12:05:43,673 iteration 877 : loss : 0.178879, loss_ce: 0.069434
2021-11-30 12:05:45,211 iteration 878 : loss : 0.173228, loss_ce: 0.071768
2021-11-30 12:05:46,659 iteration 879 : loss : 0.217493, loss_ce: 0.095963
2021-11-30 12:05:48,172 iteration 880 : loss : 0.174044, loss_ce: 0.062686
2021-11-30 12:05:49,620 iteration 881 : loss : 0.235482, loss_ce: 0.103886
2021-11-30 12:05:51,150 iteration 882 : loss : 0.200414, loss_ce: 0.084251
2021-11-30 12:05:52,625 iteration 883 : loss : 0.190738, loss_ce: 0.079132
2021-11-30 12:05:54,192 iteration 884 : loss : 0.185861, loss_ce: 0.077975
 13%|███▉                          | 52/400 [23:56<2:37:05, 27.09s/it]2021-11-30 12:05:55,759 iteration 885 : loss : 0.211172, loss_ce: 0.092364
2021-11-30 12:05:57,278 iteration 886 : loss : 0.175998, loss_ce: 0.076559
2021-11-30 12:05:58,798 iteration 887 : loss : 0.261655, loss_ce: 0.122034
2021-11-30 12:06:00,290 iteration 888 : loss : 0.174540, loss_ce: 0.065533
2021-11-30 12:06:01,835 iteration 889 : loss : 0.149864, loss_ce: 0.058381
2021-11-30 12:06:03,436 iteration 890 : loss : 0.177490, loss_ce: 0.070792
2021-11-30 12:06:04,894 iteration 891 : loss : 0.184116, loss_ce: 0.073945
2021-11-30 12:06:06,350 iteration 892 : loss : 0.174981, loss_ce: 0.080778
2021-11-30 12:06:07,794 iteration 893 : loss : 0.163334, loss_ce: 0.066255
2021-11-30 12:06:09,413 iteration 894 : loss : 0.164447, loss_ce: 0.064608
2021-11-30 12:06:10,840 iteration 895 : loss : 0.201077, loss_ce: 0.069763
2021-11-30 12:06:12,391 iteration 896 : loss : 0.221255, loss_ce: 0.093105
2021-11-30 12:06:13,972 iteration 897 : loss : 0.168012, loss_ce: 0.074366
2021-11-30 12:06:15,415 iteration 898 : loss : 0.144303, loss_ce: 0.066287
2021-11-30 12:06:16,880 iteration 899 : loss : 0.180702, loss_ce: 0.083452
2021-11-30 12:06:18,304 iteration 900 : loss : 0.165223, loss_ce: 0.075279
2021-11-30 12:06:19,741 iteration 901 : loss : 0.233243, loss_ce: 0.097361
 13%|███▉                          | 53/400 [24:22<2:33:59, 26.63s/it]2021-11-30 12:06:21,310 iteration 902 : loss : 0.142043, loss_ce: 0.067345
2021-11-30 12:06:22,895 iteration 903 : loss : 0.174447, loss_ce: 0.075533
2021-11-30 12:06:24,409 iteration 904 : loss : 0.181060, loss_ce: 0.066965
2021-11-30 12:06:25,912 iteration 905 : loss : 0.208913, loss_ce: 0.099613
2021-11-30 12:06:27,386 iteration 906 : loss : 0.173526, loss_ce: 0.070470
2021-11-30 12:06:28,952 iteration 907 : loss : 0.209593, loss_ce: 0.076807
2021-11-30 12:06:30,335 iteration 908 : loss : 0.154127, loss_ce: 0.066916
2021-11-30 12:06:31,853 iteration 909 : loss : 0.172132, loss_ce: 0.079277
2021-11-30 12:06:33,259 iteration 910 : loss : 0.191240, loss_ce: 0.082214
2021-11-30 12:06:34,739 iteration 911 : loss : 0.168960, loss_ce: 0.073515
2021-11-30 12:06:36,203 iteration 912 : loss : 0.187978, loss_ce: 0.074828
2021-11-30 12:06:37,650 iteration 913 : loss : 0.190895, loss_ce: 0.067895
2021-11-30 12:06:39,124 iteration 914 : loss : 0.238673, loss_ce: 0.099282
2021-11-30 12:06:40,552 iteration 915 : loss : 0.210152, loss_ce: 0.068315
2021-11-30 12:06:41,992 iteration 916 : loss : 0.132689, loss_ce: 0.055909
2021-11-30 12:06:43,507 iteration 917 : loss : 0.207268, loss_ce: 0.097064
2021-11-30 12:06:45,057 iteration 918 : loss : 0.202670, loss_ce: 0.082764
 14%|████                          | 54/400 [24:47<2:31:16, 26.23s/it]2021-11-30 12:06:46,642 iteration 919 : loss : 0.138848, loss_ce: 0.068471
2021-11-30 12:06:48,120 iteration 920 : loss : 0.163214, loss_ce: 0.071212
2021-11-30 12:06:49,657 iteration 921 : loss : 0.191565, loss_ce: 0.080870
2021-11-30 12:06:51,196 iteration 922 : loss : 0.181915, loss_ce: 0.085435
2021-11-30 12:06:52,637 iteration 923 : loss : 0.163137, loss_ce: 0.078972
2021-11-30 12:06:54,157 iteration 924 : loss : 0.220258, loss_ce: 0.080654
2021-11-30 12:06:55,602 iteration 925 : loss : 0.163188, loss_ce: 0.061348
2021-11-30 12:06:57,150 iteration 926 : loss : 0.166589, loss_ce: 0.069536
2021-11-30 12:06:58,560 iteration 927 : loss : 0.186446, loss_ce: 0.075289
2021-11-30 12:07:00,046 iteration 928 : loss : 0.133870, loss_ce: 0.059607
2021-11-30 12:07:01,597 iteration 929 : loss : 0.152605, loss_ce: 0.062089
2021-11-30 12:07:03,096 iteration 930 : loss : 0.161552, loss_ce: 0.071093
2021-11-30 12:07:04,600 iteration 931 : loss : 0.183516, loss_ce: 0.054417
2021-11-30 12:07:06,088 iteration 932 : loss : 0.152266, loss_ce: 0.057197
2021-11-30 12:07:07,530 iteration 933 : loss : 0.186819, loss_ce: 0.076240
2021-11-30 12:07:09,011 iteration 934 : loss : 0.147521, loss_ce: 0.058038
2021-11-30 12:07:09,011 Training Data Eval:
2021-11-30 12:07:16,369   Average segmentation loss on training set: 0.1439
2021-11-30 12:07:16,369 Validation Data Eval:
2021-11-30 12:07:18,918   Average segmentation loss on validation set: 0.2011
2021-11-30 12:07:20,530 iteration 935 : loss : 0.167556, loss_ce: 0.062258
 14%|████▏                         | 55/400 [25:22<2:46:46, 29.00s/it]2021-11-30 12:07:22,048 iteration 936 : loss : 0.144937, loss_ce: 0.057065
2021-11-30 12:07:23,553 iteration 937 : loss : 0.174000, loss_ce: 0.057754
2021-11-30 12:07:25,081 iteration 938 : loss : 0.165457, loss_ce: 0.064175
2021-11-30 12:07:26,528 iteration 939 : loss : 0.181063, loss_ce: 0.071930
2021-11-30 12:07:28,054 iteration 940 : loss : 0.187373, loss_ce: 0.077092
2021-11-30 12:07:29,674 iteration 941 : loss : 0.184259, loss_ce: 0.080915
2021-11-30 12:07:31,106 iteration 942 : loss : 0.144162, loss_ce: 0.059395
2021-11-30 12:07:32,569 iteration 943 : loss : 0.157865, loss_ce: 0.071409
2021-11-30 12:07:34,015 iteration 944 : loss : 0.140558, loss_ce: 0.060799
2021-11-30 12:07:35,473 iteration 945 : loss : 0.162952, loss_ce: 0.064675
2021-11-30 12:07:36,928 iteration 946 : loss : 0.138459, loss_ce: 0.060497
2021-11-30 12:07:38,353 iteration 947 : loss : 0.163485, loss_ce: 0.067618
2021-11-30 12:07:39,788 iteration 948 : loss : 0.164216, loss_ce: 0.075257
2021-11-30 12:07:41,270 iteration 949 : loss : 0.156239, loss_ce: 0.061263
2021-11-30 12:07:42,739 iteration 950 : loss : 0.180721, loss_ce: 0.078724
2021-11-30 12:07:44,249 iteration 951 : loss : 0.154502, loss_ce: 0.081792
2021-11-30 12:07:45,672 iteration 952 : loss : 0.139492, loss_ce: 0.061101
 14%|████▏                         | 56/400 [25:48<2:39:38, 27.85s/it]2021-11-30 12:07:47,237 iteration 953 : loss : 0.173521, loss_ce: 0.070716
2021-11-30 12:07:48,765 iteration 954 : loss : 0.137571, loss_ce: 0.059965
2021-11-30 12:07:50,165 iteration 955 : loss : 0.124916, loss_ce: 0.056684
2021-11-30 12:07:51,713 iteration 956 : loss : 0.164639, loss_ce: 0.067858
2021-11-30 12:07:53,214 iteration 957 : loss : 0.167268, loss_ce: 0.063724
2021-11-30 12:07:54,738 iteration 958 : loss : 0.186029, loss_ce: 0.067375
2021-11-30 12:07:56,327 iteration 959 : loss : 0.179688, loss_ce: 0.078797
2021-11-30 12:07:57,979 iteration 960 : loss : 0.191026, loss_ce: 0.085481
2021-11-30 12:07:59,415 iteration 961 : loss : 0.235115, loss_ce: 0.082895
2021-11-30 12:08:00,816 iteration 962 : loss : 0.185024, loss_ce: 0.062391
2021-11-30 12:08:02,380 iteration 963 : loss : 0.133777, loss_ce: 0.054890
2021-11-30 12:08:03,909 iteration 964 : loss : 0.175492, loss_ce: 0.081601
2021-11-30 12:08:05,374 iteration 965 : loss : 0.136841, loss_ce: 0.070541
2021-11-30 12:08:06,853 iteration 966 : loss : 0.150613, loss_ce: 0.067941
2021-11-30 12:08:08,348 iteration 967 : loss : 0.156982, loss_ce: 0.057624
2021-11-30 12:08:09,770 iteration 968 : loss : 0.219354, loss_ce: 0.081724
2021-11-30 12:08:11,268 iteration 969 : loss : 0.136224, loss_ce: 0.059207
 14%|████▎                         | 57/400 [26:13<2:35:19, 27.17s/it]2021-11-30 12:08:12,776 iteration 970 : loss : 0.162513, loss_ce: 0.072962
2021-11-30 12:08:14,286 iteration 971 : loss : 0.222131, loss_ce: 0.087528
2021-11-30 12:08:15,693 iteration 972 : loss : 0.151116, loss_ce: 0.065804
2021-11-30 12:08:17,195 iteration 973 : loss : 0.162970, loss_ce: 0.063340
2021-11-30 12:08:18,599 iteration 974 : loss : 0.145518, loss_ce: 0.061380
2021-11-30 12:08:20,124 iteration 975 : loss : 0.172690, loss_ce: 0.067231
2021-11-30 12:08:21,596 iteration 976 : loss : 0.141274, loss_ce: 0.057718
2021-11-30 12:08:23,076 iteration 977 : loss : 0.147476, loss_ce: 0.053140
2021-11-30 12:08:24,593 iteration 978 : loss : 0.141858, loss_ce: 0.062203
2021-11-30 12:08:26,033 iteration 979 : loss : 0.215067, loss_ce: 0.094130
2021-11-30 12:08:27,511 iteration 980 : loss : 0.134652, loss_ce: 0.050880
2021-11-30 12:08:29,104 iteration 981 : loss : 0.136138, loss_ce: 0.058652
2021-11-30 12:08:30,551 iteration 982 : loss : 0.166682, loss_ce: 0.084111
2021-11-30 12:08:32,108 iteration 983 : loss : 0.168144, loss_ce: 0.063489
2021-11-30 12:08:33,505 iteration 984 : loss : 0.153593, loss_ce: 0.069225
2021-11-30 12:08:34,977 iteration 985 : loss : 0.157657, loss_ce: 0.064963
2021-11-30 12:08:36,523 iteration 986 : loss : 0.171037, loss_ce: 0.074398
 14%|████▎                         | 58/400 [26:38<2:31:35, 26.60s/it]2021-11-30 12:08:38,111 iteration 987 : loss : 0.137890, loss_ce: 0.062146
2021-11-30 12:08:39,626 iteration 988 : loss : 0.156366, loss_ce: 0.067208
2021-11-30 12:08:41,132 iteration 989 : loss : 0.181493, loss_ce: 0.074453
2021-11-30 12:08:42,604 iteration 990 : loss : 0.181850, loss_ce: 0.078965
2021-11-30 12:08:44,065 iteration 991 : loss : 0.136515, loss_ce: 0.063979
2021-11-30 12:08:45,642 iteration 992 : loss : 0.128257, loss_ce: 0.061144
2021-11-30 12:08:47,166 iteration 993 : loss : 0.159729, loss_ce: 0.059451
2021-11-30 12:08:48,778 iteration 994 : loss : 0.170140, loss_ce: 0.076120
2021-11-30 12:08:50,287 iteration 995 : loss : 0.181836, loss_ce: 0.068346
2021-11-30 12:08:51,848 iteration 996 : loss : 0.131755, loss_ce: 0.063386
2021-11-30 12:08:53,403 iteration 997 : loss : 0.223120, loss_ce: 0.090983
2021-11-30 12:08:54,820 iteration 998 : loss : 0.190450, loss_ce: 0.069983
2021-11-30 12:08:56,361 iteration 999 : loss : 0.145869, loss_ce: 0.056561
2021-11-30 12:08:57,812 iteration 1000 : loss : 0.175631, loss_ce: 0.059955
2021-11-30 12:08:59,284 iteration 1001 : loss : 0.189504, loss_ce: 0.049193
2021-11-30 12:09:00,921 iteration 1002 : loss : 0.136031, loss_ce: 0.059653
2021-11-30 12:09:02,498 iteration 1003 : loss : 0.162433, loss_ce: 0.067441
 15%|████▍                         | 59/400 [27:04<2:30:05, 26.41s/it]2021-11-30 12:09:04,045 iteration 1004 : loss : 0.222579, loss_ce: 0.091560
2021-11-30 12:09:05,527 iteration 1005 : loss : 0.156124, loss_ce: 0.058448
2021-11-30 12:09:07,114 iteration 1006 : loss : 0.174042, loss_ce: 0.087932
2021-11-30 12:09:08,538 iteration 1007 : loss : 0.144643, loss_ce: 0.075060
2021-11-30 12:09:09,971 iteration 1008 : loss : 0.256168, loss_ce: 0.118951
2021-11-30 12:09:11,419 iteration 1009 : loss : 0.210588, loss_ce: 0.083978
2021-11-30 12:09:12,867 iteration 1010 : loss : 0.189195, loss_ce: 0.081464
2021-11-30 12:09:14,408 iteration 1011 : loss : 0.176640, loss_ce: 0.076924
2021-11-30 12:09:15,887 iteration 1012 : loss : 0.117143, loss_ce: 0.045989
2021-11-30 12:09:17,321 iteration 1013 : loss : 0.133684, loss_ce: 0.051082
2021-11-30 12:09:18,815 iteration 1014 : loss : 0.206178, loss_ce: 0.080945
2021-11-30 12:09:20,227 iteration 1015 : loss : 0.148651, loss_ce: 0.064682
2021-11-30 12:09:21,714 iteration 1016 : loss : 0.137150, loss_ce: 0.060144
2021-11-30 12:09:23,112 iteration 1017 : loss : 0.103819, loss_ce: 0.044025
2021-11-30 12:09:24,521 iteration 1018 : loss : 0.148077, loss_ce: 0.059547
2021-11-30 12:09:25,991 iteration 1019 : loss : 0.182431, loss_ce: 0.091189
2021-11-30 12:09:25,991 Training Data Eval:
2021-11-30 12:09:33,376   Average segmentation loss on training set: 0.1911
2021-11-30 12:09:33,376 Validation Data Eval:
2021-11-30 12:09:35,928   Average segmentation loss on validation set: 0.2929
2021-11-30 12:09:37,373 iteration 1020 : loss : 0.140442, loss_ce: 0.060003
 15%|████▌                         | 60/400 [27:39<2:44:04, 28.95s/it]2021-11-30 12:09:39,023 iteration 1021 : loss : 0.155584, loss_ce: 0.051785
2021-11-30 12:09:40,493 iteration 1022 : loss : 0.159645, loss_ce: 0.066534
2021-11-30 12:09:41,985 iteration 1023 : loss : 0.203978, loss_ce: 0.058287
2021-11-30 12:09:43,536 iteration 1024 : loss : 0.112260, loss_ce: 0.047086
2021-11-30 12:09:45,091 iteration 1025 : loss : 0.141214, loss_ce: 0.058822
2021-11-30 12:09:46,473 iteration 1026 : loss : 0.129965, loss_ce: 0.054691
2021-11-30 12:09:48,007 iteration 1027 : loss : 0.146748, loss_ce: 0.058318
2021-11-30 12:09:49,533 iteration 1028 : loss : 0.144218, loss_ce: 0.061674
2021-11-30 12:09:51,003 iteration 1029 : loss : 0.136119, loss_ce: 0.061526
2021-11-30 12:09:52,486 iteration 1030 : loss : 0.199984, loss_ce: 0.084435
2021-11-30 12:09:54,037 iteration 1031 : loss : 0.186498, loss_ce: 0.096495
2021-11-30 12:09:55,545 iteration 1032 : loss : 0.156617, loss_ce: 0.072339
2021-11-30 12:09:56,987 iteration 1033 : loss : 0.177930, loss_ce: 0.064898
2021-11-30 12:09:58,548 iteration 1034 : loss : 0.153618, loss_ce: 0.065057
2021-11-30 12:10:00,040 iteration 1035 : loss : 0.122421, loss_ce: 0.053740
2021-11-30 12:10:01,536 iteration 1036 : loss : 0.134842, loss_ce: 0.061610
2021-11-30 12:10:02,990 iteration 1037 : loss : 0.181586, loss_ce: 0.065012
 15%|████▌                         | 61/400 [28:05<2:37:54, 27.95s/it]2021-11-30 12:10:04,540 iteration 1038 : loss : 0.117772, loss_ce: 0.049878
2021-11-30 12:10:06,013 iteration 1039 : loss : 0.130981, loss_ce: 0.059294
2021-11-30 12:10:07,538 iteration 1040 : loss : 0.162941, loss_ce: 0.079146
2021-11-30 12:10:09,061 iteration 1041 : loss : 0.126837, loss_ce: 0.053361
2021-11-30 12:10:10,590 iteration 1042 : loss : 0.133801, loss_ce: 0.059999
2021-11-30 12:10:12,095 iteration 1043 : loss : 0.163883, loss_ce: 0.058375
2021-11-30 12:10:13,621 iteration 1044 : loss : 0.127029, loss_ce: 0.054453
2021-11-30 12:10:15,004 iteration 1045 : loss : 0.128553, loss_ce: 0.064024
2021-11-30 12:10:16,437 iteration 1046 : loss : 0.168737, loss_ce: 0.049563
2021-11-30 12:10:17,979 iteration 1047 : loss : 0.131239, loss_ce: 0.050907
2021-11-30 12:10:19,477 iteration 1048 : loss : 0.168920, loss_ce: 0.066922
2021-11-30 12:10:20,983 iteration 1049 : loss : 0.205920, loss_ce: 0.083570
2021-11-30 12:10:22,531 iteration 1050 : loss : 0.145391, loss_ce: 0.051001
2021-11-30 12:10:24,002 iteration 1051 : loss : 0.137391, loss_ce: 0.062595
2021-11-30 12:10:25,493 iteration 1052 : loss : 0.214732, loss_ce: 0.085751
2021-11-30 12:10:26,881 iteration 1053 : loss : 0.157534, loss_ce: 0.056056
2021-11-30 12:10:28,447 iteration 1054 : loss : 0.162720, loss_ce: 0.065899
 16%|████▋                         | 62/400 [28:30<2:33:15, 27.21s/it]2021-11-30 12:10:30,004 iteration 1055 : loss : 0.115129, loss_ce: 0.043747
2021-11-30 12:10:31,463 iteration 1056 : loss : 0.133804, loss_ce: 0.064161
2021-11-30 12:10:32,918 iteration 1057 : loss : 0.166543, loss_ce: 0.075628
2021-11-30 12:10:34,400 iteration 1058 : loss : 0.137875, loss_ce: 0.063779
2021-11-30 12:10:35,795 iteration 1059 : loss : 0.151624, loss_ce: 0.060865
2021-11-30 12:10:37,279 iteration 1060 : loss : 0.110844, loss_ce: 0.047797
2021-11-30 12:10:38,814 iteration 1061 : loss : 0.154810, loss_ce: 0.069795
2021-11-30 12:10:40,330 iteration 1062 : loss : 0.140902, loss_ce: 0.058232
2021-11-30 12:10:41,894 iteration 1063 : loss : 0.190150, loss_ce: 0.067890
2021-11-30 12:10:43,362 iteration 1064 : loss : 0.147822, loss_ce: 0.050873
2021-11-30 12:10:44,778 iteration 1065 : loss : 0.150562, loss_ce: 0.066151
2021-11-30 12:10:46,311 iteration 1066 : loss : 0.184273, loss_ce: 0.074316
2021-11-30 12:10:47,777 iteration 1067 : loss : 0.169865, loss_ce: 0.050902
2021-11-30 12:10:49,350 iteration 1068 : loss : 0.188468, loss_ce: 0.090747
2021-11-30 12:10:50,881 iteration 1069 : loss : 0.179658, loss_ce: 0.069245
2021-11-30 12:10:52,375 iteration 1070 : loss : 0.125490, loss_ce: 0.055297
2021-11-30 12:10:53,843 iteration 1071 : loss : 0.132944, loss_ce: 0.057404
 16%|████▋                         | 63/400 [28:56<2:29:44, 26.66s/it]2021-11-30 12:10:55,494 iteration 1072 : loss : 0.138830, loss_ce: 0.057950
2021-11-30 12:10:56,949 iteration 1073 : loss : 0.207408, loss_ce: 0.062318
2021-11-30 12:10:58,420 iteration 1074 : loss : 0.160349, loss_ce: 0.065091
2021-11-30 12:10:59,943 iteration 1075 : loss : 0.162352, loss_ce: 0.063188
2021-11-30 12:11:01,371 iteration 1076 : loss : 0.133253, loss_ce: 0.057582
2021-11-30 12:11:02,833 iteration 1077 : loss : 0.138731, loss_ce: 0.050612
2021-11-30 12:11:04,391 iteration 1078 : loss : 0.181349, loss_ce: 0.093739
2021-11-30 12:11:05,837 iteration 1079 : loss : 0.108825, loss_ce: 0.044952
2021-11-30 12:11:07,283 iteration 1080 : loss : 0.128827, loss_ce: 0.057775
2021-11-30 12:11:08,792 iteration 1081 : loss : 0.137175, loss_ce: 0.049619
2021-11-30 12:11:10,294 iteration 1082 : loss : 0.129792, loss_ce: 0.062746
2021-11-30 12:11:11,824 iteration 1083 : loss : 0.162284, loss_ce: 0.069066
2021-11-30 12:11:13,286 iteration 1084 : loss : 0.151391, loss_ce: 0.076705
2021-11-30 12:11:14,850 iteration 1085 : loss : 0.119945, loss_ce: 0.049899
2021-11-30 12:11:16,326 iteration 1086 : loss : 0.117378, loss_ce: 0.053525
2021-11-30 12:11:17,818 iteration 1087 : loss : 0.126944, loss_ce: 0.051351
2021-11-30 12:11:19,345 iteration 1088 : loss : 0.134912, loss_ce: 0.064047
 16%|████▊                         | 64/400 [29:21<2:27:21, 26.31s/it]2021-11-30 12:11:20,852 iteration 1089 : loss : 0.144794, loss_ce: 0.057714
2021-11-30 12:11:22,301 iteration 1090 : loss : 0.119281, loss_ce: 0.059627
2021-11-30 12:11:23,786 iteration 1091 : loss : 0.128053, loss_ce: 0.051975
2021-11-30 12:11:25,276 iteration 1092 : loss : 0.087169, loss_ce: 0.041529
2021-11-30 12:11:26,776 iteration 1093 : loss : 0.136385, loss_ce: 0.048479
2021-11-30 12:11:28,352 iteration 1094 : loss : 0.120383, loss_ce: 0.050861
2021-11-30 12:11:29,801 iteration 1095 : loss : 0.119708, loss_ce: 0.056601
2021-11-30 12:11:31,413 iteration 1096 : loss : 0.121625, loss_ce: 0.055134
2021-11-30 12:11:32,913 iteration 1097 : loss : 0.148519, loss_ce: 0.057208
2021-11-30 12:11:34,361 iteration 1098 : loss : 0.101344, loss_ce: 0.051485
2021-11-30 12:11:35,890 iteration 1099 : loss : 0.154478, loss_ce: 0.056767
2021-11-30 12:11:37,457 iteration 1100 : loss : 0.153837, loss_ce: 0.070903
2021-11-30 12:11:38,928 iteration 1101 : loss : 0.120027, loss_ce: 0.048902
2021-11-30 12:11:40,489 iteration 1102 : loss : 0.196485, loss_ce: 0.062025
2021-11-30 12:11:41,939 iteration 1103 : loss : 0.158657, loss_ce: 0.064287
2021-11-30 12:11:43,394 iteration 1104 : loss : 0.158899, loss_ce: 0.068561
2021-11-30 12:11:43,394 Training Data Eval:
2021-11-30 12:11:50,802   Average segmentation loss on training set: 0.2569
2021-11-30 12:11:50,802 Validation Data Eval:
2021-11-30 12:11:53,388   Average segmentation loss on validation set: 0.3808
2021-11-30 12:11:54,947 iteration 1105 : loss : 0.143019, loss_ce: 0.046305
 16%|████▉                         | 65/400 [29:57<2:42:28, 29.10s/it]2021-11-30 12:11:56,429 iteration 1106 : loss : 0.185972, loss_ce: 0.072292
2021-11-30 12:11:57,894 iteration 1107 : loss : 0.150643, loss_ce: 0.072006
2021-11-30 12:11:59,399 iteration 1108 : loss : 0.183013, loss_ce: 0.052986
2021-11-30 12:12:00,817 iteration 1109 : loss : 0.116511, loss_ce: 0.053181
2021-11-30 12:12:02,309 iteration 1110 : loss : 0.159585, loss_ce: 0.068909
2021-11-30 12:12:03,779 iteration 1111 : loss : 0.101009, loss_ce: 0.047607
2021-11-30 12:12:05,260 iteration 1112 : loss : 0.129564, loss_ce: 0.055642
2021-11-30 12:12:06,706 iteration 1113 : loss : 0.109706, loss_ce: 0.044009
2021-11-30 12:12:08,239 iteration 1114 : loss : 0.111008, loss_ce: 0.050849
2021-11-30 12:12:09,874 iteration 1115 : loss : 0.142379, loss_ce: 0.064056
2021-11-30 12:12:11,391 iteration 1116 : loss : 0.186552, loss_ce: 0.065680
2021-11-30 12:12:12,986 iteration 1117 : loss : 0.142796, loss_ce: 0.048603
2021-11-30 12:12:14,517 iteration 1118 : loss : 0.155706, loss_ce: 0.055740
2021-11-30 12:12:15,966 iteration 1119 : loss : 0.126962, loss_ce: 0.059403
2021-11-30 12:12:17,417 iteration 1120 : loss : 0.144142, loss_ce: 0.060529
2021-11-30 12:12:18,939 iteration 1121 : loss : 0.121483, loss_ce: 0.047751
2021-11-30 12:12:20,453 iteration 1122 : loss : 0.141462, loss_ce: 0.070891
 16%|████▉                         | 66/400 [30:22<2:35:59, 28.02s/it]2021-11-30 12:12:21,963 iteration 1123 : loss : 0.171438, loss_ce: 0.074905
2021-11-30 12:12:23,479 iteration 1124 : loss : 0.125802, loss_ce: 0.061846
2021-11-30 12:12:24,962 iteration 1125 : loss : 0.114128, loss_ce: 0.041423
2021-11-30 12:12:26,427 iteration 1126 : loss : 0.165096, loss_ce: 0.060682
2021-11-30 12:12:27,877 iteration 1127 : loss : 0.138944, loss_ce: 0.062685
2021-11-30 12:12:29,433 iteration 1128 : loss : 0.157997, loss_ce: 0.056484
2021-11-30 12:12:30,979 iteration 1129 : loss : 0.121700, loss_ce: 0.042660
2021-11-30 12:12:32,426 iteration 1130 : loss : 0.116504, loss_ce: 0.055130
2021-11-30 12:12:33,897 iteration 1131 : loss : 0.100226, loss_ce: 0.045494
2021-11-30 12:12:35,418 iteration 1132 : loss : 0.103182, loss_ce: 0.043241
2021-11-30 12:12:36,819 iteration 1133 : loss : 0.105788, loss_ce: 0.051688
2021-11-30 12:12:38,313 iteration 1134 : loss : 0.139712, loss_ce: 0.051529
2021-11-30 12:12:39,748 iteration 1135 : loss : 0.144064, loss_ce: 0.062966
2021-11-30 12:12:41,224 iteration 1136 : loss : 0.089393, loss_ce: 0.037598
2021-11-30 12:12:42,695 iteration 1137 : loss : 0.150515, loss_ce: 0.065867
2021-11-30 12:12:44,231 iteration 1138 : loss : 0.125598, loss_ce: 0.060541
2021-11-30 12:12:45,726 iteration 1139 : loss : 0.142575, loss_ce: 0.049942
 17%|█████                         | 67/400 [30:48<2:30:57, 27.20s/it]2021-11-30 12:12:47,296 iteration 1140 : loss : 0.127796, loss_ce: 0.056547
2021-11-30 12:12:48,726 iteration 1141 : loss : 0.124037, loss_ce: 0.045703
2021-11-30 12:12:50,292 iteration 1142 : loss : 0.125183, loss_ce: 0.056392
2021-11-30 12:12:51,778 iteration 1143 : loss : 0.116921, loss_ce: 0.050900
2021-11-30 12:12:53,307 iteration 1144 : loss : 0.159269, loss_ce: 0.078242
2021-11-30 12:12:54,728 iteration 1145 : loss : 0.108430, loss_ce: 0.042463
2021-11-30 12:12:56,241 iteration 1146 : loss : 0.187417, loss_ce: 0.082685
2021-11-30 12:12:57,642 iteration 1147 : loss : 0.122217, loss_ce: 0.054741
2021-11-30 12:12:59,197 iteration 1148 : loss : 0.129191, loss_ce: 0.048989
2021-11-30 12:13:00,625 iteration 1149 : loss : 0.124971, loss_ce: 0.060015
2021-11-30 12:13:02,106 iteration 1150 : loss : 0.114372, loss_ce: 0.050912
2021-11-30 12:13:03,610 iteration 1151 : loss : 0.124674, loss_ce: 0.047859
2021-11-30 12:13:05,233 iteration 1152 : loss : 0.115718, loss_ce: 0.049636
2021-11-30 12:13:06,613 iteration 1153 : loss : 0.120615, loss_ce: 0.048849
2021-11-30 12:13:08,145 iteration 1154 : loss : 0.112967, loss_ce: 0.046218
2021-11-30 12:13:09,600 iteration 1155 : loss : 0.125529, loss_ce: 0.047229
2021-11-30 12:13:11,131 iteration 1156 : loss : 0.155436, loss_ce: 0.074420
 17%|█████                         | 68/400 [31:13<2:27:29, 26.66s/it]2021-11-30 12:13:12,659 iteration 1157 : loss : 0.182737, loss_ce: 0.053610
2021-11-30 12:13:14,159 iteration 1158 : loss : 0.125987, loss_ce: 0.051830
2021-11-30 12:13:15,721 iteration 1159 : loss : 0.102094, loss_ce: 0.038246
2021-11-30 12:13:17,327 iteration 1160 : loss : 0.139223, loss_ce: 0.055515
2021-11-30 12:13:18,864 iteration 1161 : loss : 0.102014, loss_ce: 0.042873
2021-11-30 12:13:20,367 iteration 1162 : loss : 0.095716, loss_ce: 0.039902
2021-11-30 12:13:21,812 iteration 1163 : loss : 0.152396, loss_ce: 0.067382
2021-11-30 12:13:23,287 iteration 1164 : loss : 0.131333, loss_ce: 0.057967
2021-11-30 12:13:24,764 iteration 1165 : loss : 0.127694, loss_ce: 0.060650
2021-11-30 12:13:26,338 iteration 1166 : loss : 0.091150, loss_ce: 0.038853
2021-11-30 12:13:27,785 iteration 1167 : loss : 0.107566, loss_ce: 0.047884
2021-11-30 12:13:29,249 iteration 1168 : loss : 0.091543, loss_ce: 0.041117
2021-11-30 12:13:30,707 iteration 1169 : loss : 0.117154, loss_ce: 0.051530
2021-11-30 12:13:32,153 iteration 1170 : loss : 0.100680, loss_ce: 0.046223
2021-11-30 12:13:33,652 iteration 1171 : loss : 0.092765, loss_ce: 0.038416
2021-11-30 12:13:35,134 iteration 1172 : loss : 0.144114, loss_ce: 0.067695
2021-11-30 12:13:36,642 iteration 1173 : loss : 0.121580, loss_ce: 0.043662
 17%|█████▏                        | 69/400 [31:38<2:25:09, 26.31s/it]2021-11-30 12:13:38,148 iteration 1174 : loss : 0.111543, loss_ce: 0.044276
2021-11-30 12:13:39,633 iteration 1175 : loss : 0.091876, loss_ce: 0.042891
2021-11-30 12:13:41,112 iteration 1176 : loss : 0.090607, loss_ce: 0.043450
2021-11-30 12:13:42,572 iteration 1177 : loss : 0.111424, loss_ce: 0.043762
2021-11-30 12:13:44,109 iteration 1178 : loss : 0.081879, loss_ce: 0.035170
2021-11-30 12:13:45,683 iteration 1179 : loss : 0.115673, loss_ce: 0.053560
2021-11-30 12:13:47,132 iteration 1180 : loss : 0.128857, loss_ce: 0.059065
2021-11-30 12:13:48,598 iteration 1181 : loss : 0.123434, loss_ce: 0.051674
2021-11-30 12:13:50,086 iteration 1182 : loss : 0.118905, loss_ce: 0.047470
2021-11-30 12:13:51,529 iteration 1183 : loss : 0.109112, loss_ce: 0.044993
2021-11-30 12:13:52,982 iteration 1184 : loss : 0.100792, loss_ce: 0.048422
2021-11-30 12:13:54,502 iteration 1185 : loss : 0.109085, loss_ce: 0.043851
2021-11-30 12:13:55,933 iteration 1186 : loss : 0.115348, loss_ce: 0.049894
2021-11-30 12:13:57,489 iteration 1187 : loss : 0.133785, loss_ce: 0.052607
2021-11-30 12:13:58,931 iteration 1188 : loss : 0.150467, loss_ce: 0.062784
2021-11-30 12:14:00,502 iteration 1189 : loss : 0.129375, loss_ce: 0.058647
2021-11-30 12:14:00,502 Training Data Eval:
2021-11-30 12:14:07,900   Average segmentation loss on training set: 0.0853
2021-11-30 12:14:07,900 Validation Data Eval:
2021-11-30 12:14:10,475   Average segmentation loss on validation set: 0.1204
2021-11-30 12:14:12,394 Found new lowest validation loss at iteration 1189! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/UNET_RUNMC/best_val_loss.pth
2021-11-30 12:14:13,887 iteration 1190 : loss : 0.131446, loss_ce: 0.066059
 18%|█████▎                        | 70/400 [32:16<2:42:47, 29.60s/it]2021-11-30 12:14:15,472 iteration 1191 : loss : 0.110916, loss_ce: 0.049881
2021-11-30 12:14:16,859 iteration 1192 : loss : 0.096052, loss_ce: 0.044141
2021-11-30 12:14:18,389 iteration 1193 : loss : 0.129116, loss_ce: 0.045918
2021-11-30 12:14:19,823 iteration 1194 : loss : 0.091630, loss_ce: 0.036314
2021-11-30 12:14:21,199 iteration 1195 : loss : 0.114782, loss_ce: 0.053196
2021-11-30 12:14:22,588 iteration 1196 : loss : 0.110009, loss_ce: 0.050778
2021-11-30 12:14:23,941 iteration 1197 : loss : 0.097044, loss_ce: 0.043526
2021-11-30 12:14:25,439 iteration 1198 : loss : 0.093124, loss_ce: 0.049125
2021-11-30 12:14:26,892 iteration 1199 : loss : 0.100184, loss_ce: 0.041264
2021-11-30 12:14:28,346 iteration 1200 : loss : 0.134790, loss_ce: 0.043200
2021-11-30 12:14:29,772 iteration 1201 : loss : 0.100754, loss_ce: 0.047123
2021-11-30 12:14:31,278 iteration 1202 : loss : 0.126899, loss_ce: 0.050265
2021-11-30 12:14:32,803 iteration 1203 : loss : 0.114010, loss_ce: 0.042718
2021-11-30 12:14:34,271 iteration 1204 : loss : 0.134396, loss_ce: 0.051333
2021-11-30 12:14:35,752 iteration 1205 : loss : 0.145474, loss_ce: 0.069280
2021-11-30 12:14:37,242 iteration 1206 : loss : 0.088582, loss_ce: 0.040232
2021-11-30 12:14:38,781 iteration 1207 : loss : 0.085132, loss_ce: 0.036742
 18%|█████▎                        | 71/400 [32:41<2:34:31, 28.18s/it]2021-11-30 12:14:40,351 iteration 1208 : loss : 0.108207, loss_ce: 0.049953
2021-11-30 12:14:41,846 iteration 1209 : loss : 0.092518, loss_ce: 0.037540
2021-11-30 12:14:43,334 iteration 1210 : loss : 0.090668, loss_ce: 0.042557
2021-11-30 12:14:44,754 iteration 1211 : loss : 0.076532, loss_ce: 0.036459
2021-11-30 12:14:46,175 iteration 1212 : loss : 0.134394, loss_ce: 0.044245
2021-11-30 12:14:47,817 iteration 1213 : loss : 0.095045, loss_ce: 0.044653
2021-11-30 12:14:49,304 iteration 1214 : loss : 0.113642, loss_ce: 0.052841
2021-11-30 12:14:50,789 iteration 1215 : loss : 0.118979, loss_ce: 0.056251
2021-11-30 12:14:52,413 iteration 1216 : loss : 0.111003, loss_ce: 0.046783
2021-11-30 12:14:53,902 iteration 1217 : loss : 0.079237, loss_ce: 0.033700
2021-11-30 12:14:55,316 iteration 1218 : loss : 0.105213, loss_ce: 0.043488
2021-11-30 12:14:56,727 iteration 1219 : loss : 0.090531, loss_ce: 0.037916
2021-11-30 12:14:58,205 iteration 1220 : loss : 0.112220, loss_ce: 0.043031
2021-11-30 12:14:59,660 iteration 1221 : loss : 0.114749, loss_ce: 0.045423
2021-11-30 12:15:01,223 iteration 1222 : loss : 0.128550, loss_ce: 0.048709
2021-11-30 12:15:02,683 iteration 1223 : loss : 0.139126, loss_ce: 0.058195
2021-11-30 12:15:04,214 iteration 1224 : loss : 0.139106, loss_ce: 0.055507
 18%|█████▍                        | 72/400 [33:06<2:29:33, 27.36s/it]2021-11-30 12:15:05,721 iteration 1225 : loss : 0.110814, loss_ce: 0.044826
2021-11-30 12:15:07,220 iteration 1226 : loss : 0.158579, loss_ce: 0.073627
2021-11-30 12:15:08,636 iteration 1227 : loss : 0.114128, loss_ce: 0.044561
2021-11-30 12:15:10,102 iteration 1228 : loss : 0.093195, loss_ce: 0.038786
2021-11-30 12:15:11,492 iteration 1229 : loss : 0.132456, loss_ce: 0.041835
2021-11-30 12:15:12,933 iteration 1230 : loss : 0.111499, loss_ce: 0.047145
2021-11-30 12:15:14,481 iteration 1231 : loss : 0.108182, loss_ce: 0.052662
2021-11-30 12:15:15,918 iteration 1232 : loss : 0.106141, loss_ce: 0.055450
2021-11-30 12:15:17,373 iteration 1233 : loss : 0.092616, loss_ce: 0.040092
2021-11-30 12:15:18,890 iteration 1234 : loss : 0.098104, loss_ce: 0.047857
2021-11-30 12:15:20,399 iteration 1235 : loss : 0.123855, loss_ce: 0.044835
2021-11-30 12:15:21,842 iteration 1236 : loss : 0.095950, loss_ce: 0.037940
2021-11-30 12:15:23,227 iteration 1237 : loss : 0.169155, loss_ce: 0.071681
2021-11-30 12:15:24,759 iteration 1238 : loss : 0.112454, loss_ce: 0.044736
2021-11-30 12:15:26,171 iteration 1239 : loss : 0.069291, loss_ce: 0.033769
2021-11-30 12:15:27,617 iteration 1240 : loss : 0.092232, loss_ce: 0.040561
2021-11-30 12:15:29,061 iteration 1241 : loss : 0.132096, loss_ce: 0.064433
 18%|█████▍                        | 73/400 [33:31<2:24:59, 26.60s/it]2021-11-30 12:15:30,629 iteration 1242 : loss : 0.113219, loss_ce: 0.056363
2021-11-30 12:15:32,076 iteration 1243 : loss : 0.112365, loss_ce: 0.042590
2021-11-30 12:15:33,547 iteration 1244 : loss : 0.094161, loss_ce: 0.045914
2021-11-30 12:15:35,052 iteration 1245 : loss : 0.104881, loss_ce: 0.044764
2021-11-30 12:15:36,532 iteration 1246 : loss : 0.086929, loss_ce: 0.036977
2021-11-30 12:15:37,972 iteration 1247 : loss : 0.118093, loss_ce: 0.049237
2021-11-30 12:15:39,485 iteration 1248 : loss : 0.098167, loss_ce: 0.036260
2021-11-30 12:15:40,937 iteration 1249 : loss : 0.091738, loss_ce: 0.038041
2021-11-30 12:15:42,527 iteration 1250 : loss : 0.104171, loss_ce: 0.043707
2021-11-30 12:15:44,153 iteration 1251 : loss : 0.105765, loss_ce: 0.053941
2021-11-30 12:15:45,677 iteration 1252 : loss : 0.095464, loss_ce: 0.046706
2021-11-30 12:15:47,054 iteration 1253 : loss : 0.084428, loss_ce: 0.036893
2021-11-30 12:15:48,527 iteration 1254 : loss : 0.098103, loss_ce: 0.047308
2021-11-30 12:15:50,013 iteration 1255 : loss : 0.087694, loss_ce: 0.037555
2021-11-30 12:15:51,483 iteration 1256 : loss : 0.083334, loss_ce: 0.037771
2021-11-30 12:15:52,958 iteration 1257 : loss : 0.126226, loss_ce: 0.042307
2021-11-30 12:15:54,458 iteration 1258 : loss : 0.112278, loss_ce: 0.061570
 18%|█████▌                        | 74/400 [33:56<2:22:34, 26.24s/it]2021-11-30 12:15:55,951 iteration 1259 : loss : 0.099945, loss_ce: 0.046124
2021-11-30 12:15:57,517 iteration 1260 : loss : 0.074115, loss_ce: 0.035236
2021-11-30 12:15:58,984 iteration 1261 : loss : 0.098559, loss_ce: 0.034762
2021-11-30 12:16:00,549 iteration 1262 : loss : 0.083280, loss_ce: 0.039994
2021-11-30 12:16:02,142 iteration 1263 : loss : 0.185786, loss_ce: 0.088105
2021-11-30 12:16:03,590 iteration 1264 : loss : 0.087632, loss_ce: 0.041152
2021-11-30 12:16:05,138 iteration 1265 : loss : 0.082367, loss_ce: 0.047539
2021-11-30 12:16:06,680 iteration 1266 : loss : 0.101713, loss_ce: 0.037325
2021-11-30 12:16:08,206 iteration 1267 : loss : 0.110934, loss_ce: 0.051111
2021-11-30 12:16:09,665 iteration 1268 : loss : 0.119169, loss_ce: 0.046451
2021-11-30 12:16:11,244 iteration 1269 : loss : 0.162755, loss_ce: 0.049245
2021-11-30 12:16:12,771 iteration 1270 : loss : 0.092006, loss_ce: 0.045264
2021-11-30 12:16:14,221 iteration 1271 : loss : 0.110011, loss_ce: 0.052190
2021-11-30 12:16:15,587 iteration 1272 : loss : 0.091215, loss_ce: 0.035865
2021-11-30 12:16:17,033 iteration 1273 : loss : 0.106525, loss_ce: 0.039946
2021-11-30 12:16:18,523 iteration 1274 : loss : 0.102542, loss_ce: 0.035787
2021-11-30 12:16:18,523 Training Data Eval:
2021-11-30 12:16:25,922   Average segmentation loss on training set: 0.0698
2021-11-30 12:16:25,923 Validation Data Eval:
2021-11-30 12:16:28,493   Average segmentation loss on validation set: 0.1288
2021-11-30 12:16:29,973 iteration 1275 : loss : 0.095046, loss_ce: 0.038116
 19%|█████▋                        | 75/400 [34:32<2:37:12, 29.02s/it]2021-11-30 12:16:31,424 iteration 1276 : loss : 0.108434, loss_ce: 0.047502
2021-11-30 12:16:33,023 iteration 1277 : loss : 0.080858, loss_ce: 0.038832
2021-11-30 12:16:34,460 iteration 1278 : loss : 0.065509, loss_ce: 0.029741
2021-11-30 12:16:36,058 iteration 1279 : loss : 0.108801, loss_ce: 0.045309
2021-11-30 12:16:37,598 iteration 1280 : loss : 0.112259, loss_ce: 0.050743
2021-11-30 12:16:39,084 iteration 1281 : loss : 0.104109, loss_ce: 0.046692
2021-11-30 12:16:40,498 iteration 1282 : loss : 0.064412, loss_ce: 0.033453
2021-11-30 12:16:42,047 iteration 1283 : loss : 0.112042, loss_ce: 0.037972
2021-11-30 12:16:43,474 iteration 1284 : loss : 0.091264, loss_ce: 0.039155
2021-11-30 12:16:45,036 iteration 1285 : loss : 0.086778, loss_ce: 0.036479
2021-11-30 12:16:46,644 iteration 1286 : loss : 0.126156, loss_ce: 0.041896
2021-11-30 12:16:48,121 iteration 1287 : loss : 0.090008, loss_ce: 0.050185
2021-11-30 12:16:49,563 iteration 1288 : loss : 0.115609, loss_ce: 0.055692
2021-11-30 12:16:51,044 iteration 1289 : loss : 0.127621, loss_ce: 0.059334
2021-11-30 12:16:52,566 iteration 1290 : loss : 0.131764, loss_ce: 0.051511
2021-11-30 12:16:54,113 iteration 1291 : loss : 0.120823, loss_ce: 0.058602
2021-11-30 12:16:55,673 iteration 1292 : loss : 0.134439, loss_ce: 0.053657
 19%|█████▋                        | 76/400 [34:58<2:31:21, 28.03s/it]2021-11-30 12:16:57,269 iteration 1293 : loss : 0.098848, loss_ce: 0.044207
2021-11-30 12:16:58,784 iteration 1294 : loss : 0.116945, loss_ce: 0.047634
2021-11-30 12:17:00,346 iteration 1295 : loss : 0.112127, loss_ce: 0.038794
2021-11-30 12:17:01,857 iteration 1296 : loss : 0.105748, loss_ce: 0.045473
2021-11-30 12:17:03,308 iteration 1297 : loss : 0.095882, loss_ce: 0.049630
2021-11-30 12:17:04,722 iteration 1298 : loss : 0.064989, loss_ce: 0.033641
2021-11-30 12:17:06,219 iteration 1299 : loss : 0.103765, loss_ce: 0.052040
2021-11-30 12:17:07,694 iteration 1300 : loss : 0.088534, loss_ce: 0.038505
2021-11-30 12:17:09,222 iteration 1301 : loss : 0.081371, loss_ce: 0.038045
2021-11-30 12:17:10,683 iteration 1302 : loss : 0.096844, loss_ce: 0.050977
2021-11-30 12:17:12,136 iteration 1303 : loss : 0.099474, loss_ce: 0.046114
2021-11-30 12:17:13,624 iteration 1304 : loss : 0.091667, loss_ce: 0.038272
2021-11-30 12:17:15,115 iteration 1305 : loss : 0.112098, loss_ce: 0.044620
2021-11-30 12:17:16,646 iteration 1306 : loss : 0.129182, loss_ce: 0.046395
2021-11-30 12:17:18,175 iteration 1307 : loss : 0.093551, loss_ce: 0.050880
2021-11-30 12:17:19,664 iteration 1308 : loss : 0.115336, loss_ce: 0.038459
2021-11-30 12:17:21,074 iteration 1309 : loss : 0.114319, loss_ce: 0.040540
 19%|█████▊                        | 77/400 [35:23<2:26:38, 27.24s/it]2021-11-30 12:17:22,690 iteration 1310 : loss : 0.112505, loss_ce: 0.042363
2021-11-30 12:17:24,144 iteration 1311 : loss : 0.095100, loss_ce: 0.044963
2021-11-30 12:17:25,607 iteration 1312 : loss : 0.070757, loss_ce: 0.032666
2021-11-30 12:17:27,052 iteration 1313 : loss : 0.100123, loss_ce: 0.034567
2021-11-30 12:17:28,556 iteration 1314 : loss : 0.090601, loss_ce: 0.048318
2021-11-30 12:17:30,081 iteration 1315 : loss : 0.110628, loss_ce: 0.043199
2021-11-30 12:17:31,702 iteration 1316 : loss : 0.106706, loss_ce: 0.047403
2021-11-30 12:17:33,176 iteration 1317 : loss : 0.109788, loss_ce: 0.057719
2021-11-30 12:17:34,744 iteration 1318 : loss : 0.223143, loss_ce: 0.051551
2021-11-30 12:17:36,267 iteration 1319 : loss : 0.082355, loss_ce: 0.040550
2021-11-30 12:17:37,709 iteration 1320 : loss : 0.075969, loss_ce: 0.033274
2021-11-30 12:17:39,210 iteration 1321 : loss : 0.107956, loss_ce: 0.037651
2021-11-30 12:17:40,633 iteration 1322 : loss : 0.091896, loss_ce: 0.043416
2021-11-30 12:17:42,106 iteration 1323 : loss : 0.102996, loss_ce: 0.040621
2021-11-30 12:17:43,628 iteration 1324 : loss : 0.089088, loss_ce: 0.039857
2021-11-30 12:17:45,184 iteration 1325 : loss : 0.092655, loss_ce: 0.038514
2021-11-30 12:17:46,570 iteration 1326 : loss : 0.079363, loss_ce: 0.038750
 20%|█████▊                        | 78/400 [35:48<2:23:21, 26.71s/it]2021-11-30 12:17:48,077 iteration 1327 : loss : 0.100672, loss_ce: 0.037121
2021-11-30 12:17:49,590 iteration 1328 : loss : 0.136426, loss_ce: 0.068857
2021-11-30 12:17:51,058 iteration 1329 : loss : 0.069233, loss_ce: 0.036444
2021-11-30 12:17:52,505 iteration 1330 : loss : 0.155132, loss_ce: 0.047914
2021-11-30 12:17:53,914 iteration 1331 : loss : 0.100864, loss_ce: 0.051321
2021-11-30 12:17:55,345 iteration 1332 : loss : 0.073441, loss_ce: 0.031272
2021-11-30 12:17:56,813 iteration 1333 : loss : 0.087487, loss_ce: 0.032445
2021-11-30 12:17:58,297 iteration 1334 : loss : 0.097961, loss_ce: 0.048025
2021-11-30 12:17:59,760 iteration 1335 : loss : 0.081331, loss_ce: 0.033455
2021-11-30 12:18:01,228 iteration 1336 : loss : 0.103062, loss_ce: 0.055802
2021-11-30 12:18:02,771 iteration 1337 : loss : 0.106019, loss_ce: 0.052243
2021-11-30 12:18:04,284 iteration 1338 : loss : 0.100624, loss_ce: 0.034234
2021-11-30 12:18:05,822 iteration 1339 : loss : 0.106622, loss_ce: 0.051884
2021-11-30 12:18:07,366 iteration 1340 : loss : 0.099542, loss_ce: 0.043928
2021-11-30 12:18:08,866 iteration 1341 : loss : 0.111214, loss_ce: 0.046962
2021-11-30 12:18:10,342 iteration 1342 : loss : 0.115553, loss_ce: 0.053219
2021-11-30 12:18:11,758 iteration 1343 : loss : 0.075194, loss_ce: 0.037792
 20%|█████▉                        | 79/400 [36:14<2:20:28, 26.26s/it]2021-11-30 12:18:13,286 iteration 1344 : loss : 0.121948, loss_ce: 0.048970
2021-11-30 12:18:14,856 iteration 1345 : loss : 0.134939, loss_ce: 0.057787
2021-11-30 12:18:16,299 iteration 1346 : loss : 0.136693, loss_ce: 0.063126
2021-11-30 12:18:17,865 iteration 1347 : loss : 0.079079, loss_ce: 0.037793
2021-11-30 12:18:19,470 iteration 1348 : loss : 0.123947, loss_ce: 0.051331
2021-11-30 12:18:20,967 iteration 1349 : loss : 0.100134, loss_ce: 0.047119
2021-11-30 12:18:22,444 iteration 1350 : loss : 0.111328, loss_ce: 0.054836
2021-11-30 12:18:23,925 iteration 1351 : loss : 0.084638, loss_ce: 0.040999
2021-11-30 12:18:25,479 iteration 1352 : loss : 0.080453, loss_ce: 0.041835
2021-11-30 12:18:26,992 iteration 1353 : loss : 0.095409, loss_ce: 0.038440
2021-11-30 12:18:28,520 iteration 1354 : loss : 0.111511, loss_ce: 0.046074
2021-11-30 12:18:30,126 iteration 1355 : loss : 0.171250, loss_ce: 0.045314
2021-11-30 12:18:31,623 iteration 1356 : loss : 0.081919, loss_ce: 0.038329
2021-11-30 12:18:33,152 iteration 1357 : loss : 0.068916, loss_ce: 0.034160
2021-11-30 12:18:34,690 iteration 1358 : loss : 0.108535, loss_ce: 0.047855
2021-11-30 12:18:36,177 iteration 1359 : loss : 0.098074, loss_ce: 0.042944
2021-11-30 12:18:36,177 Training Data Eval:
2021-11-30 12:18:43,535   Average segmentation loss on training set: 0.0662
2021-11-30 12:18:43,535 Validation Data Eval:
2021-11-30 12:18:46,083   Average segmentation loss on validation set: 0.1400
2021-11-30 12:18:47,651 iteration 1360 : loss : 0.081637, loss_ce: 0.029728
 20%|██████                        | 80/400 [36:49<2:35:26, 29.15s/it]2021-11-30 12:18:49,306 iteration 1361 : loss : 0.091742, loss_ce: 0.039636
2021-11-30 12:18:50,802 iteration 1362 : loss : 0.116371, loss_ce: 0.041917
2021-11-30 12:18:52,489 iteration 1363 : loss : 0.120856, loss_ce: 0.053922
2021-11-30 12:18:53,832 iteration 1364 : loss : 0.107013, loss_ce: 0.034702
2021-11-30 12:18:55,401 iteration 1365 : loss : 0.100653, loss_ce: 0.046938
2021-11-30 12:18:56,909 iteration 1366 : loss : 0.091939, loss_ce: 0.039536
2021-11-30 12:18:58,380 iteration 1367 : loss : 0.096233, loss_ce: 0.041514
2021-11-30 12:18:59,886 iteration 1368 : loss : 0.066937, loss_ce: 0.036797
2021-11-30 12:19:01,351 iteration 1369 : loss : 0.079562, loss_ce: 0.034577
2021-11-30 12:19:02,886 iteration 1370 : loss : 0.093068, loss_ce: 0.045805
2021-11-30 12:19:04,348 iteration 1371 : loss : 0.069804, loss_ce: 0.029605
2021-11-30 12:19:05,791 iteration 1372 : loss : 0.093552, loss_ce: 0.044120
2021-11-30 12:19:07,242 iteration 1373 : loss : 0.087118, loss_ce: 0.039509
2021-11-30 12:19:08,728 iteration 1374 : loss : 0.075082, loss_ce: 0.032324
2021-11-30 12:19:10,241 iteration 1375 : loss : 0.104013, loss_ce: 0.050323
2021-11-30 12:19:11,757 iteration 1376 : loss : 0.098702, loss_ce: 0.037408
2021-11-30 12:19:13,228 iteration 1377 : loss : 0.065048, loss_ce: 0.028960
 20%|██████                        | 81/400 [37:15<2:29:17, 28.08s/it]2021-11-30 12:19:14,683 iteration 1378 : loss : 0.124964, loss_ce: 0.050963
2021-11-30 12:19:16,232 iteration 1379 : loss : 0.076041, loss_ce: 0.035988
2021-11-30 12:19:17,731 iteration 1380 : loss : 0.089227, loss_ce: 0.039136
2021-11-30 12:19:19,327 iteration 1381 : loss : 0.095764, loss_ce: 0.042314
2021-11-30 12:19:20,725 iteration 1382 : loss : 0.083615, loss_ce: 0.037359
2021-11-30 12:19:22,299 iteration 1383 : loss : 0.099059, loss_ce: 0.043462
2021-11-30 12:19:23,781 iteration 1384 : loss : 0.089993, loss_ce: 0.039065
2021-11-30 12:19:25,212 iteration 1385 : loss : 0.110873, loss_ce: 0.045625
2021-11-30 12:19:26,675 iteration 1386 : loss : 0.070572, loss_ce: 0.033571
2021-11-30 12:19:28,149 iteration 1387 : loss : 0.093217, loss_ce: 0.041854
2021-11-30 12:19:29,619 iteration 1388 : loss : 0.099911, loss_ce: 0.037038
2021-11-30 12:19:31,111 iteration 1389 : loss : 0.100995, loss_ce: 0.047328
2021-11-30 12:19:32,644 iteration 1390 : loss : 0.082479, loss_ce: 0.037484
2021-11-30 12:19:34,207 iteration 1391 : loss : 0.081415, loss_ce: 0.033469
2021-11-30 12:19:35,732 iteration 1392 : loss : 0.085205, loss_ce: 0.043294
2021-11-30 12:19:37,175 iteration 1393 : loss : 0.078127, loss_ce: 0.033757
2021-11-30 12:19:38,719 iteration 1394 : loss : 0.087517, loss_ce: 0.043364
 20%|██████▏                       | 82/400 [37:41<2:24:40, 27.30s/it]2021-11-30 12:19:40,230 iteration 1395 : loss : 0.082097, loss_ce: 0.040596
2021-11-30 12:19:41,697 iteration 1396 : loss : 0.108901, loss_ce: 0.056856
2021-11-30 12:19:43,246 iteration 1397 : loss : 0.095832, loss_ce: 0.040530
2021-11-30 12:19:44,808 iteration 1398 : loss : 0.101757, loss_ce: 0.046561
2021-11-30 12:19:46,223 iteration 1399 : loss : 0.088409, loss_ce: 0.038922
2021-11-30 12:19:47,617 iteration 1400 : loss : 0.085926, loss_ce: 0.034475
2021-11-30 12:19:49,169 iteration 1401 : loss : 0.087071, loss_ce: 0.035966
2021-11-30 12:19:50,663 iteration 1402 : loss : 0.096461, loss_ce: 0.044937
2021-11-30 12:19:52,096 iteration 1403 : loss : 0.087025, loss_ce: 0.042691
2021-11-30 12:19:53,520 iteration 1404 : loss : 0.070010, loss_ce: 0.035524
2021-11-30 12:19:55,009 iteration 1405 : loss : 0.108361, loss_ce: 0.041612
2021-11-30 12:19:56,505 iteration 1406 : loss : 0.086348, loss_ce: 0.032871
2021-11-30 12:19:58,047 iteration 1407 : loss : 0.076545, loss_ce: 0.036683
2021-11-30 12:19:59,537 iteration 1408 : loss : 0.088017, loss_ce: 0.038495
2021-11-30 12:20:01,030 iteration 1409 : loss : 0.121909, loss_ce: 0.043086
2021-11-30 12:20:02,598 iteration 1410 : loss : 0.088871, loss_ce: 0.035355
2021-11-30 12:20:04,055 iteration 1411 : loss : 0.090136, loss_ce: 0.038402
 21%|██████▏                       | 83/400 [38:06<2:21:07, 26.71s/it]2021-11-30 12:20:05,551 iteration 1412 : loss : 0.097319, loss_ce: 0.040279
2021-11-30 12:20:07,092 iteration 1413 : loss : 0.120903, loss_ce: 0.044159
2021-11-30 12:20:08,558 iteration 1414 : loss : 0.090357, loss_ce: 0.042738
2021-11-30 12:20:10,104 iteration 1415 : loss : 0.082717, loss_ce: 0.030936
2021-11-30 12:20:11,565 iteration 1416 : loss : 0.078763, loss_ce: 0.036365
2021-11-30 12:20:13,012 iteration 1417 : loss : 0.109726, loss_ce: 0.038865
2021-11-30 12:20:14,472 iteration 1418 : loss : 0.092620, loss_ce: 0.041390
2021-11-30 12:20:15,899 iteration 1419 : loss : 0.080675, loss_ce: 0.037385
2021-11-30 12:20:17,430 iteration 1420 : loss : 0.101693, loss_ce: 0.038602
2021-11-30 12:20:19,007 iteration 1421 : loss : 0.099442, loss_ce: 0.042100
2021-11-30 12:20:20,518 iteration 1422 : loss : 0.078165, loss_ce: 0.028992
2021-11-30 12:20:21,939 iteration 1423 : loss : 0.079728, loss_ce: 0.037417
2021-11-30 12:20:23,478 iteration 1424 : loss : 0.067567, loss_ce: 0.033670
2021-11-30 12:20:24,994 iteration 1425 : loss : 0.053919, loss_ce: 0.028099
2021-11-30 12:20:26,436 iteration 1426 : loss : 0.071104, loss_ce: 0.036310
2021-11-30 12:20:27,910 iteration 1427 : loss : 0.109967, loss_ce: 0.049077
2021-11-30 12:20:29,299 iteration 1428 : loss : 0.127563, loss_ce: 0.052983
 21%|██████▎                       | 84/400 [38:31<2:18:21, 26.27s/it]2021-11-30 12:20:30,926 iteration 1429 : loss : 0.092094, loss_ce: 0.048142
2021-11-30 12:20:32,371 iteration 1430 : loss : 0.197114, loss_ce: 0.055753
2021-11-30 12:20:33,856 iteration 1431 : loss : 0.126518, loss_ce: 0.061109
2021-11-30 12:20:35,342 iteration 1432 : loss : 0.088554, loss_ce: 0.044850
2021-11-30 12:20:36,887 iteration 1433 : loss : 0.079196, loss_ce: 0.035608
2021-11-30 12:20:38,342 iteration 1434 : loss : 0.097257, loss_ce: 0.044624
2021-11-30 12:20:39,852 iteration 1435 : loss : 0.079591, loss_ce: 0.038014
2021-11-30 12:20:41,386 iteration 1436 : loss : 0.083576, loss_ce: 0.039011
2021-11-30 12:20:42,923 iteration 1437 : loss : 0.083098, loss_ce: 0.030916
2021-11-30 12:20:44,409 iteration 1438 : loss : 0.064061, loss_ce: 0.034868
2021-11-30 12:20:45,946 iteration 1439 : loss : 0.088511, loss_ce: 0.039168
2021-11-30 12:20:47,479 iteration 1440 : loss : 0.079289, loss_ce: 0.039196
2021-11-30 12:20:48,960 iteration 1441 : loss : 0.059522, loss_ce: 0.029855
2021-11-30 12:20:50,449 iteration 1442 : loss : 0.071364, loss_ce: 0.033720
2021-11-30 12:20:51,979 iteration 1443 : loss : 0.099614, loss_ce: 0.041341
2021-11-30 12:20:53,489 iteration 1444 : loss : 0.152091, loss_ce: 0.046583
2021-11-30 12:20:53,489 Training Data Eval:
2021-11-30 12:21:00,834   Average segmentation loss on training set: 0.0596
2021-11-30 12:21:00,835 Validation Data Eval:
2021-11-30 12:21:03,382   Average segmentation loss on validation set: 0.1123
2021-11-30 12:21:05,466 Found new lowest validation loss at iteration 1444! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/UNET_RUNMC/best_val_loss.pth
2021-11-30 12:21:06,951 iteration 1445 : loss : 0.078828, loss_ce: 0.033894
 21%|██████▍                       | 85/400 [39:09<2:35:50, 29.68s/it]2021-11-30 12:21:08,557 iteration 1446 : loss : 0.078791, loss_ce: 0.029691
2021-11-30 12:21:09,989 iteration 1447 : loss : 0.083595, loss_ce: 0.033275
2021-11-30 12:21:11,363 iteration 1448 : loss : 0.074185, loss_ce: 0.041774
2021-11-30 12:21:12,826 iteration 1449 : loss : 0.068878, loss_ce: 0.030455
2021-11-30 12:21:14,398 iteration 1450 : loss : 0.080550, loss_ce: 0.042878
2021-11-30 12:21:15,981 iteration 1451 : loss : 0.083451, loss_ce: 0.035953
2021-11-30 12:21:17,405 iteration 1452 : loss : 0.065697, loss_ce: 0.028003
2021-11-30 12:21:18,839 iteration 1453 : loss : 0.109480, loss_ce: 0.046510
2021-11-30 12:21:20,322 iteration 1454 : loss : 0.113361, loss_ce: 0.045063
2021-11-30 12:21:21,728 iteration 1455 : loss : 0.065349, loss_ce: 0.031627
2021-11-30 12:21:23,285 iteration 1456 : loss : 0.148821, loss_ce: 0.042115
2021-11-30 12:21:24,739 iteration 1457 : loss : 0.081785, loss_ce: 0.038525
2021-11-30 12:21:26,191 iteration 1458 : loss : 0.081353, loss_ce: 0.032062
2021-11-30 12:21:27,650 iteration 1459 : loss : 0.081274, loss_ce: 0.028599
2021-11-30 12:21:29,228 iteration 1460 : loss : 0.132303, loss_ce: 0.060534
2021-11-30 12:21:30,739 iteration 1461 : loss : 0.099857, loss_ce: 0.047762
2021-11-30 12:21:32,292 iteration 1462 : loss : 0.065254, loss_ce: 0.034555
 22%|██████▍                       | 86/400 [39:34<2:28:32, 28.38s/it]2021-11-30 12:21:33,829 iteration 1463 : loss : 0.067141, loss_ce: 0.033774
2021-11-30 12:21:35,292 iteration 1464 : loss : 0.088298, loss_ce: 0.037661
2021-11-30 12:21:36,806 iteration 1465 : loss : 0.092959, loss_ce: 0.031454
2021-11-30 12:21:38,330 iteration 1466 : loss : 0.060857, loss_ce: 0.029149
2021-11-30 12:21:39,833 iteration 1467 : loss : 0.076970, loss_ce: 0.041750
2021-11-30 12:21:41,302 iteration 1468 : loss : 0.064061, loss_ce: 0.033516
2021-11-30 12:21:42,799 iteration 1469 : loss : 0.079163, loss_ce: 0.032807
2021-11-30 12:21:44,315 iteration 1470 : loss : 0.129730, loss_ce: 0.046615
2021-11-30 12:21:45,820 iteration 1471 : loss : 0.101223, loss_ce: 0.058206
2021-11-30 12:21:47,203 iteration 1472 : loss : 0.072998, loss_ce: 0.032070
2021-11-30 12:21:48,804 iteration 1473 : loss : 0.080606, loss_ce: 0.037029
2021-11-30 12:21:50,357 iteration 1474 : loss : 0.083884, loss_ce: 0.034395
2021-11-30 12:21:51,845 iteration 1475 : loss : 0.101471, loss_ce: 0.040278
2021-11-30 12:21:53,451 iteration 1476 : loss : 0.128096, loss_ce: 0.049379
2021-11-30 12:21:54,988 iteration 1477 : loss : 0.068042, loss_ce: 0.027678
2021-11-30 12:21:56,548 iteration 1478 : loss : 0.090411, loss_ce: 0.037138
2021-11-30 12:21:58,001 iteration 1479 : loss : 0.081426, loss_ce: 0.033798
 22%|██████▌                       | 87/400 [40:00<2:23:53, 27.58s/it]2021-11-30 12:21:59,542 iteration 1480 : loss : 0.084532, loss_ce: 0.037261
2021-11-30 12:22:01,010 iteration 1481 : loss : 0.073882, loss_ce: 0.034621
2021-11-30 12:22:02,495 iteration 1482 : loss : 0.062349, loss_ce: 0.030987
2021-11-30 12:22:04,073 iteration 1483 : loss : 0.068075, loss_ce: 0.031325
2021-11-30 12:22:05,709 iteration 1484 : loss : 0.091132, loss_ce: 0.034610
2021-11-30 12:22:07,254 iteration 1485 : loss : 0.071464, loss_ce: 0.036424
2021-11-30 12:22:08,700 iteration 1486 : loss : 0.077876, loss_ce: 0.033814
2021-11-30 12:22:10,171 iteration 1487 : loss : 0.097540, loss_ce: 0.055753
2021-11-30 12:22:11,683 iteration 1488 : loss : 0.081491, loss_ce: 0.034856
2021-11-30 12:22:13,203 iteration 1489 : loss : 0.057203, loss_ce: 0.028424
2021-11-30 12:22:14,737 iteration 1490 : loss : 0.116809, loss_ce: 0.048827
2021-11-30 12:22:16,303 iteration 1491 : loss : 0.078881, loss_ce: 0.038659
2021-11-30 12:22:17,755 iteration 1492 : loss : 0.061414, loss_ce: 0.030008
2021-11-30 12:22:19,307 iteration 1493 : loss : 0.096251, loss_ce: 0.039791
2021-11-30 12:22:20,780 iteration 1494 : loss : 0.075871, loss_ce: 0.035316
2021-11-30 12:22:22,251 iteration 1495 : loss : 0.087413, loss_ce: 0.037411
2021-11-30 12:22:23,742 iteration 1496 : loss : 0.088176, loss_ce: 0.029690
 22%|██████▌                       | 88/400 [40:26<2:20:33, 27.03s/it]2021-11-30 12:22:25,236 iteration 1497 : loss : 0.129518, loss_ce: 0.057800
2021-11-30 12:22:26,708 iteration 1498 : loss : 0.070751, loss_ce: 0.027712
2021-11-30 12:22:28,241 iteration 1499 : loss : 0.078225, loss_ce: 0.041792
2021-11-30 12:22:29,640 iteration 1500 : loss : 0.091559, loss_ce: 0.034080
2021-11-30 12:22:31,116 iteration 1501 : loss : 0.085674, loss_ce: 0.034420
2021-11-30 12:22:32,631 iteration 1502 : loss : 0.083119, loss_ce: 0.034412
2021-11-30 12:22:34,108 iteration 1503 : loss : 0.047651, loss_ce: 0.026780
2021-11-30 12:22:35,583 iteration 1504 : loss : 0.064145, loss_ce: 0.027809
2021-11-30 12:22:37,008 iteration 1505 : loss : 0.072866, loss_ce: 0.031302
2021-11-30 12:22:38,498 iteration 1506 : loss : 0.074069, loss_ce: 0.034730
2021-11-30 12:22:39,993 iteration 1507 : loss : 0.092355, loss_ce: 0.043022
2021-11-30 12:22:41,446 iteration 1508 : loss : 0.086679, loss_ce: 0.042517
2021-11-30 12:22:42,938 iteration 1509 : loss : 0.072904, loss_ce: 0.030531
2021-11-30 12:22:44,380 iteration 1510 : loss : 0.069807, loss_ce: 0.029986
2021-11-30 12:22:45,879 iteration 1511 : loss : 0.089300, loss_ce: 0.046586
2021-11-30 12:22:47,311 iteration 1512 : loss : 0.067075, loss_ce: 0.030937
2021-11-30 12:22:48,815 iteration 1513 : loss : 0.076247, loss_ce: 0.038609
 22%|██████▋                       | 89/400 [40:51<2:17:03, 26.44s/it]2021-11-30 12:22:50,344 iteration 1514 : loss : 0.086348, loss_ce: 0.036755
2021-11-30 12:22:51,842 iteration 1515 : loss : 0.089873, loss_ce: 0.042026
2021-11-30 12:22:53,357 iteration 1516 : loss : 0.068089, loss_ce: 0.031170
2021-11-30 12:22:54,835 iteration 1517 : loss : 0.077333, loss_ce: 0.037575
2021-11-30 12:22:56,355 iteration 1518 : loss : 0.061019, loss_ce: 0.025063
2021-11-30 12:22:57,812 iteration 1519 : loss : 0.064049, loss_ce: 0.029232
2021-11-30 12:22:59,258 iteration 1520 : loss : 0.098178, loss_ce: 0.040417
2021-11-30 12:23:00,710 iteration 1521 : loss : 0.078252, loss_ce: 0.031701
2021-11-30 12:23:02,297 iteration 1522 : loss : 0.064984, loss_ce: 0.029549
2021-11-30 12:23:03,711 iteration 1523 : loss : 0.074813, loss_ce: 0.038940
2021-11-30 12:23:05,206 iteration 1524 : loss : 0.100035, loss_ce: 0.039956
2021-11-30 12:23:06,652 iteration 1525 : loss : 0.071148, loss_ce: 0.033462
2021-11-30 12:23:08,197 iteration 1526 : loss : 0.064343, loss_ce: 0.031510
2021-11-30 12:23:09,743 iteration 1527 : loss : 0.096988, loss_ce: 0.049152
2021-11-30 12:23:11,286 iteration 1528 : loss : 0.064542, loss_ce: 0.030263
2021-11-30 12:23:12,730 iteration 1529 : loss : 0.076220, loss_ce: 0.032107
2021-11-30 12:23:12,730 Training Data Eval:
2021-11-30 12:23:20,091   Average segmentation loss on training set: 0.0518
2021-11-30 12:23:20,092 Validation Data Eval:
2021-11-30 12:23:22,661   Average segmentation loss on validation set: 0.1097
2021-11-30 12:23:24,583 Found new lowest validation loss at iteration 1529! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/UNET_RUNMC/best_val_loss.pth
2021-11-30 12:23:26,058 iteration 1530 : loss : 0.078369, loss_ce: 0.040458
 22%|██████▊                       | 90/400 [41:28<2:33:21, 29.68s/it]2021-11-30 12:23:27,716 iteration 1531 : loss : 0.083624, loss_ce: 0.034463
2021-11-30 12:23:29,219 iteration 1532 : loss : 0.073919, loss_ce: 0.031334
2021-11-30 12:23:30,633 iteration 1533 : loss : 0.064541, loss_ce: 0.033929
2021-11-30 12:23:32,139 iteration 1534 : loss : 0.077754, loss_ce: 0.033607
2021-11-30 12:23:33,580 iteration 1535 : loss : 0.067755, loss_ce: 0.034640
2021-11-30 12:23:35,004 iteration 1536 : loss : 0.099577, loss_ce: 0.053641
2021-11-30 12:23:36,401 iteration 1537 : loss : 0.110217, loss_ce: 0.040884
2021-11-30 12:23:37,788 iteration 1538 : loss : 0.196749, loss_ce: 0.051457
2021-11-30 12:23:39,235 iteration 1539 : loss : 0.076663, loss_ce: 0.033144
2021-11-30 12:23:40,696 iteration 1540 : loss : 0.075938, loss_ce: 0.038059
2021-11-30 12:23:42,079 iteration 1541 : loss : 0.058149, loss_ce: 0.026717
2021-11-30 12:23:43,577 iteration 1542 : loss : 0.069141, loss_ce: 0.036441
2021-11-30 12:23:45,053 iteration 1543 : loss : 0.094744, loss_ce: 0.039493
2021-11-30 12:23:46,480 iteration 1544 : loss : 0.068782, loss_ce: 0.033822
2021-11-30 12:23:47,972 iteration 1545 : loss : 0.084944, loss_ce: 0.040908
2021-11-30 12:23:49,447 iteration 1546 : loss : 0.065617, loss_ce: 0.033661
2021-11-30 12:23:50,916 iteration 1547 : loss : 0.074798, loss_ce: 0.031645
 23%|██████▊                       | 91/400 [41:53<2:25:25, 28.24s/it]2021-11-30 12:23:52,470 iteration 1548 : loss : 0.077165, loss_ce: 0.031619
2021-11-30 12:23:53,886 iteration 1549 : loss : 0.058768, loss_ce: 0.029616
2021-11-30 12:23:55,482 iteration 1550 : loss : 0.076370, loss_ce: 0.036334
2021-11-30 12:23:56,949 iteration 1551 : loss : 0.094538, loss_ce: 0.041717
2021-11-30 12:23:58,430 iteration 1552 : loss : 0.055161, loss_ce: 0.029407
2021-11-30 12:23:59,819 iteration 1553 : loss : 0.064608, loss_ce: 0.035113
2021-11-30 12:24:01,285 iteration 1554 : loss : 0.099790, loss_ce: 0.042243
2021-11-30 12:24:02,840 iteration 1555 : loss : 0.070167, loss_ce: 0.030821
2021-11-30 12:24:04,476 iteration 1556 : loss : 0.116536, loss_ce: 0.048236
2021-11-30 12:24:06,025 iteration 1557 : loss : 0.157473, loss_ce: 0.044677
2021-11-30 12:24:07,491 iteration 1558 : loss : 0.104913, loss_ce: 0.038370
2021-11-30 12:24:09,089 iteration 1559 : loss : 0.068658, loss_ce: 0.033921
2021-11-30 12:24:10,551 iteration 1560 : loss : 0.073461, loss_ce: 0.027724
2021-11-30 12:24:12,009 iteration 1561 : loss : 0.092051, loss_ce: 0.041125
2021-11-30 12:24:13,499 iteration 1562 : loss : 0.077486, loss_ce: 0.041408
2021-11-30 12:24:14,970 iteration 1563 : loss : 0.096062, loss_ce: 0.053126
2021-11-30 12:24:16,456 iteration 1564 : loss : 0.095282, loss_ce: 0.035539
 23%|██████▉                       | 92/400 [42:18<2:20:47, 27.43s/it]2021-11-30 12:24:18,042 iteration 1565 : loss : 0.063685, loss_ce: 0.029275
2021-11-30 12:24:19,500 iteration 1566 : loss : 0.082325, loss_ce: 0.041842
2021-11-30 12:24:21,026 iteration 1567 : loss : 0.093658, loss_ce: 0.032637
2021-11-30 12:24:22,618 iteration 1568 : loss : 0.072658, loss_ce: 0.036378
2021-11-30 12:24:24,078 iteration 1569 : loss : 0.075907, loss_ce: 0.035413
2021-11-30 12:24:25,576 iteration 1570 : loss : 0.061842, loss_ce: 0.029400
2021-11-30 12:24:27,047 iteration 1571 : loss : 0.084821, loss_ce: 0.038676
2021-11-30 12:24:28,535 iteration 1572 : loss : 0.094409, loss_ce: 0.047695
2021-11-30 12:24:30,032 iteration 1573 : loss : 0.093107, loss_ce: 0.031642
2021-11-30 12:24:31,528 iteration 1574 : loss : 0.072949, loss_ce: 0.038633
2021-11-30 12:24:33,048 iteration 1575 : loss : 0.097995, loss_ce: 0.045442
2021-11-30 12:24:34,514 iteration 1576 : loss : 0.077841, loss_ce: 0.033076
2021-11-30 12:24:35,945 iteration 1577 : loss : 0.098659, loss_ce: 0.031459
2021-11-30 12:24:37,322 iteration 1578 : loss : 0.051451, loss_ce: 0.027126
2021-11-30 12:24:38,785 iteration 1579 : loss : 0.073204, loss_ce: 0.032816
2021-11-30 12:24:40,295 iteration 1580 : loss : 0.096896, loss_ce: 0.033637
2021-11-30 12:24:41,809 iteration 1581 : loss : 0.094272, loss_ce: 0.044964
 23%|██████▉                       | 93/400 [42:44<2:17:09, 26.81s/it]2021-11-30 12:24:43,399 iteration 1582 : loss : 0.071917, loss_ce: 0.042674
2021-11-30 12:24:44,872 iteration 1583 : loss : 0.066099, loss_ce: 0.026132
2021-11-30 12:24:46,310 iteration 1584 : loss : 0.060812, loss_ce: 0.027667
2021-11-30 12:24:47,794 iteration 1585 : loss : 0.062072, loss_ce: 0.030963
2021-11-30 12:24:49,195 iteration 1586 : loss : 0.082943, loss_ce: 0.043731
2021-11-30 12:24:50,673 iteration 1587 : loss : 0.077415, loss_ce: 0.032478
2021-11-30 12:24:52,145 iteration 1588 : loss : 0.066493, loss_ce: 0.025953
2021-11-30 12:24:53,613 iteration 1589 : loss : 0.082216, loss_ce: 0.038948
2021-11-30 12:24:55,182 iteration 1590 : loss : 0.109542, loss_ce: 0.046680
2021-11-30 12:24:56,650 iteration 1591 : loss : 0.072977, loss_ce: 0.038113
2021-11-30 12:24:58,050 iteration 1592 : loss : 0.063101, loss_ce: 0.030937
2021-11-30 12:24:59,561 iteration 1593 : loss : 0.113132, loss_ce: 0.047857
2021-11-30 12:25:01,037 iteration 1594 : loss : 0.065575, loss_ce: 0.028049
2021-11-30 12:25:02,464 iteration 1595 : loss : 0.080754, loss_ce: 0.046154
2021-11-30 12:25:03,866 iteration 1596 : loss : 0.104182, loss_ce: 0.037513
2021-11-30 12:25:05,366 iteration 1597 : loss : 0.092506, loss_ce: 0.032895
2021-11-30 12:25:06,900 iteration 1598 : loss : 0.095411, loss_ce: 0.035379
 24%|███████                       | 94/400 [43:09<2:14:04, 26.29s/it]2021-11-30 12:25:08,480 iteration 1599 : loss : 0.063166, loss_ce: 0.033846
2021-11-30 12:25:09,939 iteration 1600 : loss : 0.066287, loss_ce: 0.033574
2021-11-30 12:25:11,458 iteration 1601 : loss : 0.076051, loss_ce: 0.036344
2021-11-30 12:25:12,960 iteration 1602 : loss : 0.101610, loss_ce: 0.034964
2021-11-30 12:25:14,430 iteration 1603 : loss : 0.061812, loss_ce: 0.028254
2021-11-30 12:25:15,875 iteration 1604 : loss : 0.090436, loss_ce: 0.030642
2021-11-30 12:25:17,332 iteration 1605 : loss : 0.081364, loss_ce: 0.035392
2021-11-30 12:25:18,828 iteration 1606 : loss : 0.059294, loss_ce: 0.027694
2021-11-30 12:25:20,301 iteration 1607 : loss : 0.080482, loss_ce: 0.034381
2021-11-30 12:25:21,867 iteration 1608 : loss : 0.101989, loss_ce: 0.044501
2021-11-30 12:25:23,308 iteration 1609 : loss : 0.065732, loss_ce: 0.029963
2021-11-30 12:25:24,879 iteration 1610 : loss : 0.076969, loss_ce: 0.033821
2021-11-30 12:25:26,463 iteration 1611 : loss : 0.100362, loss_ce: 0.050503
2021-11-30 12:25:28,063 iteration 1612 : loss : 0.059733, loss_ce: 0.026949
2021-11-30 12:25:29,554 iteration 1613 : loss : 0.084686, loss_ce: 0.032378
2021-11-30 12:25:31,104 iteration 1614 : loss : 0.104990, loss_ce: 0.051998
2021-11-30 12:25:31,104 Training Data Eval:
2021-11-30 12:25:38,459   Average segmentation loss on training set: 0.0584
2021-11-30 12:25:38,460 Validation Data Eval:
2021-11-30 12:25:41,015   Average segmentation loss on validation set: 0.1010
2021-11-30 12:25:42,935 Found new lowest validation loss at iteration 1614! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/UNET_RUNMC/best_val_loss.pth
2021-11-30 12:25:44,477 iteration 1615 : loss : 0.084231, loss_ce: 0.035137
 24%|███████▏                      | 95/400 [43:46<2:30:51, 29.68s/it]2021-11-30 12:25:45,970 iteration 1616 : loss : 0.070727, loss_ce: 0.033999
2021-11-30 12:25:47,399 iteration 1617 : loss : 0.074154, loss_ce: 0.033686
2021-11-30 12:25:48,779 iteration 1618 : loss : 0.079117, loss_ce: 0.031872
2021-11-30 12:25:50,222 iteration 1619 : loss : 0.096990, loss_ce: 0.062256
2021-11-30 12:25:51,808 iteration 1620 : loss : 0.079077, loss_ce: 0.031127
2021-11-30 12:25:53,406 iteration 1621 : loss : 0.069701, loss_ce: 0.028149
2021-11-30 12:25:54,853 iteration 1622 : loss : 0.079152, loss_ce: 0.035154
2021-11-30 12:25:56,261 iteration 1623 : loss : 0.068095, loss_ce: 0.027210
2021-11-30 12:25:57,732 iteration 1624 : loss : 0.071450, loss_ce: 0.033710
2021-11-30 12:25:59,114 iteration 1625 : loss : 0.066574, loss_ce: 0.028306
2021-11-30 12:26:00,531 iteration 1626 : loss : 0.072756, loss_ce: 0.034531
2021-11-30 12:26:02,144 iteration 1627 : loss : 0.089869, loss_ce: 0.038226
2021-11-30 12:26:03,752 iteration 1628 : loss : 0.081934, loss_ce: 0.041017
2021-11-30 12:26:05,235 iteration 1629 : loss : 0.066791, loss_ce: 0.027964
2021-11-30 12:26:06,669 iteration 1630 : loss : 0.046181, loss_ce: 0.023022
2021-11-30 12:26:08,113 iteration 1631 : loss : 0.101436, loss_ce: 0.040596
2021-11-30 12:26:09,557 iteration 1632 : loss : 0.067355, loss_ce: 0.031759
 24%|███████▏                      | 96/400 [44:11<2:23:21, 28.29s/it]2021-11-30 12:26:11,097 iteration 1633 : loss : 0.058102, loss_ce: 0.025143
2021-11-30 12:26:12,615 iteration 1634 : loss : 0.079076, loss_ce: 0.033088
2021-11-30 12:26:14,085 iteration 1635 : loss : 0.072456, loss_ce: 0.029746
2021-11-30 12:26:15,500 iteration 1636 : loss : 0.067705, loss_ce: 0.035796
2021-11-30 12:26:17,017 iteration 1637 : loss : 0.088318, loss_ce: 0.032198
2021-11-30 12:26:18,499 iteration 1638 : loss : 0.070140, loss_ce: 0.034427
2021-11-30 12:26:20,053 iteration 1639 : loss : 0.079523, loss_ce: 0.038866
2021-11-30 12:26:21,506 iteration 1640 : loss : 0.072424, loss_ce: 0.029102
2021-11-30 12:26:23,083 iteration 1641 : loss : 0.069443, loss_ce: 0.033652
2021-11-30 12:26:24,516 iteration 1642 : loss : 0.062352, loss_ce: 0.029229
2021-11-30 12:26:25,936 iteration 1643 : loss : 0.079939, loss_ce: 0.031445
2021-11-30 12:26:27,478 iteration 1644 : loss : 0.070081, loss_ce: 0.031176
2021-11-30 12:26:29,063 iteration 1645 : loss : 0.071331, loss_ce: 0.036552
2021-11-30 12:26:30,502 iteration 1646 : loss : 0.084019, loss_ce: 0.033735
2021-11-30 12:26:31,982 iteration 1647 : loss : 0.123173, loss_ce: 0.047142
2021-11-30 12:26:33,375 iteration 1648 : loss : 0.052877, loss_ce: 0.026348
2021-11-30 12:26:34,788 iteration 1649 : loss : 0.052830, loss_ce: 0.027191
 24%|███████▎                      | 97/400 [44:37<2:18:16, 27.38s/it]2021-11-30 12:26:36,460 iteration 1650 : loss : 0.073365, loss_ce: 0.039498
2021-11-30 12:26:37,909 iteration 1651 : loss : 0.099771, loss_ce: 0.034894
2021-11-30 12:26:39,402 iteration 1652 : loss : 0.099007, loss_ce: 0.033095
2021-11-30 12:26:40,855 iteration 1653 : loss : 0.067797, loss_ce: 0.025997
2021-11-30 12:26:42,320 iteration 1654 : loss : 0.073919, loss_ce: 0.029597
2021-11-30 12:26:43,749 iteration 1655 : loss : 0.069788, loss_ce: 0.030329
2021-11-30 12:26:45,296 iteration 1656 : loss : 0.109428, loss_ce: 0.043249
2021-11-30 12:26:46,774 iteration 1657 : loss : 0.068928, loss_ce: 0.035273
2021-11-30 12:26:48,317 iteration 1658 : loss : 0.116473, loss_ce: 0.060688
2021-11-30 12:26:49,890 iteration 1659 : loss : 0.068653, loss_ce: 0.034939
2021-11-30 12:26:51,364 iteration 1660 : loss : 0.065183, loss_ce: 0.028831
2021-11-30 12:26:52,776 iteration 1661 : loss : 0.072721, loss_ce: 0.030542
2021-11-30 12:26:54,311 iteration 1662 : loss : 0.079417, loss_ce: 0.031911
2021-11-30 12:26:55,864 iteration 1663 : loss : 0.059657, loss_ce: 0.031636
2021-11-30 12:26:57,353 iteration 1664 : loss : 0.099985, loss_ce: 0.041521
2021-11-30 12:26:58,818 iteration 1665 : loss : 0.065687, loss_ce: 0.032323
2021-11-30 12:27:00,331 iteration 1666 : loss : 0.068953, loss_ce: 0.023952
 24%|███████▎                      | 98/400 [45:02<2:15:01, 26.82s/it]2021-11-30 12:27:01,819 iteration 1667 : loss : 0.066036, loss_ce: 0.037825
2021-11-30 12:27:03,389 iteration 1668 : loss : 0.076963, loss_ce: 0.032109
2021-11-30 12:27:04,916 iteration 1669 : loss : 0.086816, loss_ce: 0.034277
2021-11-30 12:27:06,332 iteration 1670 : loss : 0.053363, loss_ce: 0.023652
2021-11-30 12:27:07,769 iteration 1671 : loss : 0.051382, loss_ce: 0.027319
2021-11-30 12:27:09,321 iteration 1672 : loss : 0.080062, loss_ce: 0.037576
2021-11-30 12:27:10,834 iteration 1673 : loss : 0.079729, loss_ce: 0.033566
2021-11-30 12:27:12,411 iteration 1674 : loss : 0.086930, loss_ce: 0.035693
2021-11-30 12:27:13,870 iteration 1675 : loss : 0.105698, loss_ce: 0.034211
2021-11-30 12:27:15,415 iteration 1676 : loss : 0.070125, loss_ce: 0.037963
2021-11-30 12:27:16,921 iteration 1677 : loss : 0.101401, loss_ce: 0.035696
2021-11-30 12:27:18,460 iteration 1678 : loss : 0.075924, loss_ce: 0.034016
2021-11-30 12:27:19,929 iteration 1679 : loss : 0.100429, loss_ce: 0.042870
2021-11-30 12:27:21,456 iteration 1680 : loss : 0.084786, loss_ce: 0.038093
2021-11-30 12:27:22,926 iteration 1681 : loss : 0.071711, loss_ce: 0.029469
2021-11-30 12:27:24,432 iteration 1682 : loss : 0.066730, loss_ce: 0.032379
2021-11-30 12:27:25,894 iteration 1683 : loss : 0.081443, loss_ce: 0.043324
 25%|███████▍                      | 99/400 [45:28<2:12:41, 26.45s/it]2021-11-30 12:27:27,486 iteration 1684 : loss : 0.076795, loss_ce: 0.029906
2021-11-30 12:27:28,934 iteration 1685 : loss : 0.066233, loss_ce: 0.035474
2021-11-30 12:27:30,332 iteration 1686 : loss : 0.048550, loss_ce: 0.023141
2021-11-30 12:27:31,791 iteration 1687 : loss : 0.065111, loss_ce: 0.027679
2021-11-30 12:27:33,270 iteration 1688 : loss : 0.066694, loss_ce: 0.031114
2021-11-30 12:27:34,722 iteration 1689 : loss : 0.057987, loss_ce: 0.025306
2021-11-30 12:27:36,223 iteration 1690 : loss : 0.100733, loss_ce: 0.042278
2021-11-30 12:27:37,674 iteration 1691 : loss : 0.062513, loss_ce: 0.028950
2021-11-30 12:27:39,198 iteration 1692 : loss : 0.086650, loss_ce: 0.036846
2021-11-30 12:27:40,768 iteration 1693 : loss : 0.066075, loss_ce: 0.033851
2021-11-30 12:27:42,286 iteration 1694 : loss : 0.069281, loss_ce: 0.029538
2021-11-30 12:27:43,694 iteration 1695 : loss : 0.067602, loss_ce: 0.026393
2021-11-30 12:27:45,165 iteration 1696 : loss : 0.093030, loss_ce: 0.038174
2021-11-30 12:27:46,707 iteration 1697 : loss : 0.069956, loss_ce: 0.026654
2021-11-30 12:27:48,189 iteration 1698 : loss : 0.052848, loss_ce: 0.023758
2021-11-30 12:27:49,557 iteration 1699 : loss : 0.065425, loss_ce: 0.034201
2021-11-30 12:27:49,557 Training Data Eval:
2021-11-30 12:27:56,924   Average segmentation loss on training set: 0.0437
2021-11-30 12:27:56,925 Validation Data Eval:
2021-11-30 12:27:59,502   Average segmentation loss on validation set: 0.1281
2021-11-30 12:28:01,019 iteration 1700 : loss : 0.065834, loss_ce: 0.026840
 25%|███████▎                     | 100/400 [46:03<2:25:14, 29.05s/it]2021-11-30 12:28:02,611 iteration 1701 : loss : 0.068642, loss_ce: 0.033988
2021-11-30 12:28:04,122 iteration 1702 : loss : 0.066937, loss_ce: 0.031687
2021-11-30 12:28:05,540 iteration 1703 : loss : 0.061923, loss_ce: 0.029627
2021-11-30 12:28:07,093 iteration 1704 : loss : 0.066454, loss_ce: 0.028554
2021-11-30 12:28:08,593 iteration 1705 : loss : 0.068225, loss_ce: 0.034460
2021-11-30 12:28:10,108 iteration 1706 : loss : 0.078888, loss_ce: 0.042452
2021-11-30 12:28:11,573 iteration 1707 : loss : 0.059887, loss_ce: 0.026012
2021-11-30 12:28:13,076 iteration 1708 : loss : 0.082809, loss_ce: 0.033560
2021-11-30 12:28:14,469 iteration 1709 : loss : 0.064079, loss_ce: 0.032586
2021-11-30 12:28:15,875 iteration 1710 : loss : 0.055785, loss_ce: 0.027411
2021-11-30 12:28:17,385 iteration 1711 : loss : 0.061714, loss_ce: 0.029514
2021-11-30 12:28:18,816 iteration 1712 : loss : 0.042918, loss_ce: 0.019417
2021-11-30 12:28:20,248 iteration 1713 : loss : 0.050231, loss_ce: 0.022618
2021-11-30 12:28:21,841 iteration 1714 : loss : 0.081880, loss_ce: 0.028592
2021-11-30 12:28:23,361 iteration 1715 : loss : 0.087882, loss_ce: 0.029570
2021-11-30 12:28:24,940 iteration 1716 : loss : 0.084120, loss_ce: 0.034122
2021-11-30 12:28:26,427 iteration 1717 : loss : 0.077073, loss_ce: 0.033729
 25%|███████▎                     | 101/400 [46:28<2:19:19, 27.96s/it]2021-11-30 12:28:27,914 iteration 1718 : loss : 0.059718, loss_ce: 0.024428
2021-11-30 12:28:29,419 iteration 1719 : loss : 0.073257, loss_ce: 0.032934
2021-11-30 12:28:30,805 iteration 1720 : loss : 0.058462, loss_ce: 0.022310
2021-11-30 12:28:32,299 iteration 1721 : loss : 0.058311, loss_ce: 0.030566
2021-11-30 12:28:33,855 iteration 1722 : loss : 0.075962, loss_ce: 0.033152
2021-11-30 12:28:35,342 iteration 1723 : loss : 0.085852, loss_ce: 0.040415
2021-11-30 12:28:36,880 iteration 1724 : loss : 0.064047, loss_ce: 0.025955
2021-11-30 12:28:38,484 iteration 1725 : loss : 0.098578, loss_ce: 0.044201
2021-11-30 12:28:40,045 iteration 1726 : loss : 0.081525, loss_ce: 0.024788
2021-11-30 12:28:41,549 iteration 1727 : loss : 0.059661, loss_ce: 0.030115
2021-11-30 12:28:43,010 iteration 1728 : loss : 0.078377, loss_ce: 0.030064
2021-11-30 12:28:44,487 iteration 1729 : loss : 0.053327, loss_ce: 0.027781
2021-11-30 12:28:45,998 iteration 1730 : loss : 0.072208, loss_ce: 0.029609
2021-11-30 12:28:47,568 iteration 1731 : loss : 0.076326, loss_ce: 0.031147
2021-11-30 12:28:49,037 iteration 1732 : loss : 0.053219, loss_ce: 0.029261
2021-11-30 12:28:50,625 iteration 1733 : loss : 0.083162, loss_ce: 0.039159
2021-11-30 12:28:52,076 iteration 1734 : loss : 0.056349, loss_ce: 0.023814
 26%|███████▍                     | 102/400 [46:54<2:15:24, 27.26s/it]2021-11-30 12:28:53,562 iteration 1735 : loss : 0.074401, loss_ce: 0.031084
2021-11-30 12:28:55,036 iteration 1736 : loss : 0.047639, loss_ce: 0.024399
2021-11-30 12:28:56,514 iteration 1737 : loss : 0.052801, loss_ce: 0.026447
2021-11-30 12:28:57,949 iteration 1738 : loss : 0.060273, loss_ce: 0.026933
2021-11-30 12:28:59,533 iteration 1739 : loss : 0.072940, loss_ce: 0.035302
2021-11-30 12:29:01,055 iteration 1740 : loss : 0.097318, loss_ce: 0.039326
2021-11-30 12:29:02,537 iteration 1741 : loss : 0.066432, loss_ce: 0.026295
2021-11-30 12:29:04,151 iteration 1742 : loss : 0.062570, loss_ce: 0.031851
2021-11-30 12:29:05,616 iteration 1743 : loss : 0.075805, loss_ce: 0.032564
2021-11-30 12:29:07,139 iteration 1744 : loss : 0.059955, loss_ce: 0.027302
2021-11-30 12:29:08,777 iteration 1745 : loss : 0.089033, loss_ce: 0.047729
2021-11-30 12:29:10,233 iteration 1746 : loss : 0.053943, loss_ce: 0.025195
2021-11-30 12:29:11,687 iteration 1747 : loss : 0.049245, loss_ce: 0.025251
2021-11-30 12:29:13,184 iteration 1748 : loss : 0.085556, loss_ce: 0.043149
2021-11-30 12:29:14,686 iteration 1749 : loss : 0.069853, loss_ce: 0.029893
2021-11-30 12:29:16,126 iteration 1750 : loss : 0.062109, loss_ce: 0.029282
2021-11-30 12:29:17,706 iteration 1751 : loss : 0.081931, loss_ce: 0.035131
 26%|███████▍                     | 103/400 [47:20<2:12:31, 26.77s/it]2021-11-30 12:29:19,210 iteration 1752 : loss : 0.064973, loss_ce: 0.027063
2021-11-30 12:29:20,644 iteration 1753 : loss : 0.072074, loss_ce: 0.036226
2021-11-30 12:29:22,101 iteration 1754 : loss : 0.059200, loss_ce: 0.029372
2021-11-30 12:29:23,582 iteration 1755 : loss : 0.070877, loss_ce: 0.031054
2021-11-30 12:29:25,087 iteration 1756 : loss : 0.091235, loss_ce: 0.047701
2021-11-30 12:29:26,643 iteration 1757 : loss : 0.062517, loss_ce: 0.033470
2021-11-30 12:29:28,167 iteration 1758 : loss : 0.101782, loss_ce: 0.048402
2021-11-30 12:29:29,573 iteration 1759 : loss : 0.086908, loss_ce: 0.031238
2021-11-30 12:29:31,055 iteration 1760 : loss : 0.066695, loss_ce: 0.029127
2021-11-30 12:29:32,508 iteration 1761 : loss : 0.055429, loss_ce: 0.025875
2021-11-30 12:29:33,989 iteration 1762 : loss : 0.066788, loss_ce: 0.029595
2021-11-30 12:29:35,390 iteration 1763 : loss : 0.060608, loss_ce: 0.029401
2021-11-30 12:29:36,979 iteration 1764 : loss : 0.054828, loss_ce: 0.022126
2021-11-30 12:29:38,478 iteration 1765 : loss : 0.081613, loss_ce: 0.030377
2021-11-30 12:29:40,026 iteration 1766 : loss : 0.063867, loss_ce: 0.029999
2021-11-30 12:29:41,527 iteration 1767 : loss : 0.070984, loss_ce: 0.035423
2021-11-30 12:29:42,993 iteration 1768 : loss : 0.087547, loss_ce: 0.038921
 26%|███████▌                     | 104/400 [47:45<2:09:54, 26.33s/it]2021-11-30 12:29:44,587 iteration 1769 : loss : 0.059684, loss_ce: 0.028717
2021-11-30 12:29:46,060 iteration 1770 : loss : 0.060103, loss_ce: 0.026452
2021-11-30 12:29:47,486 iteration 1771 : loss : 0.066179, loss_ce: 0.024178
2021-11-30 12:29:49,024 iteration 1772 : loss : 0.073071, loss_ce: 0.035579
2021-11-30 12:29:50,557 iteration 1773 : loss : 0.077269, loss_ce: 0.037137
2021-11-30 12:29:52,005 iteration 1774 : loss : 0.062270, loss_ce: 0.026583
2021-11-30 12:29:53,467 iteration 1775 : loss : 0.061409, loss_ce: 0.025609
2021-11-30 12:29:54,927 iteration 1776 : loss : 0.072639, loss_ce: 0.029480
2021-11-30 12:29:56,372 iteration 1777 : loss : 0.061332, loss_ce: 0.027809
2021-11-30 12:29:57,917 iteration 1778 : loss : 0.069988, loss_ce: 0.036660
2021-11-30 12:29:59,395 iteration 1779 : loss : 0.065345, loss_ce: 0.032329
2021-11-30 12:30:00,907 iteration 1780 : loss : 0.105798, loss_ce: 0.035131
2021-11-30 12:30:02,424 iteration 1781 : loss : 0.075478, loss_ce: 0.031820
2021-11-30 12:30:03,824 iteration 1782 : loss : 0.084432, loss_ce: 0.027566
2021-11-30 12:30:05,281 iteration 1783 : loss : 0.063153, loss_ce: 0.032957
2021-11-30 12:30:06,790 iteration 1784 : loss : 0.082324, loss_ce: 0.046533
2021-11-30 12:30:06,790 Training Data Eval:
2021-11-30 12:30:14,154   Average segmentation loss on training set: 0.0429
2021-11-30 12:30:14,154 Validation Data Eval:
2021-11-30 12:30:16,721   Average segmentation loss on validation set: 0.1232
2021-11-30 12:30:18,216 iteration 1785 : loss : 0.054698, loss_ce: 0.029425
 26%|███████▌                     | 105/400 [48:20<2:22:34, 29.00s/it]2021-11-30 12:30:19,815 iteration 1786 : loss : 0.064524, loss_ce: 0.026523
2021-11-30 12:30:21,260 iteration 1787 : loss : 0.066722, loss_ce: 0.020779
2021-11-30 12:30:22,757 iteration 1788 : loss : 0.054235, loss_ce: 0.025322
2021-11-30 12:30:24,264 iteration 1789 : loss : 0.069125, loss_ce: 0.037547
2021-11-30 12:30:25,932 iteration 1790 : loss : 0.044133, loss_ce: 0.020516
2021-11-30 12:30:27,442 iteration 1791 : loss : 0.054796, loss_ce: 0.026549
2021-11-30 12:30:28,938 iteration 1792 : loss : 0.052537, loss_ce: 0.024291
2021-11-30 12:30:30,464 iteration 1793 : loss : 0.093099, loss_ce: 0.049000
2021-11-30 12:30:31,946 iteration 1794 : loss : 0.065974, loss_ce: 0.037829
2021-11-30 12:30:33,325 iteration 1795 : loss : 0.057159, loss_ce: 0.027108
2021-11-30 12:30:34,915 iteration 1796 : loss : 0.073590, loss_ce: 0.035030
2021-11-30 12:30:36,361 iteration 1797 : loss : 0.056017, loss_ce: 0.027938
2021-11-30 12:30:37,871 iteration 1798 : loss : 0.088361, loss_ce: 0.030571
2021-11-30 12:30:39,434 iteration 1799 : loss : 0.066936, loss_ce: 0.028810
2021-11-30 12:30:40,847 iteration 1800 : loss : 0.061275, loss_ce: 0.021702
2021-11-30 12:30:42,334 iteration 1801 : loss : 0.080135, loss_ce: 0.029474
2021-11-30 12:30:43,882 iteration 1802 : loss : 0.058405, loss_ce: 0.024085
 26%|███████▋                     | 106/400 [48:46<2:17:10, 28.00s/it]2021-11-30 12:30:45,493 iteration 1803 : loss : 0.069308, loss_ce: 0.029881
2021-11-30 12:30:46,906 iteration 1804 : loss : 0.067428, loss_ce: 0.027249
2021-11-30 12:30:48,538 iteration 1805 : loss : 0.087795, loss_ce: 0.033231
2021-11-30 12:30:50,124 iteration 1806 : loss : 0.074544, loss_ce: 0.039646
2021-11-30 12:30:51,590 iteration 1807 : loss : 0.062056, loss_ce: 0.028869
2021-11-30 12:30:53,121 iteration 1808 : loss : 0.055839, loss_ce: 0.026758
2021-11-30 12:30:54,583 iteration 1809 : loss : 0.068389, loss_ce: 0.033266
2021-11-30 12:30:56,100 iteration 1810 : loss : 0.076439, loss_ce: 0.036630
2021-11-30 12:30:57,561 iteration 1811 : loss : 0.073293, loss_ce: 0.034611
2021-11-30 12:30:59,025 iteration 1812 : loss : 0.064610, loss_ce: 0.028572
2021-11-30 12:31:00,527 iteration 1813 : loss : 0.066268, loss_ce: 0.034923
2021-11-30 12:31:01,991 iteration 1814 : loss : 0.075340, loss_ce: 0.030107
2021-11-30 12:31:03,577 iteration 1815 : loss : 0.076556, loss_ce: 0.029535
2021-11-30 12:31:05,080 iteration 1816 : loss : 0.052103, loss_ce: 0.025749
2021-11-30 12:31:06,654 iteration 1817 : loss : 0.065570, loss_ce: 0.030208
2021-11-30 12:31:08,086 iteration 1818 : loss : 0.044108, loss_ce: 0.021453
2021-11-30 12:31:09,633 iteration 1819 : loss : 0.087072, loss_ce: 0.037501
 27%|███████▊                     | 107/400 [49:11<2:13:25, 27.32s/it]2021-11-30 12:31:11,048 iteration 1820 : loss : 0.058349, loss_ce: 0.028486
2021-11-30 12:31:12,662 iteration 1821 : loss : 0.066457, loss_ce: 0.026027
2021-11-30 12:31:14,205 iteration 1822 : loss : 0.096607, loss_ce: 0.055486
2021-11-30 12:31:15,781 iteration 1823 : loss : 0.072964, loss_ce: 0.037237
2021-11-30 12:31:17,297 iteration 1824 : loss : 0.104688, loss_ce: 0.033680
2021-11-30 12:31:18,744 iteration 1825 : loss : 0.061938, loss_ce: 0.028476
2021-11-30 12:31:20,334 iteration 1826 : loss : 0.074643, loss_ce: 0.026050
2021-11-30 12:31:21,780 iteration 1827 : loss : 0.045986, loss_ce: 0.022219
2021-11-30 12:31:23,288 iteration 1828 : loss : 0.056372, loss_ce: 0.027388
2021-11-30 12:31:24,743 iteration 1829 : loss : 0.076814, loss_ce: 0.034675
2021-11-30 12:31:26,315 iteration 1830 : loss : 0.094511, loss_ce: 0.033796
2021-11-30 12:31:27,817 iteration 1831 : loss : 0.052269, loss_ce: 0.025698
2021-11-30 12:31:29,284 iteration 1832 : loss : 0.053215, loss_ce: 0.024912
2021-11-30 12:31:30,698 iteration 1833 : loss : 0.075335, loss_ce: 0.035061
2021-11-30 12:31:32,173 iteration 1834 : loss : 0.080182, loss_ce: 0.032029
2021-11-30 12:31:33,673 iteration 1835 : loss : 0.061008, loss_ce: 0.025569
2021-11-30 12:31:35,139 iteration 1836 : loss : 0.083847, loss_ce: 0.034088
 27%|███████▊                     | 108/400 [49:37<2:10:19, 26.78s/it]2021-11-30 12:31:36,745 iteration 1837 : loss : 0.064337, loss_ce: 0.027238
2021-11-30 12:31:38,148 iteration 1838 : loss : 0.050336, loss_ce: 0.024682
2021-11-30 12:31:39,597 iteration 1839 : loss : 0.067207, loss_ce: 0.032458
2021-11-30 12:31:41,117 iteration 1840 : loss : 0.057721, loss_ce: 0.028357
2021-11-30 12:31:42,556 iteration 1841 : loss : 0.062834, loss_ce: 0.030117
2021-11-30 12:31:44,026 iteration 1842 : loss : 0.050401, loss_ce: 0.021029
2021-11-30 12:31:45,553 iteration 1843 : loss : 0.056880, loss_ce: 0.031970
2021-11-30 12:31:47,014 iteration 1844 : loss : 0.056525, loss_ce: 0.023122
2021-11-30 12:31:48,486 iteration 1845 : loss : 0.047020, loss_ce: 0.019293
2021-11-30 12:31:50,071 iteration 1846 : loss : 0.066148, loss_ce: 0.033377
2021-11-30 12:31:51,519 iteration 1847 : loss : 0.050736, loss_ce: 0.023477
2021-11-30 12:31:52,998 iteration 1848 : loss : 0.078709, loss_ce: 0.034435
2021-11-30 12:31:54,444 iteration 1849 : loss : 0.073367, loss_ce: 0.035704
2021-11-30 12:31:56,078 iteration 1850 : loss : 0.097359, loss_ce: 0.038996
2021-11-30 12:31:57,556 iteration 1851 : loss : 0.096101, loss_ce: 0.035108
2021-11-30 12:31:59,045 iteration 1852 : loss : 0.061599, loss_ce: 0.032843
2021-11-30 12:32:00,499 iteration 1853 : loss : 0.058734, loss_ce: 0.029043
 27%|███████▉                     | 109/400 [50:02<2:07:48, 26.35s/it]2021-11-30 12:32:02,007 iteration 1854 : loss : 0.051579, loss_ce: 0.025813
2021-11-30 12:32:03,557 iteration 1855 : loss : 0.053630, loss_ce: 0.025492
2021-11-30 12:32:05,124 iteration 1856 : loss : 0.067993, loss_ce: 0.028105
2021-11-30 12:32:06,689 iteration 1857 : loss : 0.059135, loss_ce: 0.025255
2021-11-30 12:32:08,203 iteration 1858 : loss : 0.065732, loss_ce: 0.030996
2021-11-30 12:32:09,713 iteration 1859 : loss : 0.063714, loss_ce: 0.030003
2021-11-30 12:32:11,149 iteration 1860 : loss : 0.056876, loss_ce: 0.025024
2021-11-30 12:32:12,710 iteration 1861 : loss : 0.051011, loss_ce: 0.024419
2021-11-30 12:32:14,144 iteration 1862 : loss : 0.055596, loss_ce: 0.030396
2021-11-30 12:32:15,648 iteration 1863 : loss : 0.075431, loss_ce: 0.039354
2021-11-30 12:32:17,213 iteration 1864 : loss : 0.086630, loss_ce: 0.027594
2021-11-30 12:32:18,625 iteration 1865 : loss : 0.057622, loss_ce: 0.028475
2021-11-30 12:32:20,201 iteration 1866 : loss : 0.041598, loss_ce: 0.023236
2021-11-30 12:32:21,695 iteration 1867 : loss : 0.044606, loss_ce: 0.024355
2021-11-30 12:32:23,123 iteration 1868 : loss : 0.067387, loss_ce: 0.028100
2021-11-30 12:32:24,556 iteration 1869 : loss : 0.081332, loss_ce: 0.030921
2021-11-30 12:32:24,556 Training Data Eval:
2021-11-30 12:32:31,965   Average segmentation loss on training set: 0.0472
2021-11-30 12:32:31,966 Validation Data Eval:
2021-11-30 12:32:34,542   Average segmentation loss on validation set: 0.0983
2021-11-30 12:32:36,746 Found new lowest validation loss at iteration 1869! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/UNET_RUNMC/best_val_loss.pth
2021-11-30 12:32:38,276 iteration 1870 : loss : 0.064859, loss_ce: 0.029274
 28%|███████▉                     | 110/400 [50:40<2:23:56, 29.78s/it]2021-11-30 12:32:39,822 iteration 1871 : loss : 0.050845, loss_ce: 0.023464
2021-11-30 12:32:41,374 iteration 1872 : loss : 0.113132, loss_ce: 0.035761
2021-11-30 12:32:42,776 iteration 1873 : loss : 0.072792, loss_ce: 0.032679
2021-11-30 12:32:44,300 iteration 1874 : loss : 0.080720, loss_ce: 0.043810
2021-11-30 12:32:45,658 iteration 1875 : loss : 0.068287, loss_ce: 0.030441
2021-11-30 12:32:47,196 iteration 1876 : loss : 0.069572, loss_ce: 0.025848
2021-11-30 12:32:48,704 iteration 1877 : loss : 0.075172, loss_ce: 0.034684
2021-11-30 12:32:50,126 iteration 1878 : loss : 0.053676, loss_ce: 0.026971
2021-11-30 12:32:51,530 iteration 1879 : loss : 0.053764, loss_ce: 0.023953
2021-11-30 12:32:52,939 iteration 1880 : loss : 0.066014, loss_ce: 0.034652
2021-11-30 12:32:54,394 iteration 1881 : loss : 0.086950, loss_ce: 0.032926
2021-11-30 12:32:55,813 iteration 1882 : loss : 0.057308, loss_ce: 0.026579
2021-11-30 12:32:57,227 iteration 1883 : loss : 0.074005, loss_ce: 0.028265
2021-11-30 12:32:58,783 iteration 1884 : loss : 0.081471, loss_ce: 0.037212
2021-11-30 12:33:00,185 iteration 1885 : loss : 0.051971, loss_ce: 0.026415
2021-11-30 12:33:01,776 iteration 1886 : loss : 0.064330, loss_ce: 0.032520
2021-11-30 12:33:03,360 iteration 1887 : loss : 0.090866, loss_ce: 0.040433
 28%|████████                     | 111/400 [51:05<2:16:39, 28.37s/it]2021-11-30 12:33:04,879 iteration 1888 : loss : 0.063865, loss_ce: 0.031837
2021-11-30 12:33:06,431 iteration 1889 : loss : 0.047264, loss_ce: 0.023381
2021-11-30 12:33:07,961 iteration 1890 : loss : 0.061834, loss_ce: 0.025711
2021-11-30 12:33:09,500 iteration 1891 : loss : 0.079831, loss_ce: 0.031932
2021-11-30 12:33:10,997 iteration 1892 : loss : 0.060518, loss_ce: 0.028257
2021-11-30 12:33:12,521 iteration 1893 : loss : 0.074542, loss_ce: 0.044502
2021-11-30 12:33:14,004 iteration 1894 : loss : 0.067273, loss_ce: 0.032379
2021-11-30 12:33:15,479 iteration 1895 : loss : 0.068596, loss_ce: 0.026089
2021-11-30 12:33:17,106 iteration 1896 : loss : 0.090108, loss_ce: 0.048553
2021-11-30 12:33:18,592 iteration 1897 : loss : 0.073074, loss_ce: 0.028508
2021-11-30 12:33:20,177 iteration 1898 : loss : 0.093515, loss_ce: 0.038796
2021-11-30 12:33:21,660 iteration 1899 : loss : 0.061709, loss_ce: 0.033849
2021-11-30 12:33:23,078 iteration 1900 : loss : 0.064130, loss_ce: 0.030322
2021-11-30 12:33:24,565 iteration 1901 : loss : 0.053955, loss_ce: 0.023635
2021-11-30 12:33:26,063 iteration 1902 : loss : 0.054950, loss_ce: 0.028893
2021-11-30 12:33:27,613 iteration 1903 : loss : 0.087659, loss_ce: 0.029539
2021-11-30 12:33:29,219 iteration 1904 : loss : 0.064828, loss_ce: 0.028163
 28%|████████                     | 112/400 [51:31<2:12:33, 27.62s/it]2021-11-30 12:33:30,694 iteration 1905 : loss : 0.045462, loss_ce: 0.024025
2021-11-30 12:33:32,249 iteration 1906 : loss : 0.058690, loss_ce: 0.025496
2021-11-30 12:33:33,873 iteration 1907 : loss : 0.063246, loss_ce: 0.027714
2021-11-30 12:33:35,401 iteration 1908 : loss : 0.056645, loss_ce: 0.028466
2021-11-30 12:33:36,961 iteration 1909 : loss : 0.068177, loss_ce: 0.036600
2021-11-30 12:33:38,517 iteration 1910 : loss : 0.054506, loss_ce: 0.026500
2021-11-30 12:33:40,157 iteration 1911 : loss : 0.056320, loss_ce: 0.024520
2021-11-30 12:33:41,673 iteration 1912 : loss : 0.070453, loss_ce: 0.023372
2021-11-30 12:33:43,151 iteration 1913 : loss : 0.049043, loss_ce: 0.021782
2021-11-30 12:33:44,586 iteration 1914 : loss : 0.052504, loss_ce: 0.026239
2021-11-30 12:33:46,080 iteration 1915 : loss : 0.073365, loss_ce: 0.029038
2021-11-30 12:33:47,575 iteration 1916 : loss : 0.062565, loss_ce: 0.028721
2021-11-30 12:33:49,101 iteration 1917 : loss : 0.070028, loss_ce: 0.029302
2021-11-30 12:33:50,551 iteration 1918 : loss : 0.083532, loss_ce: 0.036372
2021-11-30 12:33:52,018 iteration 1919 : loss : 0.056140, loss_ce: 0.028427
2021-11-30 12:33:53,499 iteration 1920 : loss : 0.053003, loss_ce: 0.024812
2021-11-30 12:33:55,021 iteration 1921 : loss : 0.055112, loss_ce: 0.023369
 28%|████████▏                    | 113/400 [51:57<2:09:28, 27.07s/it]2021-11-30 12:33:56,493 iteration 1922 : loss : 0.055481, loss_ce: 0.024855
2021-11-30 12:33:57,902 iteration 1923 : loss : 0.044132, loss_ce: 0.023204
2021-11-30 12:33:59,422 iteration 1924 : loss : 0.095025, loss_ce: 0.051663
2021-11-30 12:34:00,853 iteration 1925 : loss : 0.060998, loss_ce: 0.029222
2021-11-30 12:34:02,293 iteration 1926 : loss : 0.053214, loss_ce: 0.026486
2021-11-30 12:34:03,815 iteration 1927 : loss : 0.092527, loss_ce: 0.041853
2021-11-30 12:34:05,349 iteration 1928 : loss : 0.044493, loss_ce: 0.021071
2021-11-30 12:34:06,802 iteration 1929 : loss : 0.053502, loss_ce: 0.025075
2021-11-30 12:34:08,296 iteration 1930 : loss : 0.073665, loss_ce: 0.026992
2021-11-30 12:34:09,769 iteration 1931 : loss : 0.068555, loss_ce: 0.025720
2021-11-30 12:34:11,257 iteration 1932 : loss : 0.054489, loss_ce: 0.022144
2021-11-30 12:34:12,737 iteration 1933 : loss : 0.069927, loss_ce: 0.030831
2021-11-30 12:34:14,105 iteration 1934 : loss : 0.046694, loss_ce: 0.024958
2021-11-30 12:34:15,572 iteration 1935 : loss : 0.060586, loss_ce: 0.032537
2021-11-30 12:34:17,122 iteration 1936 : loss : 0.043668, loss_ce: 0.021248
2021-11-30 12:34:18,631 iteration 1937 : loss : 0.050424, loss_ce: 0.025151
2021-11-30 12:34:20,137 iteration 1938 : loss : 0.062549, loss_ce: 0.025146
 28%|████████▎                    | 114/400 [52:22<2:06:15, 26.49s/it]2021-11-30 12:34:21,586 iteration 1939 : loss : 0.047740, loss_ce: 0.023708
2021-11-30 12:34:23,074 iteration 1940 : loss : 0.050837, loss_ce: 0.023571
2021-11-30 12:34:24,621 iteration 1941 : loss : 0.060199, loss_ce: 0.027959
2021-11-30 12:34:26,106 iteration 1942 : loss : 0.078021, loss_ce: 0.027583
2021-11-30 12:34:27,512 iteration 1943 : loss : 0.066419, loss_ce: 0.027947
2021-11-30 12:34:28,979 iteration 1944 : loss : 0.051681, loss_ce: 0.024662
2021-11-30 12:34:30,452 iteration 1945 : loss : 0.057129, loss_ce: 0.033615
2021-11-30 12:34:31,921 iteration 1946 : loss : 0.072059, loss_ce: 0.029252
2021-11-30 12:34:33,373 iteration 1947 : loss : 0.074774, loss_ce: 0.031467
2021-11-30 12:34:34,839 iteration 1948 : loss : 0.062209, loss_ce: 0.030802
2021-11-30 12:34:36,233 iteration 1949 : loss : 0.060356, loss_ce: 0.032577
2021-11-30 12:34:37,775 iteration 1950 : loss : 0.060860, loss_ce: 0.026760
2021-11-30 12:34:39,386 iteration 1951 : loss : 0.070958, loss_ce: 0.030137
2021-11-30 12:34:40,797 iteration 1952 : loss : 0.075685, loss_ce: 0.026208
2021-11-30 12:34:42,306 iteration 1953 : loss : 0.081134, loss_ce: 0.045721
2021-11-30 12:34:43,873 iteration 1954 : loss : 0.058113, loss_ce: 0.025023
2021-11-30 12:34:43,874 Training Data Eval:
2021-11-30 12:34:51,262   Average segmentation loss on training set: 0.0358
2021-11-30 12:34:51,262 Validation Data Eval:
2021-11-30 12:34:53,843   Average segmentation loss on validation set: 0.0950
2021-11-30 12:34:55,798 Found new lowest validation loss at iteration 1954! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/UNET_RUNMC/best_val_loss.pth
2021-11-30 12:34:57,246 iteration 1955 : loss : 0.039685, loss_ce: 0.021805
 29%|████████▎                    | 115/400 [52:59<2:20:57, 29.68s/it]2021-11-30 12:34:58,807 iteration 1956 : loss : 0.061803, loss_ce: 0.029030
2021-11-30 12:35:00,191 iteration 1957 : loss : 0.051725, loss_ce: 0.026613
2021-11-30 12:35:01,606 iteration 1958 : loss : 0.055768, loss_ce: 0.029416
2021-11-30 12:35:03,136 iteration 1959 : loss : 0.098433, loss_ce: 0.027312
2021-11-30 12:35:04,553 iteration 1960 : loss : 0.073096, loss_ce: 0.029003
2021-11-30 12:35:06,016 iteration 1961 : loss : 0.060212, loss_ce: 0.028544
2021-11-30 12:35:07,439 iteration 1962 : loss : 0.045288, loss_ce: 0.022529
2021-11-30 12:35:08,905 iteration 1963 : loss : 0.057763, loss_ce: 0.027066
2021-11-30 12:35:10,337 iteration 1964 : loss : 0.066063, loss_ce: 0.023275
2021-11-30 12:35:11,764 iteration 1965 : loss : 0.057809, loss_ce: 0.030437
2021-11-30 12:35:13,215 iteration 1966 : loss : 0.061994, loss_ce: 0.032683
2021-11-30 12:35:14,652 iteration 1967 : loss : 0.058373, loss_ce: 0.022632
2021-11-30 12:35:16,082 iteration 1968 : loss : 0.065337, loss_ce: 0.034645
2021-11-30 12:35:17,617 iteration 1969 : loss : 0.066104, loss_ce: 0.028552
2021-11-30 12:35:19,254 iteration 1970 : loss : 0.071025, loss_ce: 0.029691
2021-11-30 12:35:20,631 iteration 1971 : loss : 0.038519, loss_ce: 0.018567
2021-11-30 12:35:22,244 iteration 1972 : loss : 0.073250, loss_ce: 0.032434
 29%|████████▍                    | 116/400 [53:24<2:13:49, 28.27s/it]2021-11-30 12:35:23,713 iteration 1973 : loss : 0.046236, loss_ce: 0.023225
2021-11-30 12:35:25,132 iteration 1974 : loss : 0.056108, loss_ce: 0.027342
2021-11-30 12:35:26,718 iteration 1975 : loss : 0.059973, loss_ce: 0.027731
2021-11-30 12:35:28,107 iteration 1976 : loss : 0.053355, loss_ce: 0.026758
2021-11-30 12:35:29,664 iteration 1977 : loss : 0.076978, loss_ce: 0.035713
2021-11-30 12:35:31,095 iteration 1978 : loss : 0.052542, loss_ce: 0.025522
2021-11-30 12:35:32,547 iteration 1979 : loss : 0.053327, loss_ce: 0.030614
2021-11-30 12:35:34,085 iteration 1980 : loss : 0.055254, loss_ce: 0.023544
2021-11-30 12:35:35,504 iteration 1981 : loss : 0.057681, loss_ce: 0.027535
2021-11-30 12:35:37,058 iteration 1982 : loss : 0.050959, loss_ce: 0.021474
2021-11-30 12:35:38,649 iteration 1983 : loss : 0.063806, loss_ce: 0.033494
2021-11-30 12:35:40,102 iteration 1984 : loss : 0.061440, loss_ce: 0.022802
2021-11-30 12:35:41,566 iteration 1985 : loss : 0.044884, loss_ce: 0.020411
2021-11-30 12:35:43,003 iteration 1986 : loss : 0.066003, loss_ce: 0.024291
2021-11-30 12:35:44,431 iteration 1987 : loss : 0.033258, loss_ce: 0.017853
2021-11-30 12:35:46,014 iteration 1988 : loss : 0.058751, loss_ce: 0.028079
2021-11-30 12:35:47,494 iteration 1989 : loss : 0.050426, loss_ce: 0.026125
 29%|████████▍                    | 117/400 [53:49<2:09:03, 27.36s/it]2021-11-30 12:35:49,039 iteration 1990 : loss : 0.075496, loss_ce: 0.036523
2021-11-30 12:35:50,434 iteration 1991 : loss : 0.048354, loss_ce: 0.025120
2021-11-30 12:35:51,957 iteration 1992 : loss : 0.050267, loss_ce: 0.027183
2021-11-30 12:35:53,434 iteration 1993 : loss : 0.077643, loss_ce: 0.025956
2021-11-30 12:35:54,951 iteration 1994 : loss : 0.048103, loss_ce: 0.023223
2021-11-30 12:35:56,428 iteration 1995 : loss : 0.069422, loss_ce: 0.028682
2021-11-30 12:35:57,923 iteration 1996 : loss : 0.055618, loss_ce: 0.027034
2021-11-30 12:35:59,409 iteration 1997 : loss : 0.056124, loss_ce: 0.025381
2021-11-30 12:36:00,933 iteration 1998 : loss : 0.089713, loss_ce: 0.041540
2021-11-30 12:36:02,438 iteration 1999 : loss : 0.065129, loss_ce: 0.031852
2021-11-30 12:36:04,065 iteration 2000 : loss : 0.067619, loss_ce: 0.022871
2021-11-30 12:36:05,551 iteration 2001 : loss : 0.059495, loss_ce: 0.026160
2021-11-30 12:36:06,965 iteration 2002 : loss : 0.066322, loss_ce: 0.028455
2021-11-30 12:36:08,312 iteration 2003 : loss : 0.037320, loss_ce: 0.021197
2021-11-30 12:36:09,766 iteration 2004 : loss : 0.090001, loss_ce: 0.030717
2021-11-30 12:36:11,292 iteration 2005 : loss : 0.044270, loss_ce: 0.022655
2021-11-30 12:36:12,815 iteration 2006 : loss : 0.060690, loss_ce: 0.025542
 30%|████████▌                    | 118/400 [54:15<2:05:42, 26.75s/it]2021-11-30 12:36:14,414 iteration 2007 : loss : 0.054657, loss_ce: 0.025613
2021-11-30 12:36:16,024 iteration 2008 : loss : 0.059301, loss_ce: 0.023664
2021-11-30 12:36:17,497 iteration 2009 : loss : 0.055898, loss_ce: 0.027147
2021-11-30 12:36:18,969 iteration 2010 : loss : 0.059595, loss_ce: 0.027397
2021-11-30 12:36:20,555 iteration 2011 : loss : 0.064932, loss_ce: 0.027045
2021-11-30 12:36:22,046 iteration 2012 : loss : 0.057000, loss_ce: 0.030373
2021-11-30 12:36:23,458 iteration 2013 : loss : 0.049656, loss_ce: 0.024363
2021-11-30 12:36:24,923 iteration 2014 : loss : 0.060743, loss_ce: 0.026612
2021-11-30 12:36:26,461 iteration 2015 : loss : 0.051858, loss_ce: 0.025398
2021-11-30 12:36:27,922 iteration 2016 : loss : 0.058898, loss_ce: 0.024094
2021-11-30 12:36:29,454 iteration 2017 : loss : 0.045946, loss_ce: 0.019514
2021-11-30 12:36:31,065 iteration 2018 : loss : 0.095163, loss_ce: 0.046027
2021-11-30 12:36:32,532 iteration 2019 : loss : 0.061377, loss_ce: 0.026076
2021-11-30 12:36:33,990 iteration 2020 : loss : 0.043407, loss_ce: 0.019763
2021-11-30 12:36:35,515 iteration 2021 : loss : 0.059344, loss_ce: 0.024715
2021-11-30 12:36:37,035 iteration 2022 : loss : 0.048732, loss_ce: 0.025140
2021-11-30 12:36:38,601 iteration 2023 : loss : 0.065334, loss_ce: 0.028321
 30%|████████▋                    | 119/400 [54:40<2:03:54, 26.46s/it]2021-11-30 12:36:40,073 iteration 2024 : loss : 0.065495, loss_ce: 0.027814
2021-11-30 12:36:41,564 iteration 2025 : loss : 0.044485, loss_ce: 0.023522
2021-11-30 12:36:43,124 iteration 2026 : loss : 0.065220, loss_ce: 0.028968
2021-11-30 12:36:44,588 iteration 2027 : loss : 0.046030, loss_ce: 0.017881
2021-11-30 12:36:46,015 iteration 2028 : loss : 0.049109, loss_ce: 0.028163
2021-11-30 12:36:47,475 iteration 2029 : loss : 0.051113, loss_ce: 0.025238
2021-11-30 12:36:48,937 iteration 2030 : loss : 0.063082, loss_ce: 0.028180
2021-11-30 12:36:50,436 iteration 2031 : loss : 0.045521, loss_ce: 0.021635
2021-11-30 12:36:52,031 iteration 2032 : loss : 0.061512, loss_ce: 0.028792
2021-11-30 12:36:53,640 iteration 2033 : loss : 0.062587, loss_ce: 0.030362
2021-11-30 12:36:55,070 iteration 2034 : loss : 0.049541, loss_ce: 0.024193
2021-11-30 12:36:56,556 iteration 2035 : loss : 0.055852, loss_ce: 0.025195
2021-11-30 12:36:58,089 iteration 2036 : loss : 0.057285, loss_ce: 0.032050
2021-11-30 12:36:59,571 iteration 2037 : loss : 0.071451, loss_ce: 0.024960
2021-11-30 12:37:01,129 iteration 2038 : loss : 0.063411, loss_ce: 0.029766
2021-11-30 12:37:02,734 iteration 2039 : loss : 0.066525, loss_ce: 0.027688
2021-11-30 12:37:02,734 Training Data Eval:
2021-11-30 12:37:10,084   Average segmentation loss on training set: 0.0367
2021-11-30 12:37:10,085 Validation Data Eval:
2021-11-30 12:37:12,640   Average segmentation loss on validation set: 0.1027
2021-11-30 12:37:14,239 iteration 2040 : loss : 0.068816, loss_ce: 0.029776
 30%|████████▋                    | 120/400 [55:16<2:16:21, 29.22s/it]2021-11-30 12:37:15,843 iteration 2041 : loss : 0.077853, loss_ce: 0.026578
2021-11-30 12:37:17,317 iteration 2042 : loss : 0.064957, loss_ce: 0.027717
2021-11-30 12:37:18,786 iteration 2043 : loss : 0.044019, loss_ce: 0.021098
2021-11-30 12:37:20,256 iteration 2044 : loss : 0.054277, loss_ce: 0.025483
2021-11-30 12:37:21,702 iteration 2045 : loss : 0.048562, loss_ce: 0.020544
2021-11-30 12:37:23,186 iteration 2046 : loss : 0.081763, loss_ce: 0.040495
2021-11-30 12:37:24,629 iteration 2047 : loss : 0.054506, loss_ce: 0.026500
2021-11-30 12:37:26,126 iteration 2048 : loss : 0.047247, loss_ce: 0.022929
2021-11-30 12:37:27,590 iteration 2049 : loss : 0.068349, loss_ce: 0.024245
2021-11-30 12:37:29,101 iteration 2050 : loss : 0.067032, loss_ce: 0.034380
2021-11-30 12:37:30,612 iteration 2051 : loss : 0.062553, loss_ce: 0.028775
2021-11-30 12:37:32,042 iteration 2052 : loss : 0.072575, loss_ce: 0.031790
2021-11-30 12:37:33,557 iteration 2053 : loss : 0.065319, loss_ce: 0.024477
2021-11-30 12:37:35,039 iteration 2054 : loss : 0.046497, loss_ce: 0.022218
2021-11-30 12:37:36,499 iteration 2055 : loss : 0.073937, loss_ce: 0.026806
2021-11-30 12:37:38,035 iteration 2056 : loss : 0.054642, loss_ce: 0.026422
2021-11-30 12:37:39,461 iteration 2057 : loss : 0.056106, loss_ce: 0.028903
 30%|████████▊                    | 121/400 [55:41<2:10:16, 28.02s/it]2021-11-30 12:37:41,013 iteration 2058 : loss : 0.050789, loss_ce: 0.027677
2021-11-30 12:37:42,561 iteration 2059 : loss : 0.083093, loss_ce: 0.032758
2021-11-30 12:37:43,956 iteration 2060 : loss : 0.055201, loss_ce: 0.022692
2021-11-30 12:37:45,469 iteration 2061 : loss : 0.081367, loss_ce: 0.035558
2021-11-30 12:37:46,934 iteration 2062 : loss : 0.073682, loss_ce: 0.032173
2021-11-30 12:37:48,403 iteration 2063 : loss : 0.092358, loss_ce: 0.025618
2021-11-30 12:37:49,816 iteration 2064 : loss : 0.048526, loss_ce: 0.021823
2021-11-30 12:37:51,269 iteration 2065 : loss : 0.062157, loss_ce: 0.026048
2021-11-30 12:37:52,769 iteration 2066 : loss : 0.073536, loss_ce: 0.033087
2021-11-30 12:37:54,290 iteration 2067 : loss : 0.045416, loss_ce: 0.023475
2021-11-30 12:37:55,833 iteration 2068 : loss : 0.060635, loss_ce: 0.030044
2021-11-30 12:37:57,296 iteration 2069 : loss : 0.051020, loss_ce: 0.022859
2021-11-30 12:37:58,844 iteration 2070 : loss : 0.060104, loss_ce: 0.028699
2021-11-30 12:38:00,395 iteration 2071 : loss : 0.079443, loss_ce: 0.032760
2021-11-30 12:38:01,889 iteration 2072 : loss : 0.063818, loss_ce: 0.028963
2021-11-30 12:38:03,421 iteration 2073 : loss : 0.087664, loss_ce: 0.032913
2021-11-30 12:38:04,948 iteration 2074 : loss : 0.061666, loss_ce: 0.034192
 30%|████████▊                    | 122/400 [56:07<2:06:18, 27.26s/it]2021-11-30 12:38:06,462 iteration 2075 : loss : 0.048423, loss_ce: 0.021807
2021-11-30 12:38:07,964 iteration 2076 : loss : 0.048729, loss_ce: 0.019253
2021-11-30 12:38:09,484 iteration 2077 : loss : 0.068173, loss_ce: 0.038541
2021-11-30 12:38:11,105 iteration 2078 : loss : 0.080030, loss_ce: 0.032676
2021-11-30 12:38:12,563 iteration 2079 : loss : 0.062929, loss_ce: 0.025722
2021-11-30 12:38:14,098 iteration 2080 : loss : 0.064568, loss_ce: 0.025428
2021-11-30 12:38:15,487 iteration 2081 : loss : 0.053750, loss_ce: 0.025305
2021-11-30 12:38:16,858 iteration 2082 : loss : 0.036108, loss_ce: 0.018712
2021-11-30 12:38:18,335 iteration 2083 : loss : 0.070375, loss_ce: 0.036834
2021-11-30 12:38:19,815 iteration 2084 : loss : 0.099483, loss_ce: 0.049693
2021-11-30 12:38:21,268 iteration 2085 : loss : 0.055267, loss_ce: 0.019990
2021-11-30 12:38:22,767 iteration 2086 : loss : 0.066514, loss_ce: 0.030087
2021-11-30 12:38:24,197 iteration 2087 : loss : 0.048819, loss_ce: 0.023407
2021-11-30 12:38:25,750 iteration 2088 : loss : 0.053753, loss_ce: 0.026905
2021-11-30 12:38:27,280 iteration 2089 : loss : 0.050776, loss_ce: 0.026457
2021-11-30 12:38:28,693 iteration 2090 : loss : 0.047840, loss_ce: 0.022398
2021-11-30 12:38:30,232 iteration 2091 : loss : 0.059250, loss_ce: 0.028533
 31%|████████▉                    | 123/400 [56:32<2:03:06, 26.67s/it]2021-11-30 12:38:31,702 iteration 2092 : loss : 0.048451, loss_ce: 0.023134
2021-11-30 12:38:33,169 iteration 2093 : loss : 0.069819, loss_ce: 0.029748
2021-11-30 12:38:34,647 iteration 2094 : loss : 0.056044, loss_ce: 0.020735
2021-11-30 12:38:36,061 iteration 2095 : loss : 0.033760, loss_ce: 0.018039
2021-11-30 12:38:37,637 iteration 2096 : loss : 0.049586, loss_ce: 0.026656
2021-11-30 12:38:39,145 iteration 2097 : loss : 0.093732, loss_ce: 0.037007
2021-11-30 12:38:40,655 iteration 2098 : loss : 0.064557, loss_ce: 0.028412
2021-11-30 12:38:42,164 iteration 2099 : loss : 0.047021, loss_ce: 0.019811
2021-11-30 12:38:43,688 iteration 2100 : loss : 0.066413, loss_ce: 0.036645
2021-11-30 12:38:45,192 iteration 2101 : loss : 0.046434, loss_ce: 0.020874
2021-11-30 12:38:46,731 iteration 2102 : loss : 0.052950, loss_ce: 0.028343
2021-11-30 12:38:48,118 iteration 2103 : loss : 0.045948, loss_ce: 0.023659
2021-11-30 12:38:49,555 iteration 2104 : loss : 0.075564, loss_ce: 0.025715
2021-11-30 12:38:51,152 iteration 2105 : loss : 0.077528, loss_ce: 0.030612
2021-11-30 12:38:52,590 iteration 2106 : loss : 0.037343, loss_ce: 0.020839
2021-11-30 12:38:54,168 iteration 2107 : loss : 0.046350, loss_ce: 0.021169
2021-11-30 12:38:55,675 iteration 2108 : loss : 0.078431, loss_ce: 0.034924
 31%|████████▉                    | 124/400 [56:58<2:00:58, 26.30s/it]2021-11-30 12:38:57,211 iteration 2109 : loss : 0.066727, loss_ce: 0.032488
2021-11-30 12:38:58,730 iteration 2110 : loss : 0.055498, loss_ce: 0.022935
2021-11-30 12:39:00,291 iteration 2111 : loss : 0.066119, loss_ce: 0.020911
2021-11-30 12:39:01,787 iteration 2112 : loss : 0.081661, loss_ce: 0.041683
2021-11-30 12:39:03,242 iteration 2113 : loss : 0.057862, loss_ce: 0.031112
2021-11-30 12:39:04,773 iteration 2114 : loss : 0.060522, loss_ce: 0.034267
2021-11-30 12:39:06,306 iteration 2115 : loss : 0.058908, loss_ce: 0.022320
2021-11-30 12:39:07,817 iteration 2116 : loss : 0.050498, loss_ce: 0.022035
2021-11-30 12:39:09,317 iteration 2117 : loss : 0.050469, loss_ce: 0.022269
2021-11-30 12:39:10,854 iteration 2118 : loss : 0.069882, loss_ce: 0.023856
2021-11-30 12:39:12,395 iteration 2119 : loss : 0.073394, loss_ce: 0.029407
2021-11-30 12:39:13,899 iteration 2120 : loss : 0.086155, loss_ce: 0.028689
2021-11-30 12:39:15,285 iteration 2121 : loss : 0.045579, loss_ce: 0.020417
2021-11-30 12:39:16,741 iteration 2122 : loss : 0.054227, loss_ce: 0.022908
2021-11-30 12:39:18,307 iteration 2123 : loss : 0.105652, loss_ce: 0.048210
2021-11-30 12:39:19,885 iteration 2124 : loss : 0.064527, loss_ce: 0.031619
2021-11-30 12:39:19,885 Training Data Eval:
2021-11-30 12:39:27,244   Average segmentation loss on training set: 0.0358
2021-11-30 12:39:27,245 Validation Data Eval:
2021-11-30 12:39:29,789   Average segmentation loss on validation set: 0.0903
2021-11-30 12:39:31,713 Found new lowest validation loss at iteration 2124! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/UNET_RUNMC/best_val_loss.pth
2021-11-30 12:39:33,219 iteration 2125 : loss : 0.068426, loss_ce: 0.037282
 31%|█████████                    | 125/400 [57:35<2:16:01, 29.68s/it]2021-11-30 12:39:34,802 iteration 2126 : loss : 0.059715, loss_ce: 0.026021
2021-11-30 12:39:36,279 iteration 2127 : loss : 0.083302, loss_ce: 0.043880
2021-11-30 12:39:37,752 iteration 2128 : loss : 0.055654, loss_ce: 0.023072
2021-11-30 12:39:39,182 iteration 2129 : loss : 0.058540, loss_ce: 0.026068
2021-11-30 12:39:40,689 iteration 2130 : loss : 0.085552, loss_ce: 0.030209
2021-11-30 12:39:42,124 iteration 2131 : loss : 0.064957, loss_ce: 0.025023
2021-11-30 12:39:43,587 iteration 2132 : loss : 0.037497, loss_ce: 0.017691
2021-11-30 12:39:45,122 iteration 2133 : loss : 0.071913, loss_ce: 0.029784
2021-11-30 12:39:46,496 iteration 2134 : loss : 0.046022, loss_ce: 0.025676
2021-11-30 12:39:47,959 iteration 2135 : loss : 0.043693, loss_ce: 0.021378
2021-11-30 12:39:49,424 iteration 2136 : loss : 0.049244, loss_ce: 0.026417
2021-11-30 12:39:50,919 iteration 2137 : loss : 0.061878, loss_ce: 0.030870
2021-11-30 12:39:52,376 iteration 2138 : loss : 0.051142, loss_ce: 0.024144
2021-11-30 12:39:53,929 iteration 2139 : loss : 0.063298, loss_ce: 0.033639
2021-11-30 12:39:55,429 iteration 2140 : loss : 0.065125, loss_ce: 0.031194
2021-11-30 12:39:57,011 iteration 2141 : loss : 0.077006, loss_ce: 0.028032
2021-11-30 12:39:58,475 iteration 2142 : loss : 0.056666, loss_ce: 0.023088
 32%|█████████▏                   | 126/400 [58:00<2:09:26, 28.35s/it]2021-11-30 12:39:59,941 iteration 2143 : loss : 0.049444, loss_ce: 0.023800
2021-11-30 12:40:01,451 iteration 2144 : loss : 0.046560, loss_ce: 0.019277
2021-11-30 12:40:02,980 iteration 2145 : loss : 0.069076, loss_ce: 0.026465
2021-11-30 12:40:04,424 iteration 2146 : loss : 0.032740, loss_ce: 0.018405
2021-11-30 12:40:05,921 iteration 2147 : loss : 0.079808, loss_ce: 0.031167
2021-11-30 12:40:07,382 iteration 2148 : loss : 0.047746, loss_ce: 0.023726
2021-11-30 12:40:08,895 iteration 2149 : loss : 0.054891, loss_ce: 0.023910
2021-11-30 12:40:10,349 iteration 2150 : loss : 0.049214, loss_ce: 0.025833
2021-11-30 12:40:11,900 iteration 2151 : loss : 0.061980, loss_ce: 0.028247
2021-11-30 12:40:13,332 iteration 2152 : loss : 0.049627, loss_ce: 0.025231
2021-11-30 12:40:14,771 iteration 2153 : loss : 0.051288, loss_ce: 0.022722
2021-11-30 12:40:16,427 iteration 2154 : loss : 0.064346, loss_ce: 0.026127
2021-11-30 12:40:17,919 iteration 2155 : loss : 0.049278, loss_ce: 0.021242
2021-11-30 12:40:19,350 iteration 2156 : loss : 0.062626, loss_ce: 0.030576
2021-11-30 12:40:21,003 iteration 2157 : loss : 0.101541, loss_ce: 0.035502
2021-11-30 12:40:22,598 iteration 2158 : loss : 0.070000, loss_ce: 0.035444
2021-11-30 12:40:24,112 iteration 2159 : loss : 0.079922, loss_ce: 0.027934
 32%|█████████▏                   | 127/400 [58:26<2:05:15, 27.53s/it]2021-11-30 12:40:25,610 iteration 2160 : loss : 0.061646, loss_ce: 0.026070
2021-11-30 12:40:27,043 iteration 2161 : loss : 0.037393, loss_ce: 0.017811
2021-11-30 12:40:28,519 iteration 2162 : loss : 0.045389, loss_ce: 0.023729
2021-11-30 12:40:29,970 iteration 2163 : loss : 0.047420, loss_ce: 0.023480
2021-11-30 12:40:31,506 iteration 2164 : loss : 0.059953, loss_ce: 0.033056
2021-11-30 12:40:33,020 iteration 2165 : loss : 0.069261, loss_ce: 0.029811
2021-11-30 12:40:34,452 iteration 2166 : loss : 0.049087, loss_ce: 0.024700
2021-11-30 12:40:35,856 iteration 2167 : loss : 0.050779, loss_ce: 0.022144
2021-11-30 12:40:37,319 iteration 2168 : loss : 0.036957, loss_ce: 0.019098
2021-11-30 12:40:38,840 iteration 2169 : loss : 0.066569, loss_ce: 0.029622
2021-11-30 12:40:40,215 iteration 2170 : loss : 0.053329, loss_ce: 0.021289
2021-11-30 12:40:41,686 iteration 2171 : loss : 0.063710, loss_ce: 0.026300
2021-11-30 12:40:43,130 iteration 2172 : loss : 0.072778, loss_ce: 0.029101
2021-11-30 12:40:44,579 iteration 2173 : loss : 0.054552, loss_ce: 0.019932
2021-11-30 12:40:46,067 iteration 2174 : loss : 0.060080, loss_ce: 0.026513
2021-11-30 12:40:47,642 iteration 2175 : loss : 0.040732, loss_ce: 0.020744
2021-11-30 12:40:49,010 iteration 2176 : loss : 0.039679, loss_ce: 0.022128
 32%|█████████▎                   | 128/400 [58:51<2:01:14, 26.75s/it]2021-11-30 12:40:50,482 iteration 2177 : loss : 0.073546, loss_ce: 0.043274
2021-11-30 12:40:51,981 iteration 2178 : loss : 0.044936, loss_ce: 0.019989
2021-11-30 12:40:53,545 iteration 2179 : loss : 0.059924, loss_ce: 0.022305
2021-11-30 12:40:55,070 iteration 2180 : loss : 0.042997, loss_ce: 0.021607
2021-11-30 12:40:56,477 iteration 2181 : loss : 0.043905, loss_ce: 0.019275
2021-11-30 12:40:57,956 iteration 2182 : loss : 0.079276, loss_ce: 0.033052
2021-11-30 12:40:59,447 iteration 2183 : loss : 0.065756, loss_ce: 0.023506
2021-11-30 12:41:00,865 iteration 2184 : loss : 0.077302, loss_ce: 0.039757
2021-11-30 12:41:02,288 iteration 2185 : loss : 0.045396, loss_ce: 0.020634
2021-11-30 12:41:03,863 iteration 2186 : loss : 0.060715, loss_ce: 0.026264
2021-11-30 12:41:05,451 iteration 2187 : loss : 0.061732, loss_ce: 0.023729
2021-11-30 12:41:07,114 iteration 2188 : loss : 0.084326, loss_ce: 0.046231
2021-11-30 12:41:08,754 iteration 2189 : loss : 0.066624, loss_ce: 0.026887
2021-11-30 12:41:10,234 iteration 2190 : loss : 0.076371, loss_ce: 0.038570
2021-11-30 12:41:11,738 iteration 2191 : loss : 0.035465, loss_ce: 0.017031
2021-11-30 12:41:13,213 iteration 2192 : loss : 0.049004, loss_ce: 0.021521
2021-11-30 12:41:14,733 iteration 2193 : loss : 0.046677, loss_ce: 0.021070
 32%|█████████▎                   | 129/400 [59:17<1:59:24, 26.44s/it]2021-11-30 12:41:16,318 iteration 2194 : loss : 0.051784, loss_ce: 0.023386
2021-11-30 12:41:17,833 iteration 2195 : loss : 0.061575, loss_ce: 0.032470
2021-11-30 12:41:19,370 iteration 2196 : loss : 0.053680, loss_ce: 0.026117
2021-11-30 12:41:20,960 iteration 2197 : loss : 0.050509, loss_ce: 0.022406
2021-11-30 12:41:22,455 iteration 2198 : loss : 0.054574, loss_ce: 0.022729
2021-11-30 12:41:23,943 iteration 2199 : loss : 0.050010, loss_ce: 0.024226
2021-11-30 12:41:25,492 iteration 2200 : loss : 0.092323, loss_ce: 0.023941
2021-11-30 12:41:26,911 iteration 2201 : loss : 0.042386, loss_ce: 0.020317
2021-11-30 12:41:28,501 iteration 2202 : loss : 0.060826, loss_ce: 0.029357
2021-11-30 12:41:30,024 iteration 2203 : loss : 0.083697, loss_ce: 0.030717
2021-11-30 12:41:31,444 iteration 2204 : loss : 0.036068, loss_ce: 0.016631
2021-11-30 12:41:32,940 iteration 2205 : loss : 0.056251, loss_ce: 0.031664
2021-11-30 12:41:34,535 iteration 2206 : loss : 0.060489, loss_ce: 0.025873
2021-11-30 12:41:36,063 iteration 2207 : loss : 0.054679, loss_ce: 0.021085
2021-11-30 12:41:37,616 iteration 2208 : loss : 0.071105, loss_ce: 0.024611
2021-11-30 12:41:39,112 iteration 2209 : loss : 0.079569, loss_ce: 0.029594
2021-11-30 12:41:39,112 Training Data Eval:
2021-11-30 12:41:46,478   Average segmentation loss on training set: 0.0326
2021-11-30 12:41:46,479 Validation Data Eval:
2021-11-30 12:41:49,051   Average segmentation loss on validation set: 0.0955
2021-11-30 12:41:50,582 iteration 2210 : loss : 0.060243, loss_ce: 0.030421
 32%|█████████▍                   | 130/400 [59:52<2:11:40, 29.26s/it]2021-11-30 12:41:52,136 iteration 2211 : loss : 0.053968, loss_ce: 0.020527
2021-11-30 12:41:53,575 iteration 2212 : loss : 0.052351, loss_ce: 0.024675
2021-11-30 12:41:55,076 iteration 2213 : loss : 0.064141, loss_ce: 0.021448
2021-11-30 12:41:56,478 iteration 2214 : loss : 0.039914, loss_ce: 0.018398
2021-11-30 12:41:57,898 iteration 2215 : loss : 0.045092, loss_ce: 0.019817
2021-11-30 12:41:59,383 iteration 2216 : loss : 0.072858, loss_ce: 0.036178
2021-11-30 12:42:00,824 iteration 2217 : loss : 0.061662, loss_ce: 0.032956
2021-11-30 12:42:02,290 iteration 2218 : loss : 0.036034, loss_ce: 0.016864
2021-11-30 12:42:03,821 iteration 2219 : loss : 0.059411, loss_ce: 0.028874
2021-11-30 12:42:05,397 iteration 2220 : loss : 0.055150, loss_ce: 0.029118
2021-11-30 12:42:06,974 iteration 2221 : loss : 0.070845, loss_ce: 0.026271
2021-11-30 12:42:08,380 iteration 2222 : loss : 0.049965, loss_ce: 0.022631
2021-11-30 12:42:09,808 iteration 2223 : loss : 0.053746, loss_ce: 0.022325
2021-11-30 12:42:11,369 iteration 2224 : loss : 0.056049, loss_ce: 0.025947
2021-11-30 12:42:12,880 iteration 2225 : loss : 0.052139, loss_ce: 0.022573
2021-11-30 12:42:14,441 iteration 2226 : loss : 0.056698, loss_ce: 0.029123
2021-11-30 12:42:15,981 iteration 2227 : loss : 0.063219, loss_ce: 0.023436
 33%|████████▊                  | 131/400 [1:00:18<2:05:59, 28.10s/it]2021-11-30 12:42:17,602 iteration 2228 : loss : 0.075934, loss_ce: 0.035371
2021-11-30 12:42:19,095 iteration 2229 : loss : 0.043729, loss_ce: 0.021579
2021-11-30 12:42:20,714 iteration 2230 : loss : 0.060489, loss_ce: 0.028238
2021-11-30 12:42:22,175 iteration 2231 : loss : 0.087873, loss_ce: 0.054001
2021-11-30 12:42:23,642 iteration 2232 : loss : 0.047025, loss_ce: 0.026139
2021-11-30 12:42:25,343 iteration 2233 : loss : 0.111727, loss_ce: 0.031793
2021-11-30 12:42:26,859 iteration 2234 : loss : 0.054489, loss_ce: 0.020702
2021-11-30 12:42:28,362 iteration 2235 : loss : 0.051959, loss_ce: 0.024524
2021-11-30 12:42:29,834 iteration 2236 : loss : 0.042822, loss_ce: 0.021535
2021-11-30 12:42:31,348 iteration 2237 : loss : 0.061923, loss_ce: 0.029724
2021-11-30 12:42:32,799 iteration 2238 : loss : 0.053886, loss_ce: 0.020940
2021-11-30 12:42:34,282 iteration 2239 : loss : 0.051920, loss_ce: 0.021486
2021-11-30 12:42:35,692 iteration 2240 : loss : 0.062371, loss_ce: 0.022063
2021-11-30 12:42:37,159 iteration 2241 : loss : 0.051200, loss_ce: 0.021140
2021-11-30 12:42:38,695 iteration 2242 : loss : 0.068407, loss_ce: 0.027370
2021-11-30 12:42:40,218 iteration 2243 : loss : 0.064570, loss_ce: 0.030713
2021-11-30 12:42:41,691 iteration 2244 : loss : 0.033158, loss_ce: 0.016474
 33%|████████▉                  | 132/400 [1:00:44<2:02:19, 27.39s/it]2021-11-30 12:42:43,223 iteration 2245 : loss : 0.038567, loss_ce: 0.017780
2021-11-30 12:42:44,618 iteration 2246 : loss : 0.050029, loss_ce: 0.018958
2021-11-30 12:42:46,146 iteration 2247 : loss : 0.057246, loss_ce: 0.031995
2021-11-30 12:42:47,724 iteration 2248 : loss : 0.080553, loss_ce: 0.035549
2021-11-30 12:42:49,208 iteration 2249 : loss : 0.046973, loss_ce: 0.023824
2021-11-30 12:42:50,696 iteration 2250 : loss : 0.042345, loss_ce: 0.018253
2021-11-30 12:42:52,221 iteration 2251 : loss : 0.062542, loss_ce: 0.029031
2021-11-30 12:42:53,700 iteration 2252 : loss : 0.062391, loss_ce: 0.024914
2021-11-30 12:42:55,292 iteration 2253 : loss : 0.092812, loss_ce: 0.039074
2021-11-30 12:42:56,832 iteration 2254 : loss : 0.064454, loss_ce: 0.029088
2021-11-30 12:42:58,319 iteration 2255 : loss : 0.042897, loss_ce: 0.020456
2021-11-30 12:42:59,801 iteration 2256 : loss : 0.042755, loss_ce: 0.022410
2021-11-30 12:43:01,247 iteration 2257 : loss : 0.063843, loss_ce: 0.025991
2021-11-30 12:43:02,687 iteration 2258 : loss : 0.071434, loss_ce: 0.031662
2021-11-30 12:43:04,253 iteration 2259 : loss : 0.044838, loss_ce: 0.023161
2021-11-30 12:43:05,675 iteration 2260 : loss : 0.045197, loss_ce: 0.021735
2021-11-30 12:43:07,186 iteration 2261 : loss : 0.071989, loss_ce: 0.031289
 33%|████████▉                  | 133/400 [1:01:09<1:59:20, 26.82s/it]2021-11-30 12:43:08,669 iteration 2262 : loss : 0.044041, loss_ce: 0.025331
2021-11-30 12:43:10,213 iteration 2263 : loss : 0.086945, loss_ce: 0.034924
2021-11-30 12:43:11,786 iteration 2264 : loss : 0.063652, loss_ce: 0.025247
2021-11-30 12:43:13,298 iteration 2265 : loss : 0.047539, loss_ce: 0.021589
2021-11-30 12:43:14,741 iteration 2266 : loss : 0.056244, loss_ce: 0.025775
2021-11-30 12:43:16,190 iteration 2267 : loss : 0.042197, loss_ce: 0.019047
2021-11-30 12:43:17,685 iteration 2268 : loss : 0.049097, loss_ce: 0.024176
2021-11-30 12:43:19,123 iteration 2269 : loss : 0.044576, loss_ce: 0.026081
2021-11-30 12:43:20,604 iteration 2270 : loss : 0.047893, loss_ce: 0.020479
2021-11-30 12:43:22,068 iteration 2271 : loss : 0.048752, loss_ce: 0.022054
2021-11-30 12:43:23,548 iteration 2272 : loss : 0.055838, loss_ce: 0.030342
2021-11-30 12:43:25,064 iteration 2273 : loss : 0.047200, loss_ce: 0.021615
2021-11-30 12:43:26,579 iteration 2274 : loss : 0.048803, loss_ce: 0.020319
2021-11-30 12:43:28,136 iteration 2275 : loss : 0.075068, loss_ce: 0.028419
2021-11-30 12:43:29,556 iteration 2276 : loss : 0.036725, loss_ce: 0.018970
2021-11-30 12:43:31,036 iteration 2277 : loss : 0.068221, loss_ce: 0.030669
2021-11-30 12:43:32,470 iteration 2278 : loss : 0.041433, loss_ce: 0.018530
 34%|█████████                  | 134/400 [1:01:34<1:56:51, 26.36s/it]2021-11-30 12:43:33,952 iteration 2279 : loss : 0.078915, loss_ce: 0.034239
2021-11-30 12:43:35,458 iteration 2280 : loss : 0.057696, loss_ce: 0.028877
2021-11-30 12:43:36,957 iteration 2281 : loss : 0.085765, loss_ce: 0.028298
2021-11-30 12:43:38,490 iteration 2282 : loss : 0.040370, loss_ce: 0.018775
2021-11-30 12:43:40,073 iteration 2283 : loss : 0.071036, loss_ce: 0.034197
2021-11-30 12:43:41,500 iteration 2284 : loss : 0.068212, loss_ce: 0.033392
2021-11-30 12:43:42,976 iteration 2285 : loss : 0.048671, loss_ce: 0.025569
2021-11-30 12:43:44,448 iteration 2286 : loss : 0.055804, loss_ce: 0.025398
2021-11-30 12:43:45,932 iteration 2287 : loss : 0.043344, loss_ce: 0.022458
2021-11-30 12:43:47,483 iteration 2288 : loss : 0.054802, loss_ce: 0.022372
2021-11-30 12:43:48,933 iteration 2289 : loss : 0.050732, loss_ce: 0.017586
2021-11-30 12:43:50,492 iteration 2290 : loss : 0.057512, loss_ce: 0.033802
2021-11-30 12:43:51,965 iteration 2291 : loss : 0.043611, loss_ce: 0.021366
2021-11-30 12:43:53,427 iteration 2292 : loss : 0.063618, loss_ce: 0.031442
2021-11-30 12:43:54,827 iteration 2293 : loss : 0.061357, loss_ce: 0.038544
2021-11-30 12:43:56,364 iteration 2294 : loss : 0.089577, loss_ce: 0.036292
2021-11-30 12:43:56,364 Training Data Eval:
2021-11-30 12:44:03,767   Average segmentation loss on training set: 0.0444
2021-11-30 12:44:03,767 Validation Data Eval:
2021-11-30 12:44:06,332   Average segmentation loss on validation set: 0.0911
2021-11-30 12:44:07,942 iteration 2295 : loss : 0.062178, loss_ce: 0.029285
 34%|█████████                  | 135/400 [1:02:10<2:08:29, 29.09s/it]2021-11-30 12:44:09,421 iteration 2296 : loss : 0.038739, loss_ce: 0.018946
2021-11-30 12:44:10,913 iteration 2297 : loss : 0.061630, loss_ce: 0.031064
2021-11-30 12:44:12,383 iteration 2298 : loss : 0.066412, loss_ce: 0.035525
2021-11-30 12:44:13,954 iteration 2299 : loss : 0.072353, loss_ce: 0.037179
2021-11-30 12:44:15,407 iteration 2300 : loss : 0.041810, loss_ce: 0.021897
2021-11-30 12:44:16,982 iteration 2301 : loss : 0.044687, loss_ce: 0.017473
2021-11-30 12:44:18,487 iteration 2302 : loss : 0.088246, loss_ce: 0.032582
2021-11-30 12:44:20,003 iteration 2303 : loss : 0.047976, loss_ce: 0.019684
2021-11-30 12:44:21,578 iteration 2304 : loss : 0.061710, loss_ce: 0.024500
2021-11-30 12:44:23,092 iteration 2305 : loss : 0.051241, loss_ce: 0.023859
2021-11-30 12:44:24,690 iteration 2306 : loss : 0.055156, loss_ce: 0.029711
2021-11-30 12:44:26,161 iteration 2307 : loss : 0.080358, loss_ce: 0.030219
2021-11-30 12:44:27,603 iteration 2308 : loss : 0.057148, loss_ce: 0.023542
2021-11-30 12:44:29,066 iteration 2309 : loss : 0.048193, loss_ce: 0.026908
2021-11-30 12:44:30,634 iteration 2310 : loss : 0.055017, loss_ce: 0.026553
2021-11-30 12:44:32,026 iteration 2311 : loss : 0.063658, loss_ce: 0.025478
2021-11-30 12:44:33,519 iteration 2312 : loss : 0.064718, loss_ce: 0.025850
 34%|█████████▏                 | 136/400 [1:02:35<2:03:20, 28.03s/it]2021-11-30 12:44:35,089 iteration 2313 : loss : 0.059062, loss_ce: 0.024904
2021-11-30 12:44:36,458 iteration 2314 : loss : 0.036240, loss_ce: 0.016801
2021-11-30 12:44:37,901 iteration 2315 : loss : 0.043607, loss_ce: 0.018551
2021-11-30 12:44:39,478 iteration 2316 : loss : 0.062315, loss_ce: 0.030348
2021-11-30 12:44:40,894 iteration 2317 : loss : 0.046266, loss_ce: 0.019668
2021-11-30 12:44:42,359 iteration 2318 : loss : 0.052066, loss_ce: 0.027638
2021-11-30 12:44:43,868 iteration 2319 : loss : 0.070186, loss_ce: 0.029595
2021-11-30 12:44:45,454 iteration 2320 : loss : 0.050044, loss_ce: 0.025924
2021-11-30 12:44:46,930 iteration 2321 : loss : 0.057826, loss_ce: 0.027418
2021-11-30 12:44:48,416 iteration 2322 : loss : 0.048210, loss_ce: 0.024325
2021-11-30 12:44:49,790 iteration 2323 : loss : 0.038297, loss_ce: 0.019791
2021-11-30 12:44:51,249 iteration 2324 : loss : 0.048872, loss_ce: 0.024977
2021-11-30 12:44:52,621 iteration 2325 : loss : 0.038027, loss_ce: 0.020842
2021-11-30 12:44:54,109 iteration 2326 : loss : 0.081302, loss_ce: 0.020804
2021-11-30 12:44:55,566 iteration 2327 : loss : 0.045225, loss_ce: 0.020864
2021-11-30 12:44:57,062 iteration 2328 : loss : 0.050460, loss_ce: 0.024407
2021-11-30 12:44:58,498 iteration 2329 : loss : 0.047946, loss_ce: 0.022231
 34%|█████████▏                 | 137/400 [1:03:00<1:58:53, 27.12s/it]2021-11-30 12:45:00,061 iteration 2330 : loss : 0.047418, loss_ce: 0.023149
2021-11-30 12:45:01,472 iteration 2331 : loss : 0.044039, loss_ce: 0.021390
2021-11-30 12:45:03,032 iteration 2332 : loss : 0.048335, loss_ce: 0.021901
2021-11-30 12:45:04,513 iteration 2333 : loss : 0.054754, loss_ce: 0.026774
2021-11-30 12:45:06,134 iteration 2334 : loss : 0.047924, loss_ce: 0.022531
2021-11-30 12:45:07,738 iteration 2335 : loss : 0.060505, loss_ce: 0.026881
2021-11-30 12:45:09,302 iteration 2336 : loss : 0.063406, loss_ce: 0.031889
2021-11-30 12:45:10,769 iteration 2337 : loss : 0.068742, loss_ce: 0.026293
2021-11-30 12:45:12,292 iteration 2338 : loss : 0.061868, loss_ce: 0.027356
2021-11-30 12:45:13,822 iteration 2339 : loss : 0.060239, loss_ce: 0.023927
2021-11-30 12:45:15,232 iteration 2340 : loss : 0.042803, loss_ce: 0.022197
2021-11-30 12:45:16,748 iteration 2341 : loss : 0.057511, loss_ce: 0.027823
2021-11-30 12:45:18,271 iteration 2342 : loss : 0.041443, loss_ce: 0.019855
2021-11-30 12:45:19,917 iteration 2343 : loss : 0.056634, loss_ce: 0.023418
2021-11-30 12:45:21,372 iteration 2344 : loss : 0.060610, loss_ce: 0.022351
2021-11-30 12:45:22,805 iteration 2345 : loss : 0.055904, loss_ce: 0.032173
2021-11-30 12:45:24,292 iteration 2346 : loss : 0.055194, loss_ce: 0.025210
 34%|█████████▎                 | 138/400 [1:03:26<1:56:41, 26.72s/it]2021-11-30 12:45:25,947 iteration 2347 : loss : 0.076049, loss_ce: 0.034626
2021-11-30 12:45:27,453 iteration 2348 : loss : 0.048426, loss_ce: 0.022892
2021-11-30 12:45:28,903 iteration 2349 : loss : 0.046413, loss_ce: 0.020724
2021-11-30 12:45:30,379 iteration 2350 : loss : 0.068069, loss_ce: 0.024655
2021-11-30 12:45:31,956 iteration 2351 : loss : 0.060093, loss_ce: 0.030720
2021-11-30 12:45:33,394 iteration 2352 : loss : 0.044485, loss_ce: 0.024098
2021-11-30 12:45:34,861 iteration 2353 : loss : 0.041436, loss_ce: 0.021512
2021-11-30 12:45:36,307 iteration 2354 : loss : 0.035715, loss_ce: 0.020769
2021-11-30 12:45:37,775 iteration 2355 : loss : 0.049915, loss_ce: 0.021277
2021-11-30 12:45:39,328 iteration 2356 : loss : 0.056193, loss_ce: 0.024656
2021-11-30 12:45:40,822 iteration 2357 : loss : 0.067712, loss_ce: 0.024171
2021-11-30 12:45:42,315 iteration 2358 : loss : 0.050951, loss_ce: 0.022992
2021-11-30 12:45:43,765 iteration 2359 : loss : 0.050223, loss_ce: 0.021941
2021-11-30 12:45:45,193 iteration 2360 : loss : 0.066997, loss_ce: 0.022268
2021-11-30 12:45:46,677 iteration 2361 : loss : 0.056875, loss_ce: 0.023050
2021-11-30 12:45:48,310 iteration 2362 : loss : 0.066787, loss_ce: 0.030316
2021-11-30 12:45:49,810 iteration 2363 : loss : 0.044133, loss_ce: 0.023512
 35%|█████████▍                 | 139/400 [1:03:52<1:54:40, 26.36s/it]2021-11-30 12:45:51,476 iteration 2364 : loss : 0.052155, loss_ce: 0.021333
2021-11-30 12:45:53,059 iteration 2365 : loss : 0.054912, loss_ce: 0.022757
2021-11-30 12:45:54,601 iteration 2366 : loss : 0.068365, loss_ce: 0.027557
2021-11-30 12:45:56,011 iteration 2367 : loss : 0.050999, loss_ce: 0.017052
2021-11-30 12:45:57,438 iteration 2368 : loss : 0.041420, loss_ce: 0.021823
2021-11-30 12:45:58,921 iteration 2369 : loss : 0.053835, loss_ce: 0.019839
2021-11-30 12:46:00,387 iteration 2370 : loss : 0.051397, loss_ce: 0.022187
2021-11-30 12:46:01,934 iteration 2371 : loss : 0.040788, loss_ce: 0.021318
2021-11-30 12:46:03,376 iteration 2372 : loss : 0.053732, loss_ce: 0.019496
2021-11-30 12:46:04,831 iteration 2373 : loss : 0.052840, loss_ce: 0.024521
2021-11-30 12:46:06,261 iteration 2374 : loss : 0.052139, loss_ce: 0.024796
2021-11-30 12:46:07,711 iteration 2375 : loss : 0.047616, loss_ce: 0.021043
2021-11-30 12:46:09,235 iteration 2376 : loss : 0.057094, loss_ce: 0.031334
2021-11-30 12:46:10,753 iteration 2377 : loss : 0.055163, loss_ce: 0.025645
2021-11-30 12:46:12,249 iteration 2378 : loss : 0.040655, loss_ce: 0.019846
2021-11-30 12:46:13,775 iteration 2379 : loss : 0.054856, loss_ce: 0.025984
2021-11-30 12:46:13,775 Training Data Eval:
2021-11-30 12:46:21,148   Average segmentation loss on training set: 0.0301
2021-11-30 12:46:21,149 Validation Data Eval:
2021-11-30 12:46:23,701   Average segmentation loss on validation set: 0.1030
2021-11-30 12:46:25,155 iteration 2380 : loss : 0.036635, loss_ce: 0.020253
 35%|█████████▍                 | 140/400 [1:04:27<2:05:53, 29.05s/it]2021-11-30 12:46:26,658 iteration 2381 : loss : 0.046750, loss_ce: 0.019967
2021-11-30 12:46:28,193 iteration 2382 : loss : 0.056737, loss_ce: 0.021999
2021-11-30 12:46:29,634 iteration 2383 : loss : 0.040534, loss_ce: 0.018008
2021-11-30 12:46:31,169 iteration 2384 : loss : 0.064622, loss_ce: 0.027666
2021-11-30 12:46:32,731 iteration 2385 : loss : 0.053071, loss_ce: 0.022222
2021-11-30 12:46:34,223 iteration 2386 : loss : 0.044915, loss_ce: 0.019446
2021-11-30 12:46:35,685 iteration 2387 : loss : 0.051882, loss_ce: 0.020993
2021-11-30 12:46:37,185 iteration 2388 : loss : 0.051939, loss_ce: 0.026540
2021-11-30 12:46:38,616 iteration 2389 : loss : 0.025959, loss_ce: 0.013366
2021-11-30 12:46:40,125 iteration 2390 : loss : 0.051190, loss_ce: 0.029251
2021-11-30 12:46:41,602 iteration 2391 : loss : 0.052062, loss_ce: 0.031616
2021-11-30 12:46:43,132 iteration 2392 : loss : 0.056960, loss_ce: 0.023712
2021-11-30 12:46:44,639 iteration 2393 : loss : 0.060003, loss_ce: 0.023509
2021-11-30 12:46:46,029 iteration 2394 : loss : 0.044834, loss_ce: 0.019860
2021-11-30 12:46:47,601 iteration 2395 : loss : 0.047311, loss_ce: 0.022938
2021-11-30 12:46:49,041 iteration 2396 : loss : 0.034841, loss_ce: 0.019788
2021-11-30 12:46:50,552 iteration 2397 : loss : 0.050638, loss_ce: 0.022858
 35%|█████████▌                 | 141/400 [1:04:52<2:00:41, 27.96s/it]2021-11-30 12:46:52,079 iteration 2398 : loss : 0.053890, loss_ce: 0.024678
2021-11-30 12:46:53,535 iteration 2399 : loss : 0.057039, loss_ce: 0.019839
2021-11-30 12:46:55,091 iteration 2400 : loss : 0.045221, loss_ce: 0.024282
2021-11-30 12:46:56,582 iteration 2401 : loss : 0.039411, loss_ce: 0.017954
2021-11-30 12:46:58,118 iteration 2402 : loss : 0.052243, loss_ce: 0.028755
2021-11-30 12:46:59,553 iteration 2403 : loss : 0.047380, loss_ce: 0.025949
2021-11-30 12:47:00,972 iteration 2404 : loss : 0.042918, loss_ce: 0.022591
2021-11-30 12:47:02,386 iteration 2405 : loss : 0.057104, loss_ce: 0.025359
2021-11-30 12:47:04,042 iteration 2406 : loss : 0.064170, loss_ce: 0.023713
2021-11-30 12:47:05,463 iteration 2407 : loss : 0.060811, loss_ce: 0.033805
2021-11-30 12:47:06,892 iteration 2408 : loss : 0.033494, loss_ce: 0.018295
2021-11-30 12:47:08,438 iteration 2409 : loss : 0.037684, loss_ce: 0.017980
2021-11-30 12:47:09,978 iteration 2410 : loss : 0.051225, loss_ce: 0.023540
2021-11-30 12:47:11,514 iteration 2411 : loss : 0.060166, loss_ce: 0.021604
2021-11-30 12:47:12,959 iteration 2412 : loss : 0.030799, loss_ce: 0.014920
2021-11-30 12:47:14,528 iteration 2413 : loss : 0.075354, loss_ce: 0.034569
2021-11-30 12:47:16,015 iteration 2414 : loss : 0.039056, loss_ce: 0.019395
 36%|█████████▌                 | 142/400 [1:05:18<1:56:59, 27.21s/it]2021-11-30 12:47:17,588 iteration 2415 : loss : 0.056386, loss_ce: 0.023138
2021-11-30 12:47:19,043 iteration 2416 : loss : 0.047253, loss_ce: 0.022101
2021-11-30 12:47:20,528 iteration 2417 : loss : 0.035717, loss_ce: 0.016475
2021-11-30 12:47:22,114 iteration 2418 : loss : 0.043004, loss_ce: 0.019360
2021-11-30 12:47:23,599 iteration 2419 : loss : 0.065615, loss_ce: 0.035905
2021-11-30 12:47:25,200 iteration 2420 : loss : 0.060677, loss_ce: 0.032202
2021-11-30 12:47:26,763 iteration 2421 : loss : 0.062750, loss_ce: 0.025166
2021-11-30 12:47:28,313 iteration 2422 : loss : 0.052994, loss_ce: 0.023455
2021-11-30 12:47:29,784 iteration 2423 : loss : 0.057182, loss_ce: 0.027042
2021-11-30 12:47:31,247 iteration 2424 : loss : 0.055128, loss_ce: 0.026435
2021-11-30 12:47:32,861 iteration 2425 : loss : 0.060369, loss_ce: 0.023116
2021-11-30 12:47:34,323 iteration 2426 : loss : 0.051618, loss_ce: 0.019702
2021-11-30 12:47:35,732 iteration 2427 : loss : 0.041561, loss_ce: 0.022049
2021-11-30 12:47:37,187 iteration 2428 : loss : 0.054349, loss_ce: 0.019418
2021-11-30 12:47:38,626 iteration 2429 : loss : 0.056956, loss_ce: 0.024184
2021-11-30 12:47:40,074 iteration 2430 : loss : 0.032314, loss_ce: 0.015565
2021-11-30 12:47:41,567 iteration 2431 : loss : 0.051097, loss_ce: 0.027906
 36%|█████████▋                 | 143/400 [1:05:43<1:54:23, 26.71s/it]2021-11-30 12:47:43,070 iteration 2432 : loss : 0.046990, loss_ce: 0.020106
2021-11-30 12:47:44,547 iteration 2433 : loss : 0.055316, loss_ce: 0.023273
2021-11-30 12:47:46,019 iteration 2434 : loss : 0.051554, loss_ce: 0.021248
2021-11-30 12:47:47,610 iteration 2435 : loss : 0.079245, loss_ce: 0.045777
2021-11-30 12:47:49,005 iteration 2436 : loss : 0.045487, loss_ce: 0.023499
2021-11-30 12:47:50,538 iteration 2437 : loss : 0.046514, loss_ce: 0.020448
2021-11-30 12:47:52,080 iteration 2438 : loss : 0.051154, loss_ce: 0.021591
2021-11-30 12:47:53,617 iteration 2439 : loss : 0.055362, loss_ce: 0.023276
2021-11-30 12:47:55,154 iteration 2440 : loss : 0.068882, loss_ce: 0.029152
2021-11-30 12:47:56,630 iteration 2441 : loss : 0.044638, loss_ce: 0.020801
2021-11-30 12:47:58,106 iteration 2442 : loss : 0.044930, loss_ce: 0.021834
2021-11-30 12:47:59,506 iteration 2443 : loss : 0.044023, loss_ce: 0.019913
2021-11-30 12:48:00,976 iteration 2444 : loss : 0.057455, loss_ce: 0.026253
2021-11-30 12:48:02,467 iteration 2445 : loss : 0.043989, loss_ce: 0.022014
2021-11-30 12:48:04,044 iteration 2446 : loss : 0.049799, loss_ce: 0.025970
2021-11-30 12:48:05,455 iteration 2447 : loss : 0.046193, loss_ce: 0.018253
2021-11-30 12:48:06,992 iteration 2448 : loss : 0.043437, loss_ce: 0.019491
 36%|█████████▋                 | 144/400 [1:06:09<1:52:19, 26.33s/it]2021-11-30 12:48:08,459 iteration 2449 : loss : 0.057461, loss_ce: 0.024266
2021-11-30 12:48:09,889 iteration 2450 : loss : 0.041302, loss_ce: 0.024038
2021-11-30 12:48:11,351 iteration 2451 : loss : 0.039768, loss_ce: 0.016902
2021-11-30 12:48:12,794 iteration 2452 : loss : 0.045024, loss_ce: 0.020131
2021-11-30 12:48:14,300 iteration 2453 : loss : 0.054750, loss_ce: 0.023123
2021-11-30 12:48:15,773 iteration 2454 : loss : 0.045203, loss_ce: 0.019584
2021-11-30 12:48:17,223 iteration 2455 : loss : 0.050941, loss_ce: 0.027859
2021-11-30 12:48:18,660 iteration 2456 : loss : 0.051906, loss_ce: 0.024580
2021-11-30 12:48:20,130 iteration 2457 : loss : 0.057201, loss_ce: 0.025267
2021-11-30 12:48:21,565 iteration 2458 : loss : 0.040150, loss_ce: 0.018482
2021-11-30 12:48:22,947 iteration 2459 : loss : 0.032212, loss_ce: 0.016509
2021-11-30 12:48:24,402 iteration 2460 : loss : 0.051165, loss_ce: 0.017160
2021-11-30 12:48:25,967 iteration 2461 : loss : 0.045718, loss_ce: 0.019748
2021-11-30 12:48:27,470 iteration 2462 : loss : 0.059250, loss_ce: 0.022311
2021-11-30 12:48:28,925 iteration 2463 : loss : 0.047253, loss_ce: 0.023818
2021-11-30 12:48:30,424 iteration 2464 : loss : 0.037241, loss_ce: 0.018498
2021-11-30 12:48:30,424 Training Data Eval:
2021-11-30 12:48:37,808   Average segmentation loss on training set: 0.0300
2021-11-30 12:48:37,808 Validation Data Eval:
2021-11-30 12:48:40,350   Average segmentation loss on validation set: 0.1039
2021-11-30 12:48:41,838 iteration 2465 : loss : 0.054273, loss_ce: 0.024514
 36%|█████████▊                 | 145/400 [1:06:44<2:02:44, 28.88s/it]2021-11-30 12:48:43,457 iteration 2466 : loss : 0.050949, loss_ce: 0.022700
2021-11-30 12:48:45,024 iteration 2467 : loss : 0.050965, loss_ce: 0.023982
2021-11-30 12:48:46,472 iteration 2468 : loss : 0.045226, loss_ce: 0.020686
2021-11-30 12:48:47,885 iteration 2469 : loss : 0.040378, loss_ce: 0.020718
2021-11-30 12:48:49,355 iteration 2470 : loss : 0.047291, loss_ce: 0.019314
2021-11-30 12:48:50,938 iteration 2471 : loss : 0.060210, loss_ce: 0.024856
2021-11-30 12:48:52,387 iteration 2472 : loss : 0.066693, loss_ce: 0.029438
2021-11-30 12:48:53,937 iteration 2473 : loss : 0.050456, loss_ce: 0.023747
2021-11-30 12:48:55,403 iteration 2474 : loss : 0.038148, loss_ce: 0.016673
2021-11-30 12:48:56,925 iteration 2475 : loss : 0.041664, loss_ce: 0.019200
2021-11-30 12:48:58,364 iteration 2476 : loss : 0.062265, loss_ce: 0.033826
2021-11-30 12:48:59,966 iteration 2477 : loss : 0.050643, loss_ce: 0.023553
2021-11-30 12:49:01,492 iteration 2478 : loss : 0.046642, loss_ce: 0.021254
2021-11-30 12:49:02,883 iteration 2479 : loss : 0.047408, loss_ce: 0.020939
2021-11-30 12:49:04,380 iteration 2480 : loss : 0.053559, loss_ce: 0.021464
2021-11-30 12:49:05,938 iteration 2481 : loss : 0.037718, loss_ce: 0.018993
2021-11-30 12:49:07,396 iteration 2482 : loss : 0.045479, loss_ce: 0.022582
 36%|█████████▊                 | 146/400 [1:07:09<1:58:01, 27.88s/it]2021-11-30 12:49:09,029 iteration 2483 : loss : 0.065879, loss_ce: 0.034054
2021-11-30 12:49:10,526 iteration 2484 : loss : 0.053469, loss_ce: 0.016564
2021-11-30 12:49:11,970 iteration 2485 : loss : 0.040232, loss_ce: 0.021032
2021-11-30 12:49:13,439 iteration 2486 : loss : 0.040288, loss_ce: 0.020416
2021-11-30 12:49:14,909 iteration 2487 : loss : 0.057882, loss_ce: 0.021770
2021-11-30 12:49:16,443 iteration 2488 : loss : 0.047971, loss_ce: 0.023118
2021-11-30 12:49:17,917 iteration 2489 : loss : 0.050962, loss_ce: 0.026060
2021-11-30 12:49:19,483 iteration 2490 : loss : 0.048933, loss_ce: 0.023059
2021-11-30 12:49:21,057 iteration 2491 : loss : 0.038343, loss_ce: 0.021868
2021-11-30 12:49:22,422 iteration 2492 : loss : 0.032390, loss_ce: 0.016442
2021-11-30 12:49:23,942 iteration 2493 : loss : 0.049380, loss_ce: 0.023196
2021-11-30 12:49:25,407 iteration 2494 : loss : 0.054632, loss_ce: 0.020038
2021-11-30 12:49:26,868 iteration 2495 : loss : 0.051794, loss_ce: 0.031331
2021-11-30 12:49:28,565 iteration 2496 : loss : 0.058164, loss_ce: 0.020656
2021-11-30 12:49:30,094 iteration 2497 : loss : 0.054855, loss_ce: 0.022574
2021-11-30 12:49:31,636 iteration 2498 : loss : 0.050048, loss_ce: 0.023094
2021-11-30 12:49:33,061 iteration 2499 : loss : 0.047168, loss_ce: 0.019370
 37%|█████████▉                 | 147/400 [1:07:35<1:54:47, 27.22s/it]2021-11-30 12:49:34,581 iteration 2500 : loss : 0.038278, loss_ce: 0.018553
2021-11-30 12:49:36,047 iteration 2501 : loss : 0.041013, loss_ce: 0.017923
2021-11-30 12:49:37,610 iteration 2502 : loss : 0.063736, loss_ce: 0.030874
2021-11-30 12:49:39,025 iteration 2503 : loss : 0.047342, loss_ce: 0.021078
2021-11-30 12:49:40,466 iteration 2504 : loss : 0.060039, loss_ce: 0.026344
2021-11-30 12:49:42,001 iteration 2505 : loss : 0.053316, loss_ce: 0.025085
2021-11-30 12:49:43,484 iteration 2506 : loss : 0.047327, loss_ce: 0.018780
2021-11-30 12:49:44,926 iteration 2507 : loss : 0.047099, loss_ce: 0.024692
2021-11-30 12:49:46,388 iteration 2508 : loss : 0.052668, loss_ce: 0.021575
2021-11-30 12:49:47,852 iteration 2509 : loss : 0.056436, loss_ce: 0.022246
2021-11-30 12:49:49,296 iteration 2510 : loss : 0.048212, loss_ce: 0.025660
2021-11-30 12:49:50,784 iteration 2511 : loss : 0.041117, loss_ce: 0.021348
2021-11-30 12:49:52,293 iteration 2512 : loss : 0.053284, loss_ce: 0.023275
2021-11-30 12:49:53,830 iteration 2513 : loss : 0.053877, loss_ce: 0.020277
2021-11-30 12:49:55,318 iteration 2514 : loss : 0.043218, loss_ce: 0.018153
2021-11-30 12:49:56,788 iteration 2515 : loss : 0.034279, loss_ce: 0.020214
2021-11-30 12:49:58,249 iteration 2516 : loss : 0.037181, loss_ce: 0.019709
 37%|█████████▉                 | 148/400 [1:08:00<1:51:45, 26.61s/it]2021-11-30 12:49:59,747 iteration 2517 : loss : 0.044855, loss_ce: 0.020929
2021-11-30 12:50:01,239 iteration 2518 : loss : 0.071078, loss_ce: 0.028446
2021-11-30 12:50:02,759 iteration 2519 : loss : 0.045469, loss_ce: 0.020763
2021-11-30 12:50:04,245 iteration 2520 : loss : 0.058810, loss_ce: 0.026599
2021-11-30 12:50:05,696 iteration 2521 : loss : 0.037283, loss_ce: 0.017608
2021-11-30 12:50:07,162 iteration 2522 : loss : 0.043668, loss_ce: 0.023978
2021-11-30 12:50:08,695 iteration 2523 : loss : 0.046485, loss_ce: 0.016783
2021-11-30 12:50:10,175 iteration 2524 : loss : 0.042431, loss_ce: 0.022864
2021-11-30 12:50:11,676 iteration 2525 : loss : 0.041724, loss_ce: 0.020525
2021-11-30 12:50:13,193 iteration 2526 : loss : 0.055358, loss_ce: 0.023654
2021-11-30 12:50:14,709 iteration 2527 : loss : 0.052259, loss_ce: 0.023746
2021-11-30 12:50:16,224 iteration 2528 : loss : 0.057130, loss_ce: 0.024833
2021-11-30 12:50:17,720 iteration 2529 : loss : 0.070130, loss_ce: 0.031012
2021-11-30 12:50:19,227 iteration 2530 : loss : 0.043946, loss_ce: 0.024035
2021-11-30 12:50:20,803 iteration 2531 : loss : 0.038911, loss_ce: 0.018457
2021-11-30 12:50:22,273 iteration 2532 : loss : 0.054666, loss_ce: 0.023149
2021-11-30 12:50:23,777 iteration 2533 : loss : 0.049617, loss_ce: 0.018841
 37%|██████████                 | 149/400 [1:08:26<1:49:57, 26.28s/it]2021-11-30 12:50:25,248 iteration 2534 : loss : 0.057601, loss_ce: 0.027918
2021-11-30 12:50:26,814 iteration 2535 : loss : 0.071416, loss_ce: 0.027698
2021-11-30 12:50:28,252 iteration 2536 : loss : 0.045621, loss_ce: 0.026107
2021-11-30 12:50:29,703 iteration 2537 : loss : 0.042849, loss_ce: 0.020335
2021-11-30 12:50:31,089 iteration 2538 : loss : 0.035794, loss_ce: 0.019222
2021-11-30 12:50:32,637 iteration 2539 : loss : 0.040100, loss_ce: 0.020238
2021-11-30 12:50:34,049 iteration 2540 : loss : 0.037181, loss_ce: 0.015715
2021-11-30 12:50:35,517 iteration 2541 : loss : 0.040145, loss_ce: 0.020792
2021-11-30 12:50:36,952 iteration 2542 : loss : 0.030528, loss_ce: 0.015093
2021-11-30 12:50:38,391 iteration 2543 : loss : 0.040763, loss_ce: 0.018441
2021-11-30 12:50:39,877 iteration 2544 : loss : 0.067618, loss_ce: 0.027273
2021-11-30 12:50:41,493 iteration 2545 : loss : 0.043214, loss_ce: 0.021521
2021-11-30 12:50:43,128 iteration 2546 : loss : 0.060208, loss_ce: 0.029067
2021-11-30 12:50:44,536 iteration 2547 : loss : 0.036325, loss_ce: 0.021128
2021-11-30 12:50:46,032 iteration 2548 : loss : 0.064740, loss_ce: 0.033661
2021-11-30 12:50:47,515 iteration 2549 : loss : 0.064218, loss_ce: 0.023658
2021-11-30 12:50:47,515 Training Data Eval:
2021-11-30 12:50:54,859   Average segmentation loss on training set: 0.0297
2021-11-30 12:50:54,859 Validation Data Eval:
2021-11-30 12:50:57,407   Average segmentation loss on validation set: 0.0846
2021-11-30 12:50:59,366 Found new lowest validation loss at iteration 2549! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/UNET_RUNMC/best_val_loss.pth
2021-11-30 12:51:00,801 iteration 2550 : loss : 0.037856, loss_ce: 0.016744
 38%|██████████▏                | 150/400 [1:09:03<2:02:56, 29.51s/it]2021-11-30 12:51:02,273 iteration 2551 : loss : 0.040555, loss_ce: 0.021595
2021-11-30 12:51:03,784 iteration 2552 : loss : 0.033721, loss_ce: 0.017928
2021-11-30 12:51:05,145 iteration 2553 : loss : 0.026372, loss_ce: 0.013425
2021-11-30 12:51:06,604 iteration 2554 : loss : 0.059134, loss_ce: 0.024214
2021-11-30 12:51:08,023 iteration 2555 : loss : 0.043611, loss_ce: 0.016503
2021-11-30 12:51:09,512 iteration 2556 : loss : 0.052585, loss_ce: 0.027337
2021-11-30 12:51:10,945 iteration 2557 : loss : 0.046194, loss_ce: 0.023972
2021-11-30 12:51:12,434 iteration 2558 : loss : 0.056254, loss_ce: 0.030221
2021-11-30 12:51:14,013 iteration 2559 : loss : 0.045968, loss_ce: 0.019702
2021-11-30 12:51:15,484 iteration 2560 : loss : 0.042292, loss_ce: 0.023863
2021-11-30 12:51:17,075 iteration 2561 : loss : 0.042370, loss_ce: 0.019120
2021-11-30 12:51:18,581 iteration 2562 : loss : 0.043647, loss_ce: 0.020884
2021-11-30 12:51:20,092 iteration 2563 : loss : 0.062757, loss_ce: 0.031467
2021-11-30 12:51:21,532 iteration 2564 : loss : 0.054347, loss_ce: 0.027080
2021-11-30 12:51:22,975 iteration 2565 : loss : 0.054447, loss_ce: 0.026286
2021-11-30 12:51:24,436 iteration 2566 : loss : 0.043288, loss_ce: 0.020824
2021-11-30 12:51:25,919 iteration 2567 : loss : 0.069087, loss_ce: 0.022905
 38%|██████████▏                | 151/400 [1:09:28<1:56:59, 28.19s/it]2021-11-30 12:51:27,494 iteration 2568 : loss : 0.049411, loss_ce: 0.025639
2021-11-30 12:51:28,959 iteration 2569 : loss : 0.049878, loss_ce: 0.027246
2021-11-30 12:51:30,425 iteration 2570 : loss : 0.054861, loss_ce: 0.025968
2021-11-30 12:51:31,882 iteration 2571 : loss : 0.040130, loss_ce: 0.018579
2021-11-30 12:51:33,300 iteration 2572 : loss : 0.048306, loss_ce: 0.017243
2021-11-30 12:51:34,733 iteration 2573 : loss : 0.048914, loss_ce: 0.022377
2021-11-30 12:51:36,206 iteration 2574 : loss : 0.048946, loss_ce: 0.021497
2021-11-30 12:51:37,817 iteration 2575 : loss : 0.065800, loss_ce: 0.027507
2021-11-30 12:51:39,311 iteration 2576 : loss : 0.045399, loss_ce: 0.020214
2021-11-30 12:51:40,777 iteration 2577 : loss : 0.065442, loss_ce: 0.028798
2021-11-30 12:51:42,268 iteration 2578 : loss : 0.072707, loss_ce: 0.027030
2021-11-30 12:51:43,689 iteration 2579 : loss : 0.047967, loss_ce: 0.015500
2021-11-30 12:51:45,241 iteration 2580 : loss : 0.062917, loss_ce: 0.020255
2021-11-30 12:51:46,763 iteration 2581 : loss : 0.061913, loss_ce: 0.026393
2021-11-30 12:51:48,313 iteration 2582 : loss : 0.050135, loss_ce: 0.023839
2021-11-30 12:51:49,785 iteration 2583 : loss : 0.040108, loss_ce: 0.018776
2021-11-30 12:51:51,181 iteration 2584 : loss : 0.037279, loss_ce: 0.019180
 38%|██████████▎                | 152/400 [1:09:53<1:52:54, 27.32s/it]2021-11-30 12:51:52,718 iteration 2585 : loss : 0.041357, loss_ce: 0.017275
2021-11-30 12:51:54,198 iteration 2586 : loss : 0.057086, loss_ce: 0.025911
2021-11-30 12:51:55,611 iteration 2587 : loss : 0.053402, loss_ce: 0.021329
2021-11-30 12:51:57,054 iteration 2588 : loss : 0.049234, loss_ce: 0.026128
2021-11-30 12:51:58,581 iteration 2589 : loss : 0.057368, loss_ce: 0.023507
2021-11-30 12:52:00,128 iteration 2590 : loss : 0.053852, loss_ce: 0.026154
2021-11-30 12:52:01,573 iteration 2591 : loss : 0.034421, loss_ce: 0.016118
2021-11-30 12:52:03,020 iteration 2592 : loss : 0.048049, loss_ce: 0.019642
2021-11-30 12:52:04,474 iteration 2593 : loss : 0.042934, loss_ce: 0.022280
2021-11-30 12:52:06,086 iteration 2594 : loss : 0.060550, loss_ce: 0.029399
2021-11-30 12:52:07,616 iteration 2595 : loss : 0.048010, loss_ce: 0.020248
2021-11-30 12:52:09,063 iteration 2596 : loss : 0.040394, loss_ce: 0.021679
2021-11-30 12:52:10,485 iteration 2597 : loss : 0.042566, loss_ce: 0.019035
2021-11-30 12:52:11,951 iteration 2598 : loss : 0.036417, loss_ce: 0.018610
2021-11-30 12:52:13,406 iteration 2599 : loss : 0.055369, loss_ce: 0.019181
2021-11-30 12:52:14,874 iteration 2600 : loss : 0.047368, loss_ce: 0.020697
2021-11-30 12:52:16,296 iteration 2601 : loss : 0.049150, loss_ce: 0.024890
 38%|██████████▎                | 153/400 [1:10:18<1:49:43, 26.65s/it]2021-11-30 12:52:17,863 iteration 2602 : loss : 0.049565, loss_ce: 0.026405
2021-11-30 12:52:19,272 iteration 2603 : loss : 0.043077, loss_ce: 0.019382
2021-11-30 12:52:20,750 iteration 2604 : loss : 0.045453, loss_ce: 0.019754
2021-11-30 12:52:22,369 iteration 2605 : loss : 0.036179, loss_ce: 0.016331
2021-11-30 12:52:23,834 iteration 2606 : loss : 0.046365, loss_ce: 0.022729
2021-11-30 12:52:25,328 iteration 2607 : loss : 0.053230, loss_ce: 0.025262
2021-11-30 12:52:26,752 iteration 2608 : loss : 0.052731, loss_ce: 0.030188
2021-11-30 12:52:28,196 iteration 2609 : loss : 0.052994, loss_ce: 0.020999
2021-11-30 12:52:29,812 iteration 2610 : loss : 0.060620, loss_ce: 0.029276
2021-11-30 12:52:31,249 iteration 2611 : loss : 0.060403, loss_ce: 0.023807
2021-11-30 12:52:32,628 iteration 2612 : loss : 0.037962, loss_ce: 0.020523
2021-11-30 12:52:34,100 iteration 2613 : loss : 0.072612, loss_ce: 0.025358
2021-11-30 12:52:35,647 iteration 2614 : loss : 0.056624, loss_ce: 0.021524
2021-11-30 12:52:37,083 iteration 2615 : loss : 0.055534, loss_ce: 0.025193
2021-11-30 12:52:38,540 iteration 2616 : loss : 0.042369, loss_ce: 0.019208
2021-11-30 12:52:40,110 iteration 2617 : loss : 0.045561, loss_ce: 0.017409
2021-11-30 12:52:41,606 iteration 2618 : loss : 0.062353, loss_ce: 0.033472
 38%|██████████▍                | 154/400 [1:10:43<1:47:36, 26.25s/it]2021-11-30 12:52:43,204 iteration 2619 : loss : 0.046321, loss_ce: 0.017224
2021-11-30 12:52:44,644 iteration 2620 : loss : 0.049666, loss_ce: 0.022710
2021-11-30 12:52:46,156 iteration 2621 : loss : 0.044117, loss_ce: 0.020063
2021-11-30 12:52:47,658 iteration 2622 : loss : 0.050944, loss_ce: 0.026332
2021-11-30 12:52:49,168 iteration 2623 : loss : 0.068001, loss_ce: 0.035705
2021-11-30 12:52:50,588 iteration 2624 : loss : 0.035142, loss_ce: 0.016227
2021-11-30 12:52:52,052 iteration 2625 : loss : 0.033970, loss_ce: 0.018005
2021-11-30 12:52:53,606 iteration 2626 : loss : 0.041350, loss_ce: 0.022739
2021-11-30 12:52:55,121 iteration 2627 : loss : 0.061493, loss_ce: 0.021569
2021-11-30 12:52:56,562 iteration 2628 : loss : 0.046624, loss_ce: 0.020409
2021-11-30 12:52:58,103 iteration 2629 : loss : 0.050321, loss_ce: 0.022480
2021-11-30 12:52:59,506 iteration 2630 : loss : 0.036154, loss_ce: 0.015927
2021-11-30 12:53:01,009 iteration 2631 : loss : 0.046976, loss_ce: 0.024783
2021-11-30 12:53:02,522 iteration 2632 : loss : 0.058217, loss_ce: 0.025345
2021-11-30 12:53:03,968 iteration 2633 : loss : 0.040652, loss_ce: 0.020849
2021-11-30 12:53:05,472 iteration 2634 : loss : 0.032075, loss_ce: 0.015078
2021-11-30 12:53:05,473 Training Data Eval:
2021-11-30 12:53:12,843   Average segmentation loss on training set: 0.0273
2021-11-30 12:53:12,844 Validation Data Eval:
2021-11-30 12:53:15,405   Average segmentation loss on validation set: 0.0787
2021-11-30 12:53:17,393 Found new lowest validation loss at iteration 2634! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/UNET_RUNMC/best_val_loss.pth
2021-11-30 12:53:18,965 iteration 2635 : loss : 0.047933, loss_ce: 0.024914
 39%|██████████▍                | 155/400 [1:11:21<2:00:47, 29.58s/it]2021-11-30 12:53:20,406 iteration 2636 : loss : 0.035609, loss_ce: 0.015131
2021-11-30 12:53:22,007 iteration 2637 : loss : 0.054345, loss_ce: 0.022678
2021-11-30 12:53:23,468 iteration 2638 : loss : 0.036556, loss_ce: 0.018761
2021-11-30 12:53:24,785 iteration 2639 : loss : 0.036292, loss_ce: 0.016776
2021-11-30 12:53:26,168 iteration 2640 : loss : 0.047800, loss_ce: 0.023186
2021-11-30 12:53:27,703 iteration 2641 : loss : 0.050058, loss_ce: 0.023598
2021-11-30 12:53:29,210 iteration 2642 : loss : 0.060574, loss_ce: 0.020978
2021-11-30 12:53:30,710 iteration 2643 : loss : 0.046834, loss_ce: 0.021509
2021-11-30 12:53:32,110 iteration 2644 : loss : 0.041018, loss_ce: 0.017096
2021-11-30 12:53:33,516 iteration 2645 : loss : 0.046761, loss_ce: 0.027864
2021-11-30 12:53:34,930 iteration 2646 : loss : 0.048656, loss_ce: 0.019503
2021-11-30 12:53:36,446 iteration 2647 : loss : 0.055739, loss_ce: 0.031459
2021-11-30 12:53:37,829 iteration 2648 : loss : 0.025442, loss_ce: 0.015388
2021-11-30 12:53:39,371 iteration 2649 : loss : 0.058520, loss_ce: 0.029395
2021-11-30 12:53:40,901 iteration 2650 : loss : 0.045278, loss_ce: 0.023363
2021-11-30 12:53:42,356 iteration 2651 : loss : 0.045973, loss_ce: 0.021652
2021-11-30 12:53:43,859 iteration 2652 : loss : 0.040254, loss_ce: 0.020385
 39%|██████████▌                | 156/400 [1:11:46<1:54:35, 28.18s/it]2021-11-30 12:53:45,520 iteration 2653 : loss : 0.047706, loss_ce: 0.021204
2021-11-30 12:53:47,000 iteration 2654 : loss : 0.038337, loss_ce: 0.018299
2021-11-30 12:53:48,432 iteration 2655 : loss : 0.036593, loss_ce: 0.018157
2021-11-30 12:53:49,855 iteration 2656 : loss : 0.034228, loss_ce: 0.018590
2021-11-30 12:53:51,278 iteration 2657 : loss : 0.031096, loss_ce: 0.015742
2021-11-30 12:53:52,738 iteration 2658 : loss : 0.045073, loss_ce: 0.019929
2021-11-30 12:53:54,360 iteration 2659 : loss : 0.060427, loss_ce: 0.029712
2021-11-30 12:53:55,829 iteration 2660 : loss : 0.042536, loss_ce: 0.019407
2021-11-30 12:53:57,216 iteration 2661 : loss : 0.040355, loss_ce: 0.018570
2021-11-30 12:53:58,747 iteration 2662 : loss : 0.058532, loss_ce: 0.026818
2021-11-30 12:54:00,171 iteration 2663 : loss : 0.050397, loss_ce: 0.024256
2021-11-30 12:54:01,732 iteration 2664 : loss : 0.046423, loss_ce: 0.019834
2021-11-30 12:54:03,186 iteration 2665 : loss : 0.044539, loss_ce: 0.020468
2021-11-30 12:54:04,682 iteration 2666 : loss : 0.043684, loss_ce: 0.017888
2021-11-30 12:54:06,210 iteration 2667 : loss : 0.031215, loss_ce: 0.014941
2021-11-30 12:54:07,702 iteration 2668 : loss : 0.057092, loss_ce: 0.023439
2021-11-30 12:54:09,243 iteration 2669 : loss : 0.045352, loss_ce: 0.022617
 39%|██████████▌                | 157/400 [1:12:11<1:50:43, 27.34s/it]2021-11-30 12:54:10,786 iteration 2670 : loss : 0.072566, loss_ce: 0.028846
2021-11-30 12:54:12,280 iteration 2671 : loss : 0.057938, loss_ce: 0.019276
2021-11-30 12:54:13,783 iteration 2672 : loss : 0.033721, loss_ce: 0.016721
2021-11-30 12:54:15,318 iteration 2673 : loss : 0.086205, loss_ce: 0.041989
2021-11-30 12:54:16,761 iteration 2674 : loss : 0.043848, loss_ce: 0.023258
2021-11-30 12:54:18,302 iteration 2675 : loss : 0.062647, loss_ce: 0.026764
2021-11-30 12:54:19,791 iteration 2676 : loss : 0.041390, loss_ce: 0.016425
2021-11-30 12:54:21,208 iteration 2677 : loss : 0.039546, loss_ce: 0.020705
2021-11-30 12:54:22,612 iteration 2678 : loss : 0.035575, loss_ce: 0.015739
2021-11-30 12:54:24,142 iteration 2679 : loss : 0.045068, loss_ce: 0.023708
2021-11-30 12:54:25,706 iteration 2680 : loss : 0.045741, loss_ce: 0.023839
2021-11-30 12:54:27,175 iteration 2681 : loss : 0.047676, loss_ce: 0.023920
2021-11-30 12:54:28,596 iteration 2682 : loss : 0.037219, loss_ce: 0.018283
2021-11-30 12:54:30,114 iteration 2683 : loss : 0.055898, loss_ce: 0.028092
2021-11-30 12:54:31,568 iteration 2684 : loss : 0.057384, loss_ce: 0.018832
2021-11-30 12:54:33,053 iteration 2685 : loss : 0.045663, loss_ce: 0.023251
2021-11-30 12:54:34,520 iteration 2686 : loss : 0.042147, loss_ce: 0.020048
 40%|██████████▋                | 158/400 [1:12:36<1:47:47, 26.72s/it]2021-11-30 12:54:36,027 iteration 2687 : loss : 0.030574, loss_ce: 0.017109
2021-11-30 12:54:37,487 iteration 2688 : loss : 0.041219, loss_ce: 0.020037
2021-11-30 12:54:38,836 iteration 2689 : loss : 0.036382, loss_ce: 0.018087
2021-11-30 12:54:40,309 iteration 2690 : loss : 0.041575, loss_ce: 0.021046
2021-11-30 12:54:41,797 iteration 2691 : loss : 0.039670, loss_ce: 0.016126
2021-11-30 12:54:43,292 iteration 2692 : loss : 0.053756, loss_ce: 0.021146
2021-11-30 12:54:44,741 iteration 2693 : loss : 0.044442, loss_ce: 0.021890
2021-11-30 12:54:46,132 iteration 2694 : loss : 0.046599, loss_ce: 0.020446
2021-11-30 12:54:47,676 iteration 2695 : loss : 0.054031, loss_ce: 0.023344
2021-11-30 12:54:49,281 iteration 2696 : loss : 0.064095, loss_ce: 0.029287
2021-11-30 12:54:50,844 iteration 2697 : loss : 0.051043, loss_ce: 0.029677
2021-11-30 12:54:52,254 iteration 2698 : loss : 0.036719, loss_ce: 0.018486
2021-11-30 12:54:53,675 iteration 2699 : loss : 0.040456, loss_ce: 0.017561
2021-11-30 12:54:55,111 iteration 2700 : loss : 0.052573, loss_ce: 0.020135
2021-11-30 12:54:56,611 iteration 2701 : loss : 0.070423, loss_ce: 0.033726
2021-11-30 12:54:58,134 iteration 2702 : loss : 0.062620, loss_ce: 0.019418
2021-11-30 12:54:59,652 iteration 2703 : loss : 0.048046, loss_ce: 0.023230
 40%|██████████▋                | 159/400 [1:13:02<1:45:24, 26.24s/it]2021-11-30 12:55:01,128 iteration 2704 : loss : 0.032096, loss_ce: 0.015415
2021-11-30 12:55:02,558 iteration 2705 : loss : 0.039776, loss_ce: 0.017198
2021-11-30 12:55:04,087 iteration 2706 : loss : 0.066618, loss_ce: 0.033561
2021-11-30 12:55:05,559 iteration 2707 : loss : 0.035863, loss_ce: 0.017638
2021-11-30 12:55:07,013 iteration 2708 : loss : 0.062631, loss_ce: 0.024535
2021-11-30 12:55:08,554 iteration 2709 : loss : 0.055673, loss_ce: 0.021935
2021-11-30 12:55:10,086 iteration 2710 : loss : 0.051095, loss_ce: 0.027104
2021-11-30 12:55:11,547 iteration 2711 : loss : 0.034783, loss_ce: 0.015956
2021-11-30 12:55:13,017 iteration 2712 : loss : 0.067451, loss_ce: 0.028420
2021-11-30 12:55:14,501 iteration 2713 : loss : 0.039624, loss_ce: 0.017430
2021-11-30 12:55:15,969 iteration 2714 : loss : 0.055934, loss_ce: 0.026761
2021-11-30 12:55:17,414 iteration 2715 : loss : 0.056573, loss_ce: 0.030881
2021-11-30 12:55:18,861 iteration 2716 : loss : 0.050816, loss_ce: 0.017932
2021-11-30 12:55:20,252 iteration 2717 : loss : 0.039700, loss_ce: 0.018578
2021-11-30 12:55:21,737 iteration 2718 : loss : 0.046212, loss_ce: 0.020840
2021-11-30 12:55:23,179 iteration 2719 : loss : 0.033532, loss_ce: 0.018874
2021-11-30 12:55:23,179 Training Data Eval:
2021-11-30 12:55:30,591   Average segmentation loss on training set: 0.0298
2021-11-30 12:55:30,592 Validation Data Eval:
2021-11-30 12:55:33,176   Average segmentation loss on validation set: 0.1031
2021-11-30 12:55:34,638 iteration 2720 : loss : 0.043307, loss_ce: 0.023148
 40%|██████████▊                | 160/400 [1:13:36<1:55:27, 28.86s/it]2021-11-30 12:55:36,221 iteration 2721 : loss : 0.038880, loss_ce: 0.016875
2021-11-30 12:55:37,732 iteration 2722 : loss : 0.032612, loss_ce: 0.018819
2021-11-30 12:55:39,293 iteration 2723 : loss : 0.044423, loss_ce: 0.023955
2021-11-30 12:55:40,719 iteration 2724 : loss : 0.045247, loss_ce: 0.020143
2021-11-30 12:55:42,175 iteration 2725 : loss : 0.046053, loss_ce: 0.016594
2021-11-30 12:55:43,734 iteration 2726 : loss : 0.046994, loss_ce: 0.019264
2021-11-30 12:55:45,207 iteration 2727 : loss : 0.049097, loss_ce: 0.022802
2021-11-30 12:55:46,742 iteration 2728 : loss : 0.044586, loss_ce: 0.020380
2021-11-30 12:55:48,252 iteration 2729 : loss : 0.069020, loss_ce: 0.037575
2021-11-30 12:55:49,814 iteration 2730 : loss : 0.047904, loss_ce: 0.021903
2021-11-30 12:55:51,281 iteration 2731 : loss : 0.044365, loss_ce: 0.020517
2021-11-30 12:55:52,662 iteration 2732 : loss : 0.034867, loss_ce: 0.017837
2021-11-30 12:55:54,132 iteration 2733 : loss : 0.036904, loss_ce: 0.020835
2021-11-30 12:55:55,639 iteration 2734 : loss : 0.049211, loss_ce: 0.019567
2021-11-30 12:55:57,133 iteration 2735 : loss : 0.046618, loss_ce: 0.024701
2021-11-30 12:55:58,594 iteration 2736 : loss : 0.039148, loss_ce: 0.015973
2021-11-30 12:56:00,046 iteration 2737 : loss : 0.060361, loss_ce: 0.021165
 40%|██████████▊                | 161/400 [1:14:02<1:50:51, 27.83s/it]2021-11-30 12:56:01,642 iteration 2738 : loss : 0.058610, loss_ce: 0.031095
2021-11-30 12:56:03,065 iteration 2739 : loss : 0.048726, loss_ce: 0.018752
2021-11-30 12:56:04,625 iteration 2740 : loss : 0.042896, loss_ce: 0.020650
2021-11-30 12:56:06,097 iteration 2741 : loss : 0.045439, loss_ce: 0.022761
2021-11-30 12:56:07,592 iteration 2742 : loss : 0.045176, loss_ce: 0.023258
2021-11-30 12:56:09,010 iteration 2743 : loss : 0.040167, loss_ce: 0.019104
2021-11-30 12:56:10,475 iteration 2744 : loss : 0.032178, loss_ce: 0.014716
2021-11-30 12:56:11,897 iteration 2745 : loss : 0.039750, loss_ce: 0.018262
2021-11-30 12:56:13,484 iteration 2746 : loss : 0.111401, loss_ce: 0.032038
2021-11-30 12:56:14,940 iteration 2747 : loss : 0.045690, loss_ce: 0.020391
2021-11-30 12:56:16,483 iteration 2748 : loss : 0.041985, loss_ce: 0.016298
2021-11-30 12:56:18,033 iteration 2749 : loss : 0.044713, loss_ce: 0.020219
2021-11-30 12:56:19,489 iteration 2750 : loss : 0.055818, loss_ce: 0.026047
2021-11-30 12:56:20,984 iteration 2751 : loss : 0.057609, loss_ce: 0.020354
2021-11-30 12:56:22,573 iteration 2752 : loss : 0.042258, loss_ce: 0.019425
2021-11-30 12:56:24,166 iteration 2753 : loss : 0.048230, loss_ce: 0.025750
2021-11-30 12:56:25,675 iteration 2754 : loss : 0.042173, loss_ce: 0.023496
 40%|██████████▉                | 162/400 [1:14:28<1:47:45, 27.17s/it]2021-11-30 12:56:27,051 iteration 2755 : loss : 0.031314, loss_ce: 0.013895
2021-11-30 12:56:28,527 iteration 2756 : loss : 0.044654, loss_ce: 0.020313
2021-11-30 12:56:30,011 iteration 2757 : loss : 0.039199, loss_ce: 0.020460
2021-11-30 12:56:31,491 iteration 2758 : loss : 0.037749, loss_ce: 0.017109
2021-11-30 12:56:32,987 iteration 2759 : loss : 0.045975, loss_ce: 0.020460
2021-11-30 12:56:34,480 iteration 2760 : loss : 0.052247, loss_ce: 0.024091
2021-11-30 12:56:36,004 iteration 2761 : loss : 0.044645, loss_ce: 0.018338
2021-11-30 12:56:37,486 iteration 2762 : loss : 0.044170, loss_ce: 0.021209
2021-11-30 12:56:39,027 iteration 2763 : loss : 0.064367, loss_ce: 0.025120
2021-11-30 12:56:40,439 iteration 2764 : loss : 0.032574, loss_ce: 0.016413
2021-11-30 12:56:42,038 iteration 2765 : loss : 0.039999, loss_ce: 0.018279
2021-11-30 12:56:43,438 iteration 2766 : loss : 0.047391, loss_ce: 0.020589
2021-11-30 12:56:44,907 iteration 2767 : loss : 0.042280, loss_ce: 0.020790
2021-11-30 12:56:46,374 iteration 2768 : loss : 0.050501, loss_ce: 0.019854
2021-11-30 12:56:47,773 iteration 2769 : loss : 0.032824, loss_ce: 0.018385
2021-11-30 12:56:49,226 iteration 2770 : loss : 0.049607, loss_ce: 0.024622
2021-11-30 12:56:50,703 iteration 2771 : loss : 0.056920, loss_ce: 0.025972
 41%|███████████                | 163/400 [1:14:53<1:44:45, 26.52s/it]2021-11-30 12:56:52,152 iteration 2772 : loss : 0.048229, loss_ce: 0.020935
2021-11-30 12:56:53,661 iteration 2773 : loss : 0.037175, loss_ce: 0.019783
2021-11-30 12:56:55,166 iteration 2774 : loss : 0.059319, loss_ce: 0.019324
2021-11-30 12:56:56,684 iteration 2775 : loss : 0.057918, loss_ce: 0.027299
2021-11-30 12:56:58,163 iteration 2776 : loss : 0.043196, loss_ce: 0.021675
2021-11-30 12:56:59,714 iteration 2777 : loss : 0.042649, loss_ce: 0.019245
2021-11-30 12:57:01,149 iteration 2778 : loss : 0.041612, loss_ce: 0.016950
2021-11-30 12:57:02,618 iteration 2779 : loss : 0.031075, loss_ce: 0.015789
2021-11-30 12:57:04,095 iteration 2780 : loss : 0.040571, loss_ce: 0.022998
2021-11-30 12:57:05,697 iteration 2781 : loss : 0.048123, loss_ce: 0.021876
2021-11-30 12:57:07,263 iteration 2782 : loss : 0.032733, loss_ce: 0.018301
2021-11-30 12:57:08,721 iteration 2783 : loss : 0.047548, loss_ce: 0.019489
2021-11-30 12:57:10,083 iteration 2784 : loss : 0.049829, loss_ce: 0.018492
2021-11-30 12:57:11,517 iteration 2785 : loss : 0.033427, loss_ce: 0.018189
2021-11-30 12:57:12,925 iteration 2786 : loss : 0.034706, loss_ce: 0.017343
2021-11-30 12:57:14,369 iteration 2787 : loss : 0.033688, loss_ce: 0.016425
2021-11-30 12:57:15,996 iteration 2788 : loss : 0.040938, loss_ce: 0.019489
 41%|███████████                | 164/400 [1:15:18<1:42:53, 26.16s/it]2021-11-30 12:57:17,542 iteration 2789 : loss : 0.033385, loss_ce: 0.014988
2021-11-30 12:57:19,127 iteration 2790 : loss : 0.063696, loss_ce: 0.029277
2021-11-30 12:57:20,528 iteration 2791 : loss : 0.031620, loss_ce: 0.014636
2021-11-30 12:57:21,994 iteration 2792 : loss : 0.044375, loss_ce: 0.020617
2021-11-30 12:57:23,491 iteration 2793 : loss : 0.041778, loss_ce: 0.017968
2021-11-30 12:57:25,027 iteration 2794 : loss : 0.039959, loss_ce: 0.021571
2021-11-30 12:57:26,489 iteration 2795 : loss : 0.055084, loss_ce: 0.025140
2021-11-30 12:57:27,941 iteration 2796 : loss : 0.045762, loss_ce: 0.018669
2021-11-30 12:57:29,488 iteration 2797 : loss : 0.038965, loss_ce: 0.020155
2021-11-30 12:57:30,983 iteration 2798 : loss : 0.050477, loss_ce: 0.018872
2021-11-30 12:57:32,565 iteration 2799 : loss : 0.060856, loss_ce: 0.024311
2021-11-30 12:57:34,047 iteration 2800 : loss : 0.046702, loss_ce: 0.024850
2021-11-30 12:57:35,426 iteration 2801 : loss : 0.094280, loss_ce: 0.025780
2021-11-30 12:57:36,868 iteration 2802 : loss : 0.029726, loss_ce: 0.013573
2021-11-30 12:57:38,373 iteration 2803 : loss : 0.052311, loss_ce: 0.023397
2021-11-30 12:57:39,738 iteration 2804 : loss : 0.047994, loss_ce: 0.022973
2021-11-30 12:57:39,739 Training Data Eval:
2021-11-30 12:57:47,136   Average segmentation loss on training set: 0.0253
2021-11-30 12:57:47,137 Validation Data Eval:
2021-11-30 12:57:49,694   Average segmentation loss on validation set: 0.0839
2021-11-30 12:57:51,300 iteration 2805 : loss : 0.059275, loss_ce: 0.024573
 41%|███████████▏               | 165/400 [1:15:53<1:53:12, 28.90s/it]2021-11-30 12:57:52,933 iteration 2806 : loss : 0.058222, loss_ce: 0.027975
2021-11-30 12:57:54,523 iteration 2807 : loss : 0.081307, loss_ce: 0.025270
2021-11-30 12:57:56,072 iteration 2808 : loss : 0.043502, loss_ce: 0.018121
2021-11-30 12:57:57,571 iteration 2809 : loss : 0.045566, loss_ce: 0.020840
2021-11-30 12:57:58,965 iteration 2810 : loss : 0.050090, loss_ce: 0.022321
2021-11-30 12:58:00,505 iteration 2811 : loss : 0.050450, loss_ce: 0.026297
2021-11-30 12:58:01,915 iteration 2812 : loss : 0.026871, loss_ce: 0.014439
2021-11-30 12:58:03,482 iteration 2813 : loss : 0.061718, loss_ce: 0.029519
2021-11-30 12:58:04,998 iteration 2814 : loss : 0.043383, loss_ce: 0.020368
2021-11-30 12:58:06,486 iteration 2815 : loss : 0.049837, loss_ce: 0.023179
2021-11-30 12:58:07,980 iteration 2816 : loss : 0.042229, loss_ce: 0.017858
2021-11-30 12:58:09,546 iteration 2817 : loss : 0.061943, loss_ce: 0.017699
2021-11-30 12:58:11,080 iteration 2818 : loss : 0.049717, loss_ce: 0.024039
2021-11-30 12:58:12,528 iteration 2819 : loss : 0.047105, loss_ce: 0.023059
2021-11-30 12:58:14,117 iteration 2820 : loss : 0.071873, loss_ce: 0.028504
2021-11-30 12:58:15,676 iteration 2821 : loss : 0.055101, loss_ce: 0.024851
2021-11-30 12:58:17,196 iteration 2822 : loss : 0.046263, loss_ce: 0.018089
 42%|███████████▏               | 166/400 [1:16:19<1:49:11, 28.00s/it]2021-11-30 12:58:18,842 iteration 2823 : loss : 0.051897, loss_ce: 0.025519
2021-11-30 12:58:20,294 iteration 2824 : loss : 0.037956, loss_ce: 0.020320
2021-11-30 12:58:21,788 iteration 2825 : loss : 0.050865, loss_ce: 0.023436
2021-11-30 12:58:23,394 iteration 2826 : loss : 0.058912, loss_ce: 0.024074
2021-11-30 12:58:25,016 iteration 2827 : loss : 0.056676, loss_ce: 0.020480
2021-11-30 12:58:26,516 iteration 2828 : loss : 0.050726, loss_ce: 0.021971
2021-11-30 12:58:27,994 iteration 2829 : loss : 0.042921, loss_ce: 0.022716
2021-11-30 12:58:29,456 iteration 2830 : loss : 0.041838, loss_ce: 0.019291
2021-11-30 12:58:30,841 iteration 2831 : loss : 0.035879, loss_ce: 0.017329
2021-11-30 12:58:32,478 iteration 2832 : loss : 0.049577, loss_ce: 0.023902
2021-11-30 12:58:33,943 iteration 2833 : loss : 0.036104, loss_ce: 0.015536
2021-11-30 12:58:35,505 iteration 2834 : loss : 0.051396, loss_ce: 0.020905
2021-11-30 12:58:37,017 iteration 2835 : loss : 0.052668, loss_ce: 0.019234
2021-11-30 12:58:38,506 iteration 2836 : loss : 0.044109, loss_ce: 0.020433
2021-11-30 12:58:40,072 iteration 2837 : loss : 0.059392, loss_ce: 0.027870
2021-11-30 12:58:41,490 iteration 2838 : loss : 0.035316, loss_ce: 0.018307
2021-11-30 12:58:43,027 iteration 2839 : loss : 0.035067, loss_ce: 0.016915
 42%|███████████▎               | 167/400 [1:16:45<1:46:11, 27.35s/it]2021-11-30 12:58:44,644 iteration 2840 : loss : 0.044469, loss_ce: 0.019295
2021-11-30 12:58:46,121 iteration 2841 : loss : 0.033141, loss_ce: 0.014380
2021-11-30 12:58:47,583 iteration 2842 : loss : 0.039917, loss_ce: 0.017138
2021-11-30 12:58:49,115 iteration 2843 : loss : 0.050155, loss_ce: 0.020074
2021-11-30 12:58:50,632 iteration 2844 : loss : 0.042126, loss_ce: 0.019797
2021-11-30 12:58:52,091 iteration 2845 : loss : 0.045734, loss_ce: 0.020214
2021-11-30 12:58:53,579 iteration 2846 : loss : 0.031132, loss_ce: 0.014803
2021-11-30 12:58:54,929 iteration 2847 : loss : 0.028376, loss_ce: 0.018385
2021-11-30 12:58:56,439 iteration 2848 : loss : 0.047666, loss_ce: 0.021130
2021-11-30 12:58:58,002 iteration 2849 : loss : 0.041099, loss_ce: 0.019070
2021-11-30 12:58:59,561 iteration 2850 : loss : 0.064754, loss_ce: 0.025579
2021-11-30 12:59:01,120 iteration 2851 : loss : 0.044327, loss_ce: 0.021071
2021-11-30 12:59:02,636 iteration 2852 : loss : 0.051268, loss_ce: 0.018672
2021-11-30 12:59:04,216 iteration 2853 : loss : 0.035295, loss_ce: 0.016950
2021-11-30 12:59:05,738 iteration 2854 : loss : 0.041008, loss_ce: 0.019334
2021-11-30 12:59:07,169 iteration 2855 : loss : 0.035315, loss_ce: 0.018618
2021-11-30 12:59:08,706 iteration 2856 : loss : 0.064343, loss_ce: 0.035189
 42%|███████████▎               | 168/400 [1:17:11<1:43:48, 26.85s/it]2021-11-30 12:59:10,130 iteration 2857 : loss : 0.040257, loss_ce: 0.016321
2021-11-30 12:59:11,646 iteration 2858 : loss : 0.052900, loss_ce: 0.027217
2021-11-30 12:59:13,106 iteration 2859 : loss : 0.031985, loss_ce: 0.016232
2021-11-30 12:59:14,626 iteration 2860 : loss : 0.049893, loss_ce: 0.018887
2021-11-30 12:59:16,028 iteration 2861 : loss : 0.034487, loss_ce: 0.018639
2021-11-30 12:59:17,624 iteration 2862 : loss : 0.087996, loss_ce: 0.032486
2021-11-30 12:59:19,060 iteration 2863 : loss : 0.051978, loss_ce: 0.019361
2021-11-30 12:59:20,592 iteration 2864 : loss : 0.042759, loss_ce: 0.020536
2021-11-30 12:59:22,119 iteration 2865 : loss : 0.047342, loss_ce: 0.020983
2021-11-30 12:59:23,574 iteration 2866 : loss : 0.053926, loss_ce: 0.022986
2021-11-30 12:59:25,169 iteration 2867 : loss : 0.053178, loss_ce: 0.025113
2021-11-30 12:59:26,601 iteration 2868 : loss : 0.041344, loss_ce: 0.018017
2021-11-30 12:59:28,198 iteration 2869 : loss : 0.043950, loss_ce: 0.021420
2021-11-30 12:59:29,714 iteration 2870 : loss : 0.042741, loss_ce: 0.018079
2021-11-30 12:59:31,150 iteration 2871 : loss : 0.045345, loss_ce: 0.021743
2021-11-30 12:59:32,542 iteration 2872 : loss : 0.040768, loss_ce: 0.020358
2021-11-30 12:59:34,096 iteration 2873 : loss : 0.042449, loss_ce: 0.017223
 42%|███████████▍               | 169/400 [1:17:36<1:41:41, 26.41s/it]2021-11-30 12:59:35,664 iteration 2874 : loss : 0.042676, loss_ce: 0.017624
2021-11-30 12:59:37,169 iteration 2875 : loss : 0.037948, loss_ce: 0.018334
2021-11-30 12:59:38,668 iteration 2876 : loss : 0.055679, loss_ce: 0.026204
2021-11-30 12:59:40,224 iteration 2877 : loss : 0.064086, loss_ce: 0.024325
2021-11-30 12:59:41,735 iteration 2878 : loss : 0.038978, loss_ce: 0.021270
2021-11-30 12:59:43,291 iteration 2879 : loss : 0.047762, loss_ce: 0.018791
2021-11-30 12:59:44,834 iteration 2880 : loss : 0.038271, loss_ce: 0.016732
2021-11-30 12:59:46,323 iteration 2881 : loss : 0.040178, loss_ce: 0.019865
2021-11-30 12:59:47,878 iteration 2882 : loss : 0.041236, loss_ce: 0.021673
2021-11-30 12:59:49,352 iteration 2883 : loss : 0.074597, loss_ce: 0.038744
2021-11-30 12:59:50,817 iteration 2884 : loss : 0.039961, loss_ce: 0.021920
2021-11-30 12:59:52,327 iteration 2885 : loss : 0.039813, loss_ce: 0.020735
2021-11-30 12:59:53,742 iteration 2886 : loss : 0.054481, loss_ce: 0.021025
2021-11-30 12:59:55,305 iteration 2887 : loss : 0.050612, loss_ce: 0.027875
2021-11-30 12:59:56,810 iteration 2888 : loss : 0.073848, loss_ce: 0.020515
2021-11-30 12:59:58,429 iteration 2889 : loss : 0.036167, loss_ce: 0.016413
2021-11-30 12:59:58,429 Training Data Eval:
2021-11-30 13:00:05,790   Average segmentation loss on training set: 0.0262
2021-11-30 13:00:05,790 Validation Data Eval:
2021-11-30 13:00:08,328   Average segmentation loss on validation set: 0.1069
2021-11-30 13:00:09,902 iteration 2890 : loss : 0.055775, loss_ce: 0.022521
 42%|███████████▍               | 170/400 [1:18:12<1:52:02, 29.23s/it]2021-11-30 13:00:11,500 iteration 2891 : loss : 0.047364, loss_ce: 0.022128
2021-11-30 13:00:13,075 iteration 2892 : loss : 0.052251, loss_ce: 0.016593
2021-11-30 13:00:14,528 iteration 2893 : loss : 0.048931, loss_ce: 0.019624
2021-11-30 13:00:16,122 iteration 2894 : loss : 0.064686, loss_ce: 0.024832
2021-11-30 13:00:17,696 iteration 2895 : loss : 0.059644, loss_ce: 0.031418
2021-11-30 13:00:19,149 iteration 2896 : loss : 0.044672, loss_ce: 0.018496
2021-11-30 13:00:20,649 iteration 2897 : loss : 0.054883, loss_ce: 0.024733
2021-11-30 13:00:22,084 iteration 2898 : loss : 0.045955, loss_ce: 0.024346
2021-11-30 13:00:23,605 iteration 2899 : loss : 0.061787, loss_ce: 0.023898
2021-11-30 13:00:25,089 iteration 2900 : loss : 0.034125, loss_ce: 0.015759
2021-11-30 13:00:26,586 iteration 2901 : loss : 0.047709, loss_ce: 0.018396
2021-11-30 13:00:27,995 iteration 2902 : loss : 0.054656, loss_ce: 0.023120
2021-11-30 13:00:29,443 iteration 2903 : loss : 0.039783, loss_ce: 0.021928
2021-11-30 13:00:30,901 iteration 2904 : loss : 0.045652, loss_ce: 0.020118
2021-11-30 13:00:32,317 iteration 2905 : loss : 0.050575, loss_ce: 0.026176
2021-11-30 13:00:33,858 iteration 2906 : loss : 0.047095, loss_ce: 0.027683
2021-11-30 13:00:35,300 iteration 2907 : loss : 0.044727, loss_ce: 0.021016
 43%|███████████▌               | 171/400 [1:18:37<1:47:10, 28.08s/it]2021-11-30 13:00:36,871 iteration 2908 : loss : 0.055780, loss_ce: 0.029267
2021-11-30 13:00:38,432 iteration 2909 : loss : 0.050679, loss_ce: 0.027732
2021-11-30 13:00:39,914 iteration 2910 : loss : 0.050763, loss_ce: 0.020976
2021-11-30 13:00:41,464 iteration 2911 : loss : 0.053893, loss_ce: 0.024320
2021-11-30 13:00:42,937 iteration 2912 : loss : 0.038612, loss_ce: 0.018476
2021-11-30 13:00:44,508 iteration 2913 : loss : 0.047189, loss_ce: 0.022901
2021-11-30 13:00:45,957 iteration 2914 : loss : 0.045621, loss_ce: 0.019502
2021-11-30 13:00:47,427 iteration 2915 : loss : 0.041807, loss_ce: 0.019092
2021-11-30 13:00:48,927 iteration 2916 : loss : 0.041839, loss_ce: 0.019894
2021-11-30 13:00:50,350 iteration 2917 : loss : 0.031696, loss_ce: 0.016689
2021-11-30 13:00:51,802 iteration 2918 : loss : 0.050268, loss_ce: 0.017677
2021-11-30 13:00:53,289 iteration 2919 : loss : 0.038646, loss_ce: 0.015879
2021-11-30 13:00:54,794 iteration 2920 : loss : 0.043641, loss_ce: 0.020807
2021-11-30 13:00:56,284 iteration 2921 : loss : 0.043359, loss_ce: 0.022926
2021-11-30 13:00:57,699 iteration 2922 : loss : 0.062218, loss_ce: 0.024336
2021-11-30 13:00:59,172 iteration 2923 : loss : 0.038223, loss_ce: 0.017100
2021-11-30 13:01:00,670 iteration 2924 : loss : 0.065541, loss_ce: 0.029446
 43%|███████████▌               | 172/400 [1:19:03<1:43:37, 27.27s/it]2021-11-30 13:01:02,097 iteration 2925 : loss : 0.037153, loss_ce: 0.020854
2021-11-30 13:01:03,673 iteration 2926 : loss : 0.065253, loss_ce: 0.030589
2021-11-30 13:01:05,158 iteration 2927 : loss : 0.038553, loss_ce: 0.018627
2021-11-30 13:01:06,519 iteration 2928 : loss : 0.050886, loss_ce: 0.019902
2021-11-30 13:01:07,994 iteration 2929 : loss : 0.045340, loss_ce: 0.021330
2021-11-30 13:01:09,566 iteration 2930 : loss : 0.057260, loss_ce: 0.028254
2021-11-30 13:01:11,019 iteration 2931 : loss : 0.046806, loss_ce: 0.019115
2021-11-30 13:01:12,454 iteration 2932 : loss : 0.038295, loss_ce: 0.018132
2021-11-30 13:01:14,025 iteration 2933 : loss : 0.045091, loss_ce: 0.017495
2021-11-30 13:01:15,512 iteration 2934 : loss : 0.042665, loss_ce: 0.017167
2021-11-30 13:01:17,008 iteration 2935 : loss : 0.040876, loss_ce: 0.016911
2021-11-30 13:01:18,472 iteration 2936 : loss : 0.031154, loss_ce: 0.013606
2021-11-30 13:01:19,966 iteration 2937 : loss : 0.040416, loss_ce: 0.022095
2021-11-30 13:01:21,472 iteration 2938 : loss : 0.053996, loss_ce: 0.022927
2021-11-30 13:01:23,009 iteration 2939 : loss : 0.051784, loss_ce: 0.022221
2021-11-30 13:01:24,607 iteration 2940 : loss : 0.052294, loss_ce: 0.027485
2021-11-30 13:01:26,018 iteration 2941 : loss : 0.034209, loss_ce: 0.015008
 43%|███████████▋               | 173/400 [1:19:28<1:40:58, 26.69s/it]2021-11-30 13:01:27,488 iteration 2942 : loss : 0.032419, loss_ce: 0.014041
2021-11-30 13:01:29,023 iteration 2943 : loss : 0.054765, loss_ce: 0.019113
2021-11-30 13:01:30,458 iteration 2944 : loss : 0.038989, loss_ce: 0.021267
2021-11-30 13:01:31,953 iteration 2945 : loss : 0.040181, loss_ce: 0.016311
2021-11-30 13:01:33,340 iteration 2946 : loss : 0.042026, loss_ce: 0.018702
2021-11-30 13:01:34,789 iteration 2947 : loss : 0.041135, loss_ce: 0.019504
2021-11-30 13:01:36,224 iteration 2948 : loss : 0.033846, loss_ce: 0.013705
2021-11-30 13:01:37,786 iteration 2949 : loss : 0.049806, loss_ce: 0.020714
2021-11-30 13:01:39,294 iteration 2950 : loss : 0.034100, loss_ce: 0.017189
2021-11-30 13:01:40,842 iteration 2951 : loss : 0.038455, loss_ce: 0.019702
2021-11-30 13:01:42,365 iteration 2952 : loss : 0.032124, loss_ce: 0.014506
2021-11-30 13:01:43,908 iteration 2953 : loss : 0.048073, loss_ce: 0.024941
2021-11-30 13:01:45,507 iteration 2954 : loss : 0.096649, loss_ce: 0.029345
2021-11-30 13:01:46,946 iteration 2955 : loss : 0.043085, loss_ce: 0.021390
2021-11-30 13:01:48,384 iteration 2956 : loss : 0.037050, loss_ce: 0.018073
2021-11-30 13:01:49,802 iteration 2957 : loss : 0.050262, loss_ce: 0.021530
2021-11-30 13:01:51,292 iteration 2958 : loss : 0.048148, loss_ce: 0.026302
 44%|███████████▋               | 174/400 [1:19:53<1:38:56, 26.27s/it]2021-11-30 13:01:52,821 iteration 2959 : loss : 0.029586, loss_ce: 0.013949
2021-11-30 13:01:54,300 iteration 2960 : loss : 0.038258, loss_ce: 0.019599
2021-11-30 13:01:55,798 iteration 2961 : loss : 0.044363, loss_ce: 0.019651
2021-11-30 13:01:57,457 iteration 2962 : loss : 0.040081, loss_ce: 0.017974
2021-11-30 13:01:58,974 iteration 2963 : loss : 0.081726, loss_ce: 0.029412
2021-11-30 13:02:00,363 iteration 2964 : loss : 0.040120, loss_ce: 0.018750
2021-11-30 13:02:01,854 iteration 2965 : loss : 0.037583, loss_ce: 0.020093
2021-11-30 13:02:03,398 iteration 2966 : loss : 0.060989, loss_ce: 0.025191
2021-11-30 13:02:04,862 iteration 2967 : loss : 0.064605, loss_ce: 0.028088
2021-11-30 13:02:06,361 iteration 2968 : loss : 0.040517, loss_ce: 0.018696
2021-11-30 13:02:07,827 iteration 2969 : loss : 0.040875, loss_ce: 0.020622
2021-11-30 13:02:09,307 iteration 2970 : loss : 0.053980, loss_ce: 0.022839
2021-11-30 13:02:10,792 iteration 2971 : loss : 0.042441, loss_ce: 0.018754
2021-11-30 13:02:12,281 iteration 2972 : loss : 0.044605, loss_ce: 0.022297
2021-11-30 13:02:13,697 iteration 2973 : loss : 0.040766, loss_ce: 0.014657
2021-11-30 13:02:15,246 iteration 2974 : loss : 0.054923, loss_ce: 0.020075
2021-11-30 13:02:15,247 Training Data Eval:
2021-11-30 13:02:22,619   Average segmentation loss on training set: 0.0264
2021-11-30 13:02:22,619 Validation Data Eval:
2021-11-30 13:02:25,170   Average segmentation loss on validation set: 0.1068
2021-11-30 13:02:26,644 iteration 2975 : loss : 0.039667, loss_ce: 0.017125
 44%|███████████▊               | 175/400 [1:20:28<1:48:43, 28.99s/it]2021-11-30 13:02:28,103 iteration 2976 : loss : 0.035209, loss_ce: 0.018472
2021-11-30 13:02:29,634 iteration 2977 : loss : 0.040643, loss_ce: 0.022964
2021-11-30 13:02:31,135 iteration 2978 : loss : 0.043526, loss_ce: 0.019239
2021-11-30 13:02:32,687 iteration 2979 : loss : 0.061750, loss_ce: 0.023241
2021-11-30 13:02:34,203 iteration 2980 : loss : 0.055522, loss_ce: 0.021329
2021-11-30 13:02:35,814 iteration 2981 : loss : 0.064659, loss_ce: 0.032281
2021-11-30 13:02:37,302 iteration 2982 : loss : 0.027982, loss_ce: 0.013986
2021-11-30 13:02:38,827 iteration 2983 : loss : 0.042499, loss_ce: 0.018614
2021-11-30 13:02:40,362 iteration 2984 : loss : 0.046155, loss_ce: 0.019585
2021-11-30 13:02:41,894 iteration 2985 : loss : 0.054512, loss_ce: 0.023297
2021-11-30 13:02:43,308 iteration 2986 : loss : 0.041694, loss_ce: 0.020702
2021-11-30 13:02:44,801 iteration 2987 : loss : 0.042084, loss_ce: 0.018275
2021-11-30 13:02:46,347 iteration 2988 : loss : 0.044298, loss_ce: 0.021539
2021-11-30 13:02:47,880 iteration 2989 : loss : 0.060044, loss_ce: 0.025388
2021-11-30 13:02:49,308 iteration 2990 : loss : 0.041662, loss_ce: 0.020326
2021-11-30 13:02:50,794 iteration 2991 : loss : 0.035519, loss_ce: 0.017158
2021-11-30 13:02:52,363 iteration 2992 : loss : 0.046518, loss_ce: 0.021247
 44%|███████████▉               | 176/400 [1:20:54<1:44:34, 28.01s/it]2021-11-30 13:02:53,838 iteration 2993 : loss : 0.049757, loss_ce: 0.024481
2021-11-30 13:02:55,345 iteration 2994 : loss : 0.047522, loss_ce: 0.016157
2021-11-30 13:02:56,822 iteration 2995 : loss : 0.047631, loss_ce: 0.019438
2021-11-30 13:02:58,313 iteration 2996 : loss : 0.035706, loss_ce: 0.019464
2021-11-30 13:02:59,784 iteration 2997 : loss : 0.060312, loss_ce: 0.020660
2021-11-30 13:03:01,278 iteration 2998 : loss : 0.050854, loss_ce: 0.023645
2021-11-30 13:03:02,837 iteration 2999 : loss : 0.081030, loss_ce: 0.020981
2021-11-30 13:03:04,414 iteration 3000 : loss : 0.041905, loss_ce: 0.021369
2021-11-30 13:03:05,972 iteration 3001 : loss : 0.087644, loss_ce: 0.023360
2021-11-30 13:03:07,396 iteration 3002 : loss : 0.034334, loss_ce: 0.016947
2021-11-30 13:03:08,851 iteration 3003 : loss : 0.036847, loss_ce: 0.016984
2021-11-30 13:03:10,306 iteration 3004 : loss : 0.036428, loss_ce: 0.018533
2021-11-30 13:03:11,799 iteration 3005 : loss : 0.039781, loss_ce: 0.019832
2021-11-30 13:03:13,325 iteration 3006 : loss : 0.043013, loss_ce: 0.019814
2021-11-30 13:03:14,839 iteration 3007 : loss : 0.060823, loss_ce: 0.028160
2021-11-30 13:03:16,381 iteration 3008 : loss : 0.044508, loss_ce: 0.018589
2021-11-30 13:03:17,796 iteration 3009 : loss : 0.048685, loss_ce: 0.021527
 44%|███████████▉               | 177/400 [1:21:20<1:41:13, 27.24s/it]2021-11-30 13:03:19,453 iteration 3010 : loss : 0.061703, loss_ce: 0.023240
2021-11-30 13:03:20,971 iteration 3011 : loss : 0.049598, loss_ce: 0.029255
2021-11-30 13:03:22,402 iteration 3012 : loss : 0.035252, loss_ce: 0.017686
2021-11-30 13:03:23,803 iteration 3013 : loss : 0.073489, loss_ce: 0.026044
2021-11-30 13:03:25,288 iteration 3014 : loss : 0.052080, loss_ce: 0.024051
2021-11-30 13:03:26,753 iteration 3015 : loss : 0.047744, loss_ce: 0.017649
2021-11-30 13:03:28,229 iteration 3016 : loss : 0.047631, loss_ce: 0.020430
2021-11-30 13:03:29,689 iteration 3017 : loss : 0.050005, loss_ce: 0.027720
2021-11-30 13:03:31,147 iteration 3018 : loss : 0.031613, loss_ce: 0.016391
2021-11-30 13:03:32,576 iteration 3019 : loss : 0.043119, loss_ce: 0.018186
2021-11-30 13:03:34,106 iteration 3020 : loss : 0.044327, loss_ce: 0.018702
2021-11-30 13:03:35,612 iteration 3021 : loss : 0.031524, loss_ce: 0.014679
2021-11-30 13:03:37,068 iteration 3022 : loss : 0.037494, loss_ce: 0.018024
2021-11-30 13:03:38,571 iteration 3023 : loss : 0.046356, loss_ce: 0.019744
2021-11-30 13:03:40,150 iteration 3024 : loss : 0.059636, loss_ce: 0.026232
2021-11-30 13:03:41,703 iteration 3025 : loss : 0.082030, loss_ce: 0.036266
2021-11-30 13:03:43,149 iteration 3026 : loss : 0.039211, loss_ce: 0.019131
 44%|████████████               | 178/400 [1:21:45<1:38:40, 26.67s/it]2021-11-30 13:03:44,738 iteration 3027 : loss : 0.052968, loss_ce: 0.022545
2021-11-30 13:03:46,165 iteration 3028 : loss : 0.045572, loss_ce: 0.022100
2021-11-30 13:03:47,669 iteration 3029 : loss : 0.042789, loss_ce: 0.018638
2021-11-30 13:03:49,126 iteration 3030 : loss : 0.035603, loss_ce: 0.018835
2021-11-30 13:03:50,622 iteration 3031 : loss : 0.055067, loss_ce: 0.023949
2021-11-30 13:03:52,062 iteration 3032 : loss : 0.035752, loss_ce: 0.016239
2021-11-30 13:03:53,588 iteration 3033 : loss : 0.046872, loss_ce: 0.019085
2021-11-30 13:03:55,135 iteration 3034 : loss : 0.037606, loss_ce: 0.017211
2021-11-30 13:03:56,641 iteration 3035 : loss : 0.039459, loss_ce: 0.020596
2021-11-30 13:03:58,109 iteration 3036 : loss : 0.056339, loss_ce: 0.032543
2021-11-30 13:03:59,599 iteration 3037 : loss : 0.043562, loss_ce: 0.020143
2021-11-30 13:04:01,136 iteration 3038 : loss : 0.045741, loss_ce: 0.019202
2021-11-30 13:04:02,559 iteration 3039 : loss : 0.047923, loss_ce: 0.020184
2021-11-30 13:04:04,056 iteration 3040 : loss : 0.054721, loss_ce: 0.020235
2021-11-30 13:04:05,596 iteration 3041 : loss : 0.033873, loss_ce: 0.017464
2021-11-30 13:04:07,060 iteration 3042 : loss : 0.039463, loss_ce: 0.020019
2021-11-30 13:04:08,736 iteration 3043 : loss : 0.051992, loss_ce: 0.019877
 45%|████████████               | 179/400 [1:22:11<1:37:02, 26.34s/it]2021-11-30 13:04:10,277 iteration 3044 : loss : 0.041220, loss_ce: 0.017619
2021-11-30 13:04:11,697 iteration 3045 : loss : 0.035604, loss_ce: 0.016548
2021-11-30 13:04:13,172 iteration 3046 : loss : 0.031890, loss_ce: 0.014892
2021-11-30 13:04:14,787 iteration 3047 : loss : 0.049355, loss_ce: 0.027317
2021-11-30 13:04:16,214 iteration 3048 : loss : 0.039205, loss_ce: 0.017587
2021-11-30 13:04:17,706 iteration 3049 : loss : 0.054758, loss_ce: 0.025404
2021-11-30 13:04:19,243 iteration 3050 : loss : 0.035743, loss_ce: 0.019141
2021-11-30 13:04:20,758 iteration 3051 : loss : 0.050762, loss_ce: 0.017506
2021-11-30 13:04:22,228 iteration 3052 : loss : 0.039344, loss_ce: 0.017618
2021-11-30 13:04:23,816 iteration 3053 : loss : 0.028733, loss_ce: 0.015741
2021-11-30 13:04:25,234 iteration 3054 : loss : 0.035097, loss_ce: 0.014079
2021-11-30 13:04:26,740 iteration 3055 : loss : 0.054105, loss_ce: 0.023264
2021-11-30 13:04:28,203 iteration 3056 : loss : 0.037775, loss_ce: 0.018301
2021-11-30 13:04:29,626 iteration 3057 : loss : 0.041282, loss_ce: 0.022717
2021-11-30 13:04:31,138 iteration 3058 : loss : 0.033815, loss_ce: 0.017173
2021-11-30 13:04:32,676 iteration 3059 : loss : 0.033443, loss_ce: 0.013353
2021-11-30 13:04:32,676 Training Data Eval:
2021-11-30 13:04:40,033   Average segmentation loss on training set: 0.0252
2021-11-30 13:04:40,033 Validation Data Eval:
2021-11-30 13:04:42,582   Average segmentation loss on validation set: 0.0768
2021-11-30 13:04:44,517 Found new lowest validation loss at iteration 3059! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/UNET_RUNMC/best_val_loss.pth
2021-11-30 13:04:45,953 iteration 3060 : loss : 0.037770, loss_ce: 0.019312
 45%|████████████▏              | 180/400 [1:22:48<1:48:33, 29.61s/it]2021-11-30 13:04:47,349 iteration 3061 : loss : 0.034159, loss_ce: 0.015480
2021-11-30 13:04:48,789 iteration 3062 : loss : 0.035054, loss_ce: 0.015880
2021-11-30 13:04:50,277 iteration 3063 : loss : 0.055373, loss_ce: 0.022976
2021-11-30 13:04:51,771 iteration 3064 : loss : 0.039967, loss_ce: 0.020123
2021-11-30 13:04:53,114 iteration 3065 : loss : 0.027717, loss_ce: 0.013558
2021-11-30 13:04:54,523 iteration 3066 : loss : 0.045579, loss_ce: 0.025714
2021-11-30 13:04:56,017 iteration 3067 : loss : 0.052739, loss_ce: 0.024915
2021-11-30 13:04:57,464 iteration 3068 : loss : 0.056453, loss_ce: 0.019787
2021-11-30 13:04:58,928 iteration 3069 : loss : 0.054920, loss_ce: 0.024633
2021-11-30 13:05:00,425 iteration 3070 : loss : 0.039482, loss_ce: 0.019929
2021-11-30 13:05:01,865 iteration 3071 : loss : 0.036807, loss_ce: 0.018224
2021-11-30 13:05:03,310 iteration 3072 : loss : 0.030645, loss_ce: 0.015538
2021-11-30 13:05:04,781 iteration 3073 : loss : 0.043286, loss_ce: 0.020061
2021-11-30 13:05:06,355 iteration 3074 : loss : 0.038551, loss_ce: 0.017626
2021-11-30 13:05:07,870 iteration 3075 : loss : 0.043931, loss_ce: 0.019048
2021-11-30 13:05:09,421 iteration 3076 : loss : 0.058309, loss_ce: 0.019726
2021-11-30 13:05:10,861 iteration 3077 : loss : 0.047977, loss_ce: 0.020038
 45%|████████████▏              | 181/400 [1:23:13<1:42:55, 28.20s/it]2021-11-30 13:05:12,342 iteration 3078 : loss : 0.037621, loss_ce: 0.019249
2021-11-30 13:05:13,962 iteration 3079 : loss : 0.034468, loss_ce: 0.014854
2021-11-30 13:05:15,362 iteration 3080 : loss : 0.038041, loss_ce: 0.014924
2021-11-30 13:05:16,873 iteration 3081 : loss : 0.047793, loss_ce: 0.018299
2021-11-30 13:05:18,342 iteration 3082 : loss : 0.050953, loss_ce: 0.015324
2021-11-30 13:05:19,885 iteration 3083 : loss : 0.040771, loss_ce: 0.020351
2021-11-30 13:05:21,238 iteration 3084 : loss : 0.033120, loss_ce: 0.016719
2021-11-30 13:05:22,752 iteration 3085 : loss : 0.036845, loss_ce: 0.018373
2021-11-30 13:05:24,348 iteration 3086 : loss : 0.040241, loss_ce: 0.022962
2021-11-30 13:05:25,813 iteration 3087 : loss : 0.057818, loss_ce: 0.026157
2021-11-30 13:05:27,318 iteration 3088 : loss : 0.029685, loss_ce: 0.012926
2021-11-30 13:05:28,907 iteration 3089 : loss : 0.049774, loss_ce: 0.021849
2021-11-30 13:05:30,302 iteration 3090 : loss : 0.036775, loss_ce: 0.019844
2021-11-30 13:05:31,788 iteration 3091 : loss : 0.045537, loss_ce: 0.023535
2021-11-30 13:05:33,423 iteration 3092 : loss : 0.047169, loss_ce: 0.021814
2021-11-30 13:05:34,860 iteration 3093 : loss : 0.031599, loss_ce: 0.015459
2021-11-30 13:05:36,346 iteration 3094 : loss : 0.034658, loss_ce: 0.016329
 46%|████████████▎              | 182/400 [1:23:38<1:39:29, 27.38s/it]2021-11-30 13:05:37,868 iteration 3095 : loss : 0.033485, loss_ce: 0.015800
2021-11-30 13:05:39,365 iteration 3096 : loss : 0.033326, loss_ce: 0.017124
2021-11-30 13:05:40,837 iteration 3097 : loss : 0.034888, loss_ce: 0.016382
2021-11-30 13:05:42,280 iteration 3098 : loss : 0.031470, loss_ce: 0.016772
2021-11-30 13:05:43,761 iteration 3099 : loss : 0.031360, loss_ce: 0.013992
2021-11-30 13:05:45,181 iteration 3100 : loss : 0.040744, loss_ce: 0.017573
2021-11-30 13:05:46,711 iteration 3101 : loss : 0.033900, loss_ce: 0.015985
2021-11-30 13:05:48,197 iteration 3102 : loss : 0.046118, loss_ce: 0.020020
2021-11-30 13:05:49,679 iteration 3103 : loss : 0.055175, loss_ce: 0.023568
2021-11-30 13:05:51,115 iteration 3104 : loss : 0.034342, loss_ce: 0.017893
2021-11-30 13:05:52,537 iteration 3105 : loss : 0.031740, loss_ce: 0.018503
2021-11-30 13:05:54,008 iteration 3106 : loss : 0.041602, loss_ce: 0.022337
2021-11-30 13:05:55,484 iteration 3107 : loss : 0.028691, loss_ce: 0.012787
2021-11-30 13:05:57,020 iteration 3108 : loss : 0.047134, loss_ce: 0.019802
2021-11-30 13:05:58,534 iteration 3109 : loss : 0.045729, loss_ce: 0.019443
2021-11-30 13:06:00,047 iteration 3110 : loss : 0.087740, loss_ce: 0.039577
2021-11-30 13:06:01,553 iteration 3111 : loss : 0.042789, loss_ce: 0.019596
 46%|████████████▎              | 183/400 [1:24:03<1:36:40, 26.73s/it]2021-11-30 13:06:03,096 iteration 3112 : loss : 0.037420, loss_ce: 0.017049
2021-11-30 13:06:04,531 iteration 3113 : loss : 0.038307, loss_ce: 0.018053
2021-11-30 13:06:06,065 iteration 3114 : loss : 0.046453, loss_ce: 0.019405
2021-11-30 13:06:07,560 iteration 3115 : loss : 0.061667, loss_ce: 0.032401
2021-11-30 13:06:09,027 iteration 3116 : loss : 0.035796, loss_ce: 0.019027
2021-11-30 13:06:10,590 iteration 3117 : loss : 0.033146, loss_ce: 0.015937
2021-11-30 13:06:12,067 iteration 3118 : loss : 0.043121, loss_ce: 0.021847
2021-11-30 13:06:13,661 iteration 3119 : loss : 0.058675, loss_ce: 0.021596
2021-11-30 13:06:15,097 iteration 3120 : loss : 0.033771, loss_ce: 0.016451
2021-11-30 13:06:16,645 iteration 3121 : loss : 0.036363, loss_ce: 0.019319
2021-11-30 13:06:18,102 iteration 3122 : loss : 0.056739, loss_ce: 0.017766
2021-11-30 13:06:19,652 iteration 3123 : loss : 0.041681, loss_ce: 0.020397
2021-11-30 13:06:21,057 iteration 3124 : loss : 0.032292, loss_ce: 0.015607
2021-11-30 13:06:22,541 iteration 3125 : loss : 0.046941, loss_ce: 0.017948
2021-11-30 13:06:24,106 iteration 3126 : loss : 0.036557, loss_ce: 0.022272
2021-11-30 13:06:25,664 iteration 3127 : loss : 0.039750, loss_ce: 0.017988
2021-11-30 13:06:27,224 iteration 3128 : loss : 0.043359, loss_ce: 0.017810
 46%|████████████▍              | 184/400 [1:24:29<1:35:05, 26.42s/it]2021-11-30 13:06:28,709 iteration 3129 : loss : 0.044169, loss_ce: 0.018826
2021-11-30 13:06:30,293 iteration 3130 : loss : 0.047614, loss_ce: 0.022541
2021-11-30 13:06:31,754 iteration 3131 : loss : 0.037599, loss_ce: 0.015472
2021-11-30 13:06:33,250 iteration 3132 : loss : 0.040984, loss_ce: 0.016045
2021-11-30 13:06:34,743 iteration 3133 : loss : 0.063708, loss_ce: 0.025593
2021-11-30 13:06:36,168 iteration 3134 : loss : 0.030429, loss_ce: 0.017026
2021-11-30 13:06:37,645 iteration 3135 : loss : 0.045357, loss_ce: 0.024101
2021-11-30 13:06:39,095 iteration 3136 : loss : 0.035187, loss_ce: 0.018300
2021-11-30 13:06:40,713 iteration 3137 : loss : 0.039171, loss_ce: 0.019079
2021-11-30 13:06:42,275 iteration 3138 : loss : 0.059076, loss_ce: 0.027219
2021-11-30 13:06:43,697 iteration 3139 : loss : 0.044893, loss_ce: 0.019298
2021-11-30 13:06:45,117 iteration 3140 : loss : 0.048665, loss_ce: 0.024226
2021-11-30 13:06:46,625 iteration 3141 : loss : 0.053527, loss_ce: 0.025010
2021-11-30 13:06:48,088 iteration 3142 : loss : 0.042739, loss_ce: 0.017857
2021-11-30 13:06:49,615 iteration 3143 : loss : 0.038398, loss_ce: 0.016842
2021-11-30 13:06:51,079 iteration 3144 : loss : 0.041730, loss_ce: 0.020743
2021-11-30 13:06:51,079 Training Data Eval:
2021-11-30 13:06:58,447   Average segmentation loss on training set: 0.0240
2021-11-30 13:06:58,447 Validation Data Eval:
2021-11-30 13:07:01,007   Average segmentation loss on validation set: 0.1041
2021-11-30 13:07:02,455 iteration 3145 : loss : 0.033035, loss_ce: 0.015867
 46%|████████████▍              | 185/400 [1:25:04<1:44:15, 29.10s/it]2021-11-30 13:07:04,290 iteration 3146 : loss : 0.060388, loss_ce: 0.023580
2021-11-30 13:07:05,818 iteration 3147 : loss : 0.051975, loss_ce: 0.025773
2021-11-30 13:07:07,234 iteration 3148 : loss : 0.036986, loss_ce: 0.016981
2021-11-30 13:07:08,768 iteration 3149 : loss : 0.037286, loss_ce: 0.015227
2021-11-30 13:07:10,289 iteration 3150 : loss : 0.043916, loss_ce: 0.020514
2021-11-30 13:07:11,806 iteration 3151 : loss : 0.062766, loss_ce: 0.024652
2021-11-30 13:07:13,319 iteration 3152 : loss : 0.034750, loss_ce: 0.015502
2021-11-30 13:07:14,825 iteration 3153 : loss : 0.044980, loss_ce: 0.022279
2021-11-30 13:07:16,389 iteration 3154 : loss : 0.044416, loss_ce: 0.021742
2021-11-30 13:07:17,924 iteration 3155 : loss : 0.035674, loss_ce: 0.016000
2021-11-30 13:07:19,337 iteration 3156 : loss : 0.047186, loss_ce: 0.018398
2021-11-30 13:07:20,847 iteration 3157 : loss : 0.051193, loss_ce: 0.024538
2021-11-30 13:07:22,294 iteration 3158 : loss : 0.061462, loss_ce: 0.022339
2021-11-30 13:07:23,826 iteration 3159 : loss : 0.056352, loss_ce: 0.030966
2021-11-30 13:07:25,353 iteration 3160 : loss : 0.036145, loss_ce: 0.019050
2021-11-30 13:07:26,925 iteration 3161 : loss : 0.033964, loss_ce: 0.015816
2021-11-30 13:07:28,425 iteration 3162 : loss : 0.050570, loss_ce: 0.020760
 46%|████████████▌              | 186/400 [1:25:30<1:40:18, 28.12s/it]2021-11-30 13:07:29,927 iteration 3163 : loss : 0.041560, loss_ce: 0.018632
2021-11-30 13:07:31,517 iteration 3164 : loss : 0.056462, loss_ce: 0.027327
2021-11-30 13:07:33,054 iteration 3165 : loss : 0.042646, loss_ce: 0.019331
2021-11-30 13:07:34,502 iteration 3166 : loss : 0.042982, loss_ce: 0.020159
2021-11-30 13:07:36,052 iteration 3167 : loss : 0.038064, loss_ce: 0.017300
2021-11-30 13:07:37,536 iteration 3168 : loss : 0.035417, loss_ce: 0.017429
2021-11-30 13:07:39,045 iteration 3169 : loss : 0.047372, loss_ce: 0.024175
2021-11-30 13:07:40,527 iteration 3170 : loss : 0.046841, loss_ce: 0.023610
2021-11-30 13:07:41,902 iteration 3171 : loss : 0.037790, loss_ce: 0.014471
2021-11-30 13:07:43,389 iteration 3172 : loss : 0.041174, loss_ce: 0.019458
2021-11-30 13:07:44,881 iteration 3173 : loss : 0.060819, loss_ce: 0.024035
2021-11-30 13:07:46,336 iteration 3174 : loss : 0.055417, loss_ce: 0.025435
2021-11-30 13:07:47,763 iteration 3175 : loss : 0.042674, loss_ce: 0.018101
2021-11-30 13:07:49,334 iteration 3176 : loss : 0.067454, loss_ce: 0.025493
2021-11-30 13:07:50,820 iteration 3177 : loss : 0.052749, loss_ce: 0.023044
2021-11-30 13:07:52,412 iteration 3178 : loss : 0.048002, loss_ce: 0.016644
2021-11-30 13:07:53,896 iteration 3179 : loss : 0.039814, loss_ce: 0.021193
 47%|████████████▌              | 187/400 [1:25:56<1:37:00, 27.33s/it]2021-11-30 13:07:55,456 iteration 3180 : loss : 0.070484, loss_ce: 0.031585
2021-11-30 13:07:56,904 iteration 3181 : loss : 0.041888, loss_ce: 0.020614
2021-11-30 13:07:58,333 iteration 3182 : loss : 0.039265, loss_ce: 0.021695
2021-11-30 13:07:59,897 iteration 3183 : loss : 0.058858, loss_ce: 0.023931
2021-11-30 13:08:01,383 iteration 3184 : loss : 0.038295, loss_ce: 0.016874
2021-11-30 13:08:02,896 iteration 3185 : loss : 0.036174, loss_ce: 0.015722
2021-11-30 13:08:04,425 iteration 3186 : loss : 0.041916, loss_ce: 0.020292
2021-11-30 13:08:05,961 iteration 3187 : loss : 0.033081, loss_ce: 0.015926
2021-11-30 13:08:07,523 iteration 3188 : loss : 0.039187, loss_ce: 0.017112
2021-11-30 13:08:08,995 iteration 3189 : loss : 0.037401, loss_ce: 0.017058
2021-11-30 13:08:10,593 iteration 3190 : loss : 0.047731, loss_ce: 0.020239
2021-11-30 13:08:12,132 iteration 3191 : loss : 0.048099, loss_ce: 0.021244
2021-11-30 13:08:13,544 iteration 3192 : loss : 0.061475, loss_ce: 0.018895
2021-11-30 13:08:15,029 iteration 3193 : loss : 0.033334, loss_ce: 0.016194
2021-11-30 13:08:16,658 iteration 3194 : loss : 0.047166, loss_ce: 0.021313
2021-11-30 13:08:18,148 iteration 3195 : loss : 0.038921, loss_ce: 0.021328
2021-11-30 13:08:19,622 iteration 3196 : loss : 0.048559, loss_ce: 0.025490
 47%|████████████▋              | 188/400 [1:26:21<1:34:51, 26.85s/it]2021-11-30 13:08:21,147 iteration 3197 : loss : 0.030715, loss_ce: 0.015764
2021-11-30 13:08:22,685 iteration 3198 : loss : 0.047160, loss_ce: 0.022724
2021-11-30 13:08:24,210 iteration 3199 : loss : 0.033967, loss_ce: 0.013507
2021-11-30 13:08:25,759 iteration 3200 : loss : 0.042745, loss_ce: 0.016622
2021-11-30 13:08:27,308 iteration 3201 : loss : 0.050668, loss_ce: 0.022475
2021-11-30 13:08:28,844 iteration 3202 : loss : 0.049078, loss_ce: 0.018436
2021-11-30 13:08:30,239 iteration 3203 : loss : 0.037198, loss_ce: 0.019672
2021-11-30 13:08:31,640 iteration 3204 : loss : 0.028935, loss_ce: 0.015465
2021-11-30 13:08:33,046 iteration 3205 : loss : 0.043872, loss_ce: 0.018200
2021-11-30 13:08:34,419 iteration 3206 : loss : 0.029025, loss_ce: 0.013047
2021-11-30 13:08:35,956 iteration 3207 : loss : 0.053764, loss_ce: 0.023397
2021-11-30 13:08:37,453 iteration 3208 : loss : 0.039220, loss_ce: 0.017694
2021-11-30 13:08:38,932 iteration 3209 : loss : 0.042473, loss_ce: 0.017551
2021-11-30 13:08:40,445 iteration 3210 : loss : 0.042835, loss_ce: 0.022408
2021-11-30 13:08:41,857 iteration 3211 : loss : 0.034953, loss_ce: 0.014320
2021-11-30 13:08:43,420 iteration 3212 : loss : 0.047653, loss_ce: 0.021811
2021-11-30 13:08:44,989 iteration 3213 : loss : 0.039466, loss_ce: 0.016870
 47%|████████████▊              | 189/400 [1:26:47<1:32:49, 26.40s/it]2021-11-30 13:08:46,471 iteration 3214 : loss : 0.027630, loss_ce: 0.015838
2021-11-30 13:08:47,966 iteration 3215 : loss : 0.039669, loss_ce: 0.020305
2021-11-30 13:08:49,439 iteration 3216 : loss : 0.060176, loss_ce: 0.023853
2021-11-30 13:08:50,964 iteration 3217 : loss : 0.051028, loss_ce: 0.023860
2021-11-30 13:08:52,469 iteration 3218 : loss : 0.050526, loss_ce: 0.020990
2021-11-30 13:08:53,949 iteration 3219 : loss : 0.047916, loss_ce: 0.021456
2021-11-30 13:08:55,373 iteration 3220 : loss : 0.034010, loss_ce: 0.018406
2021-11-30 13:08:56,953 iteration 3221 : loss : 0.055320, loss_ce: 0.027374
2021-11-30 13:08:58,419 iteration 3222 : loss : 0.036125, loss_ce: 0.013049
2021-11-30 13:08:59,936 iteration 3223 : loss : 0.030980, loss_ce: 0.014799
2021-11-30 13:09:01,508 iteration 3224 : loss : 0.055457, loss_ce: 0.019709
2021-11-30 13:09:02,978 iteration 3225 : loss : 0.056636, loss_ce: 0.029215
2021-11-30 13:09:04,453 iteration 3226 : loss : 0.044374, loss_ce: 0.020106
2021-11-30 13:09:05,949 iteration 3227 : loss : 0.045152, loss_ce: 0.021574
2021-11-30 13:09:07,406 iteration 3228 : loss : 0.044103, loss_ce: 0.017085
2021-11-30 13:09:08,838 iteration 3229 : loss : 0.044726, loss_ce: 0.017059
2021-11-30 13:09:08,838 Training Data Eval:
2021-11-30 13:09:16,213   Average segmentation loss on training set: 0.0234
2021-11-30 13:09:16,213 Validation Data Eval:
2021-11-30 13:09:18,789   Average segmentation loss on validation set: 0.0951
2021-11-30 13:09:20,211 iteration 3230 : loss : 0.039911, loss_ce: 0.017791
 48%|████████████▊              | 190/400 [1:27:22<1:41:39, 29.05s/it]2021-11-30 13:09:21,697 iteration 3231 : loss : 0.047844, loss_ce: 0.014489
2021-11-30 13:09:23,170 iteration 3232 : loss : 0.029679, loss_ce: 0.013505
2021-11-30 13:09:24,660 iteration 3233 : loss : 0.044372, loss_ce: 0.023779
2021-11-30 13:09:26,202 iteration 3234 : loss : 0.035898, loss_ce: 0.014519
2021-11-30 13:09:27,726 iteration 3235 : loss : 0.061075, loss_ce: 0.020940
2021-11-30 13:09:29,148 iteration 3236 : loss : 0.031847, loss_ce: 0.018799
2021-11-30 13:09:30,743 iteration 3237 : loss : 0.041484, loss_ce: 0.020159
2021-11-30 13:09:32,236 iteration 3238 : loss : 0.038699, loss_ce: 0.020859
2021-11-30 13:09:33,724 iteration 3239 : loss : 0.027081, loss_ce: 0.013893
2021-11-30 13:09:35,149 iteration 3240 : loss : 0.061267, loss_ce: 0.020287
2021-11-30 13:09:36,545 iteration 3241 : loss : 0.052325, loss_ce: 0.014968
2021-11-30 13:09:38,055 iteration 3242 : loss : 0.048155, loss_ce: 0.021393
2021-11-30 13:09:39,489 iteration 3243 : loss : 0.046788, loss_ce: 0.019363
2021-11-30 13:09:41,014 iteration 3244 : loss : 0.047718, loss_ce: 0.022555
2021-11-30 13:09:42,461 iteration 3245 : loss : 0.031214, loss_ce: 0.016914
2021-11-30 13:09:44,037 iteration 3246 : loss : 0.049322, loss_ce: 0.026038
2021-11-30 13:09:45,488 iteration 3247 : loss : 0.044730, loss_ce: 0.021531
 48%|████████████▉              | 191/400 [1:27:47<1:37:14, 27.92s/it]2021-11-30 13:09:46,971 iteration 3248 : loss : 0.037291, loss_ce: 0.017580
2021-11-30 13:09:48,405 iteration 3249 : loss : 0.030028, loss_ce: 0.014909
2021-11-30 13:09:49,880 iteration 3250 : loss : 0.039434, loss_ce: 0.016844
2021-11-30 13:09:51,357 iteration 3251 : loss : 0.036570, loss_ce: 0.018386
2021-11-30 13:09:52,851 iteration 3252 : loss : 0.034896, loss_ce: 0.015588
2021-11-30 13:09:54,309 iteration 3253 : loss : 0.035269, loss_ce: 0.019224
2021-11-30 13:09:55,781 iteration 3254 : loss : 0.043887, loss_ce: 0.018460
2021-11-30 13:09:57,173 iteration 3255 : loss : 0.046077, loss_ce: 0.021724
2021-11-30 13:09:58,650 iteration 3256 : loss : 0.036286, loss_ce: 0.017978
2021-11-30 13:10:00,084 iteration 3257 : loss : 0.032642, loss_ce: 0.017641
2021-11-30 13:10:01,598 iteration 3258 : loss : 0.045783, loss_ce: 0.022513
2021-11-30 13:10:03,021 iteration 3259 : loss : 0.049761, loss_ce: 0.022344
2021-11-30 13:10:04,533 iteration 3260 : loss : 0.030330, loss_ce: 0.015015
2021-11-30 13:10:05,987 iteration 3261 : loss : 0.055279, loss_ce: 0.018655
2021-11-30 13:10:07,527 iteration 3262 : loss : 0.041221, loss_ce: 0.017825
2021-11-30 13:10:09,129 iteration 3263 : loss : 0.045669, loss_ce: 0.017113
2021-11-30 13:10:10,618 iteration 3264 : loss : 0.049138, loss_ce: 0.025694
 48%|████████████▉              | 192/400 [1:28:12<1:33:53, 27.08s/it]2021-11-30 13:10:12,087 iteration 3265 : loss : 0.028113, loss_ce: 0.014479
2021-11-30 13:10:13,627 iteration 3266 : loss : 0.038909, loss_ce: 0.020772
2021-11-30 13:10:15,123 iteration 3267 : loss : 0.041612, loss_ce: 0.020103
2021-11-30 13:10:16,563 iteration 3268 : loss : 0.032706, loss_ce: 0.016228
2021-11-30 13:10:18,109 iteration 3269 : loss : 0.050647, loss_ce: 0.026084
2021-11-30 13:10:19,701 iteration 3270 : loss : 0.043158, loss_ce: 0.017131
2021-11-30 13:10:21,256 iteration 3271 : loss : 0.054316, loss_ce: 0.017577
2021-11-30 13:10:22,803 iteration 3272 : loss : 0.038056, loss_ce: 0.018045
2021-11-30 13:10:24,229 iteration 3273 : loss : 0.036857, loss_ce: 0.014137
2021-11-30 13:10:25,703 iteration 3274 : loss : 0.034581, loss_ce: 0.018920
2021-11-30 13:10:27,243 iteration 3275 : loss : 0.037516, loss_ce: 0.013704
2021-11-30 13:10:28,697 iteration 3276 : loss : 0.038722, loss_ce: 0.018811
2021-11-30 13:10:30,217 iteration 3277 : loss : 0.038358, loss_ce: 0.021689
2021-11-30 13:10:31,691 iteration 3278 : loss : 0.037766, loss_ce: 0.017615
2021-11-30 13:10:33,123 iteration 3279 : loss : 0.040217, loss_ce: 0.018444
2021-11-30 13:10:34,577 iteration 3280 : loss : 0.035646, loss_ce: 0.015640
2021-11-30 13:10:36,000 iteration 3281 : loss : 0.039641, loss_ce: 0.015276
 48%|█████████████              | 193/400 [1:28:38<1:31:40, 26.57s/it]2021-11-30 13:10:37,586 iteration 3282 : loss : 0.050764, loss_ce: 0.022277
2021-11-30 13:10:39,062 iteration 3283 : loss : 0.037362, loss_ce: 0.016636
2021-11-30 13:10:40,533 iteration 3284 : loss : 0.046633, loss_ce: 0.023155
2021-11-30 13:10:42,011 iteration 3285 : loss : 0.043796, loss_ce: 0.019078
2021-11-30 13:10:43,480 iteration 3286 : loss : 0.025918, loss_ce: 0.012722
2021-11-30 13:10:44,992 iteration 3287 : loss : 0.049382, loss_ce: 0.020724
2021-11-30 13:10:46,429 iteration 3288 : loss : 0.053938, loss_ce: 0.030409
2021-11-30 13:10:47,862 iteration 3289 : loss : 0.056955, loss_ce: 0.021660
2021-11-30 13:10:49,404 iteration 3290 : loss : 0.050780, loss_ce: 0.018409
2021-11-30 13:10:50,952 iteration 3291 : loss : 0.041223, loss_ce: 0.016853
2021-11-30 13:10:52,453 iteration 3292 : loss : 0.040527, loss_ce: 0.016685
2021-11-30 13:10:53,937 iteration 3293 : loss : 0.056200, loss_ce: 0.029924
2021-11-30 13:10:55,401 iteration 3294 : loss : 0.038350, loss_ce: 0.020067
2021-11-30 13:10:56,994 iteration 3295 : loss : 0.048631, loss_ce: 0.022517
2021-11-30 13:10:58,510 iteration 3296 : loss : 0.041591, loss_ce: 0.015982
2021-11-30 13:10:59,914 iteration 3297 : loss : 0.034080, loss_ce: 0.017543
2021-11-30 13:11:01,448 iteration 3298 : loss : 0.058970, loss_ce: 0.027124
 48%|█████████████              | 194/400 [1:29:03<1:30:03, 26.23s/it]2021-11-30 13:11:02,904 iteration 3299 : loss : 0.036499, loss_ce: 0.017716
2021-11-30 13:11:04,356 iteration 3300 : loss : 0.045723, loss_ce: 0.021944
2021-11-30 13:11:05,898 iteration 3301 : loss : 0.046289, loss_ce: 0.024063
2021-11-30 13:11:07,364 iteration 3302 : loss : 0.035070, loss_ce: 0.017036
2021-11-30 13:11:08,831 iteration 3303 : loss : 0.032235, loss_ce: 0.017160
2021-11-30 13:11:10,334 iteration 3304 : loss : 0.037060, loss_ce: 0.016753
2021-11-30 13:11:11,853 iteration 3305 : loss : 0.055985, loss_ce: 0.022346
2021-11-30 13:11:13,399 iteration 3306 : loss : 0.044721, loss_ce: 0.019203
2021-11-30 13:11:14,843 iteration 3307 : loss : 0.041931, loss_ce: 0.018308
2021-11-30 13:11:16,335 iteration 3308 : loss : 0.047078, loss_ce: 0.025636
2021-11-30 13:11:17,808 iteration 3309 : loss : 0.046549, loss_ce: 0.017610
2021-11-30 13:11:19,362 iteration 3310 : loss : 0.101361, loss_ce: 0.044155
2021-11-30 13:11:20,933 iteration 3311 : loss : 0.049886, loss_ce: 0.020976
2021-11-30 13:11:22,353 iteration 3312 : loss : 0.045364, loss_ce: 0.018550
2021-11-30 13:11:23,787 iteration 3313 : loss : 0.037062, loss_ce: 0.016583
2021-11-30 13:11:25,350 iteration 3314 : loss : 0.037795, loss_ce: 0.016508
2021-11-30 13:11:25,350 Training Data Eval:
2021-11-30 13:11:32,742   Average segmentation loss on training set: 0.0220
2021-11-30 13:11:32,742 Validation Data Eval:
2021-11-30 13:11:35,307   Average segmentation loss on validation set: 0.0928
2021-11-30 13:11:36,742 iteration 3315 : loss : 0.026022, loss_ce: 0.014999
 49%|█████████████▏             | 195/400 [1:29:39<1:38:55, 28.95s/it]2021-11-30 13:11:38,235 iteration 3316 : loss : 0.023987, loss_ce: 0.012024
2021-11-30 13:11:39,697 iteration 3317 : loss : 0.046376, loss_ce: 0.020312
2021-11-30 13:11:41,288 iteration 3318 : loss : 0.049194, loss_ce: 0.021985
2021-11-30 13:11:42,740 iteration 3319 : loss : 0.034996, loss_ce: 0.015592
2021-11-30 13:11:44,196 iteration 3320 : loss : 0.030870, loss_ce: 0.015026
2021-11-30 13:11:45,699 iteration 3321 : loss : 0.028491, loss_ce: 0.012299
2021-11-30 13:11:47,206 iteration 3322 : loss : 0.047359, loss_ce: 0.024067
2021-11-30 13:11:48,640 iteration 3323 : loss : 0.038262, loss_ce: 0.015978
2021-11-30 13:11:50,222 iteration 3324 : loss : 0.039724, loss_ce: 0.021069
2021-11-30 13:11:51,662 iteration 3325 : loss : 0.036516, loss_ce: 0.015544
2021-11-30 13:11:53,202 iteration 3326 : loss : 0.046190, loss_ce: 0.021980
2021-11-30 13:11:54,803 iteration 3327 : loss : 0.076030, loss_ce: 0.028440
2021-11-30 13:11:56,281 iteration 3328 : loss : 0.033372, loss_ce: 0.015221
2021-11-30 13:11:57,713 iteration 3329 : loss : 0.033159, loss_ce: 0.017099
2021-11-30 13:11:59,151 iteration 3330 : loss : 0.052698, loss_ce: 0.020729
2021-11-30 13:12:00,637 iteration 3331 : loss : 0.029341, loss_ce: 0.014155
2021-11-30 13:12:02,034 iteration 3332 : loss : 0.027036, loss_ce: 0.015513
 49%|█████████████▏             | 196/400 [1:30:04<1:34:42, 27.85s/it]2021-11-30 13:12:03,554 iteration 3333 : loss : 0.036498, loss_ce: 0.017522
2021-11-30 13:12:05,077 iteration 3334 : loss : 0.035203, loss_ce: 0.018273
2021-11-30 13:12:06,476 iteration 3335 : loss : 0.026752, loss_ce: 0.014366
2021-11-30 13:12:07,952 iteration 3336 : loss : 0.060610, loss_ce: 0.025495
2021-11-30 13:12:09,493 iteration 3337 : loss : 0.046537, loss_ce: 0.023236
2021-11-30 13:12:11,080 iteration 3338 : loss : 0.045530, loss_ce: 0.025375
2021-11-30 13:12:12,555 iteration 3339 : loss : 0.057743, loss_ce: 0.020816
2021-11-30 13:12:13,998 iteration 3340 : loss : 0.034844, loss_ce: 0.015478
2021-11-30 13:12:15,468 iteration 3341 : loss : 0.034809, loss_ce: 0.015981
2021-11-30 13:12:17,018 iteration 3342 : loss : 0.063714, loss_ce: 0.031265
2021-11-30 13:12:18,536 iteration 3343 : loss : 0.039690, loss_ce: 0.017982
2021-11-30 13:12:20,031 iteration 3344 : loss : 0.054732, loss_ce: 0.021166
2021-11-30 13:12:21,475 iteration 3345 : loss : 0.036792, loss_ce: 0.016107
2021-11-30 13:12:22,958 iteration 3346 : loss : 0.035916, loss_ce: 0.018143
2021-11-30 13:12:24,465 iteration 3347 : loss : 0.077058, loss_ce: 0.022765
2021-11-30 13:12:26,048 iteration 3348 : loss : 0.053355, loss_ce: 0.019308
2021-11-30 13:12:27,553 iteration 3349 : loss : 0.036141, loss_ce: 0.017117
 49%|█████████████▎             | 197/400 [1:30:29<1:31:51, 27.15s/it]2021-11-30 13:12:29,021 iteration 3350 : loss : 0.031672, loss_ce: 0.016247
2021-11-30 13:12:30,452 iteration 3351 : loss : 0.037310, loss_ce: 0.019026
2021-11-30 13:12:31,922 iteration 3352 : loss : 0.031741, loss_ce: 0.015959
2021-11-30 13:12:33,448 iteration 3353 : loss : 0.040729, loss_ce: 0.017838
2021-11-30 13:12:34,969 iteration 3354 : loss : 0.032444, loss_ce: 0.013431
2021-11-30 13:12:36,534 iteration 3355 : loss : 0.051723, loss_ce: 0.029143
2021-11-30 13:12:38,068 iteration 3356 : loss : 0.049576, loss_ce: 0.018319
2021-11-30 13:12:39,603 iteration 3357 : loss : 0.035773, loss_ce: 0.014762
2021-11-30 13:12:41,171 iteration 3358 : loss : 0.047812, loss_ce: 0.018017
2021-11-30 13:12:42,682 iteration 3359 : loss : 0.041902, loss_ce: 0.019426
2021-11-30 13:12:44,256 iteration 3360 : loss : 0.041153, loss_ce: 0.021028
2021-11-30 13:12:45,674 iteration 3361 : loss : 0.044298, loss_ce: 0.015868
2021-11-30 13:12:47,116 iteration 3362 : loss : 0.035827, loss_ce: 0.016372
2021-11-30 13:12:48,569 iteration 3363 : loss : 0.028443, loss_ce: 0.016726
2021-11-30 13:12:50,091 iteration 3364 : loss : 0.038566, loss_ce: 0.017261
2021-11-30 13:12:51,538 iteration 3365 : loss : 0.036543, loss_ce: 0.022357
2021-11-30 13:12:53,013 iteration 3366 : loss : 0.035600, loss_ce: 0.017231
 50%|█████████████▎             | 198/400 [1:30:55<1:29:42, 26.64s/it]2021-11-30 13:12:54,535 iteration 3367 : loss : 0.047378, loss_ce: 0.020381
2021-11-30 13:12:55,998 iteration 3368 : loss : 0.041170, loss_ce: 0.021302
2021-11-30 13:12:57,505 iteration 3369 : loss : 0.036802, loss_ce: 0.016079
2021-11-30 13:12:58,989 iteration 3370 : loss : 0.037173, loss_ce: 0.018611
2021-11-30 13:13:00,369 iteration 3371 : loss : 0.022977, loss_ce: 0.012770
2021-11-30 13:13:01,838 iteration 3372 : loss : 0.043111, loss_ce: 0.023921
2021-11-30 13:13:03,317 iteration 3373 : loss : 0.030149, loss_ce: 0.017239
2021-11-30 13:13:04,757 iteration 3374 : loss : 0.038579, loss_ce: 0.019598
2021-11-30 13:13:06,204 iteration 3375 : loss : 0.043753, loss_ce: 0.019420
2021-11-30 13:13:07,656 iteration 3376 : loss : 0.035201, loss_ce: 0.017291
2021-11-30 13:13:09,197 iteration 3377 : loss : 0.055203, loss_ce: 0.022470
2021-11-30 13:13:10,681 iteration 3378 : loss : 0.042066, loss_ce: 0.018484
2021-11-30 13:13:12,071 iteration 3379 : loss : 0.028608, loss_ce: 0.015003
2021-11-30 13:13:13,621 iteration 3380 : loss : 0.055635, loss_ce: 0.025848
2021-11-30 13:13:15,043 iteration 3381 : loss : 0.115151, loss_ce: 0.028730
2021-11-30 13:13:16,534 iteration 3382 : loss : 0.060537, loss_ce: 0.027180
2021-11-30 13:13:17,996 iteration 3383 : loss : 0.037780, loss_ce: 0.017778
 50%|█████████████▍             | 199/400 [1:31:20<1:27:36, 26.15s/it]2021-11-30 13:13:19,485 iteration 3384 : loss : 0.037343, loss_ce: 0.015768
2021-11-30 13:13:21,023 iteration 3385 : loss : 0.035897, loss_ce: 0.018518
2021-11-30 13:13:22,569 iteration 3386 : loss : 0.048112, loss_ce: 0.023855
2021-11-30 13:13:24,088 iteration 3387 : loss : 0.052868, loss_ce: 0.023024
2021-11-30 13:13:25,498 iteration 3388 : loss : 0.032814, loss_ce: 0.015130
2021-11-30 13:13:27,016 iteration 3389 : loss : 0.050654, loss_ce: 0.023001
2021-11-30 13:13:28,421 iteration 3390 : loss : 0.049549, loss_ce: 0.015168
2021-11-30 13:13:30,034 iteration 3391 : loss : 0.052909, loss_ce: 0.021845
2021-11-30 13:13:31,558 iteration 3392 : loss : 0.041252, loss_ce: 0.018131
2021-11-30 13:13:33,065 iteration 3393 : loss : 0.037891, loss_ce: 0.017978
2021-11-30 13:13:34,443 iteration 3394 : loss : 0.035046, loss_ce: 0.015425
2021-11-30 13:13:36,048 iteration 3395 : loss : 0.051046, loss_ce: 0.022825
2021-11-30 13:13:37,541 iteration 3396 : loss : 0.068831, loss_ce: 0.028893
2021-11-30 13:13:39,008 iteration 3397 : loss : 0.031349, loss_ce: 0.015331
2021-11-30 13:13:40,530 iteration 3398 : loss : 0.042886, loss_ce: 0.019327
2021-11-30 13:13:42,031 iteration 3399 : loss : 0.033027, loss_ce: 0.018083
2021-11-30 13:13:42,031 Training Data Eval:
2021-11-30 13:13:49,375   Average segmentation loss on training set: 0.0208
2021-11-30 13:13:49,376 Validation Data Eval:
2021-11-30 13:13:51,927   Average segmentation loss on validation set: 0.0995
2021-11-30 13:13:53,426 iteration 3400 : loss : 0.034384, loss_ce: 0.013205
 50%|█████████████▌             | 200/400 [1:31:55<1:36:26, 28.93s/it]2021-11-30 13:13:55,110 iteration 3401 : loss : 0.041471, loss_ce: 0.019326
2021-11-30 13:13:56,640 iteration 3402 : loss : 0.032349, loss_ce: 0.014775
2021-11-30 13:13:58,075 iteration 3403 : loss : 0.046236, loss_ce: 0.023672
2021-11-30 13:13:59,526 iteration 3404 : loss : 0.031172, loss_ce: 0.014895
2021-11-30 13:14:00,983 iteration 3405 : loss : 0.049963, loss_ce: 0.024009
2021-11-30 13:14:02,534 iteration 3406 : loss : 0.049676, loss_ce: 0.020518
2021-11-30 13:14:04,024 iteration 3407 : loss : 0.046198, loss_ce: 0.021707
2021-11-30 13:14:05,533 iteration 3408 : loss : 0.035920, loss_ce: 0.016092
2021-11-30 13:14:07,073 iteration 3409 : loss : 0.057168, loss_ce: 0.028082
2021-11-30 13:14:08,579 iteration 3410 : loss : 0.039132, loss_ce: 0.017873
2021-11-30 13:14:10,120 iteration 3411 : loss : 0.036453, loss_ce: 0.018709
2021-11-30 13:14:11,544 iteration 3412 : loss : 0.038993, loss_ce: 0.016062
2021-11-30 13:14:12,955 iteration 3413 : loss : 0.028432, loss_ce: 0.014651
2021-11-30 13:14:14,469 iteration 3414 : loss : 0.046981, loss_ce: 0.015231
2021-11-30 13:14:16,013 iteration 3415 : loss : 0.050925, loss_ce: 0.020905
2021-11-30 13:14:17,549 iteration 3416 : loss : 0.027968, loss_ce: 0.011987
2021-11-30 13:14:19,140 iteration 3417 : loss : 0.066909, loss_ce: 0.030369
 50%|█████████████▌             | 201/400 [1:32:21<1:32:45, 27.97s/it]2021-11-30 13:14:20,685 iteration 3418 : loss : 0.032864, loss_ce: 0.016912
2021-11-30 13:14:22,211 iteration 3419 : loss : 0.036617, loss_ce: 0.015969
2021-11-30 13:14:23,798 iteration 3420 : loss : 0.049866, loss_ce: 0.018517
2021-11-30 13:14:25,310 iteration 3421 : loss : 0.035445, loss_ce: 0.016473
2021-11-30 13:14:26,741 iteration 3422 : loss : 0.032721, loss_ce: 0.013438
2021-11-30 13:14:28,231 iteration 3423 : loss : 0.037166, loss_ce: 0.018080
2021-11-30 13:14:29,721 iteration 3424 : loss : 0.037995, loss_ce: 0.017915
2021-11-30 13:14:31,153 iteration 3425 : loss : 0.045340, loss_ce: 0.022027
2021-11-30 13:14:32,635 iteration 3426 : loss : 0.048043, loss_ce: 0.019894
2021-11-30 13:14:34,136 iteration 3427 : loss : 0.040963, loss_ce: 0.017423
2021-11-30 13:14:35,644 iteration 3428 : loss : 0.044927, loss_ce: 0.018882
2021-11-30 13:14:37,182 iteration 3429 : loss : 0.041295, loss_ce: 0.019011
2021-11-30 13:14:38,764 iteration 3430 : loss : 0.045434, loss_ce: 0.022366
2021-11-30 13:14:40,175 iteration 3431 : loss : 0.032274, loss_ce: 0.017361
2021-11-30 13:14:41,617 iteration 3432 : loss : 0.035015, loss_ce: 0.014764
2021-11-30 13:14:43,071 iteration 3433 : loss : 0.037933, loss_ce: 0.016794
2021-11-30 13:14:44,619 iteration 3434 : loss : 0.048633, loss_ce: 0.020152
 50%|█████████████▋             | 202/400 [1:32:46<1:29:49, 27.22s/it]2021-11-30 13:14:46,061 iteration 3435 : loss : 0.029357, loss_ce: 0.012584
2021-11-30 13:14:47,502 iteration 3436 : loss : 0.028068, loss_ce: 0.014033
2021-11-30 13:14:49,088 iteration 3437 : loss : 0.038709, loss_ce: 0.019968
2021-11-30 13:14:50,631 iteration 3438 : loss : 0.032837, loss_ce: 0.018233
2021-11-30 13:14:52,161 iteration 3439 : loss : 0.038249, loss_ce: 0.016652
2021-11-30 13:14:53,610 iteration 3440 : loss : 0.027919, loss_ce: 0.014098
2021-11-30 13:14:55,081 iteration 3441 : loss : 0.044117, loss_ce: 0.019710
2021-11-30 13:14:56,693 iteration 3442 : loss : 0.039642, loss_ce: 0.017447
2021-11-30 13:14:58,224 iteration 3443 : loss : 0.045966, loss_ce: 0.020011
2021-11-30 13:14:59,784 iteration 3444 : loss : 0.041916, loss_ce: 0.018736
2021-11-30 13:15:01,278 iteration 3445 : loss : 0.064362, loss_ce: 0.023309
2021-11-30 13:15:02,763 iteration 3446 : loss : 0.032344, loss_ce: 0.017139
2021-11-30 13:15:04,238 iteration 3447 : loss : 0.034866, loss_ce: 0.015770
2021-11-30 13:15:05,783 iteration 3448 : loss : 0.047623, loss_ce: 0.015944
2021-11-30 13:15:07,297 iteration 3449 : loss : 0.042067, loss_ce: 0.019992
2021-11-30 13:15:08,781 iteration 3450 : loss : 0.061815, loss_ce: 0.030903
2021-11-30 13:15:10,350 iteration 3451 : loss : 0.052570, loss_ce: 0.023476
 51%|█████████████▋             | 203/400 [1:33:12<1:27:54, 26.77s/it]2021-11-30 13:15:11,893 iteration 3452 : loss : 0.024256, loss_ce: 0.013214
2021-11-30 13:15:13,361 iteration 3453 : loss : 0.035214, loss_ce: 0.018014
2021-11-30 13:15:14,808 iteration 3454 : loss : 0.034456, loss_ce: 0.015041
2021-11-30 13:15:16,381 iteration 3455 : loss : 0.038059, loss_ce: 0.019756
2021-11-30 13:15:17,825 iteration 3456 : loss : 0.028307, loss_ce: 0.012797
2021-11-30 13:15:19,410 iteration 3457 : loss : 0.049760, loss_ce: 0.020580
2021-11-30 13:15:20,934 iteration 3458 : loss : 0.044811, loss_ce: 0.019294
2021-11-30 13:15:22,424 iteration 3459 : loss : 0.034518, loss_ce: 0.017186
2021-11-30 13:15:23,901 iteration 3460 : loss : 0.039516, loss_ce: 0.018934
2021-11-30 13:15:25,381 iteration 3461 : loss : 0.032756, loss_ce: 0.015110
2021-11-30 13:15:26,837 iteration 3462 : loss : 0.037718, loss_ce: 0.019420
2021-11-30 13:15:28,366 iteration 3463 : loss : 0.050204, loss_ce: 0.023139
2021-11-30 13:15:29,786 iteration 3464 : loss : 0.030832, loss_ce: 0.014804
2021-11-30 13:15:31,329 iteration 3465 : loss : 0.043194, loss_ce: 0.018448
2021-11-30 13:15:32,835 iteration 3466 : loss : 0.029832, loss_ce: 0.012847
2021-11-30 13:15:34,326 iteration 3467 : loss : 0.037375, loss_ce: 0.021730
2021-11-30 13:15:35,813 iteration 3468 : loss : 0.031269, loss_ce: 0.016242
 51%|█████████████▊             | 204/400 [1:33:38<1:26:10, 26.38s/it]2021-11-30 13:15:37,323 iteration 3469 : loss : 0.055503, loss_ce: 0.017692
2021-11-30 13:15:38,878 iteration 3470 : loss : 0.045995, loss_ce: 0.017029
2021-11-30 13:15:40,291 iteration 3471 : loss : 0.035219, loss_ce: 0.016292
2021-11-30 13:15:41,794 iteration 3472 : loss : 0.041677, loss_ce: 0.020900
2021-11-30 13:15:43,401 iteration 3473 : loss : 0.041737, loss_ce: 0.019585
2021-11-30 13:15:44,866 iteration 3474 : loss : 0.036651, loss_ce: 0.015065
2021-11-30 13:15:46,426 iteration 3475 : loss : 0.069618, loss_ce: 0.024831
2021-11-30 13:15:47,863 iteration 3476 : loss : 0.037402, loss_ce: 0.014420
2021-11-30 13:15:49,371 iteration 3477 : loss : 0.061770, loss_ce: 0.021801
2021-11-30 13:15:50,851 iteration 3478 : loss : 0.030904, loss_ce: 0.015215
2021-11-30 13:15:52,310 iteration 3479 : loss : 0.034345, loss_ce: 0.016636
2021-11-30 13:15:53,715 iteration 3480 : loss : 0.033259, loss_ce: 0.017483
2021-11-30 13:15:55,205 iteration 3481 : loss : 0.039167, loss_ce: 0.017609
2021-11-30 13:15:56,704 iteration 3482 : loss : 0.047474, loss_ce: 0.019248
2021-11-30 13:15:58,268 iteration 3483 : loss : 0.042515, loss_ce: 0.020304
2021-11-30 13:15:59,649 iteration 3484 : loss : 0.035152, loss_ce: 0.016907
2021-11-30 13:15:59,649 Training Data Eval:
2021-11-30 13:16:07,034   Average segmentation loss on training set: 0.0203
2021-11-30 13:16:07,034 Validation Data Eval:
2021-11-30 13:16:09,592   Average segmentation loss on validation set: 0.0926
2021-11-30 13:16:11,187 iteration 3485 : loss : 0.057674, loss_ce: 0.026217
 51%|█████████████▊             | 205/400 [1:34:13<1:34:30, 29.08s/it]2021-11-30 13:16:12,729 iteration 3486 : loss : 0.045938, loss_ce: 0.023616
2021-11-30 13:16:14,146 iteration 3487 : loss : 0.028626, loss_ce: 0.015003
2021-11-30 13:16:15,605 iteration 3488 : loss : 0.048806, loss_ce: 0.022384
2021-11-30 13:16:17,094 iteration 3489 : loss : 0.041430, loss_ce: 0.020411
2021-11-30 13:16:18,614 iteration 3490 : loss : 0.047345, loss_ce: 0.022000
2021-11-30 13:16:20,156 iteration 3491 : loss : 0.042359, loss_ce: 0.020223
2021-11-30 13:16:21,576 iteration 3492 : loss : 0.048060, loss_ce: 0.020158
2021-11-30 13:16:23,196 iteration 3493 : loss : 0.048678, loss_ce: 0.022877
2021-11-30 13:16:24,706 iteration 3494 : loss : 0.026148, loss_ce: 0.011347
2021-11-30 13:16:26,181 iteration 3495 : loss : 0.037212, loss_ce: 0.015352
2021-11-30 13:16:27,704 iteration 3496 : loss : 0.033739, loss_ce: 0.017235
2021-11-30 13:16:29,236 iteration 3497 : loss : 0.046215, loss_ce: 0.020153
2021-11-30 13:16:30,858 iteration 3498 : loss : 0.035181, loss_ce: 0.017569
2021-11-30 13:16:32,303 iteration 3499 : loss : 0.038840, loss_ce: 0.018814
2021-11-30 13:16:33,686 iteration 3500 : loss : 0.024485, loss_ce: 0.011235
2021-11-30 13:16:35,129 iteration 3501 : loss : 0.033976, loss_ce: 0.015367
2021-11-30 13:16:36,634 iteration 3502 : loss : 0.045326, loss_ce: 0.019368
 52%|█████████████▉             | 206/400 [1:34:38<1:30:29, 27.99s/it]2021-11-30 13:16:38,187 iteration 3503 : loss : 0.042247, loss_ce: 0.017918
2021-11-30 13:16:39,643 iteration 3504 : loss : 0.027695, loss_ce: 0.011885
2021-11-30 13:16:41,019 iteration 3505 : loss : 0.035358, loss_ce: 0.017922
2021-11-30 13:16:42,523 iteration 3506 : loss : 0.043404, loss_ce: 0.017275
2021-11-30 13:16:43,989 iteration 3507 : loss : 0.046543, loss_ce: 0.016050
2021-11-30 13:16:45,589 iteration 3508 : loss : 0.055147, loss_ce: 0.024014
2021-11-30 13:16:47,091 iteration 3509 : loss : 0.032454, loss_ce: 0.014160
2021-11-30 13:16:48,521 iteration 3510 : loss : 0.044663, loss_ce: 0.021539
2021-11-30 13:16:49,925 iteration 3511 : loss : 0.027411, loss_ce: 0.015854
2021-11-30 13:16:51,471 iteration 3512 : loss : 0.048452, loss_ce: 0.017224
2021-11-30 13:16:52,973 iteration 3513 : loss : 0.038992, loss_ce: 0.015752
2021-11-30 13:16:54,586 iteration 3514 : loss : 0.069399, loss_ce: 0.036080
2021-11-30 13:16:56,060 iteration 3515 : loss : 0.054065, loss_ce: 0.021622
2021-11-30 13:16:57,499 iteration 3516 : loss : 0.024802, loss_ce: 0.013183
2021-11-30 13:16:58,957 iteration 3517 : loss : 0.041287, loss_ce: 0.018099
2021-11-30 13:17:00,418 iteration 3518 : loss : 0.040291, loss_ce: 0.024550
2021-11-30 13:17:01,923 iteration 3519 : loss : 0.031322, loss_ce: 0.014429
 52%|█████████████▉             | 207/400 [1:35:04<1:27:25, 27.18s/it]2021-11-30 13:17:03,469 iteration 3520 : loss : 0.045518, loss_ce: 0.021851
2021-11-30 13:17:04,930 iteration 3521 : loss : 0.056681, loss_ce: 0.028614
2021-11-30 13:17:06,355 iteration 3522 : loss : 0.032417, loss_ce: 0.017492
2021-11-30 13:17:07,825 iteration 3523 : loss : 0.030896, loss_ce: 0.014898
2021-11-30 13:17:09,359 iteration 3524 : loss : 0.041517, loss_ce: 0.018838
2021-11-30 13:17:10,964 iteration 3525 : loss : 0.039947, loss_ce: 0.019029
2021-11-30 13:17:12,472 iteration 3526 : loss : 0.042971, loss_ce: 0.019953
2021-11-30 13:17:13,986 iteration 3527 : loss : 0.057624, loss_ce: 0.021185
2021-11-30 13:17:15,606 iteration 3528 : loss : 0.044572, loss_ce: 0.021649
2021-11-30 13:17:17,111 iteration 3529 : loss : 0.046674, loss_ce: 0.015876
2021-11-30 13:17:18,621 iteration 3530 : loss : 0.031166, loss_ce: 0.015180
2021-11-30 13:17:20,055 iteration 3531 : loss : 0.043274, loss_ce: 0.016910
2021-11-30 13:17:21,612 iteration 3532 : loss : 0.064087, loss_ce: 0.027834
2021-11-30 13:17:23,085 iteration 3533 : loss : 0.037489, loss_ce: 0.014267
2021-11-30 13:17:24,590 iteration 3534 : loss : 0.036826, loss_ce: 0.017027
2021-11-30 13:17:26,153 iteration 3535 : loss : 0.039398, loss_ce: 0.019569
2021-11-30 13:17:27,668 iteration 3536 : loss : 0.037821, loss_ce: 0.017955
 52%|██████████████             | 208/400 [1:35:30<1:25:35, 26.75s/it]2021-11-30 13:17:29,112 iteration 3537 : loss : 0.026986, loss_ce: 0.013619
2021-11-30 13:17:30,532 iteration 3538 : loss : 0.033283, loss_ce: 0.015821
2021-11-30 13:17:31,967 iteration 3539 : loss : 0.024831, loss_ce: 0.014414
2021-11-30 13:17:33,340 iteration 3540 : loss : 0.035281, loss_ce: 0.014518
2021-11-30 13:17:34,921 iteration 3541 : loss : 0.050413, loss_ce: 0.022417
2021-11-30 13:17:36,420 iteration 3542 : loss : 0.042450, loss_ce: 0.020633
2021-11-30 13:17:38,015 iteration 3543 : loss : 0.064836, loss_ce: 0.021711
2021-11-30 13:17:39,415 iteration 3544 : loss : 0.023946, loss_ce: 0.012430
2021-11-30 13:17:40,894 iteration 3545 : loss : 0.043890, loss_ce: 0.019101
2021-11-30 13:17:42,319 iteration 3546 : loss : 0.027834, loss_ce: 0.013489
2021-11-30 13:17:43,824 iteration 3547 : loss : 0.034263, loss_ce: 0.018737
2021-11-30 13:17:45,226 iteration 3548 : loss : 0.042229, loss_ce: 0.017240
2021-11-30 13:17:46,700 iteration 3549 : loss : 0.036249, loss_ce: 0.018683
2021-11-30 13:17:48,227 iteration 3550 : loss : 0.037155, loss_ce: 0.017872
2021-11-30 13:17:49,761 iteration 3551 : loss : 0.046804, loss_ce: 0.018977
2021-11-30 13:17:51,311 iteration 3552 : loss : 0.070528, loss_ce: 0.024220
2021-11-30 13:17:52,869 iteration 3553 : loss : 0.052584, loss_ce: 0.026334
 52%|██████████████             | 209/400 [1:35:55<1:23:40, 26.28s/it]2021-11-30 13:17:54,406 iteration 3554 : loss : 0.040587, loss_ce: 0.020398
2021-11-30 13:17:55,860 iteration 3555 : loss : 0.029439, loss_ce: 0.013501
2021-11-30 13:17:57,323 iteration 3556 : loss : 0.031729, loss_ce: 0.016524
2021-11-30 13:17:58,900 iteration 3557 : loss : 0.041565, loss_ce: 0.015732
2021-11-30 13:18:00,344 iteration 3558 : loss : 0.050926, loss_ce: 0.016875
2021-11-30 13:18:01,816 iteration 3559 : loss : 0.034815, loss_ce: 0.013482
2021-11-30 13:18:03,191 iteration 3560 : loss : 0.030556, loss_ce: 0.016821
2021-11-30 13:18:04,778 iteration 3561 : loss : 0.037468, loss_ce: 0.016612
2021-11-30 13:18:06,236 iteration 3562 : loss : 0.032306, loss_ce: 0.015331
2021-11-30 13:18:07,762 iteration 3563 : loss : 0.039211, loss_ce: 0.016840
2021-11-30 13:18:09,262 iteration 3564 : loss : 0.044686, loss_ce: 0.018251
2021-11-30 13:18:10,875 iteration 3565 : loss : 0.050316, loss_ce: 0.019970
2021-11-30 13:18:12,359 iteration 3566 : loss : 0.043006, loss_ce: 0.017390
2021-11-30 13:18:13,923 iteration 3567 : loss : 0.060632, loss_ce: 0.028929
2021-11-30 13:18:15,488 iteration 3568 : loss : 0.040824, loss_ce: 0.017155
2021-11-30 13:18:17,046 iteration 3569 : loss : 0.045975, loss_ce: 0.013914
2021-11-30 13:18:17,047 Training Data Eval:
2021-11-30 13:18:24,387   Average segmentation loss on training set: 0.0216
2021-11-30 13:18:24,387 Validation Data Eval:
2021-11-30 13:18:26,940   Average segmentation loss on validation set: 0.1012
2021-11-30 13:18:28,482 iteration 3570 : loss : 0.040465, loss_ce: 0.017544
 52%|██████████████▏            | 210/400 [1:36:30<1:32:06, 29.08s/it]2021-11-30 13:18:30,039 iteration 3571 : loss : 0.057683, loss_ce: 0.025930
2021-11-30 13:18:31,540 iteration 3572 : loss : 0.033206, loss_ce: 0.012977
2021-11-30 13:18:33,027 iteration 3573 : loss : 0.057135, loss_ce: 0.024037
2021-11-30 13:18:34,489 iteration 3574 : loss : 0.023817, loss_ce: 0.013001
2021-11-30 13:18:35,956 iteration 3575 : loss : 0.035622, loss_ce: 0.016902
2021-11-30 13:18:37,452 iteration 3576 : loss : 0.039985, loss_ce: 0.018351
2021-11-30 13:18:39,013 iteration 3577 : loss : 0.030888, loss_ce: 0.016154
2021-11-30 13:18:40,542 iteration 3578 : loss : 0.045498, loss_ce: 0.022286
2021-11-30 13:18:42,022 iteration 3579 : loss : 0.040758, loss_ce: 0.015727
2021-11-30 13:18:43,549 iteration 3580 : loss : 0.032333, loss_ce: 0.013729
2021-11-30 13:18:44,995 iteration 3581 : loss : 0.057698, loss_ce: 0.017244
2021-11-30 13:18:46,483 iteration 3582 : loss : 0.032122, loss_ce: 0.016186
2021-11-30 13:18:48,010 iteration 3583 : loss : 0.048575, loss_ce: 0.019379
2021-11-30 13:18:49,453 iteration 3584 : loss : 0.039108, loss_ce: 0.018518
2021-11-30 13:18:50,939 iteration 3585 : loss : 0.029841, loss_ce: 0.013482
2021-11-30 13:18:52,356 iteration 3586 : loss : 0.038033, loss_ce: 0.016580
2021-11-30 13:18:53,828 iteration 3587 : loss : 0.038890, loss_ce: 0.022279
 53%|██████████████▏            | 211/400 [1:36:56<1:28:04, 27.96s/it]2021-11-30 13:18:55,269 iteration 3588 : loss : 0.038029, loss_ce: 0.016363
2021-11-30 13:18:56,806 iteration 3589 : loss : 0.050306, loss_ce: 0.019464
2021-11-30 13:18:58,291 iteration 3590 : loss : 0.045162, loss_ce: 0.020352
2021-11-30 13:18:59,759 iteration 3591 : loss : 0.030302, loss_ce: 0.013748
2021-11-30 13:19:01,311 iteration 3592 : loss : 0.035625, loss_ce: 0.016643
2021-11-30 13:19:02,814 iteration 3593 : loss : 0.055639, loss_ce: 0.016098
2021-11-30 13:19:04,313 iteration 3594 : loss : 0.032972, loss_ce: 0.013364
2021-11-30 13:19:05,943 iteration 3595 : loss : 0.086272, loss_ce: 0.022567
2021-11-30 13:19:07,338 iteration 3596 : loss : 0.030775, loss_ce: 0.016171
2021-11-30 13:19:08,899 iteration 3597 : loss : 0.037283, loss_ce: 0.016505
2021-11-30 13:19:10,368 iteration 3598 : loss : 0.041720, loss_ce: 0.021917
2021-11-30 13:19:11,871 iteration 3599 : loss : 0.042236, loss_ce: 0.020027
2021-11-30 13:19:13,335 iteration 3600 : loss : 0.034235, loss_ce: 0.018277
2021-11-30 13:19:14,816 iteration 3601 : loss : 0.041291, loss_ce: 0.020067
2021-11-30 13:19:16,288 iteration 3602 : loss : 0.064105, loss_ce: 0.030243
2021-11-30 13:19:17,744 iteration 3603 : loss : 0.032683, loss_ce: 0.014132
2021-11-30 13:19:19,169 iteration 3604 : loss : 0.034652, loss_ce: 0.017806
 53%|██████████████▎            | 212/400 [1:37:21<1:25:09, 27.18s/it]2021-11-30 13:19:20,672 iteration 3605 : loss : 0.027669, loss_ce: 0.013133
2021-11-30 13:19:22,153 iteration 3606 : loss : 0.033910, loss_ce: 0.015996
2021-11-30 13:19:23,582 iteration 3607 : loss : 0.033494, loss_ce: 0.015006
2021-11-30 13:19:25,007 iteration 3608 : loss : 0.040425, loss_ce: 0.019192
2021-11-30 13:19:26,534 iteration 3609 : loss : 0.036968, loss_ce: 0.015053
2021-11-30 13:19:27,983 iteration 3610 : loss : 0.060042, loss_ce: 0.027830
2021-11-30 13:19:29,507 iteration 3611 : loss : 0.040049, loss_ce: 0.019371
2021-11-30 13:19:31,047 iteration 3612 : loss : 0.035768, loss_ce: 0.015647
2021-11-30 13:19:32,534 iteration 3613 : loss : 0.040041, loss_ce: 0.018061
2021-11-30 13:19:34,048 iteration 3614 : loss : 0.029997, loss_ce: 0.012759
2021-11-30 13:19:35,496 iteration 3615 : loss : 0.036251, loss_ce: 0.015100
2021-11-30 13:19:37,062 iteration 3616 : loss : 0.048110, loss_ce: 0.018937
2021-11-30 13:19:38,527 iteration 3617 : loss : 0.042444, loss_ce: 0.018925
2021-11-30 13:19:40,004 iteration 3618 : loss : 0.037375, loss_ce: 0.018488
2021-11-30 13:19:41,506 iteration 3619 : loss : 0.032427, loss_ce: 0.016826
2021-11-30 13:19:43,026 iteration 3620 : loss : 0.037496, loss_ce: 0.018882
2021-11-30 13:19:44,484 iteration 3621 : loss : 0.035749, loss_ce: 0.017329
 53%|██████████████▍            | 213/400 [1:37:46<1:22:57, 26.62s/it]2021-11-30 13:19:46,099 iteration 3622 : loss : 0.056974, loss_ce: 0.021426
2021-11-30 13:19:47,584 iteration 3623 : loss : 0.033896, loss_ce: 0.016854
2021-11-30 13:19:49,128 iteration 3624 : loss : 0.032624, loss_ce: 0.014050
2021-11-30 13:19:50,662 iteration 3625 : loss : 0.062101, loss_ce: 0.027911
2021-11-30 13:19:52,143 iteration 3626 : loss : 0.032801, loss_ce: 0.016352
2021-11-30 13:19:53,670 iteration 3627 : loss : 0.041860, loss_ce: 0.022289
2021-11-30 13:19:55,171 iteration 3628 : loss : 0.039962, loss_ce: 0.016225
2021-11-30 13:19:56,734 iteration 3629 : loss : 0.039753, loss_ce: 0.019054
2021-11-30 13:19:58,214 iteration 3630 : loss : 0.028958, loss_ce: 0.014204
2021-11-30 13:19:59,612 iteration 3631 : loss : 0.029964, loss_ce: 0.016723
2021-11-30 13:20:01,019 iteration 3632 : loss : 0.025285, loss_ce: 0.013745
2021-11-30 13:20:02,557 iteration 3633 : loss : 0.041986, loss_ce: 0.019659
2021-11-30 13:20:04,034 iteration 3634 : loss : 0.042572, loss_ce: 0.017479
2021-11-30 13:20:05,475 iteration 3635 : loss : 0.030979, loss_ce: 0.013424
2021-11-30 13:20:06,914 iteration 3636 : loss : 0.034224, loss_ce: 0.016545
2021-11-30 13:20:08,472 iteration 3637 : loss : 0.051651, loss_ce: 0.020606
2021-11-30 13:20:09,894 iteration 3638 : loss : 0.027810, loss_ce: 0.012556
 54%|██████████████▍            | 214/400 [1:38:12<1:21:24, 26.26s/it]2021-11-30 13:20:11,420 iteration 3639 : loss : 0.037090, loss_ce: 0.019994
2021-11-30 13:20:12,963 iteration 3640 : loss : 0.040013, loss_ce: 0.018293
2021-11-30 13:20:14,439 iteration 3641 : loss : 0.044963, loss_ce: 0.016156
2021-11-30 13:20:15,963 iteration 3642 : loss : 0.041235, loss_ce: 0.020720
2021-11-30 13:20:17,446 iteration 3643 : loss : 0.044695, loss_ce: 0.023444
2021-11-30 13:20:18,924 iteration 3644 : loss : 0.036792, loss_ce: 0.021280
2021-11-30 13:20:20,366 iteration 3645 : loss : 0.030052, loss_ce: 0.015111
2021-11-30 13:20:21,860 iteration 3646 : loss : 0.029239, loss_ce: 0.014100
2021-11-30 13:20:23,389 iteration 3647 : loss : 0.038897, loss_ce: 0.019331
2021-11-30 13:20:25,003 iteration 3648 : loss : 0.060494, loss_ce: 0.026124
2021-11-30 13:20:26,518 iteration 3649 : loss : 0.043837, loss_ce: 0.016868
2021-11-30 13:20:28,014 iteration 3650 : loss : 0.050290, loss_ce: 0.020285
2021-11-30 13:20:29,523 iteration 3651 : loss : 0.035844, loss_ce: 0.016228
2021-11-30 13:20:30,946 iteration 3652 : loss : 0.040242, loss_ce: 0.019604
2021-11-30 13:20:32,493 iteration 3653 : loss : 0.066171, loss_ce: 0.019202
2021-11-30 13:20:33,977 iteration 3654 : loss : 0.034626, loss_ce: 0.014381
2021-11-30 13:20:33,977 Training Data Eval:
2021-11-30 13:20:41,333   Average segmentation loss on training set: 0.0200
2021-11-30 13:20:41,333 Validation Data Eval:
2021-11-30 13:20:43,865   Average segmentation loss on validation set: 0.0802
2021-11-30 13:20:45,264 iteration 3655 : loss : 0.037915, loss_ce: 0.014915
 54%|██████████████▌            | 215/400 [1:38:47<1:29:23, 28.99s/it]2021-11-30 13:20:46,887 iteration 3656 : loss : 0.100988, loss_ce: 0.027671
2021-11-30 13:20:48,339 iteration 3657 : loss : 0.038002, loss_ce: 0.022754
2021-11-30 13:20:49,823 iteration 3658 : loss : 0.053502, loss_ce: 0.028527
2021-11-30 13:20:51,357 iteration 3659 : loss : 0.029889, loss_ce: 0.013416
2021-11-30 13:20:52,968 iteration 3660 : loss : 0.052789, loss_ce: 0.022137
2021-11-30 13:20:54,578 iteration 3661 : loss : 0.040606, loss_ce: 0.020099
2021-11-30 13:20:55,973 iteration 3662 : loss : 0.025240, loss_ce: 0.012858
2021-11-30 13:20:57,432 iteration 3663 : loss : 0.027537, loss_ce: 0.011779
2021-11-30 13:20:58,912 iteration 3664 : loss : 0.039452, loss_ce: 0.014936
2021-11-30 13:21:00,386 iteration 3665 : loss : 0.028669, loss_ce: 0.015313
2021-11-30 13:21:01,995 iteration 3666 : loss : 0.059134, loss_ce: 0.028307
2021-11-30 13:21:03,533 iteration 3667 : loss : 0.052995, loss_ce: 0.022277
2021-11-30 13:21:05,044 iteration 3668 : loss : 0.040083, loss_ce: 0.018107
2021-11-30 13:21:06,533 iteration 3669 : loss : 0.042212, loss_ce: 0.024863
2021-11-30 13:21:07,979 iteration 3670 : loss : 0.053279, loss_ce: 0.021673
2021-11-30 13:21:09,518 iteration 3671 : loss : 0.041312, loss_ce: 0.020097
2021-11-30 13:21:10,965 iteration 3672 : loss : 0.035483, loss_ce: 0.016726
 54%|██████████████▌            | 216/400 [1:39:13<1:25:51, 28.00s/it]2021-11-30 13:21:12,479 iteration 3673 : loss : 0.040908, loss_ce: 0.021540
2021-11-30 13:21:13,964 iteration 3674 : loss : 0.043493, loss_ce: 0.022008
2021-11-30 13:21:15,442 iteration 3675 : loss : 0.033113, loss_ce: 0.020684
2021-11-30 13:21:16,908 iteration 3676 : loss : 0.026887, loss_ce: 0.011778
2021-11-30 13:21:18,379 iteration 3677 : loss : 0.040852, loss_ce: 0.021070
2021-11-30 13:21:19,959 iteration 3678 : loss : 0.052979, loss_ce: 0.021616
2021-11-30 13:21:21,431 iteration 3679 : loss : 0.033535, loss_ce: 0.016041
2021-11-30 13:21:22,897 iteration 3680 : loss : 0.059807, loss_ce: 0.019608
2021-11-30 13:21:24,431 iteration 3681 : loss : 0.031948, loss_ce: 0.014323
2021-11-30 13:21:25,882 iteration 3682 : loss : 0.036768, loss_ce: 0.017693
2021-11-30 13:21:27,417 iteration 3683 : loss : 0.122480, loss_ce: 0.022113
2021-11-30 13:21:28,865 iteration 3684 : loss : 0.026065, loss_ce: 0.011596
2021-11-30 13:21:30,335 iteration 3685 : loss : 0.047840, loss_ce: 0.025906
2021-11-30 13:21:31,842 iteration 3686 : loss : 0.047493, loss_ce: 0.024065
2021-11-30 13:21:33,342 iteration 3687 : loss : 0.032869, loss_ce: 0.014118
2021-11-30 13:21:34,825 iteration 3688 : loss : 0.072670, loss_ce: 0.034413
2021-11-30 13:21:36,242 iteration 3689 : loss : 0.047976, loss_ce: 0.021677
 54%|██████████████▋            | 217/400 [1:39:38<1:22:55, 27.19s/it]2021-11-30 13:21:37,728 iteration 3690 : loss : 0.031339, loss_ce: 0.012183
2021-11-30 13:21:39,299 iteration 3691 : loss : 0.074845, loss_ce: 0.019474
2021-11-30 13:21:40,779 iteration 3692 : loss : 0.039570, loss_ce: 0.019248
2021-11-30 13:21:42,336 iteration 3693 : loss : 0.051376, loss_ce: 0.022639
2021-11-30 13:21:43,832 iteration 3694 : loss : 0.028008, loss_ce: 0.014028
2021-11-30 13:21:45,268 iteration 3695 : loss : 0.033603, loss_ce: 0.015486
2021-11-30 13:21:46,808 iteration 3696 : loss : 0.047869, loss_ce: 0.027246
2021-11-30 13:21:48,292 iteration 3697 : loss : 0.034986, loss_ce: 0.013709
2021-11-30 13:21:49,768 iteration 3698 : loss : 0.040936, loss_ce: 0.018929
2021-11-30 13:21:51,322 iteration 3699 : loss : 0.063015, loss_ce: 0.027608
2021-11-30 13:21:52,802 iteration 3700 : loss : 0.048121, loss_ce: 0.024079
2021-11-30 13:21:54,244 iteration 3701 : loss : 0.042635, loss_ce: 0.019095
2021-11-30 13:21:55,682 iteration 3702 : loss : 0.062774, loss_ce: 0.026927
2021-11-30 13:21:57,309 iteration 3703 : loss : 0.047663, loss_ce: 0.024831
2021-11-30 13:21:58,742 iteration 3704 : loss : 0.035469, loss_ce: 0.013841
2021-11-30 13:22:00,174 iteration 3705 : loss : 0.031835, loss_ce: 0.013252
2021-11-30 13:22:01,603 iteration 3706 : loss : 0.034637, loss_ce: 0.012160
 55%|██████████████▋            | 218/400 [1:40:03<1:20:47, 26.64s/it]2021-11-30 13:22:03,144 iteration 3707 : loss : 0.071322, loss_ce: 0.030327
2021-11-30 13:22:04,628 iteration 3708 : loss : 0.037980, loss_ce: 0.019023
2021-11-30 13:22:06,112 iteration 3709 : loss : 0.029890, loss_ce: 0.012643
2021-11-30 13:22:07,643 iteration 3710 : loss : 0.043221, loss_ce: 0.018259
2021-11-30 13:22:09,098 iteration 3711 : loss : 0.026728, loss_ce: 0.013482
2021-11-30 13:22:10,615 iteration 3712 : loss : 0.054603, loss_ce: 0.020486
2021-11-30 13:22:12,061 iteration 3713 : loss : 0.033654, loss_ce: 0.015517
2021-11-30 13:22:13,506 iteration 3714 : loss : 0.045533, loss_ce: 0.018193
2021-11-30 13:22:14,982 iteration 3715 : loss : 0.044362, loss_ce: 0.020388
2021-11-30 13:22:16,439 iteration 3716 : loss : 0.035392, loss_ce: 0.016573
2021-11-30 13:22:17,953 iteration 3717 : loss : 0.046045, loss_ce: 0.021240
2021-11-30 13:22:19,494 iteration 3718 : loss : 0.031093, loss_ce: 0.016597
2021-11-30 13:22:20,923 iteration 3719 : loss : 0.029869, loss_ce: 0.012259
2021-11-30 13:22:22,442 iteration 3720 : loss : 0.031606, loss_ce: 0.018248
2021-11-30 13:22:23,952 iteration 3721 : loss : 0.029616, loss_ce: 0.015092
2021-11-30 13:22:25,412 iteration 3722 : loss : 0.032464, loss_ce: 0.015863
2021-11-30 13:22:26,859 iteration 3723 : loss : 0.043527, loss_ce: 0.017238
 55%|██████████████▊            | 219/400 [1:40:29<1:19:05, 26.22s/it]2021-11-30 13:22:28,315 iteration 3724 : loss : 0.038687, loss_ce: 0.022266
2021-11-30 13:22:29,761 iteration 3725 : loss : 0.034285, loss_ce: 0.016814
2021-11-30 13:22:31,321 iteration 3726 : loss : 0.051323, loss_ce: 0.028183
2021-11-30 13:22:32,817 iteration 3727 : loss : 0.031412, loss_ce: 0.015743
2021-11-30 13:22:34,327 iteration 3728 : loss : 0.054930, loss_ce: 0.019144
2021-11-30 13:22:35,775 iteration 3729 : loss : 0.032441, loss_ce: 0.012799
2021-11-30 13:22:37,258 iteration 3730 : loss : 0.033540, loss_ce: 0.017201
2021-11-30 13:22:38,713 iteration 3731 : loss : 0.035682, loss_ce: 0.015024
2021-11-30 13:22:40,223 iteration 3732 : loss : 0.035235, loss_ce: 0.014608
2021-11-30 13:22:41,657 iteration 3733 : loss : 0.037257, loss_ce: 0.017840
2021-11-30 13:22:43,161 iteration 3734 : loss : 0.029001, loss_ce: 0.014676
2021-11-30 13:22:44,633 iteration 3735 : loss : 0.034457, loss_ce: 0.012809
2021-11-30 13:22:46,177 iteration 3736 : loss : 0.044879, loss_ce: 0.023133
2021-11-30 13:22:47,623 iteration 3737 : loss : 0.031631, loss_ce: 0.014346
2021-11-30 13:22:49,079 iteration 3738 : loss : 0.027491, loss_ce: 0.012768
2021-11-30 13:22:50,522 iteration 3739 : loss : 0.041255, loss_ce: 0.018668
2021-11-30 13:22:50,522 Training Data Eval:
2021-11-30 13:22:57,901   Average segmentation loss on training set: 0.0192
2021-11-30 13:22:57,902 Validation Data Eval:
2021-11-30 13:23:00,464   Average segmentation loss on validation set: 0.0832
2021-11-30 13:23:02,132 iteration 3740 : loss : 0.050131, loss_ce: 0.023695
 55%|██████████████▊            | 220/400 [1:41:04<1:26:48, 28.94s/it]2021-11-30 13:23:03,820 iteration 3741 : loss : 0.045308, loss_ce: 0.016390
2021-11-30 13:23:05,282 iteration 3742 : loss : 0.043227, loss_ce: 0.024094
2021-11-30 13:23:06,764 iteration 3743 : loss : 0.040842, loss_ce: 0.019768
2021-11-30 13:23:08,297 iteration 3744 : loss : 0.051332, loss_ce: 0.020286
2021-11-30 13:23:09,850 iteration 3745 : loss : 0.034876, loss_ce: 0.013864
2021-11-30 13:23:11,404 iteration 3746 : loss : 0.037627, loss_ce: 0.016713
2021-11-30 13:23:12,857 iteration 3747 : loss : 0.038897, loss_ce: 0.015975
2021-11-30 13:23:14,328 iteration 3748 : loss : 0.034226, loss_ce: 0.018611
2021-11-30 13:23:15,880 iteration 3749 : loss : 0.041944, loss_ce: 0.017271
2021-11-30 13:23:17,461 iteration 3750 : loss : 0.048840, loss_ce: 0.018665
2021-11-30 13:23:18,963 iteration 3751 : loss : 0.052862, loss_ce: 0.019139
2021-11-30 13:23:20,434 iteration 3752 : loss : 0.044009, loss_ce: 0.015017
2021-11-30 13:23:21,882 iteration 3753 : loss : 0.039530, loss_ce: 0.015892
2021-11-30 13:23:23,398 iteration 3754 : loss : 0.033421, loss_ce: 0.015076
2021-11-30 13:23:24,951 iteration 3755 : loss : 0.035625, loss_ce: 0.017840
2021-11-30 13:23:26,365 iteration 3756 : loss : 0.034217, loss_ce: 0.017800
2021-11-30 13:23:27,803 iteration 3757 : loss : 0.033697, loss_ce: 0.017092
 55%|██████████████▉            | 221/400 [1:41:30<1:23:25, 27.96s/it]2021-11-30 13:23:29,441 iteration 3758 : loss : 0.040451, loss_ce: 0.016221
2021-11-30 13:23:30,868 iteration 3759 : loss : 0.032688, loss_ce: 0.014547
2021-11-30 13:23:32,335 iteration 3760 : loss : 0.032835, loss_ce: 0.015975
2021-11-30 13:23:33,862 iteration 3761 : loss : 0.043775, loss_ce: 0.020179
2021-11-30 13:23:35,407 iteration 3762 : loss : 0.032841, loss_ce: 0.017699
2021-11-30 13:23:36,980 iteration 3763 : loss : 0.049496, loss_ce: 0.020164
2021-11-30 13:23:38,454 iteration 3764 : loss : 0.036066, loss_ce: 0.016386
2021-11-30 13:23:39,887 iteration 3765 : loss : 0.042930, loss_ce: 0.018337
2021-11-30 13:23:41,435 iteration 3766 : loss : 0.077640, loss_ce: 0.027789
2021-11-30 13:23:42,908 iteration 3767 : loss : 0.028526, loss_ce: 0.012480
2021-11-30 13:23:44,338 iteration 3768 : loss : 0.038297, loss_ce: 0.015094
2021-11-30 13:23:45,809 iteration 3769 : loss : 0.033620, loss_ce: 0.017313
2021-11-30 13:23:47,354 iteration 3770 : loss : 0.057006, loss_ce: 0.014567
2021-11-30 13:23:48,967 iteration 3771 : loss : 0.046644, loss_ce: 0.019729
2021-11-30 13:23:50,489 iteration 3772 : loss : 0.048717, loss_ce: 0.016289
2021-11-30 13:23:51,999 iteration 3773 : loss : 0.031247, loss_ce: 0.015833
2021-11-30 13:23:53,572 iteration 3774 : loss : 0.078452, loss_ce: 0.040258
 56%|██████████████▉            | 222/400 [1:41:55<1:20:59, 27.30s/it]2021-11-30 13:23:55,092 iteration 3775 : loss : 0.033200, loss_ce: 0.015606
2021-11-30 13:23:56,501 iteration 3776 : loss : 0.043670, loss_ce: 0.021091
2021-11-30 13:23:57,966 iteration 3777 : loss : 0.029334, loss_ce: 0.011430
2021-11-30 13:23:59,410 iteration 3778 : loss : 0.027373, loss_ce: 0.013792
2021-11-30 13:24:00,865 iteration 3779 : loss : 0.035556, loss_ce: 0.017651
2021-11-30 13:24:02,377 iteration 3780 : loss : 0.042586, loss_ce: 0.017174
2021-11-30 13:24:03,883 iteration 3781 : loss : 0.044083, loss_ce: 0.020419
2021-11-30 13:24:05,382 iteration 3782 : loss : 0.086972, loss_ce: 0.024413
2021-11-30 13:24:06,893 iteration 3783 : loss : 0.028960, loss_ce: 0.015468
2021-11-30 13:24:08,410 iteration 3784 : loss : 0.035244, loss_ce: 0.019598
2021-11-30 13:24:09,854 iteration 3785 : loss : 0.037067, loss_ce: 0.019089
2021-11-30 13:24:11,295 iteration 3786 : loss : 0.034567, loss_ce: 0.018241
2021-11-30 13:24:12,742 iteration 3787 : loss : 0.032279, loss_ce: 0.014420
2021-11-30 13:24:14,295 iteration 3788 : loss : 0.049205, loss_ce: 0.016143
2021-11-30 13:24:15,771 iteration 3789 : loss : 0.043172, loss_ce: 0.016827
2021-11-30 13:24:17,306 iteration 3790 : loss : 0.047932, loss_ce: 0.020973
2021-11-30 13:24:18,798 iteration 3791 : loss : 0.032606, loss_ce: 0.015818
 56%|███████████████            | 223/400 [1:42:21<1:18:42, 26.68s/it]2021-11-30 13:24:20,284 iteration 3792 : loss : 0.029695, loss_ce: 0.013406
2021-11-30 13:24:21,805 iteration 3793 : loss : 0.036454, loss_ce: 0.016514
2021-11-30 13:24:23,303 iteration 3794 : loss : 0.042667, loss_ce: 0.019139
2021-11-30 13:24:24,795 iteration 3795 : loss : 0.034954, loss_ce: 0.014486
2021-11-30 13:24:26,188 iteration 3796 : loss : 0.027635, loss_ce: 0.012547
2021-11-30 13:24:27,657 iteration 3797 : loss : 0.031499, loss_ce: 0.013773
2021-11-30 13:24:29,084 iteration 3798 : loss : 0.026349, loss_ce: 0.013158
2021-11-30 13:24:30,563 iteration 3799 : loss : 0.049040, loss_ce: 0.019203
2021-11-30 13:24:32,025 iteration 3800 : loss : 0.028865, loss_ce: 0.011453
2021-11-30 13:24:33,495 iteration 3801 : loss : 0.073094, loss_ce: 0.021935
2021-11-30 13:24:34,959 iteration 3802 : loss : 0.042730, loss_ce: 0.022228
2021-11-30 13:24:36,433 iteration 3803 : loss : 0.027714, loss_ce: 0.011827
2021-11-30 13:24:37,893 iteration 3804 : loss : 0.033742, loss_ce: 0.017343
2021-11-30 13:24:39,349 iteration 3805 : loss : 0.046402, loss_ce: 0.021010
2021-11-30 13:24:40,784 iteration 3806 : loss : 0.044644, loss_ce: 0.025833
2021-11-30 13:24:42,302 iteration 3807 : loss : 0.034906, loss_ce: 0.017239
2021-11-30 13:24:43,799 iteration 3808 : loss : 0.048435, loss_ce: 0.020897
 56%|███████████████            | 224/400 [1:42:46<1:16:46, 26.18s/it]2021-11-30 13:24:45,274 iteration 3809 : loss : 0.030645, loss_ce: 0.013368
2021-11-30 13:24:46,718 iteration 3810 : loss : 0.028374, loss_ce: 0.013910
2021-11-30 13:24:48,181 iteration 3811 : loss : 0.032434, loss_ce: 0.012847
2021-11-30 13:24:49,664 iteration 3812 : loss : 0.040979, loss_ce: 0.024141
2021-11-30 13:24:51,125 iteration 3813 : loss : 0.037564, loss_ce: 0.019639
2021-11-30 13:24:52,642 iteration 3814 : loss : 0.034548, loss_ce: 0.014971
2021-11-30 13:24:54,112 iteration 3815 : loss : 0.031371, loss_ce: 0.014686
2021-11-30 13:24:55,613 iteration 3816 : loss : 0.030076, loss_ce: 0.014385
2021-11-30 13:24:57,054 iteration 3817 : loss : 0.029715, loss_ce: 0.013714
2021-11-30 13:24:58,584 iteration 3818 : loss : 0.042220, loss_ce: 0.021589
2021-11-30 13:25:00,127 iteration 3819 : loss : 0.039752, loss_ce: 0.021475
2021-11-30 13:25:01,639 iteration 3820 : loss : 0.038247, loss_ce: 0.016869
2021-11-30 13:25:03,148 iteration 3821 : loss : 0.041961, loss_ce: 0.019791
2021-11-30 13:25:04,573 iteration 3822 : loss : 0.049741, loss_ce: 0.020981
2021-11-30 13:25:06,153 iteration 3823 : loss : 0.041546, loss_ce: 0.014489
2021-11-30 13:25:07,649 iteration 3824 : loss : 0.034783, loss_ce: 0.016272
2021-11-30 13:25:07,650 Training Data Eval:
2021-11-30 13:25:15,040   Average segmentation loss on training set: 0.0183
2021-11-30 13:25:15,041 Validation Data Eval:
2021-11-30 13:25:17,602   Average segmentation loss on validation set: 0.0873
2021-11-30 13:25:19,188 iteration 3825 : loss : 0.034466, loss_ce: 0.015700
 56%|███████████████▏           | 225/400 [1:43:21<1:24:24, 28.94s/it]2021-11-30 13:25:20,756 iteration 3826 : loss : 0.033897, loss_ce: 0.016351
2021-11-30 13:25:22,168 iteration 3827 : loss : 0.034775, loss_ce: 0.015819
2021-11-30 13:25:23,666 iteration 3828 : loss : 0.029686, loss_ce: 0.014136
2021-11-30 13:25:25,171 iteration 3829 : loss : 0.048051, loss_ce: 0.018029
2021-11-30 13:25:26,632 iteration 3830 : loss : 0.028138, loss_ce: 0.013676
2021-11-30 13:25:28,082 iteration 3831 : loss : 0.030730, loss_ce: 0.014051
2021-11-30 13:25:29,517 iteration 3832 : loss : 0.035564, loss_ce: 0.013720
2021-11-30 13:25:30,994 iteration 3833 : loss : 0.030064, loss_ce: 0.016093
2021-11-30 13:25:32,518 iteration 3834 : loss : 0.036837, loss_ce: 0.018831
2021-11-30 13:25:34,027 iteration 3835 : loss : 0.044465, loss_ce: 0.018968
2021-11-30 13:25:35,529 iteration 3836 : loss : 0.040207, loss_ce: 0.019616
2021-11-30 13:25:37,058 iteration 3837 : loss : 0.032936, loss_ce: 0.015270
2021-11-30 13:25:38,648 iteration 3838 : loss : 0.033021, loss_ce: 0.014612
2021-11-30 13:25:40,125 iteration 3839 : loss : 0.034604, loss_ce: 0.018689
2021-11-30 13:25:41,619 iteration 3840 : loss : 0.050253, loss_ce: 0.017149
2021-11-30 13:25:43,059 iteration 3841 : loss : 0.031948, loss_ce: 0.014971
2021-11-30 13:25:44,567 iteration 3842 : loss : 0.057757, loss_ce: 0.030004
 56%|███████████████▎           | 226/400 [1:43:46<1:20:49, 27.87s/it]2021-11-30 13:25:46,076 iteration 3843 : loss : 0.030876, loss_ce: 0.015564
2021-11-30 13:25:47,607 iteration 3844 : loss : 0.044793, loss_ce: 0.016255
2021-11-30 13:25:49,106 iteration 3845 : loss : 0.034319, loss_ce: 0.018959
2021-11-30 13:25:50,550 iteration 3846 : loss : 0.024296, loss_ce: 0.012229
2021-11-30 13:25:52,005 iteration 3847 : loss : 0.035154, loss_ce: 0.016319
2021-11-30 13:25:53,536 iteration 3848 : loss : 0.028500, loss_ce: 0.017149
2021-11-30 13:25:55,128 iteration 3849 : loss : 0.045513, loss_ce: 0.022634
2021-11-30 13:25:56,669 iteration 3850 : loss : 0.054989, loss_ce: 0.016445
2021-11-30 13:25:58,160 iteration 3851 : loss : 0.035291, loss_ce: 0.019698
2021-11-30 13:25:59,675 iteration 3852 : loss : 0.030578, loss_ce: 0.012309
2021-11-30 13:26:01,113 iteration 3853 : loss : 0.031235, loss_ce: 0.012082
2021-11-30 13:26:02,641 iteration 3854 : loss : 0.039153, loss_ce: 0.015312
2021-11-30 13:26:04,298 iteration 3855 : loss : 0.043165, loss_ce: 0.019849
2021-11-30 13:26:05,661 iteration 3856 : loss : 0.034551, loss_ce: 0.018526
2021-11-30 13:26:07,139 iteration 3857 : loss : 0.037093, loss_ce: 0.019543
2021-11-30 13:26:08,513 iteration 3858 : loss : 0.041375, loss_ce: 0.019189
2021-11-30 13:26:09,996 iteration 3859 : loss : 0.031036, loss_ce: 0.014930
 57%|███████████████▎           | 227/400 [1:44:12<1:18:15, 27.14s/it]2021-11-30 13:26:11,551 iteration 3860 : loss : 0.039693, loss_ce: 0.017150
2021-11-30 13:26:13,123 iteration 3861 : loss : 0.055449, loss_ce: 0.025951
2021-11-30 13:26:14,626 iteration 3862 : loss : 0.034006, loss_ce: 0.015552
2021-11-30 13:26:16,132 iteration 3863 : loss : 0.032821, loss_ce: 0.014998
2021-11-30 13:26:17,647 iteration 3864 : loss : 0.043414, loss_ce: 0.019114
2021-11-30 13:26:19,128 iteration 3865 : loss : 0.038118, loss_ce: 0.015468
2021-11-30 13:26:20,739 iteration 3866 : loss : 0.039888, loss_ce: 0.015754
2021-11-30 13:26:22,205 iteration 3867 : loss : 0.039919, loss_ce: 0.016893
2021-11-30 13:26:23,732 iteration 3868 : loss : 0.054243, loss_ce: 0.018563
2021-11-30 13:26:25,250 iteration 3869 : loss : 0.039926, loss_ce: 0.021350
2021-11-30 13:26:26,790 iteration 3870 : loss : 0.038491, loss_ce: 0.019103
2021-11-30 13:26:28,311 iteration 3871 : loss : 0.037233, loss_ce: 0.014532
2021-11-30 13:26:29,763 iteration 3872 : loss : 0.036690, loss_ce: 0.016392
2021-11-30 13:26:31,259 iteration 3873 : loss : 0.030630, loss_ce: 0.018380
2021-11-30 13:26:32,771 iteration 3874 : loss : 0.040753, loss_ce: 0.018883
2021-11-30 13:26:34,270 iteration 3875 : loss : 0.038664, loss_ce: 0.018057
2021-11-30 13:26:35,784 iteration 3876 : loss : 0.043926, loss_ce: 0.017363
 57%|███████████████▍           | 228/400 [1:44:38<1:16:37, 26.73s/it]2021-11-30 13:26:37,274 iteration 3877 : loss : 0.037268, loss_ce: 0.018229
2021-11-30 13:26:38,797 iteration 3878 : loss : 0.040361, loss_ce: 0.019801
2021-11-30 13:26:40,265 iteration 3879 : loss : 0.027354, loss_ce: 0.012345
2021-11-30 13:26:41,789 iteration 3880 : loss : 0.048443, loss_ce: 0.021260
2021-11-30 13:26:43,250 iteration 3881 : loss : 0.036997, loss_ce: 0.019778
2021-11-30 13:26:44,846 iteration 3882 : loss : 0.095185, loss_ce: 0.022302
2021-11-30 13:26:46,382 iteration 3883 : loss : 0.056622, loss_ce: 0.019201
2021-11-30 13:26:47,887 iteration 3884 : loss : 0.049911, loss_ce: 0.022404
2021-11-30 13:26:49,432 iteration 3885 : loss : 0.066050, loss_ce: 0.017674
2021-11-30 13:26:50,975 iteration 3886 : loss : 0.033777, loss_ce: 0.015698
2021-11-30 13:26:52,415 iteration 3887 : loss : 0.024187, loss_ce: 0.012292
2021-11-30 13:26:53,970 iteration 3888 : loss : 0.035675, loss_ce: 0.020056
2021-11-30 13:26:55,478 iteration 3889 : loss : 0.036142, loss_ce: 0.015891
2021-11-30 13:26:57,090 iteration 3890 : loss : 0.054868, loss_ce: 0.022762
2021-11-30 13:26:58,547 iteration 3891 : loss : 0.042802, loss_ce: 0.019949
2021-11-30 13:27:00,045 iteration 3892 : loss : 0.039927, loss_ce: 0.020284
2021-11-30 13:27:01,574 iteration 3893 : loss : 0.047941, loss_ce: 0.019833
 57%|███████████████▍           | 229/400 [1:45:03<1:15:23, 26.45s/it]2021-11-30 13:27:03,086 iteration 3894 : loss : 0.038891, loss_ce: 0.015335
2021-11-30 13:27:04,517 iteration 3895 : loss : 0.035000, loss_ce: 0.016458
2021-11-30 13:27:06,017 iteration 3896 : loss : 0.047904, loss_ce: 0.022182
2021-11-30 13:27:07,569 iteration 3897 : loss : 0.040367, loss_ce: 0.018315
2021-11-30 13:27:09,099 iteration 3898 : loss : 0.044541, loss_ce: 0.020557
2021-11-30 13:27:10,515 iteration 3899 : loss : 0.034262, loss_ce: 0.015541
2021-11-30 13:27:11,970 iteration 3900 : loss : 0.031774, loss_ce: 0.013410
2021-11-30 13:27:13,399 iteration 3901 : loss : 0.036064, loss_ce: 0.016888
2021-11-30 13:27:14,847 iteration 3902 : loss : 0.037786, loss_ce: 0.017035
2021-11-30 13:27:16,263 iteration 3903 : loss : 0.037973, loss_ce: 0.015727
2021-11-30 13:27:17,738 iteration 3904 : loss : 0.036901, loss_ce: 0.015587
2021-11-30 13:27:19,222 iteration 3905 : loss : 0.041521, loss_ce: 0.017579
2021-11-30 13:27:20,620 iteration 3906 : loss : 0.023554, loss_ce: 0.012610
2021-11-30 13:27:22,011 iteration 3907 : loss : 0.024004, loss_ce: 0.011926
2021-11-30 13:27:23,419 iteration 3908 : loss : 0.028494, loss_ce: 0.013815
2021-11-30 13:27:24,893 iteration 3909 : loss : 0.043610, loss_ce: 0.025165
2021-11-30 13:27:24,893 Training Data Eval:
2021-11-30 13:27:32,276   Average segmentation loss on training set: 0.0192
2021-11-30 13:27:32,277 Validation Data Eval:
2021-11-30 13:27:34,847   Average segmentation loss on validation set: 0.0884
2021-11-30 13:27:36,418 iteration 3910 : loss : 0.042225, loss_ce: 0.017455
 57%|███████████████▌           | 230/400 [1:45:38<1:22:05, 28.97s/it]2021-11-30 13:27:37,901 iteration 3911 : loss : 0.028798, loss_ce: 0.016001
2021-11-30 13:27:39,411 iteration 3912 : loss : 0.041378, loss_ce: 0.018500
2021-11-30 13:27:40,892 iteration 3913 : loss : 0.034842, loss_ce: 0.016008
2021-11-30 13:27:42,380 iteration 3914 : loss : 0.027087, loss_ce: 0.015165
2021-11-30 13:27:43,848 iteration 3915 : loss : 0.032755, loss_ce: 0.015473
2021-11-30 13:27:45,285 iteration 3916 : loss : 0.024550, loss_ce: 0.012180
2021-11-30 13:27:46,810 iteration 3917 : loss : 0.043068, loss_ce: 0.018973
2021-11-30 13:27:48,347 iteration 3918 : loss : 0.058628, loss_ce: 0.018821
2021-11-30 13:27:49,992 iteration 3919 : loss : 0.046164, loss_ce: 0.016860
2021-11-30 13:27:51,429 iteration 3920 : loss : 0.030687, loss_ce: 0.016415
2021-11-30 13:27:52,917 iteration 3921 : loss : 0.029676, loss_ce: 0.015006
2021-11-30 13:27:54,449 iteration 3922 : loss : 0.035131, loss_ce: 0.014846
2021-11-30 13:27:55,904 iteration 3923 : loss : 0.032192, loss_ce: 0.017901
2021-11-30 13:27:57,340 iteration 3924 : loss : 0.045938, loss_ce: 0.016321
2021-11-30 13:27:58,910 iteration 3925 : loss : 0.047482, loss_ce: 0.020857
2021-11-30 13:28:00,453 iteration 3926 : loss : 0.028164, loss_ce: 0.013034
2021-11-30 13:28:01,909 iteration 3927 : loss : 0.040239, loss_ce: 0.019101
 58%|███████████████▌           | 231/400 [1:46:04<1:18:42, 27.95s/it]2021-11-30 13:28:03,558 iteration 3928 : loss : 0.056531, loss_ce: 0.026225
2021-11-30 13:28:05,017 iteration 3929 : loss : 0.064567, loss_ce: 0.028215
2021-11-30 13:28:06,538 iteration 3930 : loss : 0.038118, loss_ce: 0.019357
2021-11-30 13:28:08,054 iteration 3931 : loss : 0.036033, loss_ce: 0.017566
2021-11-30 13:28:09,585 iteration 3932 : loss : 0.046254, loss_ce: 0.017724
2021-11-30 13:28:11,154 iteration 3933 : loss : 0.032088, loss_ce: 0.014798
2021-11-30 13:28:12,670 iteration 3934 : loss : 0.025912, loss_ce: 0.013002
2021-11-30 13:28:14,072 iteration 3935 : loss : 0.028927, loss_ce: 0.015047
2021-11-30 13:28:15,706 iteration 3936 : loss : 0.038096, loss_ce: 0.018679
2021-11-30 13:28:17,155 iteration 3937 : loss : 0.035445, loss_ce: 0.019634
2021-11-30 13:28:18,699 iteration 3938 : loss : 0.099489, loss_ce: 0.020827
2021-11-30 13:28:20,095 iteration 3939 : loss : 0.040626, loss_ce: 0.015142
2021-11-30 13:28:21,564 iteration 3940 : loss : 0.036322, loss_ce: 0.016508
2021-11-30 13:28:23,047 iteration 3941 : loss : 0.033805, loss_ce: 0.016912
2021-11-30 13:28:24,591 iteration 3942 : loss : 0.072034, loss_ce: 0.033384
2021-11-30 13:28:26,030 iteration 3943 : loss : 0.050992, loss_ce: 0.015930
2021-11-30 13:28:27,576 iteration 3944 : loss : 0.042727, loss_ce: 0.018304
 58%|███████████████▋           | 232/400 [1:46:29<1:16:16, 27.24s/it]2021-11-30 13:28:29,120 iteration 3945 : loss : 0.045257, loss_ce: 0.024109
2021-11-30 13:28:30,502 iteration 3946 : loss : 0.032817, loss_ce: 0.018379
2021-11-30 13:28:31,974 iteration 3947 : loss : 0.031087, loss_ce: 0.012878
2021-11-30 13:28:33,605 iteration 3948 : loss : 0.048622, loss_ce: 0.018378
2021-11-30 13:28:35,142 iteration 3949 : loss : 0.045953, loss_ce: 0.021277
2021-11-30 13:28:36,700 iteration 3950 : loss : 0.042102, loss_ce: 0.015845
2021-11-30 13:28:38,116 iteration 3951 : loss : 0.032109, loss_ce: 0.015913
2021-11-30 13:28:39,676 iteration 3952 : loss : 0.037626, loss_ce: 0.016848
2021-11-30 13:28:41,236 iteration 3953 : loss : 0.059540, loss_ce: 0.025607
2021-11-30 13:28:42,713 iteration 3954 : loss : 0.034731, loss_ce: 0.014676
2021-11-30 13:28:44,176 iteration 3955 : loss : 0.026429, loss_ce: 0.013338
2021-11-30 13:28:45,769 iteration 3956 : loss : 0.044229, loss_ce: 0.020447
2021-11-30 13:28:47,190 iteration 3957 : loss : 0.043231, loss_ce: 0.021109
2021-11-30 13:28:48,648 iteration 3958 : loss : 0.038439, loss_ce: 0.016796
2021-11-30 13:28:50,226 iteration 3959 : loss : 0.044702, loss_ce: 0.018851
2021-11-30 13:28:51,676 iteration 3960 : loss : 0.046662, loss_ce: 0.024083
2021-11-30 13:28:53,159 iteration 3961 : loss : 0.032543, loss_ce: 0.017387
 58%|███████████████▋           | 233/400 [1:46:55<1:14:26, 26.74s/it]2021-11-30 13:28:54,726 iteration 3962 : loss : 0.036722, loss_ce: 0.017134
2021-11-30 13:28:56,196 iteration 3963 : loss : 0.037219, loss_ce: 0.018413
2021-11-30 13:28:57,783 iteration 3964 : loss : 0.043617, loss_ce: 0.020250
2021-11-30 13:28:59,203 iteration 3965 : loss : 0.038402, loss_ce: 0.015656
2021-11-30 13:29:00,737 iteration 3966 : loss : 0.039163, loss_ce: 0.017386
2021-11-30 13:29:02,237 iteration 3967 : loss : 0.038446, loss_ce: 0.016822
2021-11-30 13:29:03,620 iteration 3968 : loss : 0.049007, loss_ce: 0.016727
2021-11-30 13:29:05,134 iteration 3969 : loss : 0.041332, loss_ce: 0.015332
2021-11-30 13:29:06,616 iteration 3970 : loss : 0.042828, loss_ce: 0.017966
2021-11-30 13:29:07,991 iteration 3971 : loss : 0.027364, loss_ce: 0.013877
2021-11-30 13:29:09,529 iteration 3972 : loss : 0.037209, loss_ce: 0.012787
2021-11-30 13:29:10,960 iteration 3973 : loss : 0.044646, loss_ce: 0.024077
2021-11-30 13:29:12,335 iteration 3974 : loss : 0.025071, loss_ce: 0.011016
2021-11-30 13:29:13,829 iteration 3975 : loss : 0.032893, loss_ce: 0.015355
2021-11-30 13:29:15,384 iteration 3976 : loss : 0.049980, loss_ce: 0.021897
2021-11-30 13:29:16,944 iteration 3977 : loss : 0.042386, loss_ce: 0.018388
2021-11-30 13:29:18,535 iteration 3978 : loss : 0.048560, loss_ce: 0.018804
 58%|███████████████▊           | 234/400 [1:47:20<1:12:51, 26.33s/it]2021-11-30 13:29:20,067 iteration 3979 : loss : 0.031414, loss_ce: 0.014583
2021-11-30 13:29:21,582 iteration 3980 : loss : 0.049554, loss_ce: 0.020345
2021-11-30 13:29:23,029 iteration 3981 : loss : 0.039518, loss_ce: 0.019886
2021-11-30 13:29:24,400 iteration 3982 : loss : 0.027259, loss_ce: 0.012880
2021-11-30 13:29:25,812 iteration 3983 : loss : 0.029983, loss_ce: 0.016597
2021-11-30 13:29:27,266 iteration 3984 : loss : 0.029367, loss_ce: 0.014485
2021-11-30 13:29:28,786 iteration 3985 : loss : 0.050125, loss_ce: 0.026634
2021-11-30 13:29:30,364 iteration 3986 : loss : 0.049548, loss_ce: 0.022279
2021-11-30 13:29:31,858 iteration 3987 : loss : 0.042131, loss_ce: 0.018827
2021-11-30 13:29:33,331 iteration 3988 : loss : 0.041600, loss_ce: 0.022275
2021-11-30 13:29:34,795 iteration 3989 : loss : 0.052737, loss_ce: 0.016658
2021-11-30 13:29:36,248 iteration 3990 : loss : 0.051957, loss_ce: 0.018850
2021-11-30 13:29:37,804 iteration 3991 : loss : 0.050862, loss_ce: 0.014498
2021-11-30 13:29:39,312 iteration 3992 : loss : 0.042639, loss_ce: 0.017105
2021-11-30 13:29:40,858 iteration 3993 : loss : 0.042114, loss_ce: 0.018297
2021-11-30 13:29:42,303 iteration 3994 : loss : 0.027859, loss_ce: 0.016193
2021-11-30 13:29:42,303 Training Data Eval:
2021-11-30 13:29:49,672   Average segmentation loss on training set: 0.0181
2021-11-30 13:29:49,672 Validation Data Eval:
2021-11-30 13:29:52,214   Average segmentation loss on validation set: 0.0939
2021-11-30 13:29:53,639 iteration 3995 : loss : 0.037294, loss_ce: 0.019691
 59%|███████████████▊           | 235/400 [1:47:55<1:19:39, 28.97s/it]2021-11-30 13:29:55,261 iteration 3996 : loss : 0.035818, loss_ce: 0.014216
2021-11-30 13:29:56,706 iteration 3997 : loss : 0.039677, loss_ce: 0.015422
2021-11-30 13:29:58,221 iteration 3998 : loss : 0.037670, loss_ce: 0.021227
2021-11-30 13:29:59,677 iteration 3999 : loss : 0.033416, loss_ce: 0.011769
2021-11-30 13:30:01,221 iteration 4000 : loss : 0.039539, loss_ce: 0.022959
2021-11-30 13:30:02,638 iteration 4001 : loss : 0.027957, loss_ce: 0.013976
2021-11-30 13:30:04,142 iteration 4002 : loss : 0.029560, loss_ce: 0.014359
2021-11-30 13:30:05,643 iteration 4003 : loss : 0.034057, loss_ce: 0.017691
2021-11-30 13:30:07,174 iteration 4004 : loss : 0.034869, loss_ce: 0.012601
2021-11-30 13:30:08,665 iteration 4005 : loss : 0.049309, loss_ce: 0.024295
2021-11-30 13:30:10,091 iteration 4006 : loss : 0.034644, loss_ce: 0.015729
2021-11-30 13:30:11,626 iteration 4007 : loss : 0.041578, loss_ce: 0.015340
2021-11-30 13:30:13,075 iteration 4008 : loss : 0.028140, loss_ce: 0.014255
2021-11-30 13:30:14,571 iteration 4009 : loss : 0.033862, loss_ce: 0.018354
2021-11-30 13:30:16,120 iteration 4010 : loss : 0.040904, loss_ce: 0.020817
2021-11-30 13:30:17,553 iteration 4011 : loss : 0.039884, loss_ce: 0.015423
2021-11-30 13:30:19,106 iteration 4012 : loss : 0.044861, loss_ce: 0.019798
 59%|███████████████▉           | 236/400 [1:48:21<1:16:17, 27.91s/it]2021-11-30 13:30:20,711 iteration 4013 : loss : 0.056402, loss_ce: 0.022518
2021-11-30 13:30:22,141 iteration 4014 : loss : 0.032840, loss_ce: 0.012813
2021-11-30 13:30:23,681 iteration 4015 : loss : 0.049437, loss_ce: 0.020892
2021-11-30 13:30:25,196 iteration 4016 : loss : 0.033397, loss_ce: 0.015589
2021-11-30 13:30:26,753 iteration 4017 : loss : 0.040411, loss_ce: 0.016606
2021-11-30 13:30:28,186 iteration 4018 : loss : 0.032429, loss_ce: 0.014409
2021-11-30 13:30:29,642 iteration 4019 : loss : 0.034806, loss_ce: 0.019155
2021-11-30 13:30:31,075 iteration 4020 : loss : 0.031004, loss_ce: 0.018555
2021-11-30 13:30:32,538 iteration 4021 : loss : 0.028394, loss_ce: 0.015140
2021-11-30 13:30:34,070 iteration 4022 : loss : 0.040076, loss_ce: 0.020665
2021-11-30 13:30:35,576 iteration 4023 : loss : 0.027423, loss_ce: 0.012425
2021-11-30 13:30:37,001 iteration 4024 : loss : 0.028584, loss_ce: 0.012363
2021-11-30 13:30:38,437 iteration 4025 : loss : 0.035809, loss_ce: 0.014726
2021-11-30 13:30:39,956 iteration 4026 : loss : 0.033705, loss_ce: 0.016319
2021-11-30 13:30:41,471 iteration 4027 : loss : 0.040754, loss_ce: 0.017069
2021-11-30 13:30:43,017 iteration 4028 : loss : 0.043653, loss_ce: 0.019075
2021-11-30 13:30:44,454 iteration 4029 : loss : 0.028792, loss_ce: 0.014948
 59%|███████████████▉           | 237/400 [1:48:46<1:13:45, 27.15s/it]2021-11-30 13:30:46,060 iteration 4030 : loss : 0.044421, loss_ce: 0.020435
2021-11-30 13:30:47,575 iteration 4031 : loss : 0.053976, loss_ce: 0.018703
2021-11-30 13:30:49,064 iteration 4032 : loss : 0.040674, loss_ce: 0.020948
2021-11-30 13:30:50,549 iteration 4033 : loss : 0.045874, loss_ce: 0.020193
2021-11-30 13:30:51,965 iteration 4034 : loss : 0.030805, loss_ce: 0.014281
2021-11-30 13:30:53,419 iteration 4035 : loss : 0.027867, loss_ce: 0.012328
2021-11-30 13:30:54,952 iteration 4036 : loss : 0.052909, loss_ce: 0.020946
2021-11-30 13:30:56,389 iteration 4037 : loss : 0.030234, loss_ce: 0.016372
2021-11-30 13:30:57,935 iteration 4038 : loss : 0.037308, loss_ce: 0.016368
2021-11-30 13:30:59,506 iteration 4039 : loss : 0.032565, loss_ce: 0.015512
2021-11-30 13:31:01,072 iteration 4040 : loss : 0.049834, loss_ce: 0.022559
2021-11-30 13:31:02,620 iteration 4041 : loss : 0.041900, loss_ce: 0.018767
2021-11-30 13:31:04,067 iteration 4042 : loss : 0.039944, loss_ce: 0.017847
2021-11-30 13:31:05,574 iteration 4043 : loss : 0.191985, loss_ce: 0.018774
2021-11-30 13:31:07,021 iteration 4044 : loss : 0.035659, loss_ce: 0.016965
2021-11-30 13:31:08,496 iteration 4045 : loss : 0.038205, loss_ce: 0.021241
2021-11-30 13:31:10,043 iteration 4046 : loss : 0.036006, loss_ce: 0.019648
 60%|████████████████           | 238/400 [1:49:12<1:12:01, 26.68s/it]2021-11-30 13:31:11,687 iteration 4047 : loss : 0.041030, loss_ce: 0.021121
2021-11-30 13:31:13,246 iteration 4048 : loss : 0.042019, loss_ce: 0.018307
2021-11-30 13:31:14,668 iteration 4049 : loss : 0.031146, loss_ce: 0.014623
2021-11-30 13:31:16,179 iteration 4050 : loss : 0.047587, loss_ce: 0.021074
2021-11-30 13:31:17,777 iteration 4051 : loss : 0.076550, loss_ce: 0.022713
2021-11-30 13:31:19,177 iteration 4052 : loss : 0.034548, loss_ce: 0.016076
2021-11-30 13:31:20,546 iteration 4053 : loss : 0.031698, loss_ce: 0.013994
2021-11-30 13:31:21,978 iteration 4054 : loss : 0.046464, loss_ce: 0.017529
2021-11-30 13:31:23,464 iteration 4055 : loss : 0.029204, loss_ce: 0.015660
2021-11-30 13:31:24,975 iteration 4056 : loss : 0.040357, loss_ce: 0.020229
2021-11-30 13:31:26,416 iteration 4057 : loss : 0.038406, loss_ce: 0.013603
2021-11-30 13:31:27,909 iteration 4058 : loss : 0.033960, loss_ce: 0.016342
2021-11-30 13:31:29,512 iteration 4059 : loss : 0.049836, loss_ce: 0.024701
2021-11-30 13:31:31,020 iteration 4060 : loss : 0.041658, loss_ce: 0.020146
2021-11-30 13:31:32,502 iteration 4061 : loss : 0.036635, loss_ce: 0.017676
2021-11-30 13:31:34,020 iteration 4062 : loss : 0.048075, loss_ce: 0.020679
2021-11-30 13:31:35,517 iteration 4063 : loss : 0.047810, loss_ce: 0.017921
 60%|████████████████▏          | 239/400 [1:49:37<1:10:36, 26.31s/it]2021-11-30 13:31:37,025 iteration 4064 : loss : 0.037200, loss_ce: 0.011993
2021-11-30 13:31:38,466 iteration 4065 : loss : 0.022437, loss_ce: 0.011309
2021-11-30 13:31:40,027 iteration 4066 : loss : 0.048656, loss_ce: 0.019700
2021-11-30 13:31:41,479 iteration 4067 : loss : 0.028198, loss_ce: 0.013861
2021-11-30 13:31:42,958 iteration 4068 : loss : 0.042876, loss_ce: 0.019327
2021-11-30 13:31:44,463 iteration 4069 : loss : 0.031868, loss_ce: 0.015716
2021-11-30 13:31:45,964 iteration 4070 : loss : 0.038842, loss_ce: 0.017540
2021-11-30 13:31:47,514 iteration 4071 : loss : 0.038923, loss_ce: 0.017719
2021-11-30 13:31:49,001 iteration 4072 : loss : 0.032162, loss_ce: 0.013779
2021-11-30 13:31:50,461 iteration 4073 : loss : 0.031743, loss_ce: 0.014620
2021-11-30 13:31:51,931 iteration 4074 : loss : 0.037852, loss_ce: 0.019269
2021-11-30 13:31:53,456 iteration 4075 : loss : 0.032590, loss_ce: 0.013366
2021-11-30 13:31:54,892 iteration 4076 : loss : 0.043590, loss_ce: 0.018831
2021-11-30 13:31:56,356 iteration 4077 : loss : 0.025831, loss_ce: 0.012541
2021-11-30 13:31:57,903 iteration 4078 : loss : 0.041693, loss_ce: 0.018322
2021-11-30 13:31:59,436 iteration 4079 : loss : 0.032547, loss_ce: 0.016322
2021-11-30 13:31:59,436 Training Data Eval:
2021-11-30 13:32:06,813   Average segmentation loss on training set: 0.0184
2021-11-30 13:32:06,813 Validation Data Eval:
2021-11-30 13:32:09,382   Average segmentation loss on validation set: 0.0733
2021-11-30 13:32:11,345 Found new lowest validation loss at iteration 4079! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/UNET_RUNMC/best_val_loss.pth
2021-11-30 13:32:12,874 iteration 4080 : loss : 0.036952, loss_ce: 0.017396
 60%|████████████████▏          | 240/400 [1:50:15<1:19:01, 29.63s/it]2021-11-30 13:32:14,413 iteration 4081 : loss : 0.038779, loss_ce: 0.017629
2021-11-30 13:32:15,884 iteration 4082 : loss : 0.031319, loss_ce: 0.014835
2021-11-30 13:32:17,281 iteration 4083 : loss : 0.025867, loss_ce: 0.014360
2021-11-30 13:32:18,742 iteration 4084 : loss : 0.027432, loss_ce: 0.011507
2021-11-30 13:32:20,136 iteration 4085 : loss : 0.033155, loss_ce: 0.013431
2021-11-30 13:32:21,721 iteration 4086 : loss : 0.055237, loss_ce: 0.022631
2021-11-30 13:32:23,195 iteration 4087 : loss : 0.090787, loss_ce: 0.019076
2021-11-30 13:32:24,566 iteration 4088 : loss : 0.041087, loss_ce: 0.022123
2021-11-30 13:32:26,015 iteration 4089 : loss : 0.051442, loss_ce: 0.019341
2021-11-30 13:32:27,365 iteration 4090 : loss : 0.037695, loss_ce: 0.015351
2021-11-30 13:32:28,824 iteration 4091 : loss : 0.049793, loss_ce: 0.021862
2021-11-30 13:32:30,336 iteration 4092 : loss : 0.034372, loss_ce: 0.015782
2021-11-30 13:32:31,841 iteration 4093 : loss : 0.066187, loss_ce: 0.024148
2021-11-30 13:32:33,304 iteration 4094 : loss : 0.039831, loss_ce: 0.019615
2021-11-30 13:32:34,804 iteration 4095 : loss : 0.048792, loss_ce: 0.022762
2021-11-30 13:32:36,266 iteration 4096 : loss : 0.038163, loss_ce: 0.022551
2021-11-30 13:32:37,733 iteration 4097 : loss : 0.033872, loss_ce: 0.016233
 60%|████████████████▎          | 241/400 [1:50:40<1:14:43, 28.20s/it]2021-11-30 13:32:39,277 iteration 4098 : loss : 0.030405, loss_ce: 0.015100
2021-11-30 13:32:40,716 iteration 4099 : loss : 0.048139, loss_ce: 0.018574
2021-11-30 13:32:42,216 iteration 4100 : loss : 0.041331, loss_ce: 0.017842
2021-11-30 13:32:43,769 iteration 4101 : loss : 0.033308, loss_ce: 0.014078
2021-11-30 13:32:45,279 iteration 4102 : loss : 0.047388, loss_ce: 0.022640
2021-11-30 13:32:46,775 iteration 4103 : loss : 0.050567, loss_ce: 0.024256
2021-11-30 13:32:48,270 iteration 4104 : loss : 0.048463, loss_ce: 0.021313
2021-11-30 13:32:49,776 iteration 4105 : loss : 0.028492, loss_ce: 0.015625
2021-11-30 13:32:51,268 iteration 4106 : loss : 0.034922, loss_ce: 0.017597
2021-11-30 13:32:52,747 iteration 4107 : loss : 0.027803, loss_ce: 0.013964
2021-11-30 13:32:54,218 iteration 4108 : loss : 0.034441, loss_ce: 0.016672
2021-11-30 13:32:55,654 iteration 4109 : loss : 0.032859, loss_ce: 0.014611
2021-11-30 13:32:57,194 iteration 4110 : loss : 0.036986, loss_ce: 0.016964
2021-11-30 13:32:58,848 iteration 4111 : loss : 0.043544, loss_ce: 0.023549
2021-11-30 13:33:00,290 iteration 4112 : loss : 0.039581, loss_ce: 0.016503
2021-11-30 13:33:01,829 iteration 4113 : loss : 0.047823, loss_ce: 0.017192
2021-11-30 13:33:03,314 iteration 4114 : loss : 0.025234, loss_ce: 0.013357
 60%|████████████████▎          | 242/400 [1:51:05<1:12:10, 27.41s/it]2021-11-30 13:33:04,793 iteration 4115 : loss : 0.043352, loss_ce: 0.023210
2021-11-30 13:33:06,343 iteration 4116 : loss : 0.042885, loss_ce: 0.020281
2021-11-30 13:33:07,765 iteration 4117 : loss : 0.023991, loss_ce: 0.011993
2021-11-30 13:33:09,316 iteration 4118 : loss : 0.041867, loss_ce: 0.020951
2021-11-30 13:33:10,920 iteration 4119 : loss : 0.057784, loss_ce: 0.020108
2021-11-30 13:33:12,435 iteration 4120 : loss : 0.038629, loss_ce: 0.013477
2021-11-30 13:33:13,867 iteration 4121 : loss : 0.035617, loss_ce: 0.015080
2021-11-30 13:33:15,320 iteration 4122 : loss : 0.045878, loss_ce: 0.018079
2021-11-30 13:33:16,745 iteration 4123 : loss : 0.024396, loss_ce: 0.013157
2021-11-30 13:33:18,295 iteration 4124 : loss : 0.039523, loss_ce: 0.020282
2021-11-30 13:33:19,824 iteration 4125 : loss : 0.038197, loss_ce: 0.020584
2021-11-30 13:33:21,340 iteration 4126 : loss : 0.041602, loss_ce: 0.019088
2021-11-30 13:33:22,787 iteration 4127 : loss : 0.021409, loss_ce: 0.012188
2021-11-30 13:33:24,390 iteration 4128 : loss : 0.070353, loss_ce: 0.024836
2021-11-30 13:33:25,818 iteration 4129 : loss : 0.036294, loss_ce: 0.013056
2021-11-30 13:33:27,238 iteration 4130 : loss : 0.027609, loss_ce: 0.011883
2021-11-30 13:33:28,714 iteration 4131 : loss : 0.031406, loss_ce: 0.017472
 61%|████████████████▍          | 243/400 [1:51:31<1:10:08, 26.81s/it]2021-11-30 13:33:30,299 iteration 4132 : loss : 0.038944, loss_ce: 0.015536
2021-11-30 13:33:31,815 iteration 4133 : loss : 0.034087, loss_ce: 0.013525
2021-11-30 13:33:33,347 iteration 4134 : loss : 0.046003, loss_ce: 0.021136
2021-11-30 13:33:34,748 iteration 4135 : loss : 0.040016, loss_ce: 0.019001
2021-11-30 13:33:36,230 iteration 4136 : loss : 0.034518, loss_ce: 0.018569
2021-11-30 13:33:37,643 iteration 4137 : loss : 0.028763, loss_ce: 0.012767
2021-11-30 13:33:39,151 iteration 4138 : loss : 0.041027, loss_ce: 0.019769
2021-11-30 13:33:40,668 iteration 4139 : loss : 0.041525, loss_ce: 0.020622
2021-11-30 13:33:42,153 iteration 4140 : loss : 0.032064, loss_ce: 0.016398
2021-11-30 13:33:43,651 iteration 4141 : loss : 0.038441, loss_ce: 0.016651
2021-11-30 13:33:45,208 iteration 4142 : loss : 0.051146, loss_ce: 0.021381
2021-11-30 13:33:46,586 iteration 4143 : loss : 0.034969, loss_ce: 0.014332
2021-11-30 13:33:48,004 iteration 4144 : loss : 0.030683, loss_ce: 0.011054
2021-11-30 13:33:49,535 iteration 4145 : loss : 0.034071, loss_ce: 0.015295
2021-11-30 13:33:50,992 iteration 4146 : loss : 0.032211, loss_ce: 0.017924
2021-11-30 13:33:52,484 iteration 4147 : loss : 0.030106, loss_ce: 0.013494
2021-11-30 13:33:53,930 iteration 4148 : loss : 0.042845, loss_ce: 0.020139
 61%|████████████████▍          | 244/400 [1:51:56<1:08:28, 26.33s/it]2021-11-30 13:33:55,387 iteration 4149 : loss : 0.030023, loss_ce: 0.013453
2021-11-30 13:33:56,889 iteration 4150 : loss : 0.031882, loss_ce: 0.012397
2021-11-30 13:33:58,429 iteration 4151 : loss : 0.056407, loss_ce: 0.018879
2021-11-30 13:33:59,934 iteration 4152 : loss : 0.032793, loss_ce: 0.016031
2021-11-30 13:34:01,349 iteration 4153 : loss : 0.023753, loss_ce: 0.011853
2021-11-30 13:34:02,854 iteration 4154 : loss : 0.029247, loss_ce: 0.013603
2021-11-30 13:34:04,325 iteration 4155 : loss : 0.028769, loss_ce: 0.012942
2021-11-30 13:34:05,726 iteration 4156 : loss : 0.025985, loss_ce: 0.012305
2021-11-30 13:34:07,275 iteration 4157 : loss : 0.053799, loss_ce: 0.030806
2021-11-30 13:34:08,717 iteration 4158 : loss : 0.028811, loss_ce: 0.012964
2021-11-30 13:34:10,186 iteration 4159 : loss : 0.026323, loss_ce: 0.013994
2021-11-30 13:34:11,802 iteration 4160 : loss : 0.057193, loss_ce: 0.021409
2021-11-30 13:34:13,377 iteration 4161 : loss : 0.040708, loss_ce: 0.016284
2021-11-30 13:34:14,936 iteration 4162 : loss : 0.055758, loss_ce: 0.022322
2021-11-30 13:34:16,476 iteration 4163 : loss : 0.062466, loss_ce: 0.033839
2021-11-30 13:34:17,963 iteration 4164 : loss : 0.040559, loss_ce: 0.018747
2021-11-30 13:34:17,963 Training Data Eval:
2021-11-30 13:34:25,329   Average segmentation loss on training set: 0.0190
2021-11-30 13:34:25,329 Validation Data Eval:
2021-11-30 13:34:27,876   Average segmentation loss on validation set: 0.0764
2021-11-30 13:34:29,418 iteration 4165 : loss : 0.042624, loss_ce: 0.016509
 61%|████████████████▌          | 245/400 [1:52:31<1:15:07, 29.08s/it]2021-11-30 13:34:30,967 iteration 4166 : loss : 0.028659, loss_ce: 0.014861
2021-11-30 13:34:32,376 iteration 4167 : loss : 0.031504, loss_ce: 0.012538
2021-11-30 13:34:33,815 iteration 4168 : loss : 0.024578, loss_ce: 0.012626
2021-11-30 13:34:35,265 iteration 4169 : loss : 0.025524, loss_ce: 0.014506
2021-11-30 13:34:36,873 iteration 4170 : loss : 0.040383, loss_ce: 0.019689
2021-11-30 13:34:38,274 iteration 4171 : loss : 0.037453, loss_ce: 0.016700
2021-11-30 13:34:39,747 iteration 4172 : loss : 0.034048, loss_ce: 0.018499
2021-11-30 13:34:41,260 iteration 4173 : loss : 0.031163, loss_ce: 0.012498
2021-11-30 13:34:42,722 iteration 4174 : loss : 0.034285, loss_ce: 0.012587
2021-11-30 13:34:44,148 iteration 4175 : loss : 0.029260, loss_ce: 0.014131
2021-11-30 13:34:45,627 iteration 4176 : loss : 0.028976, loss_ce: 0.015346
2021-11-30 13:34:47,164 iteration 4177 : loss : 0.030645, loss_ce: 0.012758
2021-11-30 13:34:48,611 iteration 4178 : loss : 0.031945, loss_ce: 0.014940
2021-11-30 13:34:50,077 iteration 4179 : loss : 0.037630, loss_ce: 0.019685
2021-11-30 13:34:51,533 iteration 4180 : loss : 0.030060, loss_ce: 0.013221
2021-11-30 13:34:53,057 iteration 4181 : loss : 0.049316, loss_ce: 0.022195
2021-11-30 13:34:54,550 iteration 4182 : loss : 0.037419, loss_ce: 0.013597
 62%|████████████████▌          | 246/400 [1:52:56<1:11:35, 27.89s/it]2021-11-30 13:34:56,098 iteration 4183 : loss : 0.038522, loss_ce: 0.016826
2021-11-30 13:34:57,626 iteration 4184 : loss : 0.049710, loss_ce: 0.022766
2021-11-30 13:34:59,088 iteration 4185 : loss : 0.033337, loss_ce: 0.017963
2021-11-30 13:35:00,553 iteration 4186 : loss : 0.038988, loss_ce: 0.014691
2021-11-30 13:35:02,082 iteration 4187 : loss : 0.059156, loss_ce: 0.019977
2021-11-30 13:35:03,516 iteration 4188 : loss : 0.035725, loss_ce: 0.014698
2021-11-30 13:35:05,120 iteration 4189 : loss : 0.037772, loss_ce: 0.019599
2021-11-30 13:35:06,661 iteration 4190 : loss : 0.048006, loss_ce: 0.025833
2021-11-30 13:35:08,215 iteration 4191 : loss : 0.046649, loss_ce: 0.021593
2021-11-30 13:35:09,603 iteration 4192 : loss : 0.024115, loss_ce: 0.011052
2021-11-30 13:35:11,120 iteration 4193 : loss : 0.049038, loss_ce: 0.028322
2021-11-30 13:35:12,655 iteration 4194 : loss : 0.030930, loss_ce: 0.013477
2021-11-30 13:35:14,129 iteration 4195 : loss : 0.048817, loss_ce: 0.017625
2021-11-30 13:35:15,661 iteration 4196 : loss : 0.023628, loss_ce: 0.012445
2021-11-30 13:35:17,107 iteration 4197 : loss : 0.032105, loss_ce: 0.013502
2021-11-30 13:35:18,562 iteration 4198 : loss : 0.026670, loss_ce: 0.013190
2021-11-30 13:35:19,968 iteration 4199 : loss : 0.031742, loss_ce: 0.014649
 62%|████████████████▋          | 247/400 [1:53:22<1:09:14, 27.15s/it]2021-11-30 13:35:21,511 iteration 4200 : loss : 0.031773, loss_ce: 0.014598
2021-11-30 13:35:23,043 iteration 4201 : loss : 0.033909, loss_ce: 0.017396
2021-11-30 13:35:24,545 iteration 4202 : loss : 0.034870, loss_ce: 0.016830
2021-11-30 13:35:26,138 iteration 4203 : loss : 0.043875, loss_ce: 0.021397
2021-11-30 13:35:27,591 iteration 4204 : loss : 0.030179, loss_ce: 0.015121
2021-11-30 13:35:29,044 iteration 4205 : loss : 0.036824, loss_ce: 0.018184
2021-11-30 13:35:30,665 iteration 4206 : loss : 0.044659, loss_ce: 0.019032
2021-11-30 13:35:32,192 iteration 4207 : loss : 0.045414, loss_ce: 0.020658
2021-11-30 13:35:33,575 iteration 4208 : loss : 0.021372, loss_ce: 0.010996
2021-11-30 13:35:35,030 iteration 4209 : loss : 0.030024, loss_ce: 0.015299
2021-11-30 13:35:36,508 iteration 4210 : loss : 0.032427, loss_ce: 0.016235
2021-11-30 13:35:38,028 iteration 4211 : loss : 0.039106, loss_ce: 0.018866
2021-11-30 13:35:39,585 iteration 4212 : loss : 0.033340, loss_ce: 0.015484
2021-11-30 13:35:41,042 iteration 4213 : loss : 0.048271, loss_ce: 0.014499
2021-11-30 13:35:42,567 iteration 4214 : loss : 0.043231, loss_ce: 0.015634
2021-11-30 13:35:44,052 iteration 4215 : loss : 0.045545, loss_ce: 0.019352
2021-11-30 13:35:45,515 iteration 4216 : loss : 0.035403, loss_ce: 0.014728
 62%|████████████████▋          | 248/400 [1:53:47<1:07:33, 26.67s/it]2021-11-30 13:35:47,110 iteration 4217 : loss : 0.042215, loss_ce: 0.017303
2021-11-30 13:35:48,580 iteration 4218 : loss : 0.031026, loss_ce: 0.013583
2021-11-30 13:35:50,031 iteration 4219 : loss : 0.030162, loss_ce: 0.014013
2021-11-30 13:35:51,399 iteration 4220 : loss : 0.033652, loss_ce: 0.011630
2021-11-30 13:35:52,979 iteration 4221 : loss : 0.052277, loss_ce: 0.023406
2021-11-30 13:35:54,547 iteration 4222 : loss : 0.039464, loss_ce: 0.015107
2021-11-30 13:35:55,995 iteration 4223 : loss : 0.033080, loss_ce: 0.014124
2021-11-30 13:35:57,586 iteration 4224 : loss : 0.084578, loss_ce: 0.025409
2021-11-30 13:35:59,099 iteration 4225 : loss : 0.042804, loss_ce: 0.017513
2021-11-30 13:36:00,638 iteration 4226 : loss : 0.034633, loss_ce: 0.018387
2021-11-30 13:36:02,146 iteration 4227 : loss : 0.024096, loss_ce: 0.012344
2021-11-30 13:36:03,577 iteration 4228 : loss : 0.033888, loss_ce: 0.017013
2021-11-30 13:36:05,053 iteration 4229 : loss : 0.036396, loss_ce: 0.016695
2021-11-30 13:36:06,530 iteration 4230 : loss : 0.073700, loss_ce: 0.040440
2021-11-30 13:36:08,057 iteration 4231 : loss : 0.036018, loss_ce: 0.017830
2021-11-30 13:36:09,580 iteration 4232 : loss : 0.037311, loss_ce: 0.015185
2021-11-30 13:36:11,079 iteration 4233 : loss : 0.040074, loss_ce: 0.023467
 62%|████████████████▊          | 249/400 [1:54:13<1:06:16, 26.34s/it]2021-11-30 13:36:12,526 iteration 4234 : loss : 0.038070, loss_ce: 0.012722
2021-11-30 13:36:13,959 iteration 4235 : loss : 0.019695, loss_ce: 0.010074
2021-11-30 13:36:15,476 iteration 4236 : loss : 0.028736, loss_ce: 0.012709
2021-11-30 13:36:16,925 iteration 4237 : loss : 0.032149, loss_ce: 0.016663
2021-11-30 13:36:18,405 iteration 4238 : loss : 0.047470, loss_ce: 0.026151
2021-11-30 13:36:20,029 iteration 4239 : loss : 0.049841, loss_ce: 0.022694
2021-11-30 13:36:21,501 iteration 4240 : loss : 0.030326, loss_ce: 0.012853
2021-11-30 13:36:22,885 iteration 4241 : loss : 0.034159, loss_ce: 0.017129
2021-11-30 13:36:24,377 iteration 4242 : loss : 0.040619, loss_ce: 0.015458
2021-11-30 13:36:25,844 iteration 4243 : loss : 0.036610, loss_ce: 0.018060
2021-11-30 13:36:27,251 iteration 4244 : loss : 0.023696, loss_ce: 0.013821
2021-11-30 13:36:28,786 iteration 4245 : loss : 0.055720, loss_ce: 0.024004
2021-11-30 13:36:30,360 iteration 4246 : loss : 0.040402, loss_ce: 0.015991
2021-11-30 13:36:31,878 iteration 4247 : loss : 0.054043, loss_ce: 0.022377
2021-11-30 13:36:33,291 iteration 4248 : loss : 0.036764, loss_ce: 0.017193
2021-11-30 13:36:34,845 iteration 4249 : loss : 0.033277, loss_ce: 0.013136
2021-11-30 13:36:34,846 Training Data Eval:
2021-11-30 13:36:42,201   Average segmentation loss on training set: 0.0173
2021-11-30 13:36:42,201 Validation Data Eval:
2021-11-30 13:36:44,744   Average segmentation loss on validation set: 0.0886
2021-11-30 13:36:46,235 iteration 4250 : loss : 0.043399, loss_ce: 0.016741
 62%|████████████████▉          | 250/400 [1:54:48<1:12:27, 28.98s/it]2021-11-30 13:36:47,856 iteration 4251 : loss : 0.041553, loss_ce: 0.016063
2021-11-30 13:36:49,404 iteration 4252 : loss : 0.044872, loss_ce: 0.022962
2021-11-30 13:36:50,884 iteration 4253 : loss : 0.043825, loss_ce: 0.020438
2021-11-30 13:36:52,401 iteration 4254 : loss : 0.025030, loss_ce: 0.011842
2021-11-30 13:36:53,942 iteration 4255 : loss : 0.046966, loss_ce: 0.019543
2021-11-30 13:36:55,323 iteration 4256 : loss : 0.036092, loss_ce: 0.017585
2021-11-30 13:36:56,788 iteration 4257 : loss : 0.034006, loss_ce: 0.015268
2021-11-30 13:36:58,204 iteration 4258 : loss : 0.040433, loss_ce: 0.015339
2021-11-30 13:36:59,713 iteration 4259 : loss : 0.039739, loss_ce: 0.020281
2021-11-30 13:37:01,211 iteration 4260 : loss : 0.049055, loss_ce: 0.018863
2021-11-30 13:37:02,739 iteration 4261 : loss : 0.027486, loss_ce: 0.013861
2021-11-30 13:37:04,187 iteration 4262 : loss : 0.027778, loss_ce: 0.014827
2021-11-30 13:37:05,711 iteration 4263 : loss : 0.029943, loss_ce: 0.014776
2021-11-30 13:37:07,254 iteration 4264 : loss : 0.040206, loss_ce: 0.018950
2021-11-30 13:37:08,733 iteration 4265 : loss : 0.059584, loss_ce: 0.019794
2021-11-30 13:37:10,266 iteration 4266 : loss : 0.042152, loss_ce: 0.016855
2021-11-30 13:37:11,805 iteration 4267 : loss : 0.041077, loss_ce: 0.017549
 63%|████████████████▉          | 251/400 [1:55:14<1:09:26, 27.96s/it]2021-11-30 13:37:13,366 iteration 4268 : loss : 0.037817, loss_ce: 0.012107
2021-11-30 13:37:14,825 iteration 4269 : loss : 0.038609, loss_ce: 0.015669
2021-11-30 13:37:16,252 iteration 4270 : loss : 0.033003, loss_ce: 0.018960
2021-11-30 13:37:17,643 iteration 4271 : loss : 0.028829, loss_ce: 0.013793
2021-11-30 13:37:19,123 iteration 4272 : loss : 0.034462, loss_ce: 0.018750
2021-11-30 13:37:20,539 iteration 4273 : loss : 0.029678, loss_ce: 0.014297
2021-11-30 13:37:22,113 iteration 4274 : loss : 0.038226, loss_ce: 0.020840
2021-11-30 13:37:23,565 iteration 4275 : loss : 0.025936, loss_ce: 0.012224
2021-11-30 13:37:25,109 iteration 4276 : loss : 0.036411, loss_ce: 0.012977
2021-11-30 13:37:26,706 iteration 4277 : loss : 0.036944, loss_ce: 0.015523
2021-11-30 13:37:28,241 iteration 4278 : loss : 0.041285, loss_ce: 0.016545
2021-11-30 13:37:29,699 iteration 4279 : loss : 0.027572, loss_ce: 0.015453
2021-11-30 13:37:31,189 iteration 4280 : loss : 0.034433, loss_ce: 0.014487
2021-11-30 13:37:32,685 iteration 4281 : loss : 0.036656, loss_ce: 0.014257
2021-11-30 13:37:34,103 iteration 4282 : loss : 0.039295, loss_ce: 0.017562
2021-11-30 13:37:35,580 iteration 4283 : loss : 0.035183, loss_ce: 0.014565
2021-11-30 13:37:37,025 iteration 4284 : loss : 0.035674, loss_ce: 0.017986
 63%|█████████████████          | 252/400 [1:55:39<1:06:56, 27.14s/it]2021-11-30 13:37:38,486 iteration 4285 : loss : 0.036065, loss_ce: 0.014182
2021-11-30 13:37:40,020 iteration 4286 : loss : 0.034013, loss_ce: 0.015023
2021-11-30 13:37:41,462 iteration 4287 : loss : 0.032124, loss_ce: 0.017389
2021-11-30 13:37:42,890 iteration 4288 : loss : 0.032823, loss_ce: 0.017111
2021-11-30 13:37:44,389 iteration 4289 : loss : 0.028080, loss_ce: 0.013700
2021-11-30 13:37:45,850 iteration 4290 : loss : 0.037754, loss_ce: 0.013998
2021-11-30 13:37:47,359 iteration 4291 : loss : 0.038235, loss_ce: 0.017684
2021-11-30 13:37:48,865 iteration 4292 : loss : 0.033060, loss_ce: 0.017067
2021-11-30 13:37:50,361 iteration 4293 : loss : 0.037315, loss_ce: 0.015121
2021-11-30 13:37:51,810 iteration 4294 : loss : 0.025956, loss_ce: 0.010344
2021-11-30 13:37:53,416 iteration 4295 : loss : 0.048497, loss_ce: 0.019875
2021-11-30 13:37:54,910 iteration 4296 : loss : 0.029565, loss_ce: 0.012100
2021-11-30 13:37:56,423 iteration 4297 : loss : 0.035864, loss_ce: 0.016623
2021-11-30 13:37:57,876 iteration 4298 : loss : 0.029036, loss_ce: 0.016259
2021-11-30 13:37:59,434 iteration 4299 : loss : 0.031109, loss_ce: 0.015513
2021-11-30 13:38:00,998 iteration 4300 : loss : 0.043109, loss_ce: 0.018170
2021-11-30 13:38:02,558 iteration 4301 : loss : 0.035228, loss_ce: 0.016543
 63%|█████████████████          | 253/400 [1:56:04<1:05:18, 26.66s/it]2021-11-30 13:38:04,151 iteration 4302 : loss : 0.045605, loss_ce: 0.022148
2021-11-30 13:38:05,698 iteration 4303 : loss : 0.052958, loss_ce: 0.015442
2021-11-30 13:38:07,130 iteration 4304 : loss : 0.032909, loss_ce: 0.015202
2021-11-30 13:38:08,574 iteration 4305 : loss : 0.037639, loss_ce: 0.017425
2021-11-30 13:38:10,010 iteration 4306 : loss : 0.029445, loss_ce: 0.013605
2021-11-30 13:38:11,424 iteration 4307 : loss : 0.023128, loss_ce: 0.013145
2021-11-30 13:38:12,937 iteration 4308 : loss : 0.039621, loss_ce: 0.016445
2021-11-30 13:38:14,501 iteration 4309 : loss : 0.035728, loss_ce: 0.016314
2021-11-30 13:38:15,993 iteration 4310 : loss : 0.026325, loss_ce: 0.013013
2021-11-30 13:38:17,431 iteration 4311 : loss : 0.046958, loss_ce: 0.018191
2021-11-30 13:38:18,863 iteration 4312 : loss : 0.040443, loss_ce: 0.016831
2021-11-30 13:38:20,316 iteration 4313 : loss : 0.025714, loss_ce: 0.014496
2021-11-30 13:38:21,768 iteration 4314 : loss : 0.032959, loss_ce: 0.013729
2021-11-30 13:38:23,279 iteration 4315 : loss : 0.031691, loss_ce: 0.013777
2021-11-30 13:38:24,717 iteration 4316 : loss : 0.035185, loss_ce: 0.015038
2021-11-30 13:38:26,268 iteration 4317 : loss : 0.041303, loss_ce: 0.018828
2021-11-30 13:38:27,694 iteration 4318 : loss : 0.030283, loss_ce: 0.015121
 64%|█████████████████▏         | 254/400 [1:56:30<1:03:45, 26.20s/it]2021-11-30 13:38:29,266 iteration 4319 : loss : 0.040137, loss_ce: 0.019496
2021-11-30 13:38:30,797 iteration 4320 : loss : 0.045993, loss_ce: 0.019263
2021-11-30 13:38:32,163 iteration 4321 : loss : 0.025702, loss_ce: 0.010670
2021-11-30 13:38:33,653 iteration 4322 : loss : 0.030301, loss_ce: 0.014530
2021-11-30 13:38:35,084 iteration 4323 : loss : 0.032189, loss_ce: 0.019209
2021-11-30 13:38:36,707 iteration 4324 : loss : 0.057567, loss_ce: 0.025986
2021-11-30 13:38:38,189 iteration 4325 : loss : 0.041574, loss_ce: 0.019835
2021-11-30 13:38:39,640 iteration 4326 : loss : 0.043333, loss_ce: 0.022934
2021-11-30 13:38:41,158 iteration 4327 : loss : 0.038538, loss_ce: 0.013893
2021-11-30 13:38:42,581 iteration 4328 : loss : 0.058392, loss_ce: 0.025766
2021-11-30 13:38:44,120 iteration 4329 : loss : 0.069849, loss_ce: 0.020900
2021-11-30 13:38:45,640 iteration 4330 : loss : 0.040314, loss_ce: 0.020808
2021-11-30 13:38:47,216 iteration 4331 : loss : 0.058097, loss_ce: 0.019430
2021-11-30 13:38:48,752 iteration 4332 : loss : 0.050846, loss_ce: 0.023395
2021-11-30 13:38:50,317 iteration 4333 : loss : 0.038051, loss_ce: 0.018215
2021-11-30 13:38:51,802 iteration 4334 : loss : 0.036693, loss_ce: 0.022373
2021-11-30 13:38:51,802 Training Data Eval:
2021-11-30 13:38:59,170   Average segmentation loss on training set: 0.0200
2021-11-30 13:38:59,170 Validation Data Eval:
2021-11-30 13:39:01,708   Average segmentation loss on validation set: 0.1016
2021-11-30 13:39:03,267 iteration 4335 : loss : 0.063458, loss_ce: 0.020371
 64%|█████████████████▏         | 255/400 [1:57:05<1:10:06, 29.01s/it]2021-11-30 13:39:04,690 iteration 4336 : loss : 0.024069, loss_ce: 0.014330
2021-11-30 13:39:06,273 iteration 4337 : loss : 0.036658, loss_ce: 0.018830
2021-11-30 13:39:07,758 iteration 4338 : loss : 0.028388, loss_ce: 0.013534
2021-11-30 13:39:09,187 iteration 4339 : loss : 0.031583, loss_ce: 0.014001
2021-11-30 13:39:10,708 iteration 4340 : loss : 0.047345, loss_ce: 0.022020
2021-11-30 13:39:12,134 iteration 4341 : loss : 0.029668, loss_ce: 0.012290
2021-11-30 13:39:13,674 iteration 4342 : loss : 0.048496, loss_ce: 0.017892
2021-11-30 13:39:15,219 iteration 4343 : loss : 0.037658, loss_ce: 0.014547
2021-11-30 13:39:16,732 iteration 4344 : loss : 0.036668, loss_ce: 0.018843
2021-11-30 13:39:18,322 iteration 4345 : loss : 0.037387, loss_ce: 0.017666
2021-11-30 13:39:19,734 iteration 4346 : loss : 0.038403, loss_ce: 0.013302
2021-11-30 13:39:21,220 iteration 4347 : loss : 0.031362, loss_ce: 0.016675
2021-11-30 13:39:22,651 iteration 4348 : loss : 0.028291, loss_ce: 0.015439
2021-11-30 13:39:24,050 iteration 4349 : loss : 0.026161, loss_ce: 0.010359
2021-11-30 13:39:25,439 iteration 4350 : loss : 0.041388, loss_ce: 0.016920
2021-11-30 13:39:26,953 iteration 4351 : loss : 0.036206, loss_ce: 0.014203
2021-11-30 13:39:28,441 iteration 4352 : loss : 0.046305, loss_ce: 0.020429
 64%|█████████████████▎         | 256/400 [1:57:30<1:06:52, 27.86s/it]2021-11-30 13:39:29,934 iteration 4353 : loss : 0.031748, loss_ce: 0.013200
2021-11-30 13:39:31,511 iteration 4354 : loss : 0.039704, loss_ce: 0.020558
2021-11-30 13:39:33,055 iteration 4355 : loss : 0.035623, loss_ce: 0.019131
2021-11-30 13:39:34,631 iteration 4356 : loss : 0.044163, loss_ce: 0.014978
2021-11-30 13:39:36,164 iteration 4357 : loss : 0.042335, loss_ce: 0.021108
2021-11-30 13:39:37,732 iteration 4358 : loss : 0.040702, loss_ce: 0.018966
2021-11-30 13:39:39,237 iteration 4359 : loss : 0.043961, loss_ce: 0.017086
2021-11-30 13:39:40,711 iteration 4360 : loss : 0.070576, loss_ce: 0.036667
2021-11-30 13:39:42,252 iteration 4361 : loss : 0.035551, loss_ce: 0.016498
2021-11-30 13:39:43,733 iteration 4362 : loss : 0.038417, loss_ce: 0.021019
2021-11-30 13:39:45,205 iteration 4363 : loss : 0.028835, loss_ce: 0.014651
2021-11-30 13:39:46,721 iteration 4364 : loss : 0.038937, loss_ce: 0.017173
2021-11-30 13:39:48,301 iteration 4365 : loss : 0.050489, loss_ce: 0.023741
2021-11-30 13:39:49,720 iteration 4366 : loss : 0.030896, loss_ce: 0.012276
2021-11-30 13:39:51,084 iteration 4367 : loss : 0.037354, loss_ce: 0.018206
2021-11-30 13:39:52,500 iteration 4368 : loss : 0.035561, loss_ce: 0.015020
2021-11-30 13:39:54,010 iteration 4369 : loss : 0.033989, loss_ce: 0.015607
 64%|█████████████████▎         | 257/400 [1:57:56<1:04:45, 27.17s/it]2021-11-30 13:39:55,558 iteration 4370 : loss : 0.030256, loss_ce: 0.013066
2021-11-30 13:39:57,059 iteration 4371 : loss : 0.043335, loss_ce: 0.022363
2021-11-30 13:39:58,591 iteration 4372 : loss : 0.032832, loss_ce: 0.014635
2021-11-30 13:40:00,091 iteration 4373 : loss : 0.031528, loss_ce: 0.018720
2021-11-30 13:40:01,611 iteration 4374 : loss : 0.029137, loss_ce: 0.012649
2021-11-30 13:40:03,125 iteration 4375 : loss : 0.047272, loss_ce: 0.026179
2021-11-30 13:40:04,577 iteration 4376 : loss : 0.037227, loss_ce: 0.016618
2021-11-30 13:40:06,059 iteration 4377 : loss : 0.037487, loss_ce: 0.013040
2021-11-30 13:40:07,482 iteration 4378 : loss : 0.030563, loss_ce: 0.013605
2021-11-30 13:40:08,948 iteration 4379 : loss : 0.038587, loss_ce: 0.014651
2021-11-30 13:40:10,340 iteration 4380 : loss : 0.027892, loss_ce: 0.012269
2021-11-30 13:40:11,850 iteration 4381 : loss : 0.046965, loss_ce: 0.020109
2021-11-30 13:40:13,347 iteration 4382 : loss : 0.088231, loss_ce: 0.023012
2021-11-30 13:40:14,816 iteration 4383 : loss : 0.043569, loss_ce: 0.015674
2021-11-30 13:40:16,359 iteration 4384 : loss : 0.048422, loss_ce: 0.021324
2021-11-30 13:40:17,904 iteration 4385 : loss : 0.043787, loss_ce: 0.018021
2021-11-30 13:40:19,357 iteration 4386 : loss : 0.028286, loss_ce: 0.015308
 64%|█████████████████▍         | 258/400 [1:58:21<1:03:00, 26.63s/it]2021-11-30 13:40:20,823 iteration 4387 : loss : 0.037648, loss_ce: 0.014798
2021-11-30 13:40:22,313 iteration 4388 : loss : 0.042898, loss_ce: 0.020811
2021-11-30 13:40:23,797 iteration 4389 : loss : 0.042503, loss_ce: 0.016373
2021-11-30 13:40:25,213 iteration 4390 : loss : 0.027620, loss_ce: 0.012567
2021-11-30 13:40:26,723 iteration 4391 : loss : 0.039739, loss_ce: 0.018254
2021-11-30 13:40:28,217 iteration 4392 : loss : 0.034500, loss_ce: 0.013506
2021-11-30 13:40:29,776 iteration 4393 : loss : 0.046898, loss_ce: 0.019481
2021-11-30 13:40:31,312 iteration 4394 : loss : 0.032080, loss_ce: 0.015649
2021-11-30 13:40:32,925 iteration 4395 : loss : 0.035432, loss_ce: 0.016938
2021-11-30 13:40:34,367 iteration 4396 : loss : 0.038840, loss_ce: 0.015596
2021-11-30 13:40:35,981 iteration 4397 : loss : 0.045439, loss_ce: 0.024858
2021-11-30 13:40:37,413 iteration 4398 : loss : 0.032576, loss_ce: 0.014907
2021-11-30 13:40:38,985 iteration 4399 : loss : 0.043701, loss_ce: 0.019711
2021-11-30 13:40:40,482 iteration 4400 : loss : 0.048149, loss_ce: 0.018996
2021-11-30 13:40:42,146 iteration 4401 : loss : 0.049585, loss_ce: 0.021023
2021-11-30 13:40:43,652 iteration 4402 : loss : 0.047176, loss_ce: 0.018729
2021-11-30 13:40:45,158 iteration 4403 : loss : 0.033617, loss_ce: 0.013831
 65%|█████████████████▍         | 259/400 [1:58:47<1:01:59, 26.38s/it]2021-11-30 13:40:46,653 iteration 4404 : loss : 0.030483, loss_ce: 0.016505
2021-11-30 13:40:48,188 iteration 4405 : loss : 0.033159, loss_ce: 0.016936
2021-11-30 13:40:49,688 iteration 4406 : loss : 0.039299, loss_ce: 0.018170
2021-11-30 13:40:51,253 iteration 4407 : loss : 0.043284, loss_ce: 0.017431
2021-11-30 13:40:52,713 iteration 4408 : loss : 0.033942, loss_ce: 0.016331
2021-11-30 13:40:54,219 iteration 4409 : loss : 0.043027, loss_ce: 0.017396
2021-11-30 13:40:55,858 iteration 4410 : loss : 0.059958, loss_ce: 0.023513
2021-11-30 13:40:57,408 iteration 4411 : loss : 0.046931, loss_ce: 0.022255
2021-11-30 13:40:58,882 iteration 4412 : loss : 0.037814, loss_ce: 0.018127
2021-11-30 13:41:00,399 iteration 4413 : loss : 0.029071, loss_ce: 0.011766
2021-11-30 13:41:01,836 iteration 4414 : loss : 0.029320, loss_ce: 0.012328
2021-11-30 13:41:03,359 iteration 4415 : loss : 0.032197, loss_ce: 0.013521
2021-11-30 13:41:04,912 iteration 4416 : loss : 0.054014, loss_ce: 0.019018
2021-11-30 13:41:06,414 iteration 4417 : loss : 0.032672, loss_ce: 0.013454
2021-11-30 13:41:07,926 iteration 4418 : loss : 0.034388, loss_ce: 0.017435
2021-11-30 13:41:09,488 iteration 4419 : loss : 0.032411, loss_ce: 0.012164
2021-11-30 13:41:09,488 Training Data Eval:
2021-11-30 13:41:16,807   Average segmentation loss on training set: 0.0179
2021-11-30 13:41:16,808 Validation Data Eval:
2021-11-30 13:41:19,361   Average segmentation loss on validation set: 0.0889
2021-11-30 13:41:20,805 iteration 4420 : loss : 0.029850, loss_ce: 0.016072
 65%|█████████████████▌         | 260/400 [1:59:23<1:08:02, 29.16s/it]2021-11-30 13:41:22,326 iteration 4421 : loss : 0.029264, loss_ce: 0.013646
2021-11-30 13:41:23,842 iteration 4422 : loss : 0.035816, loss_ce: 0.016980
2021-11-30 13:41:25,393 iteration 4423 : loss : 0.042283, loss_ce: 0.018462
2021-11-30 13:41:26,808 iteration 4424 : loss : 0.030544, loss_ce: 0.012599
2021-11-30 13:41:28,228 iteration 4425 : loss : 0.025408, loss_ce: 0.011685
2021-11-30 13:41:29,788 iteration 4426 : loss : 0.028491, loss_ce: 0.015085
2021-11-30 13:41:31,225 iteration 4427 : loss : 0.036248, loss_ce: 0.019193
2021-11-30 13:41:32,606 iteration 4428 : loss : 0.021566, loss_ce: 0.012200
2021-11-30 13:41:34,015 iteration 4429 : loss : 0.032827, loss_ce: 0.014125
2021-11-30 13:41:35,451 iteration 4430 : loss : 0.022985, loss_ce: 0.012546
2021-11-30 13:41:36,961 iteration 4431 : loss : 0.050855, loss_ce: 0.023614
2021-11-30 13:41:38,494 iteration 4432 : loss : 0.040386, loss_ce: 0.017251
2021-11-30 13:41:39,953 iteration 4433 : loss : 0.047795, loss_ce: 0.015990
2021-11-30 13:41:41,371 iteration 4434 : loss : 0.024787, loss_ce: 0.012214
2021-11-30 13:41:42,868 iteration 4435 : loss : 0.053270, loss_ce: 0.014626
2021-11-30 13:41:44,366 iteration 4436 : loss : 0.029859, loss_ce: 0.014528
2021-11-30 13:41:45,926 iteration 4437 : loss : 0.047050, loss_ce: 0.020289
 65%|█████████████████▌         | 261/400 [1:59:48<1:04:44, 27.95s/it]2021-11-30 13:41:47,529 iteration 4438 : loss : 0.051505, loss_ce: 0.016730
2021-11-30 13:41:49,029 iteration 4439 : loss : 0.034279, loss_ce: 0.015112
2021-11-30 13:41:50,507 iteration 4440 : loss : 0.030086, loss_ce: 0.013322
2021-11-30 13:41:52,119 iteration 4441 : loss : 0.035678, loss_ce: 0.014759
2021-11-30 13:41:53,664 iteration 4442 : loss : 0.029139, loss_ce: 0.017184
2021-11-30 13:41:55,156 iteration 4443 : loss : 0.039710, loss_ce: 0.018256
2021-11-30 13:41:56,565 iteration 4444 : loss : 0.035068, loss_ce: 0.013928
2021-11-30 13:41:58,046 iteration 4445 : loss : 0.044789, loss_ce: 0.021140
2021-11-30 13:41:59,584 iteration 4446 : loss : 0.032173, loss_ce: 0.016064
2021-11-30 13:42:01,043 iteration 4447 : loss : 0.027807, loss_ce: 0.012238
2021-11-30 13:42:02,560 iteration 4448 : loss : 0.057861, loss_ce: 0.027517
2021-11-30 13:42:04,061 iteration 4449 : loss : 0.039372, loss_ce: 0.019346
2021-11-30 13:42:05,623 iteration 4450 : loss : 0.047073, loss_ce: 0.017954
2021-11-30 13:42:07,082 iteration 4451 : loss : 0.034825, loss_ce: 0.017144
2021-11-30 13:42:08,604 iteration 4452 : loss : 0.036051, loss_ce: 0.017980
2021-11-30 13:42:10,064 iteration 4453 : loss : 0.028591, loss_ce: 0.011887
2021-11-30 13:42:11,510 iteration 4454 : loss : 0.034489, loss_ce: 0.017492
 66%|█████████████████▋         | 262/400 [2:00:13<1:02:38, 27.24s/it]2021-11-30 13:42:13,063 iteration 4455 : loss : 0.034206, loss_ce: 0.013028
2021-11-30 13:42:14,472 iteration 4456 : loss : 0.022869, loss_ce: 0.011064
2021-11-30 13:42:15,994 iteration 4457 : loss : 0.035702, loss_ce: 0.016972
2021-11-30 13:42:17,602 iteration 4458 : loss : 0.074861, loss_ce: 0.033078
2021-11-30 13:42:19,269 iteration 4459 : loss : 0.043766, loss_ce: 0.021865
2021-11-30 13:42:20,689 iteration 4460 : loss : 0.046038, loss_ce: 0.020084
2021-11-30 13:42:22,169 iteration 4461 : loss : 0.047372, loss_ce: 0.017011
2021-11-30 13:42:23,694 iteration 4462 : loss : 0.037151, loss_ce: 0.018702
2021-11-30 13:42:25,157 iteration 4463 : loss : 0.039530, loss_ce: 0.017152
2021-11-30 13:42:26,614 iteration 4464 : loss : 0.026976, loss_ce: 0.011963
2021-11-30 13:42:28,024 iteration 4465 : loss : 0.030961, loss_ce: 0.011674
2021-11-30 13:42:29,459 iteration 4466 : loss : 0.031504, loss_ce: 0.012730
2021-11-30 13:42:30,941 iteration 4467 : loss : 0.052847, loss_ce: 0.026082
2021-11-30 13:42:32,453 iteration 4468 : loss : 0.034446, loss_ce: 0.018066
2021-11-30 13:42:34,018 iteration 4469 : loss : 0.045470, loss_ce: 0.024614
2021-11-30 13:42:35,560 iteration 4470 : loss : 0.031326, loss_ce: 0.015193
2021-11-30 13:42:37,117 iteration 4471 : loss : 0.039591, loss_ce: 0.015615
 66%|█████████████████▊         | 263/400 [2:00:39<1:01:04, 26.74s/it]2021-11-30 13:42:38,622 iteration 4472 : loss : 0.033495, loss_ce: 0.017156
2021-11-30 13:42:40,083 iteration 4473 : loss : 0.031966, loss_ce: 0.015676
2021-11-30 13:42:41,599 iteration 4474 : loss : 0.033854, loss_ce: 0.018782
2021-11-30 13:42:43,086 iteration 4475 : loss : 0.041003, loss_ce: 0.015463
2021-11-30 13:42:44,545 iteration 4476 : loss : 0.032589, loss_ce: 0.015077
2021-11-30 13:42:46,009 iteration 4477 : loss : 0.041019, loss_ce: 0.020080
2021-11-30 13:42:47,625 iteration 4478 : loss : 0.034780, loss_ce: 0.018827
2021-11-30 13:42:49,118 iteration 4479 : loss : 0.041239, loss_ce: 0.015798
2021-11-30 13:42:50,620 iteration 4480 : loss : 0.035291, loss_ce: 0.014578
2021-11-30 13:42:52,058 iteration 4481 : loss : 0.030480, loss_ce: 0.013725
2021-11-30 13:42:53,483 iteration 4482 : loss : 0.027869, loss_ce: 0.011240
2021-11-30 13:42:54,922 iteration 4483 : loss : 0.034481, loss_ce: 0.014740
2021-11-30 13:42:56,410 iteration 4484 : loss : 0.025110, loss_ce: 0.011772
2021-11-30 13:42:57,814 iteration 4485 : loss : 0.031611, loss_ce: 0.016485
2021-11-30 13:42:59,375 iteration 4486 : loss : 0.030292, loss_ce: 0.014360
2021-11-30 13:43:00,886 iteration 4487 : loss : 0.045036, loss_ce: 0.022090
2021-11-30 13:43:02,312 iteration 4488 : loss : 0.037919, loss_ce: 0.015422
 66%|███████████████████▏         | 264/400 [2:01:04<59:34, 26.28s/it]2021-11-30 13:43:03,908 iteration 4489 : loss : 0.032309, loss_ce: 0.017010
2021-11-30 13:43:05,383 iteration 4490 : loss : 0.041135, loss_ce: 0.021514
2021-11-30 13:43:06,980 iteration 4491 : loss : 0.038140, loss_ce: 0.016730
2021-11-30 13:43:08,437 iteration 4492 : loss : 0.040947, loss_ce: 0.015508
2021-11-30 13:43:10,023 iteration 4493 : loss : 0.041778, loss_ce: 0.026190
2021-11-30 13:43:11,460 iteration 4494 : loss : 0.042809, loss_ce: 0.016126
2021-11-30 13:43:12,963 iteration 4495 : loss : 0.039690, loss_ce: 0.012639
2021-11-30 13:43:14,473 iteration 4496 : loss : 0.036180, loss_ce: 0.012923
2021-11-30 13:43:15,846 iteration 4497 : loss : 0.047795, loss_ce: 0.021365
2021-11-30 13:43:17,339 iteration 4498 : loss : 0.031422, loss_ce: 0.012817
2021-11-30 13:43:18,796 iteration 4499 : loss : 0.025834, loss_ce: 0.012660
2021-11-30 13:43:20,289 iteration 4500 : loss : 0.025434, loss_ce: 0.012198
2021-11-30 13:43:21,751 iteration 4501 : loss : 0.035710, loss_ce: 0.018158
2021-11-30 13:43:23,211 iteration 4502 : loss : 0.043068, loss_ce: 0.017549
2021-11-30 13:43:24,801 iteration 4503 : loss : 0.030697, loss_ce: 0.014403
2021-11-30 13:43:26,183 iteration 4504 : loss : 0.028003, loss_ce: 0.013447
2021-11-30 13:43:26,184 Training Data Eval:
2021-11-30 13:43:33,565   Average segmentation loss on training set: 0.0164
2021-11-30 13:43:33,565 Validation Data Eval:
2021-11-30 13:43:36,135   Average segmentation loss on validation set: 0.0819
2021-11-30 13:43:37,656 iteration 4505 : loss : 0.038136, loss_ce: 0.016074
 66%|█████████████████▉         | 265/400 [2:01:40<1:05:15, 29.01s/it]2021-11-30 13:43:39,245 iteration 4506 : loss : 0.026664, loss_ce: 0.013523
2021-11-30 13:43:40,809 iteration 4507 : loss : 0.041749, loss_ce: 0.019298
2021-11-30 13:43:42,289 iteration 4508 : loss : 0.036140, loss_ce: 0.013786
2021-11-30 13:43:43,698 iteration 4509 : loss : 0.032520, loss_ce: 0.014597
2021-11-30 13:43:45,204 iteration 4510 : loss : 0.027264, loss_ce: 0.012869
2021-11-30 13:43:46,778 iteration 4511 : loss : 0.046019, loss_ce: 0.020603
2021-11-30 13:43:48,163 iteration 4512 : loss : 0.029001, loss_ce: 0.013628
2021-11-30 13:43:49,636 iteration 4513 : loss : 0.025819, loss_ce: 0.012637
2021-11-30 13:43:51,106 iteration 4514 : loss : 0.030156, loss_ce: 0.014897
2021-11-30 13:43:52,596 iteration 4515 : loss : 0.028318, loss_ce: 0.014351
2021-11-30 13:43:54,091 iteration 4516 : loss : 0.028306, loss_ce: 0.013867
2021-11-30 13:43:55,615 iteration 4517 : loss : 0.030157, loss_ce: 0.013616
2021-11-30 13:43:57,078 iteration 4518 : loss : 0.040264, loss_ce: 0.020255
2021-11-30 13:43:58,483 iteration 4519 : loss : 0.044048, loss_ce: 0.016873
2021-11-30 13:43:59,986 iteration 4520 : loss : 0.042190, loss_ce: 0.014769
2021-11-30 13:44:01,487 iteration 4521 : loss : 0.036468, loss_ce: 0.018787
2021-11-30 13:44:03,063 iteration 4522 : loss : 0.054563, loss_ce: 0.018081
 66%|█████████████████▉         | 266/400 [2:02:05<1:02:21, 27.93s/it]2021-11-30 13:44:04,669 iteration 4523 : loss : 0.040451, loss_ce: 0.021185
2021-11-30 13:44:06,164 iteration 4524 : loss : 0.032483, loss_ce: 0.014312
2021-11-30 13:44:07,667 iteration 4525 : loss : 0.031576, loss_ce: 0.017491
2021-11-30 13:44:09,143 iteration 4526 : loss : 0.028967, loss_ce: 0.012210
2021-11-30 13:44:10,546 iteration 4527 : loss : 0.030462, loss_ce: 0.014844
2021-11-30 13:44:11,994 iteration 4528 : loss : 0.036149, loss_ce: 0.014989
2021-11-30 13:44:13,533 iteration 4529 : loss : 0.034596, loss_ce: 0.016849
2021-11-30 13:44:14,972 iteration 4530 : loss : 0.034653, loss_ce: 0.013248
2021-11-30 13:44:16,467 iteration 4531 : loss : 0.030193, loss_ce: 0.013798
2021-11-30 13:44:17,991 iteration 4532 : loss : 0.023573, loss_ce: 0.010120
2021-11-30 13:44:19,449 iteration 4533 : loss : 0.031304, loss_ce: 0.016398
2021-11-30 13:44:20,922 iteration 4534 : loss : 0.035109, loss_ce: 0.012654
2021-11-30 13:44:22,429 iteration 4535 : loss : 0.030474, loss_ce: 0.014534
2021-11-30 13:44:23,935 iteration 4536 : loss : 0.031019, loss_ce: 0.014460
2021-11-30 13:44:25,416 iteration 4537 : loss : 0.041519, loss_ce: 0.018052
2021-11-30 13:44:26,887 iteration 4538 : loss : 0.038437, loss_ce: 0.015951
2021-11-30 13:44:28,369 iteration 4539 : loss : 0.032905, loss_ce: 0.014637
 67%|██████████████████         | 267/400 [2:02:30<1:00:09, 27.14s/it]2021-11-30 13:44:29,878 iteration 4540 : loss : 0.020896, loss_ce: 0.010155
2021-11-30 13:44:31,396 iteration 4541 : loss : 0.031392, loss_ce: 0.016569
2021-11-30 13:44:32,842 iteration 4542 : loss : 0.052753, loss_ce: 0.024569
2021-11-30 13:44:34,314 iteration 4543 : loss : 0.028780, loss_ce: 0.012968
2021-11-30 13:44:35,752 iteration 4544 : loss : 0.041772, loss_ce: 0.016477
2021-11-30 13:44:37,323 iteration 4545 : loss : 0.036474, loss_ce: 0.018076
2021-11-30 13:44:38,918 iteration 4546 : loss : 0.053732, loss_ce: 0.026315
2021-11-30 13:44:40,499 iteration 4547 : loss : 0.034942, loss_ce: 0.016783
2021-11-30 13:44:41,918 iteration 4548 : loss : 0.028485, loss_ce: 0.012983
2021-11-30 13:44:43,356 iteration 4549 : loss : 0.039676, loss_ce: 0.014795
2021-11-30 13:44:44,858 iteration 4550 : loss : 0.044916, loss_ce: 0.022977
2021-11-30 13:44:46,363 iteration 4551 : loss : 0.034252, loss_ce: 0.015935
2021-11-30 13:44:47,927 iteration 4552 : loss : 0.044162, loss_ce: 0.016995
2021-11-30 13:44:49,426 iteration 4553 : loss : 0.039814, loss_ce: 0.015785
2021-11-30 13:44:50,864 iteration 4554 : loss : 0.028221, loss_ce: 0.014263
2021-11-30 13:44:52,342 iteration 4555 : loss : 0.030338, loss_ce: 0.015116
2021-11-30 13:44:53,822 iteration 4556 : loss : 0.039525, loss_ce: 0.017699
 67%|███████████████████▍         | 268/400 [2:02:56<58:35, 26.63s/it]2021-11-30 13:44:55,344 iteration 4557 : loss : 0.027385, loss_ce: 0.012708
2021-11-30 13:44:56,887 iteration 4558 : loss : 0.049936, loss_ce: 0.019426
2021-11-30 13:44:58,458 iteration 4559 : loss : 0.037604, loss_ce: 0.016318
2021-11-30 13:44:59,986 iteration 4560 : loss : 0.028725, loss_ce: 0.016050
2021-11-30 13:45:01,418 iteration 4561 : loss : 0.027817, loss_ce: 0.011532
2021-11-30 13:45:02,920 iteration 4562 : loss : 0.032200, loss_ce: 0.014987
2021-11-30 13:45:04,395 iteration 4563 : loss : 0.032818, loss_ce: 0.014017
2021-11-30 13:45:05,849 iteration 4564 : loss : 0.032055, loss_ce: 0.016071
2021-11-30 13:45:07,309 iteration 4565 : loss : 0.048740, loss_ce: 0.022089
2021-11-30 13:45:08,806 iteration 4566 : loss : 0.027489, loss_ce: 0.012339
2021-11-30 13:45:10,280 iteration 4567 : loss : 0.037602, loss_ce: 0.017034
2021-11-30 13:45:11,673 iteration 4568 : loss : 0.032197, loss_ce: 0.014558
2021-11-30 13:45:13,104 iteration 4569 : loss : 0.034213, loss_ce: 0.015526
2021-11-30 13:45:14,535 iteration 4570 : loss : 0.024463, loss_ce: 0.011447
2021-11-30 13:45:16,067 iteration 4571 : loss : 0.028385, loss_ce: 0.014861
2021-11-30 13:45:17,585 iteration 4572 : loss : 0.034789, loss_ce: 0.014566
2021-11-30 13:45:19,130 iteration 4573 : loss : 0.048455, loss_ce: 0.019854
 67%|███████████████████▌         | 269/400 [2:03:21<57:16, 26.23s/it]2021-11-30 13:45:20,606 iteration 4574 : loss : 0.024639, loss_ce: 0.011358
2021-11-30 13:45:22,114 iteration 4575 : loss : 0.044104, loss_ce: 0.015005
2021-11-30 13:45:23,525 iteration 4576 : loss : 0.034088, loss_ce: 0.015166
2021-11-30 13:45:24,975 iteration 4577 : loss : 0.032623, loss_ce: 0.017845
2021-11-30 13:45:26,461 iteration 4578 : loss : 0.028412, loss_ce: 0.013565
2021-11-30 13:45:27,897 iteration 4579 : loss : 0.028956, loss_ce: 0.015870
2021-11-30 13:45:29,352 iteration 4580 : loss : 0.028349, loss_ce: 0.013654
2021-11-30 13:45:30,834 iteration 4581 : loss : 0.051391, loss_ce: 0.022707
2021-11-30 13:45:32,312 iteration 4582 : loss : 0.046744, loss_ce: 0.018183
2021-11-30 13:45:33,787 iteration 4583 : loss : 0.037272, loss_ce: 0.017133
2021-11-30 13:45:35,248 iteration 4584 : loss : 0.031299, loss_ce: 0.013801
2021-11-30 13:45:36,725 iteration 4585 : loss : 0.047527, loss_ce: 0.018092
2021-11-30 13:45:38,153 iteration 4586 : loss : 0.029214, loss_ce: 0.015062
2021-11-30 13:45:39,600 iteration 4587 : loss : 0.029241, loss_ce: 0.012454
2021-11-30 13:45:41,053 iteration 4588 : loss : 0.038791, loss_ce: 0.016924
2021-11-30 13:45:42,557 iteration 4589 : loss : 0.038954, loss_ce: 0.016879
2021-11-30 13:45:42,558 Training Data Eval:
2021-11-30 13:45:49,928   Average segmentation loss on training set: 0.0174
2021-11-30 13:45:49,929 Validation Data Eval:
2021-11-30 13:45:52,491   Average segmentation loss on validation set: 0.0802
2021-11-30 13:45:53,976 iteration 4590 : loss : 0.030015, loss_ce: 0.016051
 68%|██████████████████▏        | 270/400 [2:03:56<1:02:26, 28.82s/it]2021-11-30 13:45:55,549 iteration 4591 : loss : 0.050634, loss_ce: 0.016452
2021-11-30 13:45:57,094 iteration 4592 : loss : 0.028695, loss_ce: 0.012253
2021-11-30 13:45:58,606 iteration 4593 : loss : 0.030900, loss_ce: 0.015552
2021-11-30 13:46:00,157 iteration 4594 : loss : 0.070063, loss_ce: 0.022538
2021-11-30 13:46:01,634 iteration 4595 : loss : 0.028878, loss_ce: 0.013622
2021-11-30 13:46:03,089 iteration 4596 : loss : 0.033534, loss_ce: 0.011527
2021-11-30 13:46:04,588 iteration 4597 : loss : 0.026699, loss_ce: 0.014424
2021-11-30 13:46:06,151 iteration 4598 : loss : 0.044228, loss_ce: 0.019669
2021-11-30 13:46:07,675 iteration 4599 : loss : 0.035023, loss_ce: 0.018773
2021-11-30 13:46:09,144 iteration 4600 : loss : 0.029166, loss_ce: 0.012467
2021-11-30 13:46:10,659 iteration 4601 : loss : 0.037241, loss_ce: 0.020037
2021-11-30 13:46:12,246 iteration 4602 : loss : 0.050804, loss_ce: 0.017231
2021-11-30 13:46:13,733 iteration 4603 : loss : 0.033095, loss_ce: 0.014174
2021-11-30 13:46:15,253 iteration 4604 : loss : 0.036239, loss_ce: 0.017932
2021-11-30 13:46:16,800 iteration 4605 : loss : 0.100562, loss_ce: 0.029778
2021-11-30 13:46:18,341 iteration 4606 : loss : 0.041413, loss_ce: 0.019231
2021-11-30 13:46:19,869 iteration 4607 : loss : 0.040802, loss_ce: 0.023820
 68%|██████████████████▎        | 271/400 [2:04:22<1:00:04, 27.94s/it]2021-11-30 13:46:21,351 iteration 4608 : loss : 0.023389, loss_ce: 0.011927
2021-11-30 13:46:22,858 iteration 4609 : loss : 0.041577, loss_ce: 0.017844
2021-11-30 13:46:24,297 iteration 4610 : loss : 0.031837, loss_ce: 0.012687
2021-11-30 13:46:25,730 iteration 4611 : loss : 0.030096, loss_ce: 0.014113
2021-11-30 13:46:27,151 iteration 4612 : loss : 0.035790, loss_ce: 0.015128
2021-11-30 13:46:28,644 iteration 4613 : loss : 0.031829, loss_ce: 0.014465
2021-11-30 13:46:30,138 iteration 4614 : loss : 0.034578, loss_ce: 0.017137
2021-11-30 13:46:31,683 iteration 4615 : loss : 0.053517, loss_ce: 0.019085
2021-11-30 13:46:33,306 iteration 4616 : loss : 0.041090, loss_ce: 0.020001
2021-11-30 13:46:34,810 iteration 4617 : loss : 0.035107, loss_ce: 0.014376
2021-11-30 13:46:36,251 iteration 4618 : loss : 0.025059, loss_ce: 0.011187
2021-11-30 13:46:37,695 iteration 4619 : loss : 0.037951, loss_ce: 0.017408
2021-11-30 13:46:39,238 iteration 4620 : loss : 0.039000, loss_ce: 0.015956
2021-11-30 13:46:40,714 iteration 4621 : loss : 0.045183, loss_ce: 0.024886
2021-11-30 13:46:42,205 iteration 4622 : loss : 0.057227, loss_ce: 0.026572
2021-11-30 13:46:43,668 iteration 4623 : loss : 0.034550, loss_ce: 0.014608
2021-11-30 13:46:45,159 iteration 4624 : loss : 0.030975, loss_ce: 0.016703
 68%|███████████████████▋         | 272/400 [2:04:47<57:54, 27.15s/it]2021-11-30 13:46:46,750 iteration 4625 : loss : 0.046855, loss_ce: 0.018589
2021-11-30 13:46:48,287 iteration 4626 : loss : 0.038252, loss_ce: 0.020027
2021-11-30 13:46:49,898 iteration 4627 : loss : 0.034120, loss_ce: 0.015572
2021-11-30 13:46:51,348 iteration 4628 : loss : 0.029396, loss_ce: 0.012446
2021-11-30 13:46:52,758 iteration 4629 : loss : 0.029490, loss_ce: 0.013766
2021-11-30 13:46:54,237 iteration 4630 : loss : 0.027396, loss_ce: 0.013514
2021-11-30 13:46:55,732 iteration 4631 : loss : 0.031984, loss_ce: 0.014093
2021-11-30 13:46:57,223 iteration 4632 : loss : 0.033712, loss_ce: 0.014383
2021-11-30 13:46:58,695 iteration 4633 : loss : 0.030450, loss_ce: 0.012787
2021-11-30 13:47:00,148 iteration 4634 : loss : 0.022794, loss_ce: 0.012700
2021-11-30 13:47:01,551 iteration 4635 : loss : 0.023113, loss_ce: 0.012246
2021-11-30 13:47:03,140 iteration 4636 : loss : 0.037105, loss_ce: 0.018826
2021-11-30 13:47:04,585 iteration 4637 : loss : 0.044253, loss_ce: 0.015942
2021-11-30 13:47:06,195 iteration 4638 : loss : 0.053285, loss_ce: 0.023747
2021-11-30 13:47:07,716 iteration 4639 : loss : 0.040481, loss_ce: 0.017587
2021-11-30 13:47:09,332 iteration 4640 : loss : 0.043513, loss_ce: 0.018281
2021-11-30 13:47:10,860 iteration 4641 : loss : 0.034511, loss_ce: 0.015450
 68%|███████████████████▊         | 273/400 [2:05:13<56:32, 26.72s/it]2021-11-30 13:47:12,396 iteration 4642 : loss : 0.051606, loss_ce: 0.014250
2021-11-30 13:47:13,879 iteration 4643 : loss : 0.029977, loss_ce: 0.014743
2021-11-30 13:47:15,424 iteration 4644 : loss : 0.037744, loss_ce: 0.015091
2021-11-30 13:47:16,986 iteration 4645 : loss : 0.045208, loss_ce: 0.023020
2021-11-30 13:47:18,487 iteration 4646 : loss : 0.045965, loss_ce: 0.015809
2021-11-30 13:47:19,885 iteration 4647 : loss : 0.027428, loss_ce: 0.012226
2021-11-30 13:47:21,343 iteration 4648 : loss : 0.037391, loss_ce: 0.015220
2021-11-30 13:47:22,869 iteration 4649 : loss : 0.028020, loss_ce: 0.013941
2021-11-30 13:47:24,423 iteration 4650 : loss : 0.045214, loss_ce: 0.017111
2021-11-30 13:47:25,871 iteration 4651 : loss : 0.031084, loss_ce: 0.017746
2021-11-30 13:47:27,455 iteration 4652 : loss : 0.039012, loss_ce: 0.018009
2021-11-30 13:47:28,918 iteration 4653 : loss : 0.032270, loss_ce: 0.015406
2021-11-30 13:47:30,389 iteration 4654 : loss : 0.036348, loss_ce: 0.015013
2021-11-30 13:47:31,954 iteration 4655 : loss : 0.042787, loss_ce: 0.018171
2021-11-30 13:47:33,538 iteration 4656 : loss : 0.041201, loss_ce: 0.020648
2021-11-30 13:47:35,061 iteration 4657 : loss : 0.052423, loss_ce: 0.025571
2021-11-30 13:47:36,576 iteration 4658 : loss : 0.044273, loss_ce: 0.019297
 68%|███████████████████▊         | 274/400 [2:05:38<55:28, 26.41s/it]2021-11-30 13:47:38,156 iteration 4659 : loss : 0.044395, loss_ce: 0.021326
2021-11-30 13:47:39,698 iteration 4660 : loss : 0.048985, loss_ce: 0.024417
2021-11-30 13:47:41,231 iteration 4661 : loss : 0.043359, loss_ce: 0.019776
2021-11-30 13:47:42,663 iteration 4662 : loss : 0.023515, loss_ce: 0.011473
2021-11-30 13:47:44,177 iteration 4663 : loss : 0.044441, loss_ce: 0.018295
2021-11-30 13:47:45,647 iteration 4664 : loss : 0.024508, loss_ce: 0.012116
2021-11-30 13:47:47,130 iteration 4665 : loss : 0.039817, loss_ce: 0.017329
2021-11-30 13:47:48,505 iteration 4666 : loss : 0.028062, loss_ce: 0.011344
2021-11-30 13:47:49,954 iteration 4667 : loss : 0.024094, loss_ce: 0.012697
2021-11-30 13:47:51,592 iteration 4668 : loss : 0.057587, loss_ce: 0.026284
2021-11-30 13:47:53,026 iteration 4669 : loss : 0.028891, loss_ce: 0.014607
2021-11-30 13:47:54,479 iteration 4670 : loss : 0.041853, loss_ce: 0.019406
2021-11-30 13:47:55,908 iteration 4671 : loss : 0.039393, loss_ce: 0.015158
2021-11-30 13:47:57,511 iteration 4672 : loss : 0.046611, loss_ce: 0.020119
2021-11-30 13:47:59,070 iteration 4673 : loss : 0.036952, loss_ce: 0.017909
2021-11-30 13:48:00,482 iteration 4674 : loss : 0.023421, loss_ce: 0.010151
2021-11-30 13:48:00,482 Training Data Eval:
2021-11-30 13:48:07,830   Average segmentation loss on training set: 0.0168
2021-11-30 13:48:07,830 Validation Data Eval:
2021-11-30 13:48:10,383   Average segmentation loss on validation set: 0.0831
2021-11-30 13:48:11,947 iteration 4675 : loss : 0.026678, loss_ce: 0.012794
 69%|██████████████████▌        | 275/400 [2:06:14<1:00:37, 29.10s/it]2021-11-30 13:48:13,596 iteration 4676 : loss : 0.039423, loss_ce: 0.014182
2021-11-30 13:48:15,077 iteration 4677 : loss : 0.039225, loss_ce: 0.015564
2021-11-30 13:48:16,592 iteration 4678 : loss : 0.038809, loss_ce: 0.015091
2021-11-30 13:48:18,166 iteration 4679 : loss : 0.059924, loss_ce: 0.019480
2021-11-30 13:48:19,658 iteration 4680 : loss : 0.033482, loss_ce: 0.017350
2021-11-30 13:48:21,121 iteration 4681 : loss : 0.042557, loss_ce: 0.016816
2021-11-30 13:48:22,590 iteration 4682 : loss : 0.023419, loss_ce: 0.011517
2021-11-30 13:48:23,969 iteration 4683 : loss : 0.036136, loss_ce: 0.013319
2021-11-30 13:48:25,528 iteration 4684 : loss : 0.040860, loss_ce: 0.021811
2021-11-30 13:48:27,055 iteration 4685 : loss : 0.061065, loss_ce: 0.024289
2021-11-30 13:48:28,536 iteration 4686 : loss : 0.036279, loss_ce: 0.013229
2021-11-30 13:48:29,993 iteration 4687 : loss : 0.033237, loss_ce: 0.017860
2021-11-30 13:48:31,398 iteration 4688 : loss : 0.021380, loss_ce: 0.012355
2021-11-30 13:48:32,825 iteration 4689 : loss : 0.021092, loss_ce: 0.011758
2021-11-30 13:48:34,263 iteration 4690 : loss : 0.031127, loss_ce: 0.014878
2021-11-30 13:48:35,792 iteration 4691 : loss : 0.056506, loss_ce: 0.032583
2021-11-30 13:48:37,246 iteration 4692 : loss : 0.034298, loss_ce: 0.013565
 69%|████████████████████         | 276/400 [2:06:39<57:46, 27.96s/it]2021-11-30 13:48:38,839 iteration 4693 : loss : 0.037534, loss_ce: 0.016478
2021-11-30 13:48:40,370 iteration 4694 : loss : 0.052669, loss_ce: 0.019960
2021-11-30 13:48:41,711 iteration 4695 : loss : 0.022394, loss_ce: 0.010640
2021-11-30 13:48:43,289 iteration 4696 : loss : 0.036325, loss_ce: 0.016315
2021-11-30 13:48:44,732 iteration 4697 : loss : 0.032324, loss_ce: 0.013908
2021-11-30 13:48:46,223 iteration 4698 : loss : 0.044573, loss_ce: 0.023833
2021-11-30 13:48:47,788 iteration 4699 : loss : 0.025587, loss_ce: 0.013335
2021-11-30 13:48:49,208 iteration 4700 : loss : 0.036212, loss_ce: 0.019648
2021-11-30 13:48:50,802 iteration 4701 : loss : 0.042159, loss_ce: 0.019959
2021-11-30 13:48:52,270 iteration 4702 : loss : 0.026137, loss_ce: 0.013144
2021-11-30 13:48:53,735 iteration 4703 : loss : 0.034017, loss_ce: 0.013220
2021-11-30 13:48:55,223 iteration 4704 : loss : 0.037507, loss_ce: 0.013914
2021-11-30 13:48:56,572 iteration 4705 : loss : 0.023186, loss_ce: 0.012269
2021-11-30 13:48:58,069 iteration 4706 : loss : 0.041111, loss_ce: 0.014167
2021-11-30 13:48:59,625 iteration 4707 : loss : 0.034686, loss_ce: 0.015485
2021-11-30 13:49:01,184 iteration 4708 : loss : 0.025030, loss_ce: 0.010988
2021-11-30 13:49:02,712 iteration 4709 : loss : 0.055040, loss_ce: 0.020607
 69%|████████████████████         | 277/400 [2:07:05<55:46, 27.21s/it]2021-11-30 13:49:04,278 iteration 4710 : loss : 0.045336, loss_ce: 0.014355
2021-11-30 13:49:05,925 iteration 4711 : loss : 0.037697, loss_ce: 0.017377
2021-11-30 13:49:07,391 iteration 4712 : loss : 0.037730, loss_ce: 0.016603
2021-11-30 13:49:08,878 iteration 4713 : loss : 0.024097, loss_ce: 0.012118
2021-11-30 13:49:10,378 iteration 4714 : loss : 0.036114, loss_ce: 0.014453
2021-11-30 13:49:11,939 iteration 4715 : loss : 0.042697, loss_ce: 0.017700
2021-11-30 13:49:13,453 iteration 4716 : loss : 0.033866, loss_ce: 0.014482
2021-11-30 13:49:14,940 iteration 4717 : loss : 0.033945, loss_ce: 0.015030
2021-11-30 13:49:16,360 iteration 4718 : loss : 0.023626, loss_ce: 0.011526
2021-11-30 13:49:17,778 iteration 4719 : loss : 0.040505, loss_ce: 0.023186
2021-11-30 13:49:19,235 iteration 4720 : loss : 0.030752, loss_ce: 0.014718
2021-11-30 13:49:20,719 iteration 4721 : loss : 0.030484, loss_ce: 0.015598
2021-11-30 13:49:22,270 iteration 4722 : loss : 0.042236, loss_ce: 0.018442
2021-11-30 13:49:23,756 iteration 4723 : loss : 0.024789, loss_ce: 0.012639
2021-11-30 13:49:25,173 iteration 4724 : loss : 0.028418, loss_ce: 0.013218
2021-11-30 13:49:26,583 iteration 4725 : loss : 0.037249, loss_ce: 0.018178
2021-11-30 13:49:28,065 iteration 4726 : loss : 0.032589, loss_ce: 0.015018
 70%|████████████████████▏        | 278/400 [2:07:30<54:12, 26.66s/it]2021-11-30 13:49:29,686 iteration 4727 : loss : 0.035142, loss_ce: 0.014582
2021-11-30 13:49:31,120 iteration 4728 : loss : 0.025675, loss_ce: 0.012956
2021-11-30 13:49:32,570 iteration 4729 : loss : 0.026477, loss_ce: 0.012291
2021-11-30 13:49:34,011 iteration 4730 : loss : 0.033119, loss_ce: 0.015532
2021-11-30 13:49:35,554 iteration 4731 : loss : 0.035624, loss_ce: 0.016341
2021-11-30 13:49:37,058 iteration 4732 : loss : 0.033456, loss_ce: 0.014418
2021-11-30 13:49:38,543 iteration 4733 : loss : 0.027164, loss_ce: 0.013122
2021-11-30 13:49:40,001 iteration 4734 : loss : 0.038628, loss_ce: 0.018804
2021-11-30 13:49:41,608 iteration 4735 : loss : 0.052176, loss_ce: 0.019509
2021-11-30 13:49:43,021 iteration 4736 : loss : 0.035030, loss_ce: 0.018181
2021-11-30 13:49:44,595 iteration 4737 : loss : 0.039092, loss_ce: 0.017769
2021-11-30 13:49:46,093 iteration 4738 : loss : 0.031916, loss_ce: 0.014639
2021-11-30 13:49:47,585 iteration 4739 : loss : 0.033190, loss_ce: 0.015383
2021-11-30 13:49:49,134 iteration 4740 : loss : 0.041195, loss_ce: 0.020865
2021-11-30 13:49:50,591 iteration 4741 : loss : 0.030778, loss_ce: 0.013685
2021-11-30 13:49:52,062 iteration 4742 : loss : 0.034688, loss_ce: 0.019243
2021-11-30 13:49:53,645 iteration 4743 : loss : 0.044480, loss_ce: 0.017478
 70%|████████████████████▏        | 279/400 [2:07:56<53:06, 26.34s/it]2021-11-30 13:49:55,250 iteration 4744 : loss : 0.038951, loss_ce: 0.015592
2021-11-30 13:49:56,726 iteration 4745 : loss : 0.046897, loss_ce: 0.017133
2021-11-30 13:49:58,154 iteration 4746 : loss : 0.030188, loss_ce: 0.016646
2021-11-30 13:49:59,600 iteration 4747 : loss : 0.034637, loss_ce: 0.015239
2021-11-30 13:50:01,094 iteration 4748 : loss : 0.032711, loss_ce: 0.013997
2021-11-30 13:50:02,596 iteration 4749 : loss : 0.042015, loss_ce: 0.016044
2021-11-30 13:50:04,133 iteration 4750 : loss : 0.039871, loss_ce: 0.017057
2021-11-30 13:50:05,591 iteration 4751 : loss : 0.033826, loss_ce: 0.015140
2021-11-30 13:50:07,102 iteration 4752 : loss : 0.031302, loss_ce: 0.014006
2021-11-30 13:50:08,653 iteration 4753 : loss : 0.029731, loss_ce: 0.012917
2021-11-30 13:50:10,159 iteration 4754 : loss : 0.029594, loss_ce: 0.012962
2021-11-30 13:50:11,764 iteration 4755 : loss : 0.034533, loss_ce: 0.013666
2021-11-30 13:50:13,195 iteration 4756 : loss : 0.033023, loss_ce: 0.017791
2021-11-30 13:50:14,661 iteration 4757 : loss : 0.043461, loss_ce: 0.019435
2021-11-30 13:50:16,121 iteration 4758 : loss : 0.028846, loss_ce: 0.011801
2021-11-30 13:50:17,621 iteration 4759 : loss : 0.023648, loss_ce: 0.011483
2021-11-30 13:50:17,622 Training Data Eval:
2021-11-30 13:50:25,007   Average segmentation loss on training set: 0.0168
2021-11-30 13:50:25,008 Validation Data Eval:
2021-11-30 13:50:27,555   Average segmentation loss on validation set: 0.0808
2021-11-30 13:50:29,045 iteration 4760 : loss : 0.028494, loss_ce: 0.015256
 70%|████████████████████▎        | 280/400 [2:08:31<58:06, 29.05s/it]2021-11-30 13:50:30,596 iteration 4761 : loss : 0.034744, loss_ce: 0.015709
2021-11-30 13:50:32,120 iteration 4762 : loss : 0.052359, loss_ce: 0.019352
2021-11-30 13:50:33,531 iteration 4763 : loss : 0.026119, loss_ce: 0.013216
2021-11-30 13:50:35,059 iteration 4764 : loss : 0.035282, loss_ce: 0.014584
2021-11-30 13:50:36,509 iteration 4765 : loss : 0.046000, loss_ce: 0.023425
2021-11-30 13:50:38,074 iteration 4766 : loss : 0.035687, loss_ce: 0.014526
2021-11-30 13:50:39,595 iteration 4767 : loss : 0.037728, loss_ce: 0.018076
2021-11-30 13:50:41,026 iteration 4768 : loss : 0.037754, loss_ce: 0.015000
2021-11-30 13:50:42,569 iteration 4769 : loss : 0.043524, loss_ce: 0.017518
2021-11-30 13:50:44,095 iteration 4770 : loss : 0.026668, loss_ce: 0.012377
2021-11-30 13:50:45,510 iteration 4771 : loss : 0.025098, loss_ce: 0.011293
2021-11-30 13:50:47,063 iteration 4772 : loss : 0.038496, loss_ce: 0.018227
2021-11-30 13:50:48,595 iteration 4773 : loss : 0.036653, loss_ce: 0.016152
2021-11-30 13:50:50,105 iteration 4774 : loss : 0.036309, loss_ce: 0.018053
2021-11-30 13:50:51,556 iteration 4775 : loss : 0.052463, loss_ce: 0.019162
2021-11-30 13:50:53,049 iteration 4776 : loss : 0.030629, loss_ce: 0.019726
2021-11-30 13:50:54,681 iteration 4777 : loss : 0.032078, loss_ce: 0.016818
 70%|████████████████████▎        | 281/400 [2:08:57<55:35, 28.03s/it]2021-11-30 13:50:56,123 iteration 4778 : loss : 0.022605, loss_ce: 0.010426
2021-11-30 13:50:57,582 iteration 4779 : loss : 0.040090, loss_ce: 0.017049
2021-11-30 13:50:58,989 iteration 4780 : loss : 0.022884, loss_ce: 0.010445
2021-11-30 13:51:00,456 iteration 4781 : loss : 0.046693, loss_ce: 0.013968
2021-11-30 13:51:02,003 iteration 4782 : loss : 0.039532, loss_ce: 0.019014
2021-11-30 13:51:03,492 iteration 4783 : loss : 0.023887, loss_ce: 0.014172
2021-11-30 13:51:04,927 iteration 4784 : loss : 0.029542, loss_ce: 0.015632
2021-11-30 13:51:06,509 iteration 4785 : loss : 0.031646, loss_ce: 0.014106
2021-11-30 13:51:08,027 iteration 4786 : loss : 0.034399, loss_ce: 0.015482
2021-11-30 13:51:09,549 iteration 4787 : loss : 0.034890, loss_ce: 0.013297
2021-11-30 13:51:11,066 iteration 4788 : loss : 0.045950, loss_ce: 0.018195
2021-11-30 13:51:12,480 iteration 4789 : loss : 0.027035, loss_ce: 0.014913
2021-11-30 13:51:14,072 iteration 4790 : loss : 0.045656, loss_ce: 0.017868
2021-11-30 13:51:15,546 iteration 4791 : loss : 0.034227, loss_ce: 0.017052
2021-11-30 13:51:17,079 iteration 4792 : loss : 0.047767, loss_ce: 0.017179
2021-11-30 13:51:18,513 iteration 4793 : loss : 0.027213, loss_ce: 0.013724
2021-11-30 13:51:19,906 iteration 4794 : loss : 0.042515, loss_ce: 0.016008
 70%|████████████████████▍        | 282/400 [2:09:22<53:27, 27.19s/it]2021-11-30 13:51:21,501 iteration 4795 : loss : 0.037325, loss_ce: 0.015291
2021-11-30 13:51:22,997 iteration 4796 : loss : 0.024407, loss_ce: 0.012364
2021-11-30 13:51:24,504 iteration 4797 : loss : 0.030151, loss_ce: 0.013909
2021-11-30 13:51:26,071 iteration 4798 : loss : 0.040380, loss_ce: 0.014815
2021-11-30 13:51:27,550 iteration 4799 : loss : 0.021816, loss_ce: 0.009858
2021-11-30 13:51:29,053 iteration 4800 : loss : 0.045741, loss_ce: 0.017002
2021-11-30 13:51:30,639 iteration 4801 : loss : 0.026467, loss_ce: 0.013252
2021-11-30 13:51:32,045 iteration 4802 : loss : 0.026593, loss_ce: 0.013639
2021-11-30 13:51:33,500 iteration 4803 : loss : 0.032133, loss_ce: 0.016413
2021-11-30 13:51:35,074 iteration 4804 : loss : 0.058778, loss_ce: 0.018504
2021-11-30 13:51:36,617 iteration 4805 : loss : 0.049420, loss_ce: 0.020758
2021-11-30 13:51:38,177 iteration 4806 : loss : 0.033205, loss_ce: 0.016277
2021-11-30 13:51:39,578 iteration 4807 : loss : 0.034289, loss_ce: 0.016644
2021-11-30 13:51:40,987 iteration 4808 : loss : 0.026372, loss_ce: 0.013701
2021-11-30 13:51:42,507 iteration 4809 : loss : 0.039742, loss_ce: 0.020593
2021-11-30 13:51:44,000 iteration 4810 : loss : 0.034673, loss_ce: 0.013005
2021-11-30 13:51:45,499 iteration 4811 : loss : 0.024810, loss_ce: 0.012980
 71%|████████████████████▌        | 283/400 [2:09:47<52:04, 26.70s/it]2021-11-30 13:51:46,978 iteration 4812 : loss : 0.030918, loss_ce: 0.014405
2021-11-30 13:51:48,488 iteration 4813 : loss : 0.033365, loss_ce: 0.012377
2021-11-30 13:51:50,026 iteration 4814 : loss : 0.038341, loss_ce: 0.013961
2021-11-30 13:51:51,428 iteration 4815 : loss : 0.033668, loss_ce: 0.015949
2021-11-30 13:51:53,040 iteration 4816 : loss : 0.035436, loss_ce: 0.017486
2021-11-30 13:51:54,513 iteration 4817 : loss : 0.040524, loss_ce: 0.020713
2021-11-30 13:51:55,974 iteration 4818 : loss : 0.026870, loss_ce: 0.013625
2021-11-30 13:51:57,376 iteration 4819 : loss : 0.029265, loss_ce: 0.013593
2021-11-30 13:51:58,838 iteration 4820 : loss : 0.034313, loss_ce: 0.015729
2021-11-30 13:52:00,437 iteration 4821 : loss : 0.054150, loss_ce: 0.027338
2021-11-30 13:52:02,020 iteration 4822 : loss : 0.049469, loss_ce: 0.019341
2021-11-30 13:52:03,498 iteration 4823 : loss : 0.031006, loss_ce: 0.014957
2021-11-30 13:52:05,018 iteration 4824 : loss : 0.039660, loss_ce: 0.017033
2021-11-30 13:52:06,468 iteration 4825 : loss : 0.032931, loss_ce: 0.014900
2021-11-30 13:52:07,915 iteration 4826 : loss : 0.028327, loss_ce: 0.012112
2021-11-30 13:52:09,394 iteration 4827 : loss : 0.026919, loss_ce: 0.013367
2021-11-30 13:52:10,850 iteration 4828 : loss : 0.030901, loss_ce: 0.014665
 71%|████████████████████▌        | 284/400 [2:10:13<50:51, 26.31s/it]2021-11-30 13:52:12,353 iteration 4829 : loss : 0.026922, loss_ce: 0.012398
2021-11-30 13:52:13,878 iteration 4830 : loss : 0.031040, loss_ce: 0.016739
2021-11-30 13:52:15,376 iteration 4831 : loss : 0.026980, loss_ce: 0.014289
2021-11-30 13:52:16,844 iteration 4832 : loss : 0.039199, loss_ce: 0.014658
2021-11-30 13:52:18,367 iteration 4833 : loss : 0.036586, loss_ce: 0.015386
2021-11-30 13:52:19,775 iteration 4834 : loss : 0.024422, loss_ce: 0.012040
2021-11-30 13:52:21,381 iteration 4835 : loss : 0.034181, loss_ce: 0.017656
2021-11-30 13:52:22,860 iteration 4836 : loss : 0.039428, loss_ce: 0.016717
2021-11-30 13:52:24,382 iteration 4837 : loss : 0.047117, loss_ce: 0.016258
2021-11-30 13:52:25,832 iteration 4838 : loss : 0.025158, loss_ce: 0.011088
2021-11-30 13:52:27,365 iteration 4839 : loss : 0.050415, loss_ce: 0.021092
2021-11-30 13:52:28,871 iteration 4840 : loss : 0.027425, loss_ce: 0.015095
2021-11-30 13:52:30,308 iteration 4841 : loss : 0.023052, loss_ce: 0.010701
2021-11-30 13:52:31,799 iteration 4842 : loss : 0.028871, loss_ce: 0.014197
2021-11-30 13:52:33,379 iteration 4843 : loss : 0.040896, loss_ce: 0.019095
2021-11-30 13:52:34,869 iteration 4844 : loss : 0.038690, loss_ce: 0.016909
2021-11-30 13:52:34,869 Training Data Eval:
2021-11-30 13:52:42,249   Average segmentation loss on training set: 0.0162
2021-11-30 13:52:42,249 Validation Data Eval:
2021-11-30 13:52:44,796   Average segmentation loss on validation set: 0.0827
2021-11-30 13:52:46,361 iteration 4845 : loss : 0.035858, loss_ce: 0.017808
 71%|████████████████████▋        | 285/400 [2:10:48<55:42, 29.06s/it]2021-11-30 13:52:47,867 iteration 4846 : loss : 0.027792, loss_ce: 0.013463
2021-11-30 13:52:49,337 iteration 4847 : loss : 0.037021, loss_ce: 0.016059
2021-11-30 13:52:50,891 iteration 4848 : loss : 0.038494, loss_ce: 0.021596
2021-11-30 13:52:52,408 iteration 4849 : loss : 0.028916, loss_ce: 0.010319
2021-11-30 13:52:53,868 iteration 4850 : loss : 0.028348, loss_ce: 0.012159
2021-11-30 13:52:55,486 iteration 4851 : loss : 0.040788, loss_ce: 0.018057
2021-11-30 13:52:56,946 iteration 4852 : loss : 0.027954, loss_ce: 0.011936
2021-11-30 13:52:58,440 iteration 4853 : loss : 0.028006, loss_ce: 0.015102
2021-11-30 13:52:59,927 iteration 4854 : loss : 0.034556, loss_ce: 0.015526
2021-11-30 13:53:01,377 iteration 4855 : loss : 0.037127, loss_ce: 0.013508
2021-11-30 13:53:02,980 iteration 4856 : loss : 0.048519, loss_ce: 0.023690
2021-11-30 13:53:04,415 iteration 4857 : loss : 0.022007, loss_ce: 0.010527
2021-11-30 13:53:05,981 iteration 4858 : loss : 0.031963, loss_ce: 0.015485
2021-11-30 13:53:07,418 iteration 4859 : loss : 0.019622, loss_ce: 0.011199
2021-11-30 13:53:08,960 iteration 4860 : loss : 0.042846, loss_ce: 0.021121
2021-11-30 13:53:10,419 iteration 4861 : loss : 0.032124, loss_ce: 0.013986
2021-11-30 13:53:11,940 iteration 4862 : loss : 0.028080, loss_ce: 0.015906
 72%|████████████████████▋        | 286/400 [2:11:14<53:14, 28.02s/it]2021-11-30 13:53:13,576 iteration 4863 : loss : 0.044933, loss_ce: 0.017801
2021-11-30 13:53:15,043 iteration 4864 : loss : 0.027485, loss_ce: 0.012818
2021-11-30 13:53:16,493 iteration 4865 : loss : 0.030638, loss_ce: 0.017306
2021-11-30 13:53:18,040 iteration 4866 : loss : 0.032172, loss_ce: 0.013984
2021-11-30 13:53:19,550 iteration 4867 : loss : 0.036379, loss_ce: 0.015194
2021-11-30 13:53:20,938 iteration 4868 : loss : 0.032461, loss_ce: 0.016357
2021-11-30 13:53:22,551 iteration 4869 : loss : 0.033870, loss_ce: 0.018364
2021-11-30 13:53:24,025 iteration 4870 : loss : 0.027109, loss_ce: 0.012602
2021-11-30 13:53:25,635 iteration 4871 : loss : 0.044558, loss_ce: 0.018254
2021-11-30 13:53:27,041 iteration 4872 : loss : 0.025133, loss_ce: 0.011112
2021-11-30 13:53:28,639 iteration 4873 : loss : 0.031946, loss_ce: 0.015562
2021-11-30 13:53:30,168 iteration 4874 : loss : 0.054458, loss_ce: 0.020311
2021-11-30 13:53:31,639 iteration 4875 : loss : 0.036378, loss_ce: 0.017401
2021-11-30 13:53:33,029 iteration 4876 : loss : 0.038270, loss_ce: 0.015583
2021-11-30 13:53:34,588 iteration 4877 : loss : 0.037304, loss_ce: 0.020662
2021-11-30 13:53:36,158 iteration 4878 : loss : 0.054437, loss_ce: 0.020579
2021-11-30 13:53:37,625 iteration 4879 : loss : 0.024257, loss_ce: 0.011498
 72%|████████████████████▊        | 287/400 [2:11:39<51:26, 27.32s/it]2021-11-30 13:53:39,145 iteration 4880 : loss : 0.037181, loss_ce: 0.018537
2021-11-30 13:53:40,572 iteration 4881 : loss : 0.029214, loss_ce: 0.014302
2021-11-30 13:53:41,978 iteration 4882 : loss : 0.027475, loss_ce: 0.013871
2021-11-30 13:53:43,505 iteration 4883 : loss : 0.030790, loss_ce: 0.015524
2021-11-30 13:53:45,026 iteration 4884 : loss : 0.045300, loss_ce: 0.016304
2021-11-30 13:53:46,385 iteration 4885 : loss : 0.023932, loss_ce: 0.012127
2021-11-30 13:53:47,866 iteration 4886 : loss : 0.041476, loss_ce: 0.013461
2021-11-30 13:53:49,283 iteration 4887 : loss : 0.022691, loss_ce: 0.010306
2021-11-30 13:53:50,808 iteration 4888 : loss : 0.041879, loss_ce: 0.015265
2021-11-30 13:53:52,303 iteration 4889 : loss : 0.055122, loss_ce: 0.023738
2021-11-30 13:53:53,735 iteration 4890 : loss : 0.031197, loss_ce: 0.018625
2021-11-30 13:53:55,097 iteration 4891 : loss : 0.024672, loss_ce: 0.012678
2021-11-30 13:53:56,639 iteration 4892 : loss : 0.029358, loss_ce: 0.014110
2021-11-30 13:53:58,139 iteration 4893 : loss : 0.036769, loss_ce: 0.017564
2021-11-30 13:53:59,620 iteration 4894 : loss : 0.026489, loss_ce: 0.014589
2021-11-30 13:54:01,087 iteration 4895 : loss : 0.044855, loss_ce: 0.014411
2021-11-30 13:54:02,521 iteration 4896 : loss : 0.051376, loss_ce: 0.019532
 72%|████████████████████▉        | 288/400 [2:12:04<49:38, 26.59s/it]2021-11-30 13:54:04,022 iteration 4897 : loss : 0.032321, loss_ce: 0.013173
2021-11-30 13:54:05,594 iteration 4898 : loss : 0.037274, loss_ce: 0.019872
2021-11-30 13:54:07,068 iteration 4899 : loss : 0.027115, loss_ce: 0.010895
2021-11-30 13:54:08,492 iteration 4900 : loss : 0.032435, loss_ce: 0.014604
2021-11-30 13:54:09,941 iteration 4901 : loss : 0.054600, loss_ce: 0.017520
2021-11-30 13:54:11,390 iteration 4902 : loss : 0.021965, loss_ce: 0.010461
2021-11-30 13:54:12,905 iteration 4903 : loss : 0.031366, loss_ce: 0.017876
2021-11-30 13:54:14,286 iteration 4904 : loss : 0.027845, loss_ce: 0.011336
2021-11-30 13:54:15,726 iteration 4905 : loss : 0.040849, loss_ce: 0.014843
2021-11-30 13:54:17,254 iteration 4906 : loss : 0.028822, loss_ce: 0.013179
2021-11-30 13:54:18,697 iteration 4907 : loss : 0.037976, loss_ce: 0.015921
2021-11-30 13:54:20,266 iteration 4908 : loss : 0.044533, loss_ce: 0.020874
2021-11-30 13:54:21,785 iteration 4909 : loss : 0.030855, loss_ce: 0.013656
2021-11-30 13:54:23,217 iteration 4910 : loss : 0.037271, loss_ce: 0.018233
2021-11-30 13:54:24,673 iteration 4911 : loss : 0.022771, loss_ce: 0.011137
2021-11-30 13:54:26,168 iteration 4912 : loss : 0.030795, loss_ce: 0.016346
2021-11-30 13:54:27,637 iteration 4913 : loss : 0.033447, loss_ce: 0.015319
 72%|████████████████████▉        | 289/400 [2:12:29<48:22, 26.15s/it]2021-11-30 13:54:29,213 iteration 4914 : loss : 0.033437, loss_ce: 0.015666
2021-11-30 13:54:30,640 iteration 4915 : loss : 0.036804, loss_ce: 0.017050
2021-11-30 13:54:32,086 iteration 4916 : loss : 0.026614, loss_ce: 0.009985
2021-11-30 13:54:33,583 iteration 4917 : loss : 0.024784, loss_ce: 0.010500
2021-11-30 13:54:35,134 iteration 4918 : loss : 0.025492, loss_ce: 0.012204
2021-11-30 13:54:36,780 iteration 4919 : loss : 0.045635, loss_ce: 0.021926
2021-11-30 13:54:38,225 iteration 4920 : loss : 0.021156, loss_ce: 0.010691
2021-11-30 13:54:39,655 iteration 4921 : loss : 0.024496, loss_ce: 0.013549
2021-11-30 13:54:41,055 iteration 4922 : loss : 0.029051, loss_ce: 0.010475
2021-11-30 13:54:42,598 iteration 4923 : loss : 0.035395, loss_ce: 0.016308
2021-11-30 13:54:44,167 iteration 4924 : loss : 0.045330, loss_ce: 0.027174
2021-11-30 13:54:45,655 iteration 4925 : loss : 0.030341, loss_ce: 0.014694
2021-11-30 13:54:47,198 iteration 4926 : loss : 0.044130, loss_ce: 0.023410
2021-11-30 13:54:48,653 iteration 4927 : loss : 0.029608, loss_ce: 0.012110
2021-11-30 13:54:50,203 iteration 4928 : loss : 0.051899, loss_ce: 0.022043
2021-11-30 13:54:51,789 iteration 4929 : loss : 0.043264, loss_ce: 0.021359
2021-11-30 13:54:51,789 Training Data Eval:
2021-11-30 13:54:59,166   Average segmentation loss on training set: 0.0158
2021-11-30 13:54:59,166 Validation Data Eval:
2021-11-30 13:55:01,730   Average segmentation loss on validation set: 0.0835
2021-11-30 13:55:03,237 iteration 4930 : loss : 0.037188, loss_ce: 0.013785
 72%|█████████████████████        | 290/400 [2:13:05<53:08, 28.98s/it]2021-11-30 13:55:04,779 iteration 4931 : loss : 0.038225, loss_ce: 0.019597
2021-11-30 13:55:06,308 iteration 4932 : loss : 0.041613, loss_ce: 0.025029
2021-11-30 13:55:07,735 iteration 4933 : loss : 0.024539, loss_ce: 0.014979
2021-11-30 13:55:09,304 iteration 4934 : loss : 0.031582, loss_ce: 0.013628
2021-11-30 13:55:10,792 iteration 4935 : loss : 0.034276, loss_ce: 0.013116
2021-11-30 13:55:12,312 iteration 4936 : loss : 0.028837, loss_ce: 0.013023
2021-11-30 13:55:13,818 iteration 4937 : loss : 0.030122, loss_ce: 0.016082
2021-11-30 13:55:15,374 iteration 4938 : loss : 0.059663, loss_ce: 0.023050
2021-11-30 13:55:16,923 iteration 4939 : loss : 0.047059, loss_ce: 0.022436
2021-11-30 13:55:18,360 iteration 4940 : loss : 0.043603, loss_ce: 0.019698
2021-11-30 13:55:19,826 iteration 4941 : loss : 0.041434, loss_ce: 0.023403
2021-11-30 13:55:21,329 iteration 4942 : loss : 0.029989, loss_ce: 0.012519
2021-11-30 13:55:22,912 iteration 4943 : loss : 0.061471, loss_ce: 0.020493
2021-11-30 13:55:24,398 iteration 4944 : loss : 0.042313, loss_ce: 0.017300
2021-11-30 13:55:25,956 iteration 4945 : loss : 0.031595, loss_ce: 0.014664
2021-11-30 13:55:27,504 iteration 4946 : loss : 0.039560, loss_ce: 0.014255
2021-11-30 13:55:28,975 iteration 4947 : loss : 0.030662, loss_ce: 0.013307
 73%|█████████████████████        | 291/400 [2:13:31<50:53, 28.01s/it]2021-11-30 13:55:30,551 iteration 4948 : loss : 0.039045, loss_ce: 0.018520
2021-11-30 13:55:32,008 iteration 4949 : loss : 0.037609, loss_ce: 0.017872
2021-11-30 13:55:33,547 iteration 4950 : loss : 0.041940, loss_ce: 0.020260
2021-11-30 13:55:35,072 iteration 4951 : loss : 0.032866, loss_ce: 0.014035
2021-11-30 13:55:36,582 iteration 4952 : loss : 0.034780, loss_ce: 0.021451
2021-11-30 13:55:38,127 iteration 4953 : loss : 0.030788, loss_ce: 0.015128
2021-11-30 13:55:39,596 iteration 4954 : loss : 0.027393, loss_ce: 0.012853
2021-11-30 13:55:41,028 iteration 4955 : loss : 0.035328, loss_ce: 0.013831
2021-11-30 13:55:42,511 iteration 4956 : loss : 0.028691, loss_ce: 0.012116
2021-11-30 13:55:43,904 iteration 4957 : loss : 0.024167, loss_ce: 0.012015
2021-11-30 13:55:45,416 iteration 4958 : loss : 0.041706, loss_ce: 0.022263
2021-11-30 13:55:46,932 iteration 4959 : loss : 0.028259, loss_ce: 0.012999
2021-11-30 13:55:48,333 iteration 4960 : loss : 0.026534, loss_ce: 0.013313
2021-11-30 13:55:49,835 iteration 4961 : loss : 0.032102, loss_ce: 0.014379
2021-11-30 13:55:51,263 iteration 4962 : loss : 0.048469, loss_ce: 0.014231
2021-11-30 13:55:52,731 iteration 4963 : loss : 0.028141, loss_ce: 0.012550
2021-11-30 13:55:54,285 iteration 4964 : loss : 0.039154, loss_ce: 0.015712
 73%|█████████████████████▏       | 292/400 [2:13:56<48:57, 27.20s/it]2021-11-30 13:55:55,905 iteration 4965 : loss : 0.039843, loss_ce: 0.017683
2021-11-30 13:55:57,457 iteration 4966 : loss : 0.041715, loss_ce: 0.018909
2021-11-30 13:55:58,944 iteration 4967 : loss : 0.033583, loss_ce: 0.016173
2021-11-30 13:56:00,454 iteration 4968 : loss : 0.031823, loss_ce: 0.016969
2021-11-30 13:56:02,065 iteration 4969 : loss : 0.047885, loss_ce: 0.018379
2021-11-30 13:56:03,493 iteration 4970 : loss : 0.023726, loss_ce: 0.012233
2021-11-30 13:56:04,954 iteration 4971 : loss : 0.039038, loss_ce: 0.022723
2021-11-30 13:56:06,476 iteration 4972 : loss : 0.031122, loss_ce: 0.015925
2021-11-30 13:56:07,980 iteration 4973 : loss : 0.033770, loss_ce: 0.014480
2021-11-30 13:56:09,614 iteration 4974 : loss : 0.039738, loss_ce: 0.015882
2021-11-30 13:56:11,089 iteration 4975 : loss : 0.036219, loss_ce: 0.013804
2021-11-30 13:56:12,521 iteration 4976 : loss : 0.027707, loss_ce: 0.012814
2021-11-30 13:56:14,062 iteration 4977 : loss : 0.059867, loss_ce: 0.015412
2021-11-30 13:56:15,531 iteration 4978 : loss : 0.035113, loss_ce: 0.012374
2021-11-30 13:56:17,022 iteration 4979 : loss : 0.033531, loss_ce: 0.014643
2021-11-30 13:56:18,517 iteration 4980 : loss : 0.043709, loss_ce: 0.015310
2021-11-30 13:56:19,988 iteration 4981 : loss : 0.032673, loss_ce: 0.013776
 73%|█████████████████████▏       | 293/400 [2:14:22<47:42, 26.75s/it]2021-11-30 13:56:21,496 iteration 4982 : loss : 0.028406, loss_ce: 0.014814
2021-11-30 13:56:22,968 iteration 4983 : loss : 0.031737, loss_ce: 0.015889
2021-11-30 13:56:24,547 iteration 4984 : loss : 0.029054, loss_ce: 0.013806
2021-11-30 13:56:25,939 iteration 4985 : loss : 0.026962, loss_ce: 0.010687
2021-11-30 13:56:27,510 iteration 4986 : loss : 0.042980, loss_ce: 0.021243
2021-11-30 13:56:28,931 iteration 4987 : loss : 0.030211, loss_ce: 0.012147
2021-11-30 13:56:30,432 iteration 4988 : loss : 0.039455, loss_ce: 0.015942
2021-11-30 13:56:31,990 iteration 4989 : loss : 0.043285, loss_ce: 0.018736
2021-11-30 13:56:33,571 iteration 4990 : loss : 0.035139, loss_ce: 0.017492
2021-11-30 13:56:35,072 iteration 4991 : loss : 0.041714, loss_ce: 0.016347
2021-11-30 13:56:36,484 iteration 4992 : loss : 0.029574, loss_ce: 0.015945
2021-11-30 13:56:37,966 iteration 4993 : loss : 0.038300, loss_ce: 0.016114
2021-11-30 13:56:39,407 iteration 4994 : loss : 0.032247, loss_ce: 0.012028
2021-11-30 13:56:40,839 iteration 4995 : loss : 0.035928, loss_ce: 0.017059
2021-11-30 13:56:42,403 iteration 4996 : loss : 0.030370, loss_ce: 0.011330
2021-11-30 13:56:43,808 iteration 4997 : loss : 0.027017, loss_ce: 0.010776
2021-11-30 13:56:45,240 iteration 4998 : loss : 0.030352, loss_ce: 0.014053
 74%|█████████████████████▎       | 294/400 [2:14:47<46:27, 26.30s/it]2021-11-30 13:56:46,750 iteration 4999 : loss : 0.022617, loss_ce: 0.010958
2021-11-30 13:56:48,214 iteration 5000 : loss : 0.028578, loss_ce: 0.013676
2021-11-30 13:56:49,690 iteration 5001 : loss : 0.032761, loss_ce: 0.015155
2021-11-30 13:56:51,167 iteration 5002 : loss : 0.025736, loss_ce: 0.012753
2021-11-30 13:56:52,678 iteration 5003 : loss : 0.045192, loss_ce: 0.015006
2021-11-30 13:56:54,144 iteration 5004 : loss : 0.035803, loss_ce: 0.014339
2021-11-30 13:56:55,624 iteration 5005 : loss : 0.031111, loss_ce: 0.015879
2021-11-30 13:56:57,176 iteration 5006 : loss : 0.045078, loss_ce: 0.021759
2021-11-30 13:56:58,708 iteration 5007 : loss : 0.044441, loss_ce: 0.021434
2021-11-30 13:57:00,244 iteration 5008 : loss : 0.041429, loss_ce: 0.019702
2021-11-30 13:57:01,782 iteration 5009 : loss : 0.035138, loss_ce: 0.012783
2021-11-30 13:57:03,281 iteration 5010 : loss : 0.040857, loss_ce: 0.018139
2021-11-30 13:57:04,741 iteration 5011 : loss : 0.031832, loss_ce: 0.016529
2021-11-30 13:57:06,248 iteration 5012 : loss : 0.031743, loss_ce: 0.014929
2021-11-30 13:57:07,675 iteration 5013 : loss : 0.033949, loss_ce: 0.015488
2021-11-30 13:57:09,168 iteration 5014 : loss : 0.031803, loss_ce: 0.014298
2021-11-30 13:57:09,168 Training Data Eval:
2021-11-30 13:57:16,558   Average segmentation loss on training set: 0.0157
2021-11-30 13:57:16,558 Validation Data Eval:
2021-11-30 13:57:19,118   Average segmentation loss on validation set: 0.0908
2021-11-30 13:57:20,609 iteration 5015 : loss : 0.031230, loss_ce: 0.012866
 74%|█████████████████████▍       | 295/400 [2:15:22<50:46, 29.02s/it]2021-11-30 13:57:22,140 iteration 5016 : loss : 0.040117, loss_ce: 0.021567
2021-11-30 13:57:23,617 iteration 5017 : loss : 0.025589, loss_ce: 0.012918
2021-11-30 13:57:25,100 iteration 5018 : loss : 0.030738, loss_ce: 0.016447
2021-11-30 13:57:26,625 iteration 5019 : loss : 0.030949, loss_ce: 0.012573
2021-11-30 13:57:28,140 iteration 5020 : loss : 0.032651, loss_ce: 0.012691
2021-11-30 13:57:29,680 iteration 5021 : loss : 0.033631, loss_ce: 0.018875
2021-11-30 13:57:31,121 iteration 5022 : loss : 0.025907, loss_ce: 0.011146
2021-11-30 13:57:32,612 iteration 5023 : loss : 0.049872, loss_ce: 0.016669
2021-11-30 13:57:34,209 iteration 5024 : loss : 0.046466, loss_ce: 0.014147
2021-11-30 13:57:35,765 iteration 5025 : loss : 0.065122, loss_ce: 0.016308
2021-11-30 13:57:37,261 iteration 5026 : loss : 0.034262, loss_ce: 0.015993
2021-11-30 13:57:38,707 iteration 5027 : loss : 0.040124, loss_ce: 0.018534
2021-11-30 13:57:40,183 iteration 5028 : loss : 0.032118, loss_ce: 0.015087
2021-11-30 13:57:41,760 iteration 5029 : loss : 0.048388, loss_ce: 0.014554
2021-11-30 13:57:43,250 iteration 5030 : loss : 0.035109, loss_ce: 0.020935
2021-11-30 13:57:44,819 iteration 5031 : loss : 0.034808, loss_ce: 0.018820
2021-11-30 13:57:46,313 iteration 5032 : loss : 0.045811, loss_ce: 0.024040
 74%|█████████████████████▍       | 296/400 [2:15:48<48:34, 28.03s/it]2021-11-30 13:57:47,978 iteration 5033 : loss : 0.046588, loss_ce: 0.016960
2021-11-30 13:57:49,439 iteration 5034 : loss : 0.050809, loss_ce: 0.018152
2021-11-30 13:57:50,884 iteration 5035 : loss : 0.030601, loss_ce: 0.015791
2021-11-30 13:57:52,352 iteration 5036 : loss : 0.031502, loss_ce: 0.015048
2021-11-30 13:57:53,862 iteration 5037 : loss : 0.039024, loss_ce: 0.016276
2021-11-30 13:57:55,331 iteration 5038 : loss : 0.056312, loss_ce: 0.016562
2021-11-30 13:57:56,918 iteration 5039 : loss : 0.046546, loss_ce: 0.025289
2021-11-30 13:57:58,409 iteration 5040 : loss : 0.042549, loss_ce: 0.014730
2021-11-30 13:57:59,954 iteration 5041 : loss : 0.043522, loss_ce: 0.017652
2021-11-30 13:58:01,388 iteration 5042 : loss : 0.029694, loss_ce: 0.014654
2021-11-30 13:58:02,871 iteration 5043 : loss : 0.026981, loss_ce: 0.014540
2021-11-30 13:58:04,372 iteration 5044 : loss : 0.043883, loss_ce: 0.022851
2021-11-30 13:58:05,930 iteration 5045 : loss : 0.042844, loss_ce: 0.020387
2021-11-30 13:58:07,422 iteration 5046 : loss : 0.047389, loss_ce: 0.016452
2021-11-30 13:58:08,899 iteration 5047 : loss : 0.035899, loss_ce: 0.013214
2021-11-30 13:58:10,361 iteration 5048 : loss : 0.028053, loss_ce: 0.013079
2021-11-30 13:58:11,836 iteration 5049 : loss : 0.025883, loss_ce: 0.013063
 74%|█████████████████████▌       | 297/400 [2:16:14<46:49, 27.27s/it]2021-11-30 13:58:13,394 iteration 5050 : loss : 0.045537, loss_ce: 0.017321
2021-11-30 13:58:14,856 iteration 5051 : loss : 0.044246, loss_ce: 0.018799
2021-11-30 13:58:16,333 iteration 5052 : loss : 0.031233, loss_ce: 0.015345
2021-11-30 13:58:17,793 iteration 5053 : loss : 0.024452, loss_ce: 0.011719
2021-11-30 13:58:19,282 iteration 5054 : loss : 0.028853, loss_ce: 0.011796
2021-11-30 13:58:20,757 iteration 5055 : loss : 0.028622, loss_ce: 0.012443
2021-11-30 13:58:22,227 iteration 5056 : loss : 0.042373, loss_ce: 0.013777
2021-11-30 13:58:23,756 iteration 5057 : loss : 0.036219, loss_ce: 0.018703
2021-11-30 13:58:25,311 iteration 5058 : loss : 0.042288, loss_ce: 0.021329
2021-11-30 13:58:26,682 iteration 5059 : loss : 0.031053, loss_ce: 0.011329
2021-11-30 13:58:28,173 iteration 5060 : loss : 0.048827, loss_ce: 0.015855
2021-11-30 13:58:29,695 iteration 5061 : loss : 0.026638, loss_ce: 0.013187
2021-11-30 13:58:31,207 iteration 5062 : loss : 0.045101, loss_ce: 0.017695
2021-11-30 13:58:32,744 iteration 5063 : loss : 0.034500, loss_ce: 0.015850
2021-11-30 13:58:34,193 iteration 5064 : loss : 0.027660, loss_ce: 0.013066
2021-11-30 13:58:35,656 iteration 5065 : loss : 0.033913, loss_ce: 0.015688
2021-11-30 13:58:37,160 iteration 5066 : loss : 0.022376, loss_ce: 0.013841
 74%|█████████████████████▌       | 298/400 [2:16:39<45:22, 26.69s/it]2021-11-30 13:58:38,801 iteration 5067 : loss : 0.031582, loss_ce: 0.015981
2021-11-30 13:58:40,287 iteration 5068 : loss : 0.043853, loss_ce: 0.019870
2021-11-30 13:58:41,779 iteration 5069 : loss : 0.031844, loss_ce: 0.013907
2021-11-30 13:58:43,256 iteration 5070 : loss : 0.029057, loss_ce: 0.012025
2021-11-30 13:58:44,817 iteration 5071 : loss : 0.035796, loss_ce: 0.015988
2021-11-30 13:58:46,175 iteration 5072 : loss : 0.026878, loss_ce: 0.011690
2021-11-30 13:58:47,658 iteration 5073 : loss : 0.033586, loss_ce: 0.018301
2021-11-30 13:58:49,195 iteration 5074 : loss : 0.034648, loss_ce: 0.013601
2021-11-30 13:58:50,699 iteration 5075 : loss : 0.034335, loss_ce: 0.015498
2021-11-30 13:58:52,246 iteration 5076 : loss : 0.028705, loss_ce: 0.014673
2021-11-30 13:58:53,700 iteration 5077 : loss : 0.030647, loss_ce: 0.013723
2021-11-30 13:58:55,212 iteration 5078 : loss : 0.027424, loss_ce: 0.011128
2021-11-30 13:58:56,658 iteration 5079 : loss : 0.031287, loss_ce: 0.015627
2021-11-30 13:58:58,152 iteration 5080 : loss : 0.037644, loss_ce: 0.016302
2021-11-30 13:58:59,691 iteration 5081 : loss : 0.028789, loss_ce: 0.015241
2021-11-30 13:59:01,254 iteration 5082 : loss : 0.055180, loss_ce: 0.029015
2021-11-30 13:59:02,768 iteration 5083 : loss : 0.041506, loss_ce: 0.017443
 75%|█████████████████████▋       | 299/400 [2:17:05<44:22, 26.37s/it]2021-11-30 13:59:04,281 iteration 5084 : loss : 0.025230, loss_ce: 0.012023
2021-11-30 13:59:05,767 iteration 5085 : loss : 0.037448, loss_ce: 0.015518
2021-11-30 13:59:07,239 iteration 5086 : loss : 0.028829, loss_ce: 0.012953
2021-11-30 13:59:08,792 iteration 5087 : loss : 0.033060, loss_ce: 0.014589
2021-11-30 13:59:10,293 iteration 5088 : loss : 0.031763, loss_ce: 0.013533
2021-11-30 13:59:11,710 iteration 5089 : loss : 0.028712, loss_ce: 0.012618
2021-11-30 13:59:13,172 iteration 5090 : loss : 0.032340, loss_ce: 0.015271
2021-11-30 13:59:14,783 iteration 5091 : loss : 0.043675, loss_ce: 0.019183
2021-11-30 13:59:16,270 iteration 5092 : loss : 0.021476, loss_ce: 0.012851
2021-11-30 13:59:17,731 iteration 5093 : loss : 0.038033, loss_ce: 0.017806
2021-11-30 13:59:19,163 iteration 5094 : loss : 0.025470, loss_ce: 0.011879
2021-11-30 13:59:20,723 iteration 5095 : loss : 0.047483, loss_ce: 0.019630
2021-11-30 13:59:22,172 iteration 5096 : loss : 0.020506, loss_ce: 0.010703
2021-11-30 13:59:23,623 iteration 5097 : loss : 0.027383, loss_ce: 0.014180
2021-11-30 13:59:25,135 iteration 5098 : loss : 0.040221, loss_ce: 0.015620
2021-11-30 13:59:26,611 iteration 5099 : loss : 0.035214, loss_ce: 0.014705
2021-11-30 13:59:26,612 Training Data Eval:
2021-11-30 13:59:33,978   Average segmentation loss on training set: 0.0155
2021-11-30 13:59:33,978 Validation Data Eval:
2021-11-30 13:59:36,545   Average segmentation loss on validation set: 0.0754
2021-11-30 13:59:38,054 iteration 5100 : loss : 0.024331, loss_ce: 0.014391
 75%|█████████████████████▊       | 300/400 [2:17:40<48:24, 29.04s/it]2021-11-30 13:59:39,616 iteration 5101 : loss : 0.032214, loss_ce: 0.016511
2021-11-30 13:59:41,075 iteration 5102 : loss : 0.021451, loss_ce: 0.011174
2021-11-30 13:59:42,569 iteration 5103 : loss : 0.035061, loss_ce: 0.015509
2021-11-30 13:59:44,146 iteration 5104 : loss : 0.027699, loss_ce: 0.013127
2021-11-30 13:59:45,584 iteration 5105 : loss : 0.035332, loss_ce: 0.016002
2021-11-30 13:59:47,153 iteration 5106 : loss : 0.038984, loss_ce: 0.016610
2021-11-30 13:59:48,580 iteration 5107 : loss : 0.026475, loss_ce: 0.012752
2021-11-30 13:59:50,094 iteration 5108 : loss : 0.037998, loss_ce: 0.015425
2021-11-30 13:59:51,587 iteration 5109 : loss : 0.040754, loss_ce: 0.014858
2021-11-30 13:59:53,065 iteration 5110 : loss : 0.036154, loss_ce: 0.017892
2021-11-30 13:59:54,603 iteration 5111 : loss : 0.027861, loss_ce: 0.012217
2021-11-30 13:59:56,056 iteration 5112 : loss : 0.041210, loss_ce: 0.014768
2021-11-30 13:59:57,621 iteration 5113 : loss : 0.070599, loss_ce: 0.016123
2021-11-30 13:59:59,088 iteration 5114 : loss : 0.028311, loss_ce: 0.016293
2021-11-30 14:00:00,529 iteration 5115 : loss : 0.039701, loss_ce: 0.014179
2021-11-30 14:00:01,970 iteration 5116 : loss : 0.028095, loss_ce: 0.013139
2021-11-30 14:00:03,526 iteration 5117 : loss : 0.031557, loss_ce: 0.017385
 75%|█████████████████████▊       | 301/400 [2:18:05<46:09, 27.97s/it]2021-11-30 14:00:05,062 iteration 5118 : loss : 0.023951, loss_ce: 0.011469
2021-11-30 14:00:06,568 iteration 5119 : loss : 0.033684, loss_ce: 0.013978
2021-11-30 14:00:08,020 iteration 5120 : loss : 0.053116, loss_ce: 0.019983
2021-11-30 14:00:09,527 iteration 5121 : loss : 0.031313, loss_ce: 0.017303
2021-11-30 14:00:10,966 iteration 5122 : loss : 0.037665, loss_ce: 0.017162
2021-11-30 14:00:12,441 iteration 5123 : loss : 0.031583, loss_ce: 0.011133
2021-11-30 14:00:13,970 iteration 5124 : loss : 0.033182, loss_ce: 0.016511
2021-11-30 14:00:15,354 iteration 5125 : loss : 0.037095, loss_ce: 0.021162
2021-11-30 14:00:16,793 iteration 5126 : loss : 0.034980, loss_ce: 0.014751
2021-11-30 14:00:18,331 iteration 5127 : loss : 0.042024, loss_ce: 0.016759
2021-11-30 14:00:19,797 iteration 5128 : loss : 0.021905, loss_ce: 0.011831
2021-11-30 14:00:21,328 iteration 5129 : loss : 0.030782, loss_ce: 0.012578
2021-11-30 14:00:22,827 iteration 5130 : loss : 0.028548, loss_ce: 0.011954
2021-11-30 14:00:24,231 iteration 5131 : loss : 0.019877, loss_ce: 0.009725
2021-11-30 14:00:25,717 iteration 5132 : loss : 0.030612, loss_ce: 0.015522
2021-11-30 14:00:27,162 iteration 5133 : loss : 0.039803, loss_ce: 0.014500
2021-11-30 14:00:28,596 iteration 5134 : loss : 0.028940, loss_ce: 0.015179
 76%|█████████████████████▉       | 302/400 [2:18:30<44:16, 27.10s/it]2021-11-30 14:00:30,312 iteration 5135 : loss : 0.047961, loss_ce: 0.020823
2021-11-30 14:00:31,773 iteration 5136 : loss : 0.032030, loss_ce: 0.014115
2021-11-30 14:00:33,314 iteration 5137 : loss : 0.051698, loss_ce: 0.020427
2021-11-30 14:00:34,742 iteration 5138 : loss : 0.026008, loss_ce: 0.014124
2021-11-30 14:00:36,223 iteration 5139 : loss : 0.022858, loss_ce: 0.010745
2021-11-30 14:00:37,818 iteration 5140 : loss : 0.031671, loss_ce: 0.015464
2021-11-30 14:00:39,260 iteration 5141 : loss : 0.037487, loss_ce: 0.015602
2021-11-30 14:00:40,770 iteration 5142 : loss : 0.034036, loss_ce: 0.013724
2021-11-30 14:00:42,174 iteration 5143 : loss : 0.026852, loss_ce: 0.012616
2021-11-30 14:00:43,687 iteration 5144 : loss : 0.040687, loss_ce: 0.023088
2021-11-30 14:00:45,074 iteration 5145 : loss : 0.023289, loss_ce: 0.010502
2021-11-30 14:00:46,640 iteration 5146 : loss : 0.038149, loss_ce: 0.018702
2021-11-30 14:00:48,199 iteration 5147 : loss : 0.033937, loss_ce: 0.015791
2021-11-30 14:00:49,664 iteration 5148 : loss : 0.036283, loss_ce: 0.017434
2021-11-30 14:00:51,129 iteration 5149 : loss : 0.047580, loss_ce: 0.016590
2021-11-30 14:00:52,603 iteration 5150 : loss : 0.019055, loss_ce: 0.010144
2021-11-30 14:00:54,101 iteration 5151 : loss : 0.052539, loss_ce: 0.013964
 76%|█████████████████████▉       | 303/400 [2:18:56<43:02, 26.62s/it]2021-11-30 14:00:55,584 iteration 5152 : loss : 0.062294, loss_ce: 0.012424
2021-11-30 14:00:56,970 iteration 5153 : loss : 0.023749, loss_ce: 0.011646
2021-11-30 14:00:58,544 iteration 5154 : loss : 0.038117, loss_ce: 0.018304
2021-11-30 14:00:59,966 iteration 5155 : loss : 0.020100, loss_ce: 0.009580
2021-11-30 14:01:01,429 iteration 5156 : loss : 0.041087, loss_ce: 0.018713
2021-11-30 14:01:02,839 iteration 5157 : loss : 0.025890, loss_ce: 0.011535
2021-11-30 14:01:04,288 iteration 5158 : loss : 0.027012, loss_ce: 0.012032
2021-11-30 14:01:05,821 iteration 5159 : loss : 0.031535, loss_ce: 0.013500
2021-11-30 14:01:07,246 iteration 5160 : loss : 0.038282, loss_ce: 0.016966
2021-11-30 14:01:08,723 iteration 5161 : loss : 0.042505, loss_ce: 0.018667
2021-11-30 14:01:10,147 iteration 5162 : loss : 0.033441, loss_ce: 0.016879
2021-11-30 14:01:11,655 iteration 5163 : loss : 0.033278, loss_ce: 0.015794
2021-11-30 14:01:13,154 iteration 5164 : loss : 0.041812, loss_ce: 0.021360
2021-11-30 14:01:14,551 iteration 5165 : loss : 0.028051, loss_ce: 0.011993
2021-11-30 14:01:16,033 iteration 5166 : loss : 0.030389, loss_ce: 0.015524
2021-11-30 14:01:17,536 iteration 5167 : loss : 0.044936, loss_ce: 0.019481
2021-11-30 14:01:19,088 iteration 5168 : loss : 0.045156, loss_ce: 0.020888
 76%|██████████████████████       | 304/400 [2:19:21<41:48, 26.13s/it]2021-11-30 14:01:20,751 iteration 5169 : loss : 0.042393, loss_ce: 0.019351
2021-11-30 14:01:22,176 iteration 5170 : loss : 0.027963, loss_ce: 0.011778
2021-11-30 14:01:23,716 iteration 5171 : loss : 0.030517, loss_ce: 0.015322
2021-11-30 14:01:25,122 iteration 5172 : loss : 0.030927, loss_ce: 0.013233
2021-11-30 14:01:26,466 iteration 5173 : loss : 0.025596, loss_ce: 0.013454
2021-11-30 14:01:28,023 iteration 5174 : loss : 0.035084, loss_ce: 0.016080
2021-11-30 14:01:29,473 iteration 5175 : loss : 0.037394, loss_ce: 0.019110
2021-11-30 14:01:31,085 iteration 5176 : loss : 0.047794, loss_ce: 0.020152
2021-11-30 14:01:32,513 iteration 5177 : loss : 0.041524, loss_ce: 0.018450
2021-11-30 14:01:33,987 iteration 5178 : loss : 0.023489, loss_ce: 0.010239
2021-11-30 14:01:35,518 iteration 5179 : loss : 0.026468, loss_ce: 0.011629
2021-11-30 14:01:36,995 iteration 5180 : loss : 0.032634, loss_ce: 0.013642
2021-11-30 14:01:38,419 iteration 5181 : loss : 0.037076, loss_ce: 0.012222
2021-11-30 14:01:39,900 iteration 5182 : loss : 0.025722, loss_ce: 0.012230
2021-11-30 14:01:41,350 iteration 5183 : loss : 0.028091, loss_ce: 0.014705
2021-11-30 14:01:42,778 iteration 5184 : loss : 0.047999, loss_ce: 0.017354
2021-11-30 14:01:42,778 Training Data Eval:
2021-11-30 14:01:50,174   Average segmentation loss on training set: 0.0171
2021-11-30 14:01:50,174 Validation Data Eval:
2021-11-30 14:01:52,728   Average segmentation loss on validation set: 0.1036
2021-11-30 14:01:54,210 iteration 5185 : loss : 0.036976, loss_ce: 0.018086
 76%|██████████████████████       | 305/400 [2:19:56<45:38, 28.83s/it]2021-11-30 14:01:55,770 iteration 5186 : loss : 0.032739, loss_ce: 0.017616
2021-11-30 14:01:57,208 iteration 5187 : loss : 0.026655, loss_ce: 0.013236
2021-11-30 14:01:58,663 iteration 5188 : loss : 0.025092, loss_ce: 0.012601
2021-11-30 14:02:00,122 iteration 5189 : loss : 0.026245, loss_ce: 0.013863
2021-11-30 14:02:01,557 iteration 5190 : loss : 0.028468, loss_ce: 0.013995
2021-11-30 14:02:03,116 iteration 5191 : loss : 0.046259, loss_ce: 0.016961
2021-11-30 14:02:04,644 iteration 5192 : loss : 0.038351, loss_ce: 0.016084
2021-11-30 14:02:06,066 iteration 5193 : loss : 0.023078, loss_ce: 0.010836
2021-11-30 14:02:07,593 iteration 5194 : loss : 0.038823, loss_ce: 0.018288
2021-11-30 14:02:09,039 iteration 5195 : loss : 0.027342, loss_ce: 0.012514
2021-11-30 14:02:10,416 iteration 5196 : loss : 0.025511, loss_ce: 0.010158
2021-11-30 14:02:11,833 iteration 5197 : loss : 0.025071, loss_ce: 0.011386
2021-11-30 14:02:13,278 iteration 5198 : loss : 0.025323, loss_ce: 0.014606
2021-11-30 14:02:14,776 iteration 5199 : loss : 0.022435, loss_ce: 0.010850
2021-11-30 14:02:16,242 iteration 5200 : loss : 0.035179, loss_ce: 0.013037
2021-11-30 14:02:17,741 iteration 5201 : loss : 0.032297, loss_ce: 0.013961
2021-11-30 14:02:19,314 iteration 5202 : loss : 0.042353, loss_ce: 0.021201
 76%|██████████████████████▏      | 306/400 [2:20:21<43:25, 27.71s/it]2021-11-30 14:02:20,804 iteration 5203 : loss : 0.025971, loss_ce: 0.012489
2021-11-30 14:02:22,256 iteration 5204 : loss : 0.029717, loss_ce: 0.014914
2021-11-30 14:02:23,858 iteration 5205 : loss : 0.037256, loss_ce: 0.019510
2021-11-30 14:02:25,314 iteration 5206 : loss : 0.027476, loss_ce: 0.012219
2021-11-30 14:02:26,797 iteration 5207 : loss : 0.032653, loss_ce: 0.017630
2021-11-30 14:02:28,352 iteration 5208 : loss : 0.036747, loss_ce: 0.016451
2021-11-30 14:02:29,767 iteration 5209 : loss : 0.027868, loss_ce: 0.014766
2021-11-30 14:02:31,305 iteration 5210 : loss : 0.030110, loss_ce: 0.014183
2021-11-30 14:02:32,796 iteration 5211 : loss : 0.027855, loss_ce: 0.013911
2021-11-30 14:02:34,264 iteration 5212 : loss : 0.036273, loss_ce: 0.014358
2021-11-30 14:02:35,636 iteration 5213 : loss : 0.020820, loss_ce: 0.010600
2021-11-30 14:02:37,214 iteration 5214 : loss : 0.039631, loss_ce: 0.020863
2021-11-30 14:02:38,732 iteration 5215 : loss : 0.031326, loss_ce: 0.013841
2021-11-30 14:02:40,280 iteration 5216 : loss : 0.034137, loss_ce: 0.014236
2021-11-30 14:02:41,857 iteration 5217 : loss : 0.030370, loss_ce: 0.014136
2021-11-30 14:02:43,296 iteration 5218 : loss : 0.039805, loss_ce: 0.011326
2021-11-30 14:02:44,892 iteration 5219 : loss : 0.042827, loss_ce: 0.015680
 77%|██████████████████████▎      | 307/400 [2:20:47<41:57, 27.07s/it]2021-11-30 14:02:46,418 iteration 5220 : loss : 0.039839, loss_ce: 0.015340
2021-11-30 14:02:47,892 iteration 5221 : loss : 0.035886, loss_ce: 0.014796
2021-11-30 14:02:49,316 iteration 5222 : loss : 0.024937, loss_ce: 0.011823
2021-11-30 14:02:50,838 iteration 5223 : loss : 0.036988, loss_ce: 0.016627
2021-11-30 14:02:52,308 iteration 5224 : loss : 0.030307, loss_ce: 0.013159
2021-11-30 14:02:53,853 iteration 5225 : loss : 0.029142, loss_ce: 0.012124
2021-11-30 14:02:55,304 iteration 5226 : loss : 0.027402, loss_ce: 0.012945
2021-11-30 14:02:56,886 iteration 5227 : loss : 0.043778, loss_ce: 0.021825
2021-11-30 14:02:58,314 iteration 5228 : loss : 0.026298, loss_ce: 0.012033
2021-11-30 14:02:59,925 iteration 5229 : loss : 0.035566, loss_ce: 0.016233
2021-11-30 14:03:01,285 iteration 5230 : loss : 0.024583, loss_ce: 0.012599
2021-11-30 14:03:02,787 iteration 5231 : loss : 0.032144, loss_ce: 0.015432
2021-11-30 14:03:04,294 iteration 5232 : loss : 0.028966, loss_ce: 0.011226
2021-11-30 14:03:05,828 iteration 5233 : loss : 0.028188, loss_ce: 0.013587
2021-11-30 14:03:07,304 iteration 5234 : loss : 0.028558, loss_ce: 0.014079
2021-11-30 14:03:08,806 iteration 5235 : loss : 0.029658, loss_ce: 0.013104
2021-11-30 14:03:10,337 iteration 5236 : loss : 0.038252, loss_ce: 0.015845
 77%|██████████████████████▎      | 308/400 [2:21:12<40:45, 26.58s/it]2021-11-30 14:03:11,719 iteration 5237 : loss : 0.019714, loss_ce: 0.010543
2021-11-30 14:03:13,252 iteration 5238 : loss : 0.032831, loss_ce: 0.015891
2021-11-30 14:03:14,769 iteration 5239 : loss : 0.038312, loss_ce: 0.014245
2021-11-30 14:03:16,303 iteration 5240 : loss : 0.183030, loss_ce: 0.013503
2021-11-30 14:03:17,753 iteration 5241 : loss : 0.023941, loss_ce: 0.012388
2021-11-30 14:03:19,227 iteration 5242 : loss : 0.042539, loss_ce: 0.019535
2021-11-30 14:03:20,792 iteration 5243 : loss : 0.040250, loss_ce: 0.015498
2021-11-30 14:03:22,320 iteration 5244 : loss : 0.024809, loss_ce: 0.012402
2021-11-30 14:03:23,827 iteration 5245 : loss : 0.042492, loss_ce: 0.017691
2021-11-30 14:03:25,336 iteration 5246 : loss : 0.034566, loss_ce: 0.012427
2021-11-30 14:03:26,744 iteration 5247 : loss : 0.024324, loss_ce: 0.011790
2021-11-30 14:03:28,275 iteration 5248 : loss : 0.024006, loss_ce: 0.011541
2021-11-30 14:03:29,852 iteration 5249 : loss : 0.036389, loss_ce: 0.014746
2021-11-30 14:03:31,366 iteration 5250 : loss : 0.036498, loss_ce: 0.018901
2021-11-30 14:03:32,768 iteration 5251 : loss : 0.022414, loss_ce: 0.010079
2021-11-30 14:03:34,272 iteration 5252 : loss : 0.029248, loss_ce: 0.014213
2021-11-30 14:03:35,798 iteration 5253 : loss : 0.055803, loss_ce: 0.015128
 77%|██████████████████████▍      | 309/400 [2:21:38<39:48, 26.25s/it]2021-11-30 14:03:37,340 iteration 5254 : loss : 0.022742, loss_ce: 0.010495
2021-11-30 14:03:38,822 iteration 5255 : loss : 0.044150, loss_ce: 0.024759
2021-11-30 14:03:40,247 iteration 5256 : loss : 0.027808, loss_ce: 0.014167
2021-11-30 14:03:41,731 iteration 5257 : loss : 0.028712, loss_ce: 0.013942
2021-11-30 14:03:43,269 iteration 5258 : loss : 0.026559, loss_ce: 0.009995
2021-11-30 14:03:44,713 iteration 5259 : loss : 0.042930, loss_ce: 0.013240
2021-11-30 14:03:46,225 iteration 5260 : loss : 0.022953, loss_ce: 0.011221
2021-11-30 14:03:47,728 iteration 5261 : loss : 0.037351, loss_ce: 0.019508
2021-11-30 14:03:49,203 iteration 5262 : loss : 0.023265, loss_ce: 0.012631
2021-11-30 14:03:50,691 iteration 5263 : loss : 0.035301, loss_ce: 0.018693
2021-11-30 14:03:52,124 iteration 5264 : loss : 0.023206, loss_ce: 0.011762
2021-11-30 14:03:53,662 iteration 5265 : loss : 0.032624, loss_ce: 0.014062
2021-11-30 14:03:55,240 iteration 5266 : loss : 0.031285, loss_ce: 0.013662
2021-11-30 14:03:56,721 iteration 5267 : loss : 0.049565, loss_ce: 0.019921
2021-11-30 14:03:58,216 iteration 5268 : loss : 0.035997, loss_ce: 0.013149
2021-11-30 14:03:59,817 iteration 5269 : loss : 0.047495, loss_ce: 0.018133
2021-11-30 14:03:59,817 Training Data Eval:
2021-11-30 14:04:07,170   Average segmentation loss on training set: 0.0148
2021-11-30 14:04:07,170 Validation Data Eval:
2021-11-30 14:04:09,720   Average segmentation loss on validation set: 0.0910
2021-11-30 14:04:11,225 iteration 5270 : loss : 0.030176, loss_ce: 0.015175
 78%|██████████████████████▍      | 310/400 [2:22:13<43:29, 29.00s/it]2021-11-30 14:04:12,740 iteration 5271 : loss : 0.037900, loss_ce: 0.019426
2021-11-30 14:04:14,269 iteration 5272 : loss : 0.040162, loss_ce: 0.020858
2021-11-30 14:04:15,724 iteration 5273 : loss : 0.030281, loss_ce: 0.012034
2021-11-30 14:04:17,188 iteration 5274 : loss : 0.033475, loss_ce: 0.015064
2021-11-30 14:04:18,587 iteration 5275 : loss : 0.022982, loss_ce: 0.012692
2021-11-30 14:04:20,109 iteration 5276 : loss : 0.033960, loss_ce: 0.015821
2021-11-30 14:04:21,601 iteration 5277 : loss : 0.027416, loss_ce: 0.012355
2021-11-30 14:04:22,977 iteration 5278 : loss : 0.021589, loss_ce: 0.011185
2021-11-30 14:04:24,478 iteration 5279 : loss : 0.025537, loss_ce: 0.013246
2021-11-30 14:04:26,114 iteration 5280 : loss : 0.035628, loss_ce: 0.016117
2021-11-30 14:04:27,584 iteration 5281 : loss : 0.026932, loss_ce: 0.012363
2021-11-30 14:04:29,010 iteration 5282 : loss : 0.025082, loss_ce: 0.010362
2021-11-30 14:04:30,493 iteration 5283 : loss : 0.036158, loss_ce: 0.013890
2021-11-30 14:04:31,988 iteration 5284 : loss : 0.046271, loss_ce: 0.016921
2021-11-30 14:04:33,458 iteration 5285 : loss : 0.028826, loss_ce: 0.014868
2021-11-30 14:04:34,840 iteration 5286 : loss : 0.022453, loss_ce: 0.012301
2021-11-30 14:04:36,313 iteration 5287 : loss : 0.041026, loss_ce: 0.013772
 78%|██████████████████████▌      | 311/400 [2:22:38<41:16, 27.82s/it]2021-11-30 14:04:37,895 iteration 5288 : loss : 0.028762, loss_ce: 0.013839
2021-11-30 14:04:39,414 iteration 5289 : loss : 0.033541, loss_ce: 0.014858
2021-11-30 14:04:40,873 iteration 5290 : loss : 0.046588, loss_ce: 0.012739
2021-11-30 14:04:42,314 iteration 5291 : loss : 0.053692, loss_ce: 0.026213
2021-11-30 14:04:43,849 iteration 5292 : loss : 0.028474, loss_ce: 0.013674
2021-11-30 14:04:45,351 iteration 5293 : loss : 0.037303, loss_ce: 0.018580
2021-11-30 14:04:46,883 iteration 5294 : loss : 0.038384, loss_ce: 0.017184
2021-11-30 14:04:48,472 iteration 5295 : loss : 0.042348, loss_ce: 0.019616
2021-11-30 14:04:50,004 iteration 5296 : loss : 0.034870, loss_ce: 0.013017
2021-11-30 14:04:51,546 iteration 5297 : loss : 0.025992, loss_ce: 0.011178
2021-11-30 14:04:53,086 iteration 5298 : loss : 0.043037, loss_ce: 0.018960
2021-11-30 14:04:54,629 iteration 5299 : loss : 0.025682, loss_ce: 0.012424
2021-11-30 14:04:56,182 iteration 5300 : loss : 0.035265, loss_ce: 0.014212
2021-11-30 14:04:57,593 iteration 5301 : loss : 0.030523, loss_ce: 0.014447
2021-11-30 14:04:59,014 iteration 5302 : loss : 0.027068, loss_ce: 0.016441
2021-11-30 14:05:00,457 iteration 5303 : loss : 0.025655, loss_ce: 0.012085
2021-11-30 14:05:01,871 iteration 5304 : loss : 0.025645, loss_ce: 0.012402
 78%|██████████████████████▌      | 312/400 [2:23:04<39:48, 27.15s/it]2021-11-30 14:05:03,430 iteration 5305 : loss : 0.045010, loss_ce: 0.017089
2021-11-30 14:05:05,023 iteration 5306 : loss : 0.040657, loss_ce: 0.019146
2021-11-30 14:05:06,639 iteration 5307 : loss : 0.037491, loss_ce: 0.015740
2021-11-30 14:05:08,192 iteration 5308 : loss : 0.058809, loss_ce: 0.019705
2021-11-30 14:05:09,733 iteration 5309 : loss : 0.039116, loss_ce: 0.015491
2021-11-30 14:05:11,181 iteration 5310 : loss : 0.027574, loss_ce: 0.015016
2021-11-30 14:05:12,626 iteration 5311 : loss : 0.035181, loss_ce: 0.013969
2021-11-30 14:05:14,183 iteration 5312 : loss : 0.034007, loss_ce: 0.017273
2021-11-30 14:05:15,780 iteration 5313 : loss : 0.052989, loss_ce: 0.014067
2021-11-30 14:05:17,271 iteration 5314 : loss : 0.026676, loss_ce: 0.012671
2021-11-30 14:05:18,832 iteration 5315 : loss : 0.043364, loss_ce: 0.013215
2021-11-30 14:05:20,387 iteration 5316 : loss : 0.033850, loss_ce: 0.014311
2021-11-30 14:05:21,881 iteration 5317 : loss : 0.033321, loss_ce: 0.015455
2021-11-30 14:05:23,325 iteration 5318 : loss : 0.032062, loss_ce: 0.014571
2021-11-30 14:05:24,864 iteration 5319 : loss : 0.036655, loss_ce: 0.017791
2021-11-30 14:05:26,389 iteration 5320 : loss : 0.036761, loss_ce: 0.017876
2021-11-30 14:05:27,962 iteration 5321 : loss : 0.048124, loss_ce: 0.021343
 78%|██████████████████████▋      | 313/400 [2:23:30<38:54, 26.83s/it]2021-11-30 14:05:29,409 iteration 5322 : loss : 0.026392, loss_ce: 0.014312
2021-11-30 14:05:30,926 iteration 5323 : loss : 0.032932, loss_ce: 0.015231
2021-11-30 14:05:32,341 iteration 5324 : loss : 0.029083, loss_ce: 0.012578
2021-11-30 14:05:33,792 iteration 5325 : loss : 0.049408, loss_ce: 0.013628
2021-11-30 14:05:35,343 iteration 5326 : loss : 0.033901, loss_ce: 0.014950
2021-11-30 14:05:36,785 iteration 5327 : loss : 0.031701, loss_ce: 0.013321
2021-11-30 14:05:38,252 iteration 5328 : loss : 0.050455, loss_ce: 0.028506
2021-11-30 14:05:39,811 iteration 5329 : loss : 0.038223, loss_ce: 0.015592
2021-11-30 14:05:41,312 iteration 5330 : loss : 0.036416, loss_ce: 0.015876
2021-11-30 14:05:42,787 iteration 5331 : loss : 0.045262, loss_ce: 0.017881
2021-11-30 14:05:44,292 iteration 5332 : loss : 0.046399, loss_ce: 0.017968
2021-11-30 14:05:45,819 iteration 5333 : loss : 0.035349, loss_ce: 0.019256
2021-11-30 14:05:47,324 iteration 5334 : loss : 0.047543, loss_ce: 0.019399
2021-11-30 14:05:48,860 iteration 5335 : loss : 0.027851, loss_ce: 0.010812
2021-11-30 14:05:50,354 iteration 5336 : loss : 0.032521, loss_ce: 0.015511
2021-11-30 14:05:51,801 iteration 5337 : loss : 0.029043, loss_ce: 0.013112
2021-11-30 14:05:53,257 iteration 5338 : loss : 0.027561, loss_ce: 0.015319
 78%|██████████████████████▊      | 314/400 [2:23:55<37:47, 26.37s/it]2021-11-30 14:05:54,970 iteration 5339 : loss : 0.043630, loss_ce: 0.020307
2021-11-30 14:05:56,504 iteration 5340 : loss : 0.063546, loss_ce: 0.037834
2021-11-30 14:05:58,014 iteration 5341 : loss : 0.046465, loss_ce: 0.012694
2021-11-30 14:05:59,526 iteration 5342 : loss : 0.039326, loss_ce: 0.022290
2021-11-30 14:06:00,994 iteration 5343 : loss : 0.028215, loss_ce: 0.012257
2021-11-30 14:06:02,528 iteration 5344 : loss : 0.036712, loss_ce: 0.014737
2021-11-30 14:06:03,999 iteration 5345 : loss : 0.029092, loss_ce: 0.012826
2021-11-30 14:06:05,539 iteration 5346 : loss : 0.046627, loss_ce: 0.020138
2021-11-30 14:06:07,068 iteration 5347 : loss : 0.033901, loss_ce: 0.013515
2021-11-30 14:06:08,606 iteration 5348 : loss : 0.028868, loss_ce: 0.010870
2021-11-30 14:06:10,100 iteration 5349 : loss : 0.035348, loss_ce: 0.015172
2021-11-30 14:06:11,509 iteration 5350 : loss : 0.022323, loss_ce: 0.011300
2021-11-30 14:06:12,924 iteration 5351 : loss : 0.046634, loss_ce: 0.017727
2021-11-30 14:06:14,466 iteration 5352 : loss : 0.037152, loss_ce: 0.019658
2021-11-30 14:06:16,013 iteration 5353 : loss : 0.022056, loss_ce: 0.010427
2021-11-30 14:06:17,471 iteration 5354 : loss : 0.032132, loss_ce: 0.017108
2021-11-30 14:06:17,471 Training Data Eval:
2021-11-30 14:06:24,838   Average segmentation loss on training set: 0.0159
2021-11-30 14:06:24,838 Validation Data Eval:
2021-11-30 14:06:27,414   Average segmentation loss on validation set: 0.0841
2021-11-30 14:06:28,849 iteration 5355 : loss : 0.030959, loss_ce: 0.014633
 79%|██████████████████████▊      | 315/400 [2:24:31<41:17, 29.14s/it]2021-11-30 14:06:30,419 iteration 5356 : loss : 0.038240, loss_ce: 0.019763
2021-11-30 14:06:31,840 iteration 5357 : loss : 0.036932, loss_ce: 0.012389
2021-11-30 14:06:33,488 iteration 5358 : loss : 0.045523, loss_ce: 0.020734
2021-11-30 14:06:34,882 iteration 5359 : loss : 0.027805, loss_ce: 0.013098
2021-11-30 14:06:36,392 iteration 5360 : loss : 0.034039, loss_ce: 0.017040
2021-11-30 14:06:37,896 iteration 5361 : loss : 0.037638, loss_ce: 0.014716
2021-11-30 14:06:39,418 iteration 5362 : loss : 0.032487, loss_ce: 0.011831
2021-11-30 14:06:40,921 iteration 5363 : loss : 0.029759, loss_ce: 0.014333
2021-11-30 14:06:42,419 iteration 5364 : loss : 0.051998, loss_ce: 0.014913
2021-11-30 14:06:43,894 iteration 5365 : loss : 0.023317, loss_ce: 0.011558
2021-11-30 14:06:45,413 iteration 5366 : loss : 0.031076, loss_ce: 0.014485
2021-11-30 14:06:46,970 iteration 5367 : loss : 0.032645, loss_ce: 0.015752
2021-11-30 14:06:48,444 iteration 5368 : loss : 0.026174, loss_ce: 0.013852
2021-11-30 14:06:49,969 iteration 5369 : loss : 0.022043, loss_ce: 0.010403
2021-11-30 14:06:51,449 iteration 5370 : loss : 0.019076, loss_ce: 0.010172
2021-11-30 14:06:52,954 iteration 5371 : loss : 0.028915, loss_ce: 0.015512
2021-11-30 14:06:54,431 iteration 5372 : loss : 0.029914, loss_ce: 0.012519
 79%|██████████████████████▉      | 316/400 [2:24:56<39:18, 28.07s/it]2021-11-30 14:06:56,001 iteration 5373 : loss : 0.028927, loss_ce: 0.015237
2021-11-30 14:06:57,444 iteration 5374 : loss : 0.036916, loss_ce: 0.016780
2021-11-30 14:06:58,972 iteration 5375 : loss : 0.021563, loss_ce: 0.009849
2021-11-30 14:07:00,429 iteration 5376 : loss : 0.037239, loss_ce: 0.017783
2021-11-30 14:07:01,872 iteration 5377 : loss : 0.022422, loss_ce: 0.009417
2021-11-30 14:07:03,369 iteration 5378 : loss : 0.027383, loss_ce: 0.012411
2021-11-30 14:07:04,817 iteration 5379 : loss : 0.027634, loss_ce: 0.012821
2021-11-30 14:07:06,341 iteration 5380 : loss : 0.029962, loss_ce: 0.014298
2021-11-30 14:07:07,822 iteration 5381 : loss : 0.037898, loss_ce: 0.017546
2021-11-30 14:07:09,345 iteration 5382 : loss : 0.026029, loss_ce: 0.011148
2021-11-30 14:07:10,940 iteration 5383 : loss : 0.039588, loss_ce: 0.019253
2021-11-30 14:07:12,607 iteration 5384 : loss : 0.048611, loss_ce: 0.023067
2021-11-30 14:07:14,048 iteration 5385 : loss : 0.029511, loss_ce: 0.013690
2021-11-30 14:07:15,456 iteration 5386 : loss : 0.021802, loss_ce: 0.010497
2021-11-30 14:07:16,926 iteration 5387 : loss : 0.031564, loss_ce: 0.014869
2021-11-30 14:07:18,471 iteration 5388 : loss : 0.028602, loss_ce: 0.011435
2021-11-30 14:07:20,121 iteration 5389 : loss : 0.047477, loss_ce: 0.016137
 79%|██████████████████████▉      | 317/400 [2:25:22<37:50, 27.36s/it]2021-11-30 14:07:21,642 iteration 5390 : loss : 0.047641, loss_ce: 0.015093
2021-11-30 14:07:23,173 iteration 5391 : loss : 0.034466, loss_ce: 0.014072
2021-11-30 14:07:24,700 iteration 5392 : loss : 0.038788, loss_ce: 0.023636
2021-11-30 14:07:26,174 iteration 5393 : loss : 0.043371, loss_ce: 0.014786
2021-11-30 14:07:27,623 iteration 5394 : loss : 0.031438, loss_ce: 0.018065
2021-11-30 14:07:29,154 iteration 5395 : loss : 0.033142, loss_ce: 0.012630
2021-11-30 14:07:30,594 iteration 5396 : loss : 0.034055, loss_ce: 0.018310
2021-11-30 14:07:32,146 iteration 5397 : loss : 0.039092, loss_ce: 0.014552
2021-11-30 14:07:33,649 iteration 5398 : loss : 0.031666, loss_ce: 0.014367
2021-11-30 14:07:35,074 iteration 5399 : loss : 0.032637, loss_ce: 0.012102
2021-11-30 14:07:36,668 iteration 5400 : loss : 0.046561, loss_ce: 0.018484
2021-11-30 14:07:38,200 iteration 5401 : loss : 0.033925, loss_ce: 0.017827
2021-11-30 14:07:39,704 iteration 5402 : loss : 0.038152, loss_ce: 0.015238
2021-11-30 14:07:41,172 iteration 5403 : loss : 0.030967, loss_ce: 0.014609
2021-11-30 14:07:42,607 iteration 5404 : loss : 0.028486, loss_ce: 0.011438
2021-11-30 14:07:44,082 iteration 5405 : loss : 0.031898, loss_ce: 0.015917
2021-11-30 14:07:45,640 iteration 5406 : loss : 0.027697, loss_ce: 0.011127
 80%|███████████████████████      | 318/400 [2:25:47<36:37, 26.80s/it]2021-11-30 14:07:47,228 iteration 5407 : loss : 0.034384, loss_ce: 0.015118
2021-11-30 14:07:48,707 iteration 5408 : loss : 0.033649, loss_ce: 0.013338
2021-11-30 14:07:50,289 iteration 5409 : loss : 0.034178, loss_ce: 0.017123
2021-11-30 14:07:51,741 iteration 5410 : loss : 0.038378, loss_ce: 0.021673
2021-11-30 14:07:53,227 iteration 5411 : loss : 0.027505, loss_ce: 0.013267
2021-11-30 14:07:54,736 iteration 5412 : loss : 0.026054, loss_ce: 0.013484
2021-11-30 14:07:56,200 iteration 5413 : loss : 0.038635, loss_ce: 0.019746
2021-11-30 14:07:57,711 iteration 5414 : loss : 0.030575, loss_ce: 0.014101
2021-11-30 14:07:59,292 iteration 5415 : loss : 0.045938, loss_ce: 0.020532
2021-11-30 14:08:00,724 iteration 5416 : loss : 0.020497, loss_ce: 0.009691
2021-11-30 14:08:02,253 iteration 5417 : loss : 0.030541, loss_ce: 0.012299
2021-11-30 14:08:03,708 iteration 5418 : loss : 0.046154, loss_ce: 0.020604
2021-11-30 14:08:05,254 iteration 5419 : loss : 0.035686, loss_ce: 0.012530
2021-11-30 14:08:06,810 iteration 5420 : loss : 0.028666, loss_ce: 0.013144
2021-11-30 14:08:08,335 iteration 5421 : loss : 0.032809, loss_ce: 0.016718
2021-11-30 14:08:09,701 iteration 5422 : loss : 0.021442, loss_ce: 0.012475
2021-11-30 14:08:11,239 iteration 5423 : loss : 0.050501, loss_ce: 0.014915
 80%|███████████████████████▏     | 319/400 [2:26:13<35:41, 26.44s/it]2021-11-30 14:08:12,833 iteration 5424 : loss : 0.032599, loss_ce: 0.013980
2021-11-30 14:08:14,286 iteration 5425 : loss : 0.027768, loss_ce: 0.012102
2021-11-30 14:08:15,760 iteration 5426 : loss : 0.035826, loss_ce: 0.013292
2021-11-30 14:08:17,156 iteration 5427 : loss : 0.032122, loss_ce: 0.014999
2021-11-30 14:08:18,624 iteration 5428 : loss : 0.026361, loss_ce: 0.012828
2021-11-30 14:08:20,133 iteration 5429 : loss : 0.026978, loss_ce: 0.012221
2021-11-30 14:08:21,617 iteration 5430 : loss : 0.029638, loss_ce: 0.015055
2021-11-30 14:08:23,045 iteration 5431 : loss : 0.026955, loss_ce: 0.011811
2021-11-30 14:08:24,625 iteration 5432 : loss : 0.027128, loss_ce: 0.014800
2021-11-30 14:08:26,203 iteration 5433 : loss : 0.031994, loss_ce: 0.015788
2021-11-30 14:08:27,636 iteration 5434 : loss : 0.034234, loss_ce: 0.016679
2021-11-30 14:08:29,212 iteration 5435 : loss : 0.035422, loss_ce: 0.017244
2021-11-30 14:08:30,718 iteration 5436 : loss : 0.023235, loss_ce: 0.011137
2021-11-30 14:08:32,264 iteration 5437 : loss : 0.059906, loss_ce: 0.016670
2021-11-30 14:08:33,801 iteration 5438 : loss : 0.036521, loss_ce: 0.016824
2021-11-30 14:08:35,259 iteration 5439 : loss : 0.032585, loss_ce: 0.015223
2021-11-30 14:08:35,259 Training Data Eval:
2021-11-30 14:08:42,650   Average segmentation loss on training set: 0.0144
2021-11-30 14:08:42,650 Validation Data Eval:
2021-11-30 14:08:45,204   Average segmentation loss on validation set: 0.0926
2021-11-30 14:08:46,638 iteration 5440 : loss : 0.026485, loss_ce: 0.010936
 80%|███████████████████████▏     | 320/400 [2:26:48<38:50, 29.13s/it]2021-11-30 14:08:48,169 iteration 5441 : loss : 0.026322, loss_ce: 0.011133
2021-11-30 14:08:49,612 iteration 5442 : loss : 0.028000, loss_ce: 0.012500
2021-11-30 14:08:51,079 iteration 5443 : loss : 0.028366, loss_ce: 0.012492
2021-11-30 14:08:52,484 iteration 5444 : loss : 0.027073, loss_ce: 0.010715
2021-11-30 14:08:54,007 iteration 5445 : loss : 0.038930, loss_ce: 0.017755
2021-11-30 14:08:55,456 iteration 5446 : loss : 0.026928, loss_ce: 0.014909
2021-11-30 14:08:57,046 iteration 5447 : loss : 0.041820, loss_ce: 0.016513
2021-11-30 14:08:58,474 iteration 5448 : loss : 0.030568, loss_ce: 0.016248
2021-11-30 14:08:59,966 iteration 5449 : loss : 0.051131, loss_ce: 0.029729
2021-11-30 14:09:01,450 iteration 5450 : loss : 0.030726, loss_ce: 0.014051
2021-11-30 14:09:02,875 iteration 5451 : loss : 0.030300, loss_ce: 0.012427
2021-11-30 14:09:04,370 iteration 5452 : loss : 0.043482, loss_ce: 0.018169
2021-11-30 14:09:05,786 iteration 5453 : loss : 0.025789, loss_ce: 0.013376
2021-11-30 14:09:07,330 iteration 5454 : loss : 0.048222, loss_ce: 0.028186
2021-11-30 14:09:08,872 iteration 5455 : loss : 0.036402, loss_ce: 0.012981
2021-11-30 14:09:10,378 iteration 5456 : loss : 0.026720, loss_ce: 0.009168
2021-11-30 14:09:11,825 iteration 5457 : loss : 0.025141, loss_ce: 0.010731
 80%|███████████████████████▎     | 321/400 [2:27:14<36:48, 27.95s/it]2021-11-30 14:09:13,359 iteration 5458 : loss : 0.032806, loss_ce: 0.013545
2021-11-30 14:09:14,791 iteration 5459 : loss : 0.024508, loss_ce: 0.012952
2021-11-30 14:09:16,296 iteration 5460 : loss : 0.025380, loss_ce: 0.012323
2021-11-30 14:09:17,781 iteration 5461 : loss : 0.034677, loss_ce: 0.014504
2021-11-30 14:09:19,240 iteration 5462 : loss : 0.031232, loss_ce: 0.013652
2021-11-30 14:09:20,746 iteration 5463 : loss : 0.036248, loss_ce: 0.014327
2021-11-30 14:09:22,241 iteration 5464 : loss : 0.039851, loss_ce: 0.019512
2021-11-30 14:09:23,807 iteration 5465 : loss : 0.043847, loss_ce: 0.018811
2021-11-30 14:09:25,357 iteration 5466 : loss : 0.032480, loss_ce: 0.013712
2021-11-30 14:09:26,873 iteration 5467 : loss : 0.038976, loss_ce: 0.019354
2021-11-30 14:09:28,371 iteration 5468 : loss : 0.026816, loss_ce: 0.013329
2021-11-30 14:09:29,784 iteration 5469 : loss : 0.031844, loss_ce: 0.013671
2021-11-30 14:09:31,260 iteration 5470 : loss : 0.016952, loss_ce: 0.009143
2021-11-30 14:09:32,735 iteration 5471 : loss : 0.033754, loss_ce: 0.013123
2021-11-30 14:09:34,245 iteration 5472 : loss : 0.040057, loss_ce: 0.016081
2021-11-30 14:09:35,648 iteration 5473 : loss : 0.023228, loss_ce: 0.010935
2021-11-30 14:09:37,068 iteration 5474 : loss : 0.036394, loss_ce: 0.013848
 80%|███████████████████████▎     | 322/400 [2:27:39<35:16, 27.14s/it]2021-11-30 14:09:38,636 iteration 5475 : loss : 0.039293, loss_ce: 0.015163
2021-11-30 14:09:40,179 iteration 5476 : loss : 0.028803, loss_ce: 0.014568
2021-11-30 14:09:41,666 iteration 5477 : loss : 0.097650, loss_ce: 0.022090
2021-11-30 14:09:43,064 iteration 5478 : loss : 0.024646, loss_ce: 0.010548
2021-11-30 14:09:44,549 iteration 5479 : loss : 0.026452, loss_ce: 0.013678
2021-11-30 14:09:46,029 iteration 5480 : loss : 0.030643, loss_ce: 0.012522
2021-11-30 14:09:47,477 iteration 5481 : loss : 0.032430, loss_ce: 0.014487
2021-11-30 14:09:49,071 iteration 5482 : loss : 0.045971, loss_ce: 0.014831
2021-11-30 14:09:50,449 iteration 5483 : loss : 0.026369, loss_ce: 0.012830
2021-11-30 14:09:51,948 iteration 5484 : loss : 0.038726, loss_ce: 0.016169
2021-11-30 14:09:53,436 iteration 5485 : loss : 0.031216, loss_ce: 0.015628
2021-11-30 14:09:54,914 iteration 5486 : loss : 0.050542, loss_ce: 0.028505
2021-11-30 14:09:56,408 iteration 5487 : loss : 0.036919, loss_ce: 0.016565
2021-11-30 14:09:57,856 iteration 5488 : loss : 0.026077, loss_ce: 0.014399
2021-11-30 14:09:59,296 iteration 5489 : loss : 0.032301, loss_ce: 0.018774
2021-11-30 14:10:00,867 iteration 5490 : loss : 0.032464, loss_ce: 0.014659
2021-11-30 14:10:02,306 iteration 5491 : loss : 0.021965, loss_ce: 0.009745
 81%|███████████████████████▍     | 323/400 [2:28:04<34:05, 26.57s/it]2021-11-30 14:10:03,738 iteration 5492 : loss : 0.024562, loss_ce: 0.011580
2021-11-30 14:10:05,263 iteration 5493 : loss : 0.040292, loss_ce: 0.015593
2021-11-30 14:10:06,704 iteration 5494 : loss : 0.026580, loss_ce: 0.011568
2021-11-30 14:10:08,196 iteration 5495 : loss : 0.022232, loss_ce: 0.009956
2021-11-30 14:10:09,704 iteration 5496 : loss : 0.031244, loss_ce: 0.011566
2021-11-30 14:10:11,213 iteration 5497 : loss : 0.034975, loss_ce: 0.016524
2021-11-30 14:10:12,752 iteration 5498 : loss : 0.064220, loss_ce: 0.025071
2021-11-30 14:10:14,265 iteration 5499 : loss : 0.036043, loss_ce: 0.020704
2021-11-30 14:10:15,667 iteration 5500 : loss : 0.040844, loss_ce: 0.021702
2021-11-30 14:10:17,176 iteration 5501 : loss : 0.034546, loss_ce: 0.016029
2021-11-30 14:10:18,632 iteration 5502 : loss : 0.018968, loss_ce: 0.010538
2021-11-30 14:10:20,096 iteration 5503 : loss : 0.045181, loss_ce: 0.015555
2021-11-30 14:10:21,542 iteration 5504 : loss : 0.024372, loss_ce: 0.011714
2021-11-30 14:10:23,022 iteration 5505 : loss : 0.030632, loss_ce: 0.013818
2021-11-30 14:10:24,548 iteration 5506 : loss : 0.025466, loss_ce: 0.012313
2021-11-30 14:10:26,028 iteration 5507 : loss : 0.044837, loss_ce: 0.020861
2021-11-30 14:10:27,504 iteration 5508 : loss : 0.027882, loss_ce: 0.012280
 81%|███████████████████████▍     | 324/400 [2:28:29<33:07, 26.16s/it]2021-11-30 14:10:29,030 iteration 5509 : loss : 0.032413, loss_ce: 0.012501
2021-11-30 14:10:30,604 iteration 5510 : loss : 0.026007, loss_ce: 0.012337
2021-11-30 14:10:32,062 iteration 5511 : loss : 0.026810, loss_ce: 0.012871
2021-11-30 14:10:33,605 iteration 5512 : loss : 0.031547, loss_ce: 0.014485
2021-11-30 14:10:35,062 iteration 5513 : loss : 0.022698, loss_ce: 0.011911
2021-11-30 14:10:36,468 iteration 5514 : loss : 0.026687, loss_ce: 0.013048
2021-11-30 14:10:38,032 iteration 5515 : loss : 0.069665, loss_ce: 0.020791
2021-11-30 14:10:39,500 iteration 5516 : loss : 0.029388, loss_ce: 0.014424
2021-11-30 14:10:41,007 iteration 5517 : loss : 0.033676, loss_ce: 0.016394
2021-11-30 14:10:42,495 iteration 5518 : loss : 0.045660, loss_ce: 0.018186
2021-11-30 14:10:44,016 iteration 5519 : loss : 0.045546, loss_ce: 0.018338
2021-11-30 14:10:45,515 iteration 5520 : loss : 0.026347, loss_ce: 0.013250
2021-11-30 14:10:47,028 iteration 5521 : loss : 0.030227, loss_ce: 0.015133
2021-11-30 14:10:48,509 iteration 5522 : loss : 0.039235, loss_ce: 0.015637
2021-11-30 14:10:49,962 iteration 5523 : loss : 0.019958, loss_ce: 0.009296
2021-11-30 14:10:51,418 iteration 5524 : loss : 0.033495, loss_ce: 0.016193
2021-11-30 14:10:51,419 Training Data Eval:
2021-11-30 14:10:58,777   Average segmentation loss on training set: 0.0151
2021-11-30 14:10:58,777 Validation Data Eval:
2021-11-30 14:11:01,337   Average segmentation loss on validation set: 0.0824
2021-11-30 14:11:02,830 iteration 5525 : loss : 0.035007, loss_ce: 0.019257
 81%|███████████████████████▌     | 325/400 [2:29:05<36:08, 28.91s/it]2021-11-30 14:11:04,433 iteration 5526 : loss : 0.029083, loss_ce: 0.013889
2021-11-30 14:11:05,927 iteration 5527 : loss : 0.034612, loss_ce: 0.018899
2021-11-30 14:11:07,402 iteration 5528 : loss : 0.038436, loss_ce: 0.015756
2021-11-30 14:11:08,922 iteration 5529 : loss : 0.038213, loss_ce: 0.018705
2021-11-30 14:11:10,356 iteration 5530 : loss : 0.028861, loss_ce: 0.015452
2021-11-30 14:11:11,816 iteration 5531 : loss : 0.028694, loss_ce: 0.015281
2021-11-30 14:11:13,259 iteration 5532 : loss : 0.019180, loss_ce: 0.010228
2021-11-30 14:11:14,806 iteration 5533 : loss : 0.032112, loss_ce: 0.012487
2021-11-30 14:11:16,295 iteration 5534 : loss : 0.022803, loss_ce: 0.011486
2021-11-30 14:11:17,837 iteration 5535 : loss : 0.034992, loss_ce: 0.015096
2021-11-30 14:11:19,421 iteration 5536 : loss : 0.027671, loss_ce: 0.013748
2021-11-30 14:11:20,927 iteration 5537 : loss : 0.037381, loss_ce: 0.019169
2021-11-30 14:11:22,425 iteration 5538 : loss : 0.031972, loss_ce: 0.015729
2021-11-30 14:11:24,018 iteration 5539 : loss : 0.051965, loss_ce: 0.016105
2021-11-30 14:11:25,461 iteration 5540 : loss : 0.034656, loss_ce: 0.018011
2021-11-30 14:11:26,958 iteration 5541 : loss : 0.024943, loss_ce: 0.010185
2021-11-30 14:11:28,418 iteration 5542 : loss : 0.024192, loss_ce: 0.011675
 82%|███████████████████████▋     | 326/400 [2:29:30<34:25, 27.91s/it]2021-11-30 14:11:29,997 iteration 5543 : loss : 0.034882, loss_ce: 0.014702
2021-11-30 14:11:31,516 iteration 5544 : loss : 0.024539, loss_ce: 0.013307
2021-11-30 14:11:32,985 iteration 5545 : loss : 0.028110, loss_ce: 0.013192
2021-11-30 14:11:34,435 iteration 5546 : loss : 0.038887, loss_ce: 0.015857
2021-11-30 14:11:35,915 iteration 5547 : loss : 0.029729, loss_ce: 0.013268
2021-11-30 14:11:37,467 iteration 5548 : loss : 0.051956, loss_ce: 0.019531
2021-11-30 14:11:38,903 iteration 5549 : loss : 0.025756, loss_ce: 0.012627
2021-11-30 14:11:40,408 iteration 5550 : loss : 0.022958, loss_ce: 0.011392
2021-11-30 14:11:41,901 iteration 5551 : loss : 0.032693, loss_ce: 0.014514
2021-11-30 14:11:43,326 iteration 5552 : loss : 0.027189, loss_ce: 0.012422
2021-11-30 14:11:44,839 iteration 5553 : loss : 0.034551, loss_ce: 0.015825
2021-11-30 14:11:46,387 iteration 5554 : loss : 0.031749, loss_ce: 0.013444
2021-11-30 14:11:47,910 iteration 5555 : loss : 0.046755, loss_ce: 0.016478
2021-11-30 14:11:49,312 iteration 5556 : loss : 0.028266, loss_ce: 0.012845
2021-11-30 14:11:50,871 iteration 5557 : loss : 0.050021, loss_ce: 0.015453
2021-11-30 14:11:52,367 iteration 5558 : loss : 0.030757, loss_ce: 0.012791
2021-11-30 14:11:53,756 iteration 5559 : loss : 0.030763, loss_ce: 0.013755
 82%|███████████████████████▋     | 327/400 [2:29:56<33:01, 27.14s/it]2021-11-30 14:11:55,485 iteration 5560 : loss : 0.054753, loss_ce: 0.022803
2021-11-30 14:11:56,960 iteration 5561 : loss : 0.025533, loss_ce: 0.011591
2021-11-30 14:11:58,424 iteration 5562 : loss : 0.031321, loss_ce: 0.012250
2021-11-30 14:11:59,886 iteration 5563 : loss : 0.025062, loss_ce: 0.013308
2021-11-30 14:12:01,461 iteration 5564 : loss : 0.036632, loss_ce: 0.014788
2021-11-30 14:12:02,931 iteration 5565 : loss : 0.027414, loss_ce: 0.013240
2021-11-30 14:12:04,447 iteration 5566 : loss : 0.033540, loss_ce: 0.018320
2021-11-30 14:12:06,040 iteration 5567 : loss : 0.035379, loss_ce: 0.020015
2021-11-30 14:12:07,539 iteration 5568 : loss : 0.032188, loss_ce: 0.014970
2021-11-30 14:12:09,087 iteration 5569 : loss : 0.037019, loss_ce: 0.020407
2021-11-30 14:12:10,604 iteration 5570 : loss : 0.020816, loss_ce: 0.009826
2021-11-30 14:12:12,149 iteration 5571 : loss : 0.040888, loss_ce: 0.018998
2021-11-30 14:12:13,632 iteration 5572 : loss : 0.031087, loss_ce: 0.015978
2021-11-30 14:12:15,138 iteration 5573 : loss : 0.033617, loss_ce: 0.011136
2021-11-30 14:12:16,740 iteration 5574 : loss : 0.033833, loss_ce: 0.015637
2021-11-30 14:12:18,102 iteration 5575 : loss : 0.019627, loss_ce: 0.011132
2021-11-30 14:12:19,455 iteration 5576 : loss : 0.026051, loss_ce: 0.010916
 82%|███████████████████████▊     | 328/400 [2:30:21<32:02, 26.71s/it]2021-11-30 14:12:20,995 iteration 5577 : loss : 0.038361, loss_ce: 0.018625
2021-11-30 14:12:22,441 iteration 5578 : loss : 0.031793, loss_ce: 0.011363
2021-11-30 14:12:23,925 iteration 5579 : loss : 0.028114, loss_ce: 0.011692
2021-11-30 14:12:25,449 iteration 5580 : loss : 0.024735, loss_ce: 0.011227
2021-11-30 14:12:26,897 iteration 5581 : loss : 0.031436, loss_ce: 0.014007
2021-11-30 14:12:28,414 iteration 5582 : loss : 0.031680, loss_ce: 0.012716
2021-11-30 14:12:29,876 iteration 5583 : loss : 0.035896, loss_ce: 0.013992
2021-11-30 14:12:31,467 iteration 5584 : loss : 0.034914, loss_ce: 0.016431
2021-11-30 14:12:32,951 iteration 5585 : loss : 0.028528, loss_ce: 0.013705
2021-11-30 14:12:34,441 iteration 5586 : loss : 0.049500, loss_ce: 0.023489
2021-11-30 14:12:35,920 iteration 5587 : loss : 0.055694, loss_ce: 0.029373
2021-11-30 14:12:37,490 iteration 5588 : loss : 0.037843, loss_ce: 0.014939
2021-11-30 14:12:39,040 iteration 5589 : loss : 0.041400, loss_ce: 0.019185
2021-11-30 14:12:40,533 iteration 5590 : loss : 0.039797, loss_ce: 0.017965
2021-11-30 14:12:42,028 iteration 5591 : loss : 0.029158, loss_ce: 0.014935
2021-11-30 14:12:43,500 iteration 5592 : loss : 0.023206, loss_ce: 0.010154
2021-11-30 14:12:44,961 iteration 5593 : loss : 0.035825, loss_ce: 0.020317
 82%|███████████████████████▊     | 329/400 [2:30:47<31:10, 26.35s/it]2021-11-30 14:12:46,533 iteration 5594 : loss : 0.029974, loss_ce: 0.014105
2021-11-30 14:12:47,991 iteration 5595 : loss : 0.025450, loss_ce: 0.013120
2021-11-30 14:12:49,456 iteration 5596 : loss : 0.020515, loss_ce: 0.008962
2021-11-30 14:12:50,977 iteration 5597 : loss : 0.025263, loss_ce: 0.011257
2021-11-30 14:12:52,548 iteration 5598 : loss : 0.040591, loss_ce: 0.018569
2021-11-30 14:12:54,084 iteration 5599 : loss : 0.033691, loss_ce: 0.014851
2021-11-30 14:12:55,599 iteration 5600 : loss : 0.030468, loss_ce: 0.016834
2021-11-30 14:12:57,119 iteration 5601 : loss : 0.039685, loss_ce: 0.015153
2021-11-30 14:12:58,571 iteration 5602 : loss : 0.035323, loss_ce: 0.012935
2021-11-30 14:13:00,132 iteration 5603 : loss : 0.036698, loss_ce: 0.018573
2021-11-30 14:13:01,602 iteration 5604 : loss : 0.032213, loss_ce: 0.018651
2021-11-30 14:13:03,089 iteration 5605 : loss : 0.026903, loss_ce: 0.012315
2021-11-30 14:13:04,671 iteration 5606 : loss : 0.039072, loss_ce: 0.015542
2021-11-30 14:13:06,179 iteration 5607 : loss : 0.035106, loss_ce: 0.015637
2021-11-30 14:13:07,801 iteration 5608 : loss : 0.038042, loss_ce: 0.019730
2021-11-30 14:13:09,240 iteration 5609 : loss : 0.027295, loss_ce: 0.014628
2021-11-30 14:13:09,240 Training Data Eval:
2021-11-30 14:13:16,608   Average segmentation loss on training set: 0.0145
2021-11-30 14:13:16,609 Validation Data Eval:
2021-11-30 14:13:19,155   Average segmentation loss on validation set: 0.0890
2021-11-30 14:13:20,697 iteration 5610 : loss : 0.029712, loss_ce: 0.014030
 82%|███████████████████████▉     | 330/400 [2:31:23<34:01, 29.16s/it]2021-11-30 14:13:22,300 iteration 5611 : loss : 0.033865, loss_ce: 0.019121
2021-11-30 14:13:23,811 iteration 5612 : loss : 0.030131, loss_ce: 0.012582
2021-11-30 14:13:25,299 iteration 5613 : loss : 0.036504, loss_ce: 0.018333
2021-11-30 14:13:26,764 iteration 5614 : loss : 0.034278, loss_ce: 0.014592
2021-11-30 14:13:28,286 iteration 5615 : loss : 0.028759, loss_ce: 0.013712
2021-11-30 14:13:29,837 iteration 5616 : loss : 0.035406, loss_ce: 0.019840
2021-11-30 14:13:31,342 iteration 5617 : loss : 0.031294, loss_ce: 0.013631
2021-11-30 14:13:32,778 iteration 5618 : loss : 0.028047, loss_ce: 0.014615
2021-11-30 14:13:34,302 iteration 5619 : loss : 0.041002, loss_ce: 0.017496
2021-11-30 14:13:35,844 iteration 5620 : loss : 0.033112, loss_ce: 0.012907
2021-11-30 14:13:37,302 iteration 5621 : loss : 0.031598, loss_ce: 0.014962
2021-11-30 14:13:38,891 iteration 5622 : loss : 0.037598, loss_ce: 0.018226
2021-11-30 14:13:40,337 iteration 5623 : loss : 0.033426, loss_ce: 0.013127
2021-11-30 14:13:41,853 iteration 5624 : loss : 0.029175, loss_ce: 0.014147
2021-11-30 14:13:43,395 iteration 5625 : loss : 0.043007, loss_ce: 0.019616
2021-11-30 14:13:44,893 iteration 5626 : loss : 0.031544, loss_ce: 0.014191
2021-11-30 14:13:46,362 iteration 5627 : loss : 0.036673, loss_ce: 0.014001
 83%|███████████████████████▉     | 331/400 [2:31:48<32:19, 28.11s/it]2021-11-30 14:13:47,794 iteration 5628 : loss : 0.024954, loss_ce: 0.012220
2021-11-30 14:13:49,312 iteration 5629 : loss : 0.036372, loss_ce: 0.018135
2021-11-30 14:13:50,803 iteration 5630 : loss : 0.023561, loss_ce: 0.010505
2021-11-30 14:13:52,418 iteration 5631 : loss : 0.037471, loss_ce: 0.020749
2021-11-30 14:13:53,817 iteration 5632 : loss : 0.042373, loss_ce: 0.022021
2021-11-30 14:13:55,355 iteration 5633 : loss : 0.039058, loss_ce: 0.018116
2021-11-30 14:13:56,797 iteration 5634 : loss : 0.034600, loss_ce: 0.014109
2021-11-30 14:13:58,331 iteration 5635 : loss : 0.034923, loss_ce: 0.012875
2021-11-30 14:13:59,770 iteration 5636 : loss : 0.024241, loss_ce: 0.012382
2021-11-30 14:14:01,277 iteration 5637 : loss : 0.031785, loss_ce: 0.015474
2021-11-30 14:14:02,646 iteration 5638 : loss : 0.031133, loss_ce: 0.012666
2021-11-30 14:14:04,089 iteration 5639 : loss : 0.036995, loss_ce: 0.014158
2021-11-30 14:14:05,575 iteration 5640 : loss : 0.031069, loss_ce: 0.017090
2021-11-30 14:14:07,096 iteration 5641 : loss : 0.024688, loss_ce: 0.015568
2021-11-30 14:14:08,564 iteration 5642 : loss : 0.045783, loss_ce: 0.014380
2021-11-30 14:14:10,093 iteration 5643 : loss : 0.026960, loss_ce: 0.010715
2021-11-30 14:14:11,573 iteration 5644 : loss : 0.039056, loss_ce: 0.017868
 83%|████████████████████████     | 332/400 [2:32:13<30:52, 27.25s/it]2021-11-30 14:14:13,220 iteration 5645 : loss : 0.037774, loss_ce: 0.016983
2021-11-30 14:14:14,656 iteration 5646 : loss : 0.031153, loss_ce: 0.014508
2021-11-30 14:14:16,136 iteration 5647 : loss : 0.030809, loss_ce: 0.016319
2021-11-30 14:14:17,671 iteration 5648 : loss : 0.035457, loss_ce: 0.015307
2021-11-30 14:14:19,223 iteration 5649 : loss : 0.031376, loss_ce: 0.016085
2021-11-30 14:14:20,724 iteration 5650 : loss : 0.039206, loss_ce: 0.017529
2021-11-30 14:14:22,228 iteration 5651 : loss : 0.040543, loss_ce: 0.023859
2021-11-30 14:14:23,638 iteration 5652 : loss : 0.024822, loss_ce: 0.012466
2021-11-30 14:14:25,107 iteration 5653 : loss : 0.033598, loss_ce: 0.014228
2021-11-30 14:14:26,528 iteration 5654 : loss : 0.028311, loss_ce: 0.011746
2021-11-30 14:14:28,030 iteration 5655 : loss : 0.037138, loss_ce: 0.017095
2021-11-30 14:14:29,508 iteration 5656 : loss : 0.027918, loss_ce: 0.012889
2021-11-30 14:14:30,998 iteration 5657 : loss : 0.023825, loss_ce: 0.011939
2021-11-30 14:14:32,490 iteration 5658 : loss : 0.032697, loss_ce: 0.019993
2021-11-30 14:14:33,952 iteration 5659 : loss : 0.029746, loss_ce: 0.011162
2021-11-30 14:14:35,371 iteration 5660 : loss : 0.022945, loss_ce: 0.010505
2021-11-30 14:14:36,962 iteration 5661 : loss : 0.030698, loss_ce: 0.013019
 83%|████████████████████████▏    | 333/400 [2:32:39<29:47, 26.69s/it]2021-11-30 14:14:38,498 iteration 5662 : loss : 0.027919, loss_ce: 0.011038
2021-11-30 14:14:39,982 iteration 5663 : loss : 0.032999, loss_ce: 0.015271
2021-11-30 14:14:41,477 iteration 5664 : loss : 0.049015, loss_ce: 0.018784
2021-11-30 14:14:42,874 iteration 5665 : loss : 0.021923, loss_ce: 0.010877
2021-11-30 14:14:44,403 iteration 5666 : loss : 0.025335, loss_ce: 0.011516
2021-11-30 14:14:45,935 iteration 5667 : loss : 0.026584, loss_ce: 0.012979
2021-11-30 14:14:47,429 iteration 5668 : loss : 0.027134, loss_ce: 0.015015
2021-11-30 14:14:48,958 iteration 5669 : loss : 0.054534, loss_ce: 0.031529
2021-11-30 14:14:50,476 iteration 5670 : loss : 0.024176, loss_ce: 0.011034
2021-11-30 14:14:52,005 iteration 5671 : loss : 0.032956, loss_ce: 0.013467
2021-11-30 14:14:53,482 iteration 5672 : loss : 0.042824, loss_ce: 0.017275
2021-11-30 14:14:54,924 iteration 5673 : loss : 0.031596, loss_ce: 0.013168
2021-11-30 14:14:56,495 iteration 5674 : loss : 0.028708, loss_ce: 0.015210
2021-11-30 14:14:57,959 iteration 5675 : loss : 0.028200, loss_ce: 0.014716
2021-11-30 14:14:59,513 iteration 5676 : loss : 0.036365, loss_ce: 0.014579
2021-11-30 14:15:01,052 iteration 5677 : loss : 0.026956, loss_ce: 0.014316
2021-11-30 14:15:02,551 iteration 5678 : loss : 0.031107, loss_ce: 0.013086
 84%|████████████████████████▏    | 334/400 [2:33:04<28:59, 26.35s/it]2021-11-30 14:15:04,085 iteration 5679 : loss : 0.039645, loss_ce: 0.019660
2021-11-30 14:15:05,445 iteration 5680 : loss : 0.027548, loss_ce: 0.010186
2021-11-30 14:15:06,987 iteration 5681 : loss : 0.030266, loss_ce: 0.011512
2021-11-30 14:15:08,540 iteration 5682 : loss : 0.052337, loss_ce: 0.025245
2021-11-30 14:15:10,011 iteration 5683 : loss : 0.026011, loss_ce: 0.011058
2021-11-30 14:15:11,470 iteration 5684 : loss : 0.020043, loss_ce: 0.012463
2021-11-30 14:15:13,013 iteration 5685 : loss : 0.039830, loss_ce: 0.021409
2021-11-30 14:15:14,464 iteration 5686 : loss : 0.029741, loss_ce: 0.012790
2021-11-30 14:15:15,956 iteration 5687 : loss : 0.030694, loss_ce: 0.014552
2021-11-30 14:15:17,558 iteration 5688 : loss : 0.041956, loss_ce: 0.016438
2021-11-30 14:15:19,050 iteration 5689 : loss : 0.042179, loss_ce: 0.013758
2021-11-30 14:15:20,557 iteration 5690 : loss : 0.031978, loss_ce: 0.016285
2021-11-30 14:15:22,095 iteration 5691 : loss : 0.044954, loss_ce: 0.021450
2021-11-30 14:15:23,620 iteration 5692 : loss : 0.030785, loss_ce: 0.015363
2021-11-30 14:15:25,144 iteration 5693 : loss : 0.038143, loss_ce: 0.013250
2021-11-30 14:15:26,619 iteration 5694 : loss : 0.033998, loss_ce: 0.017795
2021-11-30 14:15:26,619 Training Data Eval:
2021-11-30 14:15:33,966   Average segmentation loss on training set: 0.0148
2021-11-30 14:15:33,966 Validation Data Eval:
2021-11-30 14:15:36,518   Average segmentation loss on validation set: 0.0895
2021-11-30 14:15:37,962 iteration 5695 : loss : 0.028907, loss_ce: 0.011373
 84%|████████████████████████▎    | 335/400 [2:33:40<31:29, 29.07s/it]2021-11-30 14:15:39,456 iteration 5696 : loss : 0.024805, loss_ce: 0.014319
2021-11-30 14:15:40,899 iteration 5697 : loss : 0.026414, loss_ce: 0.013564
2021-11-30 14:15:42,453 iteration 5698 : loss : 0.032430, loss_ce: 0.014258
2021-11-30 14:15:43,878 iteration 5699 : loss : 0.031085, loss_ce: 0.014817
2021-11-30 14:15:45,353 iteration 5700 : loss : 0.023794, loss_ce: 0.011355
2021-11-30 14:15:46,787 iteration 5701 : loss : 0.027447, loss_ce: 0.013885
2021-11-30 14:15:48,386 iteration 5702 : loss : 0.029945, loss_ce: 0.013757
2021-11-30 14:15:49,897 iteration 5703 : loss : 0.038132, loss_ce: 0.017980
2021-11-30 14:15:51,406 iteration 5704 : loss : 0.039658, loss_ce: 0.017861
2021-11-30 14:15:52,918 iteration 5705 : loss : 0.037181, loss_ce: 0.014932
2021-11-30 14:15:54,315 iteration 5706 : loss : 0.023817, loss_ce: 0.010989
2021-11-30 14:15:55,783 iteration 5707 : loss : 0.029887, loss_ce: 0.014357
2021-11-30 14:15:57,211 iteration 5708 : loss : 0.028924, loss_ce: 0.012356
2021-11-30 14:15:58,709 iteration 5709 : loss : 0.026388, loss_ce: 0.011586
2021-11-30 14:16:00,203 iteration 5710 : loss : 0.032833, loss_ce: 0.012324
2021-11-30 14:16:01,626 iteration 5711 : loss : 0.027147, loss_ce: 0.012213
2021-11-30 14:16:03,152 iteration 5712 : loss : 0.032997, loss_ce: 0.015485
 84%|████████████████████████▎    | 336/400 [2:34:05<29:46, 27.91s/it]2021-11-30 14:16:04,740 iteration 5713 : loss : 0.034180, loss_ce: 0.014109
2021-11-30 14:16:06,281 iteration 5714 : loss : 0.047417, loss_ce: 0.018838
2021-11-30 14:16:07,827 iteration 5715 : loss : 0.023769, loss_ce: 0.012205
2021-11-30 14:16:09,293 iteration 5716 : loss : 0.029352, loss_ce: 0.014622
2021-11-30 14:16:10,888 iteration 5717 : loss : 0.043615, loss_ce: 0.018920
2021-11-30 14:16:12,330 iteration 5718 : loss : 0.024187, loss_ce: 0.010975
2021-11-30 14:16:13,770 iteration 5719 : loss : 0.030196, loss_ce: 0.015881
2021-11-30 14:16:15,297 iteration 5720 : loss : 0.033619, loss_ce: 0.016272
2021-11-30 14:16:16,810 iteration 5721 : loss : 0.042446, loss_ce: 0.022998
2021-11-30 14:16:18,300 iteration 5722 : loss : 0.025681, loss_ce: 0.012210
2021-11-30 14:16:19,829 iteration 5723 : loss : 0.029325, loss_ce: 0.013460
2021-11-30 14:16:21,359 iteration 5724 : loss : 0.025400, loss_ce: 0.011497
2021-11-30 14:16:22,849 iteration 5725 : loss : 0.024456, loss_ce: 0.012164
2021-11-30 14:16:24,279 iteration 5726 : loss : 0.035758, loss_ce: 0.012399
2021-11-30 14:16:25,775 iteration 5727 : loss : 0.034319, loss_ce: 0.013224
2021-11-30 14:16:27,250 iteration 5728 : loss : 0.033654, loss_ce: 0.013346
2021-11-30 14:16:28,808 iteration 5729 : loss : 0.030293, loss_ce: 0.015026
 84%|████████████████████████▍    | 337/400 [2:34:31<28:35, 27.23s/it]2021-11-30 14:16:30,299 iteration 5730 : loss : 0.024474, loss_ce: 0.013612
2021-11-30 14:16:31,856 iteration 5731 : loss : 0.053403, loss_ce: 0.028780
2021-11-30 14:16:33,290 iteration 5732 : loss : 0.029566, loss_ce: 0.013846
2021-11-30 14:16:34,842 iteration 5733 : loss : 0.045595, loss_ce: 0.015883
2021-11-30 14:16:36,387 iteration 5734 : loss : 0.030687, loss_ce: 0.015376
2021-11-30 14:16:37,816 iteration 5735 : loss : 0.029496, loss_ce: 0.011456
2021-11-30 14:16:39,263 iteration 5736 : loss : 0.029955, loss_ce: 0.013692
2021-11-30 14:16:40,843 iteration 5737 : loss : 0.042581, loss_ce: 0.018382
2021-11-30 14:16:42,293 iteration 5738 : loss : 0.026606, loss_ce: 0.011620
2021-11-30 14:16:43,742 iteration 5739 : loss : 0.038641, loss_ce: 0.017644
2021-11-30 14:16:45,232 iteration 5740 : loss : 0.039507, loss_ce: 0.016336
2021-11-30 14:16:46,716 iteration 5741 : loss : 0.036572, loss_ce: 0.018377
2021-11-30 14:16:48,215 iteration 5742 : loss : 0.043496, loss_ce: 0.014278
2021-11-30 14:16:49,732 iteration 5743 : loss : 0.020932, loss_ce: 0.011031
2021-11-30 14:16:51,274 iteration 5744 : loss : 0.044451, loss_ce: 0.019259
2021-11-30 14:16:52,836 iteration 5745 : loss : 0.046827, loss_ce: 0.019846
2021-11-30 14:16:54,303 iteration 5746 : loss : 0.035467, loss_ce: 0.013081
 84%|████████████████████████▌    | 338/400 [2:34:56<27:36, 26.71s/it]2021-11-30 14:16:55,861 iteration 5747 : loss : 0.032941, loss_ce: 0.013745
2021-11-30 14:16:57,366 iteration 5748 : loss : 0.034668, loss_ce: 0.015163
2021-11-30 14:16:58,863 iteration 5749 : loss : 0.037209, loss_ce: 0.017137
2021-11-30 14:17:00,354 iteration 5750 : loss : 0.038086, loss_ce: 0.015210
2021-11-30 14:17:01,787 iteration 5751 : loss : 0.030536, loss_ce: 0.013425
2021-11-30 14:17:03,268 iteration 5752 : loss : 0.040259, loss_ce: 0.014819
2021-11-30 14:17:04,818 iteration 5753 : loss : 0.028294, loss_ce: 0.012426
2021-11-30 14:17:06,365 iteration 5754 : loss : 0.031192, loss_ce: 0.012959
2021-11-30 14:17:07,842 iteration 5755 : loss : 0.041260, loss_ce: 0.015991
2021-11-30 14:17:09,292 iteration 5756 : loss : 0.028083, loss_ce: 0.012567
2021-11-30 14:17:10,663 iteration 5757 : loss : 0.024278, loss_ce: 0.011969
2021-11-30 14:17:12,214 iteration 5758 : loss : 0.039044, loss_ce: 0.019185
2021-11-30 14:17:13,750 iteration 5759 : loss : 0.033072, loss_ce: 0.017255
2021-11-30 14:17:15,331 iteration 5760 : loss : 0.032540, loss_ce: 0.019473
2021-11-30 14:17:16,930 iteration 5761 : loss : 0.045126, loss_ce: 0.023191
2021-11-30 14:17:18,355 iteration 5762 : loss : 0.023170, loss_ce: 0.012881
2021-11-30 14:17:19,888 iteration 5763 : loss : 0.022833, loss_ce: 0.011198
 85%|████████████████████████▌    | 339/400 [2:35:22<26:48, 26.37s/it]2021-11-30 14:17:21,423 iteration 5764 : loss : 0.030876, loss_ce: 0.013598
2021-11-30 14:17:22,973 iteration 5765 : loss : 0.058189, loss_ce: 0.015334
2021-11-30 14:17:24,484 iteration 5766 : loss : 0.035531, loss_ce: 0.021428
2021-11-30 14:17:25,958 iteration 5767 : loss : 0.030952, loss_ce: 0.018276
2021-11-30 14:17:27,556 iteration 5768 : loss : 0.028079, loss_ce: 0.013417
2021-11-30 14:17:29,109 iteration 5769 : loss : 0.037514, loss_ce: 0.014395
2021-11-30 14:17:30,559 iteration 5770 : loss : 0.024334, loss_ce: 0.010677
2021-11-30 14:17:32,099 iteration 5771 : loss : 0.039093, loss_ce: 0.017995
2021-11-30 14:17:33,596 iteration 5772 : loss : 0.028499, loss_ce: 0.012621
2021-11-30 14:17:35,004 iteration 5773 : loss : 0.032592, loss_ce: 0.013018
2021-11-30 14:17:36,407 iteration 5774 : loss : 0.036959, loss_ce: 0.013556
2021-11-30 14:17:37,914 iteration 5775 : loss : 0.023295, loss_ce: 0.011740
2021-11-30 14:17:39,494 iteration 5776 : loss : 0.050760, loss_ce: 0.025914
2021-11-30 14:17:40,905 iteration 5777 : loss : 0.027598, loss_ce: 0.014187
2021-11-30 14:17:42,380 iteration 5778 : loss : 0.039002, loss_ce: 0.014339
2021-11-30 14:17:43,858 iteration 5779 : loss : 0.025251, loss_ce: 0.011224
2021-11-30 14:17:43,858 Training Data Eval:
2021-11-30 14:17:51,211   Average segmentation loss on training set: 0.0143
2021-11-30 14:17:51,211 Validation Data Eval:
2021-11-30 14:17:53,780   Average segmentation loss on validation set: 0.0839
2021-11-30 14:17:55,271 iteration 5780 : loss : 0.039040, loss_ce: 0.014098
 85%|████████████████████████▋    | 340/400 [2:35:57<29:04, 29.08s/it]2021-11-30 14:17:56,783 iteration 5781 : loss : 0.043792, loss_ce: 0.013615
2021-11-30 14:17:58,348 iteration 5782 : loss : 0.050547, loss_ce: 0.020344
2021-11-30 14:17:59,833 iteration 5783 : loss : 0.030196, loss_ce: 0.013453
2021-11-30 14:18:01,347 iteration 5784 : loss : 0.040720, loss_ce: 0.024705
2021-11-30 14:18:02,794 iteration 5785 : loss : 0.024061, loss_ce: 0.011570
2021-11-30 14:18:04,256 iteration 5786 : loss : 0.024997, loss_ce: 0.013415
2021-11-30 14:18:05,709 iteration 5787 : loss : 0.029259, loss_ce: 0.013886
2021-11-30 14:18:07,363 iteration 5788 : loss : 0.037327, loss_ce: 0.017634
2021-11-30 14:18:08,870 iteration 5789 : loss : 0.026705, loss_ce: 0.011978
2021-11-30 14:18:10,362 iteration 5790 : loss : 0.028882, loss_ce: 0.011921
2021-11-30 14:18:11,798 iteration 5791 : loss : 0.027069, loss_ce: 0.012686
2021-11-30 14:18:13,344 iteration 5792 : loss : 0.035706, loss_ce: 0.016359
2021-11-30 14:18:14,965 iteration 5793 : loss : 0.038121, loss_ce: 0.016973
2021-11-30 14:18:16,463 iteration 5794 : loss : 0.027808, loss_ce: 0.013339
2021-11-30 14:18:17,936 iteration 5795 : loss : 0.027855, loss_ce: 0.012023
2021-11-30 14:18:19,361 iteration 5796 : loss : 0.030192, loss_ce: 0.012002
2021-11-30 14:18:20,885 iteration 5797 : loss : 0.045281, loss_ce: 0.019976
 85%|████████████████████████▋    | 341/400 [2:36:23<27:34, 28.04s/it]2021-11-30 14:18:22,450 iteration 5798 : loss : 0.042226, loss_ce: 0.021132
2021-11-30 14:18:23,980 iteration 5799 : loss : 0.032069, loss_ce: 0.014216
2021-11-30 14:18:25,461 iteration 5800 : loss : 0.028435, loss_ce: 0.013637
2021-11-30 14:18:26,942 iteration 5801 : loss : 0.039718, loss_ce: 0.015347
2021-11-30 14:18:28,372 iteration 5802 : loss : 0.021007, loss_ce: 0.010874
2021-11-30 14:18:29,841 iteration 5803 : loss : 0.035059, loss_ce: 0.018722
2021-11-30 14:18:31,312 iteration 5804 : loss : 0.027323, loss_ce: 0.012897
2021-11-30 14:18:32,902 iteration 5805 : loss : 0.044077, loss_ce: 0.017937
2021-11-30 14:18:34,373 iteration 5806 : loss : 0.049351, loss_ce: 0.017532
2021-11-30 14:18:35,949 iteration 5807 : loss : 0.043305, loss_ce: 0.019661
2021-11-30 14:18:37,472 iteration 5808 : loss : 0.033002, loss_ce: 0.017268
2021-11-30 14:18:38,906 iteration 5809 : loss : 0.023539, loss_ce: 0.009626
2021-11-30 14:18:40,368 iteration 5810 : loss : 0.035403, loss_ce: 0.014052
2021-11-30 14:18:41,799 iteration 5811 : loss : 0.022173, loss_ce: 0.009109
2021-11-30 14:18:43,305 iteration 5812 : loss : 0.036123, loss_ce: 0.014219
2021-11-30 14:18:44,722 iteration 5813 : loss : 0.027323, loss_ce: 0.014088
2021-11-30 14:18:46,227 iteration 5814 : loss : 0.035709, loss_ce: 0.017702
 86%|████████████████████████▊    | 342/400 [2:36:48<26:19, 27.23s/it]2021-11-30 14:18:47,690 iteration 5815 : loss : 0.024417, loss_ce: 0.010955
2021-11-30 14:18:49,158 iteration 5816 : loss : 0.023992, loss_ce: 0.013587
2021-11-30 14:18:50,622 iteration 5817 : loss : 0.025030, loss_ce: 0.012482
2021-11-30 14:18:52,006 iteration 5818 : loss : 0.031686, loss_ce: 0.014251
2021-11-30 14:18:53,563 iteration 5819 : loss : 0.043033, loss_ce: 0.018517
2021-11-30 14:18:55,149 iteration 5820 : loss : 0.055798, loss_ce: 0.028110
2021-11-30 14:18:56,561 iteration 5821 : loss : 0.022050, loss_ce: 0.010015
2021-11-30 14:18:58,015 iteration 5822 : loss : 0.025399, loss_ce: 0.014308
2021-11-30 14:18:59,604 iteration 5823 : loss : 0.033078, loss_ce: 0.015478
2021-11-30 14:19:01,086 iteration 5824 : loss : 0.028905, loss_ce: 0.012099
2021-11-30 14:19:02,636 iteration 5825 : loss : 0.026987, loss_ce: 0.013504
2021-11-30 14:19:04,176 iteration 5826 : loss : 0.035622, loss_ce: 0.012465
2021-11-30 14:19:05,676 iteration 5827 : loss : 0.034496, loss_ce: 0.012490
2021-11-30 14:19:07,282 iteration 5828 : loss : 0.039769, loss_ce: 0.015830
2021-11-30 14:19:08,743 iteration 5829 : loss : 0.027106, loss_ce: 0.015046
2021-11-30 14:19:10,178 iteration 5830 : loss : 0.044045, loss_ce: 0.020310
2021-11-30 14:19:11,632 iteration 5831 : loss : 0.035518, loss_ce: 0.016028
 86%|████████████████████████▊    | 343/400 [2:37:13<25:20, 26.68s/it]2021-11-30 14:19:13,160 iteration 5832 : loss : 0.022890, loss_ce: 0.010138
2021-11-30 14:19:14,684 iteration 5833 : loss : 0.033687, loss_ce: 0.019864
2021-11-30 14:19:16,094 iteration 5834 : loss : 0.020902, loss_ce: 0.009881
2021-11-30 14:19:17,534 iteration 5835 : loss : 0.032998, loss_ce: 0.012876
2021-11-30 14:19:19,087 iteration 5836 : loss : 0.054985, loss_ce: 0.030896
2021-11-30 14:19:20,587 iteration 5837 : loss : 0.032964, loss_ce: 0.010658
2021-11-30 14:19:21,995 iteration 5838 : loss : 0.022897, loss_ce: 0.009318
2021-11-30 14:19:23,486 iteration 5839 : loss : 0.030217, loss_ce: 0.012779
2021-11-30 14:19:25,019 iteration 5840 : loss : 0.041579, loss_ce: 0.017028
2021-11-30 14:19:26,482 iteration 5841 : loss : 0.032434, loss_ce: 0.015952
2021-11-30 14:19:27,942 iteration 5842 : loss : 0.015831, loss_ce: 0.008768
2021-11-30 14:19:29,398 iteration 5843 : loss : 0.030214, loss_ce: 0.014929
2021-11-30 14:19:31,046 iteration 5844 : loss : 0.045257, loss_ce: 0.015145
2021-11-30 14:19:32,532 iteration 5845 : loss : 0.037584, loss_ce: 0.018773
2021-11-30 14:19:34,042 iteration 5846 : loss : 0.040139, loss_ce: 0.019194
2021-11-30 14:19:35,539 iteration 5847 : loss : 0.033305, loss_ce: 0.015023
2021-11-30 14:19:37,016 iteration 5848 : loss : 0.029994, loss_ce: 0.013346
 86%|████████████████████████▉    | 344/400 [2:37:39<24:32, 26.29s/it]2021-11-30 14:19:38,561 iteration 5849 : loss : 0.042972, loss_ce: 0.018653
2021-11-30 14:19:39,986 iteration 5850 : loss : 0.024229, loss_ce: 0.011540
2021-11-30 14:19:41,470 iteration 5851 : loss : 0.030583, loss_ce: 0.015089
2021-11-30 14:19:42,932 iteration 5852 : loss : 0.027441, loss_ce: 0.012974
2021-11-30 14:19:44,339 iteration 5853 : loss : 0.025293, loss_ce: 0.011577
2021-11-30 14:19:45,790 iteration 5854 : loss : 0.040412, loss_ce: 0.016073
2021-11-30 14:19:47,360 iteration 5855 : loss : 0.030155, loss_ce: 0.014513
2021-11-30 14:19:48,858 iteration 5856 : loss : 0.027642, loss_ce: 0.014225
2021-11-30 14:19:50,365 iteration 5857 : loss : 0.029927, loss_ce: 0.014064
2021-11-30 14:19:51,786 iteration 5858 : loss : 0.026862, loss_ce: 0.012655
2021-11-30 14:19:53,336 iteration 5859 : loss : 0.035424, loss_ce: 0.016780
2021-11-30 14:19:54,873 iteration 5860 : loss : 0.040376, loss_ce: 0.017540
2021-11-30 14:19:56,334 iteration 5861 : loss : 0.040749, loss_ce: 0.014949
2021-11-30 14:19:57,853 iteration 5862 : loss : 0.025577, loss_ce: 0.014109
2021-11-30 14:19:59,248 iteration 5863 : loss : 0.022734, loss_ce: 0.010896
2021-11-30 14:20:00,757 iteration 5864 : loss : 0.032426, loss_ce: 0.012490
2021-11-30 14:20:00,757 Training Data Eval:
2021-11-30 14:20:08,142   Average segmentation loss on training set: 0.0150
2021-11-30 14:20:08,143 Validation Data Eval:
2021-11-30 14:20:10,705   Average segmentation loss on validation set: 0.0842
2021-11-30 14:20:12,166 iteration 5865 : loss : 0.035028, loss_ce: 0.013600
 86%|█████████████████████████    | 345/400 [2:38:14<26:32, 28.95s/it]2021-11-30 14:20:13,809 iteration 5866 : loss : 0.024162, loss_ce: 0.012521
2021-11-30 14:20:15,364 iteration 5867 : loss : 0.033360, loss_ce: 0.012595
2021-11-30 14:20:16,903 iteration 5868 : loss : 0.034417, loss_ce: 0.014929
2021-11-30 14:20:18,305 iteration 5869 : loss : 0.027668, loss_ce: 0.013124
2021-11-30 14:20:19,889 iteration 5870 : loss : 0.038846, loss_ce: 0.014759
2021-11-30 14:20:21,406 iteration 5871 : loss : 0.023293, loss_ce: 0.010957
2021-11-30 14:20:22,940 iteration 5872 : loss : 0.044303, loss_ce: 0.017178
2021-11-30 14:20:24,572 iteration 5873 : loss : 0.035171, loss_ce: 0.015149
2021-11-30 14:20:26,022 iteration 5874 : loss : 0.023655, loss_ce: 0.011365
2021-11-30 14:20:27,486 iteration 5875 : loss : 0.026374, loss_ce: 0.015375
2021-11-30 14:20:28,980 iteration 5876 : loss : 0.037786, loss_ce: 0.017031
2021-11-30 14:20:30,468 iteration 5877 : loss : 0.049352, loss_ce: 0.018089
2021-11-30 14:20:31,990 iteration 5878 : loss : 0.032493, loss_ce: 0.017336
2021-11-30 14:20:33,430 iteration 5879 : loss : 0.031173, loss_ce: 0.016389
2021-11-30 14:20:35,035 iteration 5880 : loss : 0.050399, loss_ce: 0.016291
2021-11-30 14:20:36,458 iteration 5881 : loss : 0.022456, loss_ce: 0.011193
2021-11-30 14:20:37,929 iteration 5882 : loss : 0.028632, loss_ce: 0.014840
 86%|█████████████████████████    | 346/400 [2:38:40<25:11, 28.00s/it]2021-11-30 14:20:39,452 iteration 5883 : loss : 0.022492, loss_ce: 0.010338
2021-11-30 14:20:40,903 iteration 5884 : loss : 0.031467, loss_ce: 0.014837
2021-11-30 14:20:42,535 iteration 5885 : loss : 0.055475, loss_ce: 0.022291
2021-11-30 14:20:43,948 iteration 5886 : loss : 0.028152, loss_ce: 0.011867
2021-11-30 14:20:45,393 iteration 5887 : loss : 0.032957, loss_ce: 0.015932
2021-11-30 14:20:46,982 iteration 5888 : loss : 0.040007, loss_ce: 0.015295
2021-11-30 14:20:48,570 iteration 5889 : loss : 0.035714, loss_ce: 0.016857
2021-11-30 14:20:50,076 iteration 5890 : loss : 0.022665, loss_ce: 0.010435
2021-11-30 14:20:51,627 iteration 5891 : loss : 0.040937, loss_ce: 0.015977
2021-11-30 14:20:53,233 iteration 5892 : loss : 0.037945, loss_ce: 0.017582
2021-11-30 14:20:54,679 iteration 5893 : loss : 0.023635, loss_ce: 0.010082
2021-11-30 14:20:56,269 iteration 5894 : loss : 0.035872, loss_ce: 0.017493
2021-11-30 14:20:57,803 iteration 5895 : loss : 0.034347, loss_ce: 0.014137
2021-11-30 14:20:59,274 iteration 5896 : loss : 0.024634, loss_ce: 0.013041
2021-11-30 14:21:00,840 iteration 5897 : loss : 0.046598, loss_ce: 0.024019
2021-11-30 14:21:02,383 iteration 5898 : loss : 0.021884, loss_ce: 0.010347
2021-11-30 14:21:03,831 iteration 5899 : loss : 0.029368, loss_ce: 0.013615
 87%|█████████████████████████▏   | 347/400 [2:39:06<24:10, 27.37s/it]2021-11-30 14:21:05,384 iteration 5900 : loss : 0.024801, loss_ce: 0.011442
2021-11-30 14:21:06,961 iteration 5901 : loss : 0.035145, loss_ce: 0.013394
2021-11-30 14:21:08,448 iteration 5902 : loss : 0.034389, loss_ce: 0.015447
2021-11-30 14:21:10,029 iteration 5903 : loss : 0.033499, loss_ce: 0.016228
2021-11-30 14:21:11,514 iteration 5904 : loss : 0.035540, loss_ce: 0.014523
2021-11-30 14:21:13,058 iteration 5905 : loss : 0.051230, loss_ce: 0.013577
2021-11-30 14:21:14,484 iteration 5906 : loss : 0.032956, loss_ce: 0.014801
2021-11-30 14:21:15,999 iteration 5907 : loss : 0.021664, loss_ce: 0.009847
2021-11-30 14:21:17,488 iteration 5908 : loss : 0.023927, loss_ce: 0.010651
2021-11-30 14:21:18,888 iteration 5909 : loss : 0.029528, loss_ce: 0.010685
2021-11-30 14:21:20,403 iteration 5910 : loss : 0.037105, loss_ce: 0.016910
2021-11-30 14:21:21,957 iteration 5911 : loss : 0.061245, loss_ce: 0.024641
2021-11-30 14:21:23,391 iteration 5912 : loss : 0.028627, loss_ce: 0.012610
2021-11-30 14:21:24,866 iteration 5913 : loss : 0.037261, loss_ce: 0.020138
2021-11-30 14:21:26,386 iteration 5914 : loss : 0.031026, loss_ce: 0.013996
2021-11-30 14:21:27,791 iteration 5915 : loss : 0.034686, loss_ce: 0.015782
2021-11-30 14:21:29,332 iteration 5916 : loss : 0.048843, loss_ce: 0.020638
 87%|█████████████████████████▏   | 348/400 [2:39:31<23:13, 26.81s/it]2021-11-30 14:21:30,858 iteration 5917 : loss : 0.025280, loss_ce: 0.011531
2021-11-30 14:21:32,270 iteration 5918 : loss : 0.029206, loss_ce: 0.013514
2021-11-30 14:21:33,805 iteration 5919 : loss : 0.033825, loss_ce: 0.017449
2021-11-30 14:21:35,356 iteration 5920 : loss : 0.047369, loss_ce: 0.026947
2021-11-30 14:21:36,995 iteration 5921 : loss : 0.069720, loss_ce: 0.024780
2021-11-30 14:21:38,450 iteration 5922 : loss : 0.035139, loss_ce: 0.013263
2021-11-30 14:21:39,951 iteration 5923 : loss : 0.030598, loss_ce: 0.011849
2021-11-30 14:21:41,547 iteration 5924 : loss : 0.058572, loss_ce: 0.015893
2021-11-30 14:21:43,045 iteration 5925 : loss : 0.027951, loss_ce: 0.011948
2021-11-30 14:21:44,539 iteration 5926 : loss : 0.041723, loss_ce: 0.018928
2021-11-30 14:21:46,060 iteration 5927 : loss : 0.023677, loss_ce: 0.011173
2021-11-30 14:21:47,472 iteration 5928 : loss : 0.034106, loss_ce: 0.012041
2021-11-30 14:21:48,885 iteration 5929 : loss : 0.031452, loss_ce: 0.016369
2021-11-30 14:21:50,313 iteration 5930 : loss : 0.032573, loss_ce: 0.016602
2021-11-30 14:21:51,833 iteration 5931 : loss : 0.043968, loss_ce: 0.014305
2021-11-30 14:21:53,414 iteration 5932 : loss : 0.026221, loss_ce: 0.011904
2021-11-30 14:21:54,888 iteration 5933 : loss : 0.029203, loss_ce: 0.013590
 87%|█████████████████████████▎   | 349/400 [2:39:57<22:27, 26.43s/it]2021-11-30 14:21:56,392 iteration 5934 : loss : 0.029625, loss_ce: 0.012393
2021-11-30 14:21:58,038 iteration 5935 : loss : 0.041604, loss_ce: 0.017548
2021-11-30 14:21:59,527 iteration 5936 : loss : 0.024871, loss_ce: 0.013104
2021-11-30 14:22:00,975 iteration 5937 : loss : 0.022706, loss_ce: 0.010295
2021-11-30 14:22:02,367 iteration 5938 : loss : 0.020087, loss_ce: 0.010318
2021-11-30 14:22:03,912 iteration 5939 : loss : 0.029267, loss_ce: 0.016134
2021-11-30 14:22:05,362 iteration 5940 : loss : 0.024371, loss_ce: 0.011468
2021-11-30 14:22:06,826 iteration 5941 : loss : 0.034108, loss_ce: 0.016051
2021-11-30 14:22:08,292 iteration 5942 : loss : 0.028492, loss_ce: 0.014758
2021-11-30 14:22:09,794 iteration 5943 : loss : 0.037765, loss_ce: 0.015301
2021-11-30 14:22:11,290 iteration 5944 : loss : 0.032264, loss_ce: 0.018390
2021-11-30 14:22:12,739 iteration 5945 : loss : 0.029713, loss_ce: 0.011260
2021-11-30 14:22:14,348 iteration 5946 : loss : 0.043726, loss_ce: 0.020674
2021-11-30 14:22:15,865 iteration 5947 : loss : 0.038626, loss_ce: 0.016150
2021-11-30 14:22:17,314 iteration 5948 : loss : 0.027501, loss_ce: 0.013030
2021-11-30 14:22:18,915 iteration 5949 : loss : 0.032545, loss_ce: 0.013057
2021-11-30 14:22:18,915 Training Data Eval:
2021-11-30 14:22:26,248   Average segmentation loss on training set: 0.0143
2021-11-30 14:22:26,248 Validation Data Eval:
2021-11-30 14:22:28,814   Average segmentation loss on validation set: 0.0819
2021-11-30 14:22:30,340 iteration 5950 : loss : 0.027291, loss_ce: 0.011616
 88%|█████████████████████████▍   | 350/400 [2:40:32<24:17, 29.14s/it]2021-11-30 14:22:31,836 iteration 5951 : loss : 0.023959, loss_ce: 0.010636
2021-11-30 14:22:33,330 iteration 5952 : loss : 0.028401, loss_ce: 0.013642
2021-11-30 14:22:34,840 iteration 5953 : loss : 0.034574, loss_ce: 0.016206
2021-11-30 14:22:36,293 iteration 5954 : loss : 0.032308, loss_ce: 0.014278
2021-11-30 14:22:37,780 iteration 5955 : loss : 0.030363, loss_ce: 0.013161
2021-11-30 14:22:39,304 iteration 5956 : loss : 0.037307, loss_ce: 0.022864
2021-11-30 14:22:40,822 iteration 5957 : loss : 0.021979, loss_ce: 0.011549
2021-11-30 14:22:42,279 iteration 5958 : loss : 0.022750, loss_ce: 0.010848
2021-11-30 14:22:43,773 iteration 5959 : loss : 0.032239, loss_ce: 0.015163
2021-11-30 14:22:45,213 iteration 5960 : loss : 0.025786, loss_ce: 0.010553
2021-11-30 14:22:46,642 iteration 5961 : loss : 0.044289, loss_ce: 0.012519
2021-11-30 14:22:48,121 iteration 5962 : loss : 0.049075, loss_ce: 0.015729
2021-11-30 14:22:49,647 iteration 5963 : loss : 0.036370, loss_ce: 0.017617
2021-11-30 14:22:51,118 iteration 5964 : loss : 0.027148, loss_ce: 0.013286
2021-11-30 14:22:52,537 iteration 5965 : loss : 0.033114, loss_ce: 0.018250
2021-11-30 14:22:54,078 iteration 5966 : loss : 0.048167, loss_ce: 0.015582
2021-11-30 14:22:55,596 iteration 5967 : loss : 0.027859, loss_ce: 0.014674
 88%|█████████████████████████▍   | 351/400 [2:40:57<22:50, 27.97s/it]2021-11-30 14:22:57,222 iteration 5968 : loss : 0.055760, loss_ce: 0.018114
2021-11-30 14:22:58,684 iteration 5969 : loss : 0.032065, loss_ce: 0.015094
2021-11-30 14:23:00,240 iteration 5970 : loss : 0.029691, loss_ce: 0.014211
2021-11-30 14:23:01,665 iteration 5971 : loss : 0.023735, loss_ce: 0.010511
2021-11-30 14:23:03,060 iteration 5972 : loss : 0.023135, loss_ce: 0.010512
2021-11-30 14:23:04,552 iteration 5973 : loss : 0.035768, loss_ce: 0.019061
2021-11-30 14:23:06,130 iteration 5974 : loss : 0.047702, loss_ce: 0.021586
2021-11-30 14:23:07,546 iteration 5975 : loss : 0.030035, loss_ce: 0.013572
2021-11-30 14:23:08,968 iteration 5976 : loss : 0.030195, loss_ce: 0.012528
2021-11-30 14:23:10,395 iteration 5977 : loss : 0.039677, loss_ce: 0.012958
2021-11-30 14:23:11,824 iteration 5978 : loss : 0.032380, loss_ce: 0.017396
2021-11-30 14:23:13,302 iteration 5979 : loss : 0.026531, loss_ce: 0.013953
2021-11-30 14:23:14,854 iteration 5980 : loss : 0.028312, loss_ce: 0.011511
2021-11-30 14:23:16,323 iteration 5981 : loss : 0.039600, loss_ce: 0.019335
2021-11-30 14:23:17,824 iteration 5982 : loss : 0.031807, loss_ce: 0.015066
2021-11-30 14:23:19,377 iteration 5983 : loss : 0.036629, loss_ce: 0.016432
2021-11-30 14:23:20,820 iteration 5984 : loss : 0.040030, loss_ce: 0.016404
 88%|█████████████████████████▌   | 352/400 [2:41:23<21:43, 27.15s/it]2021-11-30 14:23:22,371 iteration 5985 : loss : 0.025676, loss_ce: 0.010462
2021-11-30 14:23:23,766 iteration 5986 : loss : 0.026391, loss_ce: 0.011209
2021-11-30 14:23:25,280 iteration 5987 : loss : 0.028037, loss_ce: 0.014175
2021-11-30 14:23:26,841 iteration 5988 : loss : 0.041509, loss_ce: 0.018217
2021-11-30 14:23:28,347 iteration 5989 : loss : 0.024368, loss_ce: 0.011500
2021-11-30 14:23:29,809 iteration 5990 : loss : 0.029781, loss_ce: 0.012942
2021-11-30 14:23:31,324 iteration 5991 : loss : 0.028800, loss_ce: 0.011514
2021-11-30 14:23:32,833 iteration 5992 : loss : 0.034637, loss_ce: 0.015096
2021-11-30 14:23:34,307 iteration 5993 : loss : 0.021890, loss_ce: 0.011768
2021-11-30 14:23:35,869 iteration 5994 : loss : 0.047556, loss_ce: 0.023738
2021-11-30 14:23:37,349 iteration 5995 : loss : 0.046608, loss_ce: 0.014713
2021-11-30 14:23:38,786 iteration 5996 : loss : 0.024231, loss_ce: 0.010313
2021-11-30 14:23:40,320 iteration 5997 : loss : 0.040705, loss_ce: 0.017022
2021-11-30 14:23:41,883 iteration 5998 : loss : 0.036231, loss_ce: 0.014269
2021-11-30 14:23:43,478 iteration 5999 : loss : 0.042975, loss_ce: 0.017319
2021-11-30 14:23:44,968 iteration 6000 : loss : 0.046349, loss_ce: 0.022086
2021-11-30 14:23:46,349 iteration 6001 : loss : 0.031965, loss_ce: 0.017821
 88%|█████████████████████████▌   | 353/400 [2:41:48<20:53, 26.66s/it]2021-11-30 14:23:47,914 iteration 6002 : loss : 0.042823, loss_ce: 0.018441
2021-11-30 14:23:49,371 iteration 6003 : loss : 0.023661, loss_ce: 0.011209
2021-11-30 14:23:50,925 iteration 6004 : loss : 0.029914, loss_ce: 0.014859
2021-11-30 14:23:52,449 iteration 6005 : loss : 0.031415, loss_ce: 0.012825
2021-11-30 14:23:54,052 iteration 6006 : loss : 0.031542, loss_ce: 0.013753
2021-11-30 14:23:55,557 iteration 6007 : loss : 0.023281, loss_ce: 0.011766
2021-11-30 14:23:57,039 iteration 6008 : loss : 0.026290, loss_ce: 0.011510
2021-11-30 14:23:58,432 iteration 6009 : loss : 0.024885, loss_ce: 0.013711
2021-11-30 14:23:59,864 iteration 6010 : loss : 0.021935, loss_ce: 0.011044
2021-11-30 14:24:01,410 iteration 6011 : loss : 0.035188, loss_ce: 0.012454
2021-11-30 14:24:02,976 iteration 6012 : loss : 0.036239, loss_ce: 0.012153
2021-11-30 14:24:04,506 iteration 6013 : loss : 0.045423, loss_ce: 0.019336
2021-11-30 14:24:06,024 iteration 6014 : loss : 0.030167, loss_ce: 0.012045
2021-11-30 14:24:07,462 iteration 6015 : loss : 0.040966, loss_ce: 0.019102
2021-11-30 14:24:08,942 iteration 6016 : loss : 0.027543, loss_ce: 0.012415
2021-11-30 14:24:10,417 iteration 6017 : loss : 0.026289, loss_ce: 0.013027
2021-11-30 14:24:11,944 iteration 6018 : loss : 0.044619, loss_ce: 0.017702
 88%|█████████████████████████▋   | 354/400 [2:42:14<20:11, 26.34s/it]2021-11-30 14:24:13,492 iteration 6019 : loss : 0.042700, loss_ce: 0.025781
2021-11-30 14:24:14,959 iteration 6020 : loss : 0.027135, loss_ce: 0.016421
2021-11-30 14:24:16,474 iteration 6021 : loss : 0.075863, loss_ce: 0.021013
2021-11-30 14:24:17,909 iteration 6022 : loss : 0.031055, loss_ce: 0.013626
2021-11-30 14:24:19,461 iteration 6023 : loss : 0.030485, loss_ce: 0.014532
2021-11-30 14:24:20,923 iteration 6024 : loss : 0.028511, loss_ce: 0.012036
2021-11-30 14:24:22,380 iteration 6025 : loss : 0.033262, loss_ce: 0.015066
2021-11-30 14:24:23,893 iteration 6026 : loss : 0.029742, loss_ce: 0.013210
2021-11-30 14:24:25,400 iteration 6027 : loss : 0.036465, loss_ce: 0.015503
2021-11-30 14:24:26,913 iteration 6028 : loss : 0.028133, loss_ce: 0.013895
2021-11-30 14:24:28,363 iteration 6029 : loss : 0.035997, loss_ce: 0.013034
2021-11-30 14:24:29,766 iteration 6030 : loss : 0.026142, loss_ce: 0.014008
2021-11-30 14:24:31,268 iteration 6031 : loss : 0.028638, loss_ce: 0.015688
2021-11-30 14:24:32,692 iteration 6032 : loss : 0.030808, loss_ce: 0.011935
2021-11-30 14:24:34,246 iteration 6033 : loss : 0.037404, loss_ce: 0.016460
2021-11-30 14:24:35,746 iteration 6034 : loss : 0.024049, loss_ce: 0.011536
2021-11-30 14:24:35,746 Training Data Eval:
2021-11-30 14:24:43,112   Average segmentation loss on training set: 0.0140
2021-11-30 14:24:43,113 Validation Data Eval:
2021-11-30 14:24:45,662   Average segmentation loss on validation set: 0.0848
2021-11-30 14:24:47,052 iteration 6035 : loss : 0.020376, loss_ce: 0.010025
 89%|█████████████████████████▋   | 355/400 [2:42:49<21:43, 28.97s/it]2021-11-30 14:24:48,715 iteration 6036 : loss : 0.051108, loss_ce: 0.016127
2021-11-30 14:24:50,257 iteration 6037 : loss : 0.029970, loss_ce: 0.011555
2021-11-30 14:24:51,730 iteration 6038 : loss : 0.024805, loss_ce: 0.011001
2021-11-30 14:24:53,198 iteration 6039 : loss : 0.029121, loss_ce: 0.012649
2021-11-30 14:24:54,706 iteration 6040 : loss : 0.026066, loss_ce: 0.012135
2021-11-30 14:24:56,173 iteration 6041 : loss : 0.025753, loss_ce: 0.011260
2021-11-30 14:24:57,678 iteration 6042 : loss : 0.022779, loss_ce: 0.011836
2021-11-30 14:24:59,110 iteration 6043 : loss : 0.032083, loss_ce: 0.017151
2021-11-30 14:25:00,534 iteration 6044 : loss : 0.024923, loss_ce: 0.012516
2021-11-30 14:25:02,110 iteration 6045 : loss : 0.035471, loss_ce: 0.016073
2021-11-30 14:25:03,540 iteration 6046 : loss : 0.031211, loss_ce: 0.016367
2021-11-30 14:25:04,971 iteration 6047 : loss : 0.020024, loss_ce: 0.010387
2021-11-30 14:25:06,524 iteration 6048 : loss : 0.037838, loss_ce: 0.014068
2021-11-30 14:25:07,925 iteration 6049 : loss : 0.025868, loss_ce: 0.011337
2021-11-30 14:25:09,350 iteration 6050 : loss : 0.031057, loss_ce: 0.011252
2021-11-30 14:25:10,768 iteration 6051 : loss : 0.025158, loss_ce: 0.012671
2021-11-30 14:25:12,301 iteration 6052 : loss : 0.029576, loss_ce: 0.014725
 89%|█████████████████████████▊   | 356/400 [2:43:14<20:25, 27.85s/it]2021-11-30 14:25:13,813 iteration 6053 : loss : 0.028873, loss_ce: 0.011120
2021-11-30 14:25:15,267 iteration 6054 : loss : 0.025914, loss_ce: 0.011355
2021-11-30 14:25:16,770 iteration 6055 : loss : 0.039996, loss_ce: 0.019095
2021-11-30 14:25:18,231 iteration 6056 : loss : 0.030671, loss_ce: 0.013103
2021-11-30 14:25:19,623 iteration 6057 : loss : 0.027835, loss_ce: 0.013036
2021-11-30 14:25:21,036 iteration 6058 : loss : 0.026025, loss_ce: 0.014849
2021-11-30 14:25:22,582 iteration 6059 : loss : 0.033603, loss_ce: 0.013676
2021-11-30 14:25:24,082 iteration 6060 : loss : 0.031290, loss_ce: 0.011299
2021-11-30 14:25:25,501 iteration 6061 : loss : 0.030386, loss_ce: 0.016444
2021-11-30 14:25:27,127 iteration 6062 : loss : 0.041865, loss_ce: 0.016640
2021-11-30 14:25:28,556 iteration 6063 : loss : 0.028999, loss_ce: 0.014848
2021-11-30 14:25:30,030 iteration 6064 : loss : 0.020540, loss_ce: 0.009957
2021-11-30 14:25:31,555 iteration 6065 : loss : 0.045396, loss_ce: 0.019980
2021-11-30 14:25:32,959 iteration 6066 : loss : 0.029445, loss_ce: 0.013741
2021-11-30 14:25:34,462 iteration 6067 : loss : 0.031305, loss_ce: 0.014230
2021-11-30 14:25:35,956 iteration 6068 : loss : 0.038268, loss_ce: 0.010423
2021-11-30 14:25:37,542 iteration 6069 : loss : 0.040238, loss_ce: 0.018736
 89%|█████████████████████████▉   | 357/400 [2:43:39<19:23, 27.07s/it]2021-11-30 14:25:39,091 iteration 6070 : loss : 0.030269, loss_ce: 0.015391
2021-11-30 14:25:40,590 iteration 6071 : loss : 0.033583, loss_ce: 0.016244
2021-11-30 14:25:42,057 iteration 6072 : loss : 0.022348, loss_ce: 0.011111
2021-11-30 14:25:43,546 iteration 6073 : loss : 0.038615, loss_ce: 0.014318
2021-11-30 14:25:44,951 iteration 6074 : loss : 0.039400, loss_ce: 0.018067
2021-11-30 14:25:46,496 iteration 6075 : loss : 0.033213, loss_ce: 0.016634
2021-11-30 14:25:47,988 iteration 6076 : loss : 0.024911, loss_ce: 0.012421
2021-11-30 14:25:49,344 iteration 6077 : loss : 0.020201, loss_ce: 0.010002
2021-11-30 14:25:50,795 iteration 6078 : loss : 0.031198, loss_ce: 0.018293
2021-11-30 14:25:52,332 iteration 6079 : loss : 0.027652, loss_ce: 0.013937
2021-11-30 14:25:53,736 iteration 6080 : loss : 0.020234, loss_ce: 0.009818
2021-11-30 14:25:55,301 iteration 6081 : loss : 0.044352, loss_ce: 0.018798
2021-11-30 14:25:56,756 iteration 6082 : loss : 0.040854, loss_ce: 0.016118
2021-11-30 14:25:58,272 iteration 6083 : loss : 0.039862, loss_ce: 0.013646
2021-11-30 14:25:59,710 iteration 6084 : loss : 0.037336, loss_ce: 0.013570
2021-11-30 14:26:01,140 iteration 6085 : loss : 0.026076, loss_ce: 0.011851
2021-11-30 14:26:02,797 iteration 6086 : loss : 0.044209, loss_ce: 0.015604
 90%|█████████████████████████▉   | 358/400 [2:44:05<18:34, 26.53s/it]2021-11-30 14:26:04,273 iteration 6087 : loss : 0.028157, loss_ce: 0.013227
2021-11-30 14:26:05,750 iteration 6088 : loss : 0.030089, loss_ce: 0.015463
2021-11-30 14:26:07,201 iteration 6089 : loss : 0.035965, loss_ce: 0.019704
2021-11-30 14:26:08,673 iteration 6090 : loss : 0.026056, loss_ce: 0.009684
2021-11-30 14:26:10,167 iteration 6091 : loss : 0.025106, loss_ce: 0.013692
2021-11-30 14:26:11,685 iteration 6092 : loss : 0.022150, loss_ce: 0.011121
2021-11-30 14:26:13,077 iteration 6093 : loss : 0.028770, loss_ce: 0.011947
2021-11-30 14:26:14,505 iteration 6094 : loss : 0.027789, loss_ce: 0.011072
2021-11-30 14:26:16,021 iteration 6095 : loss : 0.027481, loss_ce: 0.013340
2021-11-30 14:26:17,537 iteration 6096 : loss : 0.036217, loss_ce: 0.013548
2021-11-30 14:26:18,951 iteration 6097 : loss : 0.028167, loss_ce: 0.015140
2021-11-30 14:26:20,445 iteration 6098 : loss : 0.036481, loss_ce: 0.016593
2021-11-30 14:26:21,981 iteration 6099 : loss : 0.050530, loss_ce: 0.016479
2021-11-30 14:26:23,549 iteration 6100 : loss : 0.044576, loss_ce: 0.018653
2021-11-30 14:26:25,081 iteration 6101 : loss : 0.032536, loss_ce: 0.015285
2021-11-30 14:26:26,523 iteration 6102 : loss : 0.028668, loss_ce: 0.011531
2021-11-30 14:26:27,893 iteration 6103 : loss : 0.018462, loss_ce: 0.009997
 90%|██████████████████████████   | 359/400 [2:44:30<17:50, 26.10s/it]2021-11-30 14:26:29,486 iteration 6104 : loss : 0.034047, loss_ce: 0.013449
2021-11-30 14:26:30,906 iteration 6105 : loss : 0.040137, loss_ce: 0.014797
2021-11-30 14:26:32,371 iteration 6106 : loss : 0.027711, loss_ce: 0.011759
2021-11-30 14:26:33,793 iteration 6107 : loss : 0.023875, loss_ce: 0.010010
2021-11-30 14:26:35,337 iteration 6108 : loss : 0.030875, loss_ce: 0.013481
2021-11-30 14:26:36,798 iteration 6109 : loss : 0.022717, loss_ce: 0.010813
2021-11-30 14:26:38,298 iteration 6110 : loss : 0.023799, loss_ce: 0.011919
2021-11-30 14:26:39,840 iteration 6111 : loss : 0.037878, loss_ce: 0.015939
2021-11-30 14:26:41,344 iteration 6112 : loss : 0.028839, loss_ce: 0.013141
2021-11-30 14:26:42,882 iteration 6113 : loss : 0.038699, loss_ce: 0.015097
2021-11-30 14:26:44,339 iteration 6114 : loss : 0.036676, loss_ce: 0.018764
2021-11-30 14:26:45,921 iteration 6115 : loss : 0.041097, loss_ce: 0.018057
2021-11-30 14:26:47,440 iteration 6116 : loss : 0.026033, loss_ce: 0.013339
2021-11-30 14:26:48,896 iteration 6117 : loss : 0.036259, loss_ce: 0.017856
2021-11-30 14:26:50,430 iteration 6118 : loss : 0.027045, loss_ce: 0.013270
2021-11-30 14:26:51,896 iteration 6119 : loss : 0.043343, loss_ce: 0.015448
2021-11-30 14:26:51,896 Training Data Eval:
2021-11-30 14:26:59,290   Average segmentation loss on training set: 0.0143
2021-11-30 14:26:59,290 Validation Data Eval:
2021-11-30 14:27:01,849   Average segmentation loss on validation set: 0.0855
2021-11-30 14:27:03,314 iteration 6120 : loss : 0.022530, loss_ce: 0.012139
 90%|██████████████████████████   | 360/400 [2:45:05<19:15, 28.90s/it]2021-11-30 14:27:04,879 iteration 6121 : loss : 0.031305, loss_ce: 0.016572
2021-11-30 14:27:06,414 iteration 6122 : loss : 0.036216, loss_ce: 0.015092
2021-11-30 14:27:07,964 iteration 6123 : loss : 0.026326, loss_ce: 0.013119
2021-11-30 14:27:09,448 iteration 6124 : loss : 0.028254, loss_ce: 0.011886
2021-11-30 14:27:11,029 iteration 6125 : loss : 0.035211, loss_ce: 0.015074
2021-11-30 14:27:12,502 iteration 6126 : loss : 0.046723, loss_ce: 0.014664
2021-11-30 14:27:13,948 iteration 6127 : loss : 0.033194, loss_ce: 0.014187
2021-11-30 14:27:15,504 iteration 6128 : loss : 0.032815, loss_ce: 0.013165
2021-11-30 14:27:16,998 iteration 6129 : loss : 0.031210, loss_ce: 0.014154
2021-11-30 14:27:18,573 iteration 6130 : loss : 0.034462, loss_ce: 0.015287
2021-11-30 14:27:20,081 iteration 6131 : loss : 0.036920, loss_ce: 0.020617
2021-11-30 14:27:21,588 iteration 6132 : loss : 0.050612, loss_ce: 0.016766
2021-11-30 14:27:23,047 iteration 6133 : loss : 0.024589, loss_ce: 0.015465
2021-11-30 14:27:24,469 iteration 6134 : loss : 0.019181, loss_ce: 0.010183
2021-11-30 14:27:25,889 iteration 6135 : loss : 0.022759, loss_ce: 0.011131
2021-11-30 14:27:27,404 iteration 6136 : loss : 0.033112, loss_ce: 0.013347
2021-11-30 14:27:28,864 iteration 6137 : loss : 0.024829, loss_ce: 0.011706
 90%|██████████████████████████▏  | 361/400 [2:45:31<18:07, 27.89s/it]2021-11-30 14:27:30,358 iteration 6138 : loss : 0.019345, loss_ce: 0.009737
2021-11-30 14:27:31,885 iteration 6139 : loss : 0.030255, loss_ce: 0.014587
2021-11-30 14:27:33,348 iteration 6140 : loss : 0.025431, loss_ce: 0.011727
2021-11-30 14:27:34,851 iteration 6141 : loss : 0.025829, loss_ce: 0.012226
2021-11-30 14:27:36,497 iteration 6142 : loss : 0.039898, loss_ce: 0.015415
2021-11-30 14:27:37,908 iteration 6143 : loss : 0.032065, loss_ce: 0.018145
2021-11-30 14:27:39,418 iteration 6144 : loss : 0.030039, loss_ce: 0.015196
2021-11-30 14:27:40,920 iteration 6145 : loss : 0.032995, loss_ce: 0.013891
2021-11-30 14:27:42,396 iteration 6146 : loss : 0.030229, loss_ce: 0.015881
2021-11-30 14:27:43,866 iteration 6147 : loss : 0.021459, loss_ce: 0.008852
2021-11-30 14:27:45,286 iteration 6148 : loss : 0.024181, loss_ce: 0.010575
2021-11-30 14:27:46,698 iteration 6149 : loss : 0.040333, loss_ce: 0.015949
2021-11-30 14:27:48,143 iteration 6150 : loss : 0.034269, loss_ce: 0.013458
2021-11-30 14:27:49,616 iteration 6151 : loss : 0.033635, loss_ce: 0.011895
2021-11-30 14:27:51,076 iteration 6152 : loss : 0.040355, loss_ce: 0.018085
2021-11-30 14:27:52,647 iteration 6153 : loss : 0.036614, loss_ce: 0.018831
2021-11-30 14:27:54,239 iteration 6154 : loss : 0.047390, loss_ce: 0.021063
 90%|██████████████████████████▏  | 362/400 [2:45:56<17:11, 27.13s/it]2021-11-30 14:27:55,735 iteration 6155 : loss : 0.030754, loss_ce: 0.016040
2021-11-30 14:27:57,213 iteration 6156 : loss : 0.019767, loss_ce: 0.010506
2021-11-30 14:27:58,743 iteration 6157 : loss : 0.028559, loss_ce: 0.012251
2021-11-30 14:28:00,196 iteration 6158 : loss : 0.026737, loss_ce: 0.011343
2021-11-30 14:28:01,587 iteration 6159 : loss : 0.020304, loss_ce: 0.010716
2021-11-30 14:28:03,302 iteration 6160 : loss : 0.038430, loss_ce: 0.019350
2021-11-30 14:28:04,691 iteration 6161 : loss : 0.026992, loss_ce: 0.011530
2021-11-30 14:28:06,190 iteration 6162 : loss : 0.023830, loss_ce: 0.010710
2021-11-30 14:28:07,664 iteration 6163 : loss : 0.023528, loss_ce: 0.010862
2021-11-30 14:28:09,105 iteration 6164 : loss : 0.024746, loss_ce: 0.010209
2021-11-30 14:28:10,610 iteration 6165 : loss : 0.026478, loss_ce: 0.012546
2021-11-30 14:28:12,144 iteration 6166 : loss : 0.029620, loss_ce: 0.015898
2021-11-30 14:28:13,542 iteration 6167 : loss : 0.019818, loss_ce: 0.009286
2021-11-30 14:28:15,150 iteration 6168 : loss : 0.039964, loss_ce: 0.014339
2021-11-30 14:28:16,626 iteration 6169 : loss : 0.027149, loss_ce: 0.012656
2021-11-30 14:28:18,127 iteration 6170 : loss : 0.027265, loss_ce: 0.014149
2021-11-30 14:28:19,607 iteration 6171 : loss : 0.029070, loss_ce: 0.011227
 91%|██████████████████████████▎  | 363/400 [2:46:21<16:24, 26.60s/it]2021-11-30 14:28:21,123 iteration 6172 : loss : 0.034270, loss_ce: 0.014215
2021-11-30 14:28:22,491 iteration 6173 : loss : 0.028744, loss_ce: 0.012411
2021-11-30 14:28:23,946 iteration 6174 : loss : 0.036658, loss_ce: 0.016943
2021-11-30 14:28:25,367 iteration 6175 : loss : 0.020996, loss_ce: 0.008817
2021-11-30 14:28:26,883 iteration 6176 : loss : 0.030324, loss_ce: 0.016402
2021-11-30 14:28:28,499 iteration 6177 : loss : 0.039594, loss_ce: 0.020058
2021-11-30 14:28:30,052 iteration 6178 : loss : 0.032749, loss_ce: 0.013736
2021-11-30 14:28:31,569 iteration 6179 : loss : 0.032385, loss_ce: 0.013109
2021-11-30 14:28:33,095 iteration 6180 : loss : 0.053391, loss_ce: 0.011850
2021-11-30 14:28:34,554 iteration 6181 : loss : 0.032048, loss_ce: 0.015203
2021-11-30 14:28:36,051 iteration 6182 : loss : 0.021487, loss_ce: 0.010582
2021-11-30 14:28:37,489 iteration 6183 : loss : 0.040727, loss_ce: 0.016727
2021-11-30 14:28:38,916 iteration 6184 : loss : 0.024338, loss_ce: 0.011538
2021-11-30 14:28:40,446 iteration 6185 : loss : 0.040630, loss_ce: 0.014186
2021-11-30 14:28:41,864 iteration 6186 : loss : 0.021436, loss_ce: 0.010545
2021-11-30 14:28:43,317 iteration 6187 : loss : 0.038954, loss_ce: 0.015688
2021-11-30 14:28:44,758 iteration 6188 : loss : 0.036717, loss_ce: 0.015136
 91%|██████████████████████████▍  | 364/400 [2:46:47<15:42, 26.17s/it]2021-11-30 14:28:46,407 iteration 6189 : loss : 0.034691, loss_ce: 0.015515
2021-11-30 14:28:47,892 iteration 6190 : loss : 0.027393, loss_ce: 0.013452
2021-11-30 14:28:49,457 iteration 6191 : loss : 0.025174, loss_ce: 0.010731
2021-11-30 14:28:50,973 iteration 6192 : loss : 0.047001, loss_ce: 0.019030
2021-11-30 14:28:52,396 iteration 6193 : loss : 0.021970, loss_ce: 0.009626
2021-11-30 14:28:53,925 iteration 6194 : loss : 0.025498, loss_ce: 0.012367
2021-11-30 14:28:55,481 iteration 6195 : loss : 0.032855, loss_ce: 0.014306
2021-11-30 14:28:56,926 iteration 6196 : loss : 0.035675, loss_ce: 0.013241
2021-11-30 14:28:58,518 iteration 6197 : loss : 0.033654, loss_ce: 0.013473
2021-11-30 14:29:00,097 iteration 6198 : loss : 0.029990, loss_ce: 0.014520
2021-11-30 14:29:01,550 iteration 6199 : loss : 0.025180, loss_ce: 0.012139
2021-11-30 14:29:03,063 iteration 6200 : loss : 0.039942, loss_ce: 0.018188
2021-11-30 14:29:04,558 iteration 6201 : loss : 0.041456, loss_ce: 0.018044
2021-11-30 14:29:06,036 iteration 6202 : loss : 0.029229, loss_ce: 0.011631
2021-11-30 14:29:07,513 iteration 6203 : loss : 0.032789, loss_ce: 0.013452
2021-11-30 14:29:09,045 iteration 6204 : loss : 0.026842, loss_ce: 0.014821
2021-11-30 14:29:09,045 Training Data Eval:
2021-11-30 14:29:16,409   Average segmentation loss on training set: 0.0141
2021-11-30 14:29:16,410 Validation Data Eval:
2021-11-30 14:29:18,961   Average segmentation loss on validation set: 0.0859
2021-11-30 14:29:20,574 iteration 6205 : loss : 0.043905, loss_ce: 0.022497
 91%|██████████████████████████▍  | 365/400 [2:47:22<16:57, 29.06s/it]2021-11-30 14:29:22,138 iteration 6206 : loss : 0.028041, loss_ce: 0.016477
2021-11-30 14:29:23,620 iteration 6207 : loss : 0.037961, loss_ce: 0.022922
2021-11-30 14:29:25,142 iteration 6208 : loss : 0.031346, loss_ce: 0.013374
2021-11-30 14:29:26,656 iteration 6209 : loss : 0.045807, loss_ce: 0.019548
2021-11-30 14:29:28,105 iteration 6210 : loss : 0.042183, loss_ce: 0.017474
2021-11-30 14:29:29,525 iteration 6211 : loss : 0.022131, loss_ce: 0.010769
2021-11-30 14:29:30,978 iteration 6212 : loss : 0.028758, loss_ce: 0.012871
2021-11-30 14:29:32,465 iteration 6213 : loss : 0.023443, loss_ce: 0.011918
2021-11-30 14:29:33,945 iteration 6214 : loss : 0.023604, loss_ce: 0.011671
2021-11-30 14:29:35,516 iteration 6215 : loss : 0.036893, loss_ce: 0.014108
2021-11-30 14:29:37,008 iteration 6216 : loss : 0.019422, loss_ce: 0.010674
2021-11-30 14:29:38,420 iteration 6217 : loss : 0.035248, loss_ce: 0.014069
2021-11-30 14:29:39,919 iteration 6218 : loss : 0.036628, loss_ce: 0.016749
2021-11-30 14:29:41,395 iteration 6219 : loss : 0.026244, loss_ce: 0.011015
2021-11-30 14:29:42,828 iteration 6220 : loss : 0.031121, loss_ce: 0.010982
2021-11-30 14:29:44,337 iteration 6221 : loss : 0.036007, loss_ce: 0.012540
2021-11-30 14:29:45,771 iteration 6222 : loss : 0.024094, loss_ce: 0.013799
 92%|██████████████████████████▌  | 366/400 [2:47:48<15:48, 27.90s/it]2021-11-30 14:29:47,358 iteration 6223 : loss : 0.038545, loss_ce: 0.019097
2021-11-30 14:29:48,838 iteration 6224 : loss : 0.027991, loss_ce: 0.014159
2021-11-30 14:29:50,309 iteration 6225 : loss : 0.041116, loss_ce: 0.017471
2021-11-30 14:29:51,752 iteration 6226 : loss : 0.036766, loss_ce: 0.014532
2021-11-30 14:29:53,185 iteration 6227 : loss : 0.027268, loss_ce: 0.013645
2021-11-30 14:29:54,692 iteration 6228 : loss : 0.025618, loss_ce: 0.011324
2021-11-30 14:29:56,194 iteration 6229 : loss : 0.020602, loss_ce: 0.011902
2021-11-30 14:29:57,709 iteration 6230 : loss : 0.027155, loss_ce: 0.011972
2021-11-30 14:29:59,253 iteration 6231 : loss : 0.046531, loss_ce: 0.020202
2021-11-30 14:30:00,763 iteration 6232 : loss : 0.022744, loss_ce: 0.011713
2021-11-30 14:30:02,236 iteration 6233 : loss : 0.025716, loss_ce: 0.012863
2021-11-30 14:30:03,684 iteration 6234 : loss : 0.031675, loss_ce: 0.010050
2021-11-30 14:30:05,134 iteration 6235 : loss : 0.031239, loss_ce: 0.019106
2021-11-30 14:30:06,621 iteration 6236 : loss : 0.032531, loss_ce: 0.017544
2021-11-30 14:30:08,132 iteration 6237 : loss : 0.036818, loss_ce: 0.018067
2021-11-30 14:30:09,692 iteration 6238 : loss : 0.031816, loss_ce: 0.012311
2021-11-30 14:30:11,194 iteration 6239 : loss : 0.032857, loss_ce: 0.012755
 92%|██████████████████████████▌  | 367/400 [2:48:13<14:56, 27.16s/it]2021-11-30 14:30:12,720 iteration 6240 : loss : 0.022084, loss_ce: 0.009526
2021-11-30 14:30:14,175 iteration 6241 : loss : 0.024469, loss_ce: 0.014196
2021-11-30 14:30:15,741 iteration 6242 : loss : 0.045723, loss_ce: 0.017933
2021-11-30 14:30:17,224 iteration 6243 : loss : 0.025898, loss_ce: 0.012536
2021-11-30 14:30:18,814 iteration 6244 : loss : 0.033443, loss_ce: 0.017906
2021-11-30 14:30:20,289 iteration 6245 : loss : 0.027015, loss_ce: 0.012481
2021-11-30 14:30:21,748 iteration 6246 : loss : 0.029297, loss_ce: 0.014774
2021-11-30 14:30:23,232 iteration 6247 : loss : 0.034694, loss_ce: 0.012238
2021-11-30 14:30:24,689 iteration 6248 : loss : 0.019707, loss_ce: 0.009894
2021-11-30 14:30:26,160 iteration 6249 : loss : 0.034811, loss_ce: 0.012926
2021-11-30 14:30:27,683 iteration 6250 : loss : 0.027275, loss_ce: 0.013031
2021-11-30 14:30:29,263 iteration 6251 : loss : 0.042274, loss_ce: 0.017878
2021-11-30 14:30:30,693 iteration 6252 : loss : 0.024476, loss_ce: 0.013269
2021-11-30 14:30:32,252 iteration 6253 : loss : 0.027972, loss_ce: 0.012036
2021-11-30 14:30:33,712 iteration 6254 : loss : 0.026474, loss_ce: 0.014215
2021-11-30 14:30:35,225 iteration 6255 : loss : 0.026549, loss_ce: 0.014127
2021-11-30 14:30:36,784 iteration 6256 : loss : 0.049288, loss_ce: 0.024631
 92%|██████████████████████████▋  | 368/400 [2:48:39<14:13, 26.69s/it]2021-11-30 14:30:38,363 iteration 6257 : loss : 0.041586, loss_ce: 0.015157
2021-11-30 14:30:39,871 iteration 6258 : loss : 0.031694, loss_ce: 0.014875
2021-11-30 14:30:41,418 iteration 6259 : loss : 0.040149, loss_ce: 0.017698
2021-11-30 14:30:42,888 iteration 6260 : loss : 0.033874, loss_ce: 0.015342
2021-11-30 14:30:44,340 iteration 6261 : loss : 0.031293, loss_ce: 0.014581
2021-11-30 14:30:45,856 iteration 6262 : loss : 0.046955, loss_ce: 0.017055
2021-11-30 14:30:47,383 iteration 6263 : loss : 0.040753, loss_ce: 0.012302
2021-11-30 14:30:48,887 iteration 6264 : loss : 0.024189, loss_ce: 0.012395
2021-11-30 14:30:50,396 iteration 6265 : loss : 0.028211, loss_ce: 0.012615
2021-11-30 14:30:51,854 iteration 6266 : loss : 0.037525, loss_ce: 0.020056
2021-11-30 14:30:53,306 iteration 6267 : loss : 0.032032, loss_ce: 0.015661
2021-11-30 14:30:54,796 iteration 6268 : loss : 0.028169, loss_ce: 0.013465
2021-11-30 14:30:56,148 iteration 6269 : loss : 0.019363, loss_ce: 0.009777
2021-11-30 14:30:57,629 iteration 6270 : loss : 0.021449, loss_ce: 0.011804
2021-11-30 14:30:59,109 iteration 6271 : loss : 0.035765, loss_ce: 0.014997
2021-11-30 14:31:00,661 iteration 6272 : loss : 0.040889, loss_ce: 0.021178
2021-11-30 14:31:02,189 iteration 6273 : loss : 0.021513, loss_ce: 0.011402
 92%|██████████████████████████▊  | 369/400 [2:49:04<13:35, 26.30s/it]2021-11-30 14:31:03,870 iteration 6274 : loss : 0.041851, loss_ce: 0.017137
2021-11-30 14:31:05,374 iteration 6275 : loss : 0.027385, loss_ce: 0.015767
2021-11-30 14:31:06,854 iteration 6276 : loss : 0.030913, loss_ce: 0.015867
2021-11-30 14:31:08,296 iteration 6277 : loss : 0.026792, loss_ce: 0.009813
2021-11-30 14:31:09,788 iteration 6278 : loss : 0.046665, loss_ce: 0.016376
2021-11-30 14:31:11,266 iteration 6279 : loss : 0.036167, loss_ce: 0.019485
2021-11-30 14:31:12,718 iteration 6280 : loss : 0.031567, loss_ce: 0.013787
2021-11-30 14:31:14,363 iteration 6281 : loss : 0.049941, loss_ce: 0.015179
2021-11-30 14:31:15,830 iteration 6282 : loss : 0.022212, loss_ce: 0.010863
2021-11-30 14:31:17,268 iteration 6283 : loss : 0.020370, loss_ce: 0.011025
2021-11-30 14:31:18,761 iteration 6284 : loss : 0.031236, loss_ce: 0.015139
2021-11-30 14:31:20,307 iteration 6285 : loss : 0.029778, loss_ce: 0.013169
2021-11-30 14:31:21,760 iteration 6286 : loss : 0.028319, loss_ce: 0.011668
2021-11-30 14:31:23,215 iteration 6287 : loss : 0.034379, loss_ce: 0.011787
2021-11-30 14:31:24,607 iteration 6288 : loss : 0.022438, loss_ce: 0.010800
2021-11-30 14:31:26,115 iteration 6289 : loss : 0.027975, loss_ce: 0.013725
2021-11-30 14:31:26,115 Training Data Eval:
2021-11-30 14:31:33,498   Average segmentation loss on training set: 0.0139
2021-11-30 14:31:33,498 Validation Data Eval:
2021-11-30 14:31:36,070   Average segmentation loss on validation set: 0.0836
2021-11-30 14:31:37,620 iteration 6290 : loss : 0.026540, loss_ce: 0.013393
 92%|██████████████████████████▊  | 370/400 [2:49:39<14:31, 29.04s/it]2021-11-30 14:31:39,169 iteration 6291 : loss : 0.019652, loss_ce: 0.009966
2021-11-30 14:31:40,654 iteration 6292 : loss : 0.026282, loss_ce: 0.011637
2021-11-30 14:31:42,170 iteration 6293 : loss : 0.043202, loss_ce: 0.021809
2021-11-30 14:31:43,701 iteration 6294 : loss : 0.041132, loss_ce: 0.014866
2021-11-30 14:31:45,181 iteration 6295 : loss : 0.027194, loss_ce: 0.014143
2021-11-30 14:31:46,754 iteration 6296 : loss : 0.037977, loss_ce: 0.016949
2021-11-30 14:31:48,288 iteration 6297 : loss : 0.037514, loss_ce: 0.016083
2021-11-30 14:31:49,813 iteration 6298 : loss : 0.038909, loss_ce: 0.017190
2021-11-30 14:31:51,288 iteration 6299 : loss : 0.034250, loss_ce: 0.013991
2021-11-30 14:31:52,734 iteration 6300 : loss : 0.052842, loss_ce: 0.019294
2021-11-30 14:31:54,183 iteration 6301 : loss : 0.031118, loss_ce: 0.020453
2021-11-30 14:31:55,690 iteration 6302 : loss : 0.029919, loss_ce: 0.013325
2021-11-30 14:31:57,106 iteration 6303 : loss : 0.025277, loss_ce: 0.009121
2021-11-30 14:31:58,522 iteration 6304 : loss : 0.033046, loss_ce: 0.013575
2021-11-30 14:32:00,053 iteration 6305 : loss : 0.038482, loss_ce: 0.019955
2021-11-30 14:32:01,530 iteration 6306 : loss : 0.050246, loss_ce: 0.014867
2021-11-30 14:32:03,014 iteration 6307 : loss : 0.024544, loss_ce: 0.013392
 93%|██████████████████████████▉  | 371/400 [2:50:05<13:30, 27.95s/it]2021-11-30 14:32:04,501 iteration 6308 : loss : 0.029764, loss_ce: 0.009998
2021-11-30 14:32:06,006 iteration 6309 : loss : 0.040693, loss_ce: 0.018747
2021-11-30 14:32:07,474 iteration 6310 : loss : 0.030733, loss_ce: 0.015305
2021-11-30 14:32:08,992 iteration 6311 : loss : 0.022180, loss_ce: 0.011206
2021-11-30 14:32:10,513 iteration 6312 : loss : 0.034444, loss_ce: 0.014910
2021-11-30 14:32:12,093 iteration 6313 : loss : 0.037805, loss_ce: 0.019643
2021-11-30 14:32:13,600 iteration 6314 : loss : 0.024509, loss_ce: 0.011042
2021-11-30 14:32:15,045 iteration 6315 : loss : 0.027736, loss_ce: 0.011779
2021-11-30 14:32:16,532 iteration 6316 : loss : 0.022650, loss_ce: 0.011016
2021-11-30 14:32:17,998 iteration 6317 : loss : 0.039561, loss_ce: 0.014368
2021-11-30 14:32:19,551 iteration 6318 : loss : 0.044409, loss_ce: 0.017061
2021-11-30 14:32:21,063 iteration 6319 : loss : 0.025414, loss_ce: 0.011437
2021-11-30 14:32:22,524 iteration 6320 : loss : 0.030079, loss_ce: 0.011089
2021-11-30 14:32:24,064 iteration 6321 : loss : 0.029651, loss_ce: 0.016300
2021-11-30 14:32:25,601 iteration 6322 : loss : 0.035614, loss_ce: 0.018245
2021-11-30 14:32:27,270 iteration 6323 : loss : 0.046046, loss_ce: 0.017432
2021-11-30 14:32:28,830 iteration 6324 : loss : 0.033471, loss_ce: 0.019174
 93%|██████████████████████████▉  | 372/400 [2:50:31<12:44, 27.31s/it]2021-11-30 14:32:30,395 iteration 6325 : loss : 0.030350, loss_ce: 0.011419
2021-11-30 14:32:31,835 iteration 6326 : loss : 0.031714, loss_ce: 0.016804
2021-11-30 14:32:33,325 iteration 6327 : loss : 0.029265, loss_ce: 0.012897
2021-11-30 14:32:34,876 iteration 6328 : loss : 0.023647, loss_ce: 0.012284
2021-11-30 14:32:36,285 iteration 6329 : loss : 0.026356, loss_ce: 0.015345
2021-11-30 14:32:37,776 iteration 6330 : loss : 0.037403, loss_ce: 0.017137
2021-11-30 14:32:39,326 iteration 6331 : loss : 0.023518, loss_ce: 0.011085
2021-11-30 14:32:40,830 iteration 6332 : loss : 0.025449, loss_ce: 0.010644
2021-11-30 14:32:42,325 iteration 6333 : loss : 0.022017, loss_ce: 0.012026
2021-11-30 14:32:43,824 iteration 6334 : loss : 0.046489, loss_ce: 0.015256
2021-11-30 14:32:45,399 iteration 6335 : loss : 0.026058, loss_ce: 0.013761
2021-11-30 14:32:46,847 iteration 6336 : loss : 0.025610, loss_ce: 0.012595
2021-11-30 14:32:48,361 iteration 6337 : loss : 0.027520, loss_ce: 0.012019
2021-11-30 14:32:49,830 iteration 6338 : loss : 0.038131, loss_ce: 0.013906
2021-11-30 14:32:51,312 iteration 6339 : loss : 0.026445, loss_ce: 0.011194
2021-11-30 14:32:52,793 iteration 6340 : loss : 0.031142, loss_ce: 0.015653
2021-11-30 14:32:54,204 iteration 6341 : loss : 0.028270, loss_ce: 0.013932
 93%|███████████████████████████  | 373/400 [2:50:56<12:01, 26.73s/it]2021-11-30 14:32:55,810 iteration 6342 : loss : 0.043697, loss_ce: 0.016713
2021-11-30 14:32:57,278 iteration 6343 : loss : 0.025598, loss_ce: 0.011515
2021-11-30 14:32:58,748 iteration 6344 : loss : 0.032589, loss_ce: 0.012237
2021-11-30 14:33:00,211 iteration 6345 : loss : 0.056151, loss_ce: 0.009168
2021-11-30 14:33:01,713 iteration 6346 : loss : 0.044359, loss_ce: 0.018694
2021-11-30 14:33:03,197 iteration 6347 : loss : 0.027856, loss_ce: 0.012734
2021-11-30 14:33:04,581 iteration 6348 : loss : 0.018359, loss_ce: 0.010194
2021-11-30 14:33:06,092 iteration 6349 : loss : 0.038059, loss_ce: 0.018432
2021-11-30 14:33:07,714 iteration 6350 : loss : 0.050269, loss_ce: 0.019218
2021-11-30 14:33:09,255 iteration 6351 : loss : 0.025752, loss_ce: 0.013723
2021-11-30 14:33:10,724 iteration 6352 : loss : 0.029929, loss_ce: 0.011828
2021-11-30 14:33:12,213 iteration 6353 : loss : 0.046791, loss_ce: 0.019178
2021-11-30 14:33:13,680 iteration 6354 : loss : 0.034158, loss_ce: 0.016746
2021-11-30 14:33:15,144 iteration 6355 : loss : 0.027180, loss_ce: 0.011837
2021-11-30 14:33:16,703 iteration 6356 : loss : 0.030449, loss_ce: 0.011045
2021-11-30 14:33:18,155 iteration 6357 : loss : 0.036794, loss_ce: 0.015988
2021-11-30 14:33:19,769 iteration 6358 : loss : 0.033800, loss_ce: 0.016529
 94%|███████████████████████████  | 374/400 [2:51:22<11:25, 26.38s/it]2021-11-30 14:33:21,377 iteration 6359 : loss : 0.040896, loss_ce: 0.014554
2021-11-30 14:33:22,820 iteration 6360 : loss : 0.029163, loss_ce: 0.015288
2021-11-30 14:33:24,333 iteration 6361 : loss : 0.044527, loss_ce: 0.017955
2021-11-30 14:33:25,872 iteration 6362 : loss : 0.037558, loss_ce: 0.017421
2021-11-30 14:33:27,422 iteration 6363 : loss : 0.039232, loss_ce: 0.017347
2021-11-30 14:33:28,869 iteration 6364 : loss : 0.030390, loss_ce: 0.011004
2021-11-30 14:33:30,357 iteration 6365 : loss : 0.050235, loss_ce: 0.027994
2021-11-30 14:33:31,930 iteration 6366 : loss : 0.032777, loss_ce: 0.015384
2021-11-30 14:33:33,543 iteration 6367 : loss : 0.035557, loss_ce: 0.015934
2021-11-30 14:33:35,077 iteration 6368 : loss : 0.036197, loss_ce: 0.017237
2021-11-30 14:33:36,549 iteration 6369 : loss : 0.026356, loss_ce: 0.013556
2021-11-30 14:33:38,037 iteration 6370 : loss : 0.032800, loss_ce: 0.014451
2021-11-30 14:33:39,640 iteration 6371 : loss : 0.040451, loss_ce: 0.017477
2021-11-30 14:33:41,089 iteration 6372 : loss : 0.016329, loss_ce: 0.009232
2021-11-30 14:33:42,545 iteration 6373 : loss : 0.024249, loss_ce: 0.012012
2021-11-30 14:33:44,048 iteration 6374 : loss : 0.035175, loss_ce: 0.016345
2021-11-30 14:33:44,048 Training Data Eval:
2021-11-30 14:33:51,407   Average segmentation loss on training set: 0.0139
2021-11-30 14:33:51,407 Validation Data Eval:
2021-11-30 14:33:53,971   Average segmentation loss on validation set: 0.0791
2021-11-30 14:33:55,498 iteration 6375 : loss : 0.041219, loss_ce: 0.015457
 94%|███████████████████████████▏ | 375/400 [2:51:57<12:09, 29.18s/it]2021-11-30 14:33:57,100 iteration 6376 : loss : 0.042189, loss_ce: 0.015413
2021-11-30 14:33:58,597 iteration 6377 : loss : 0.034595, loss_ce: 0.014745
2021-11-30 14:33:59,990 iteration 6378 : loss : 0.026606, loss_ce: 0.013841
2021-11-30 14:34:01,466 iteration 6379 : loss : 0.043483, loss_ce: 0.015355
2021-11-30 14:34:02,909 iteration 6380 : loss : 0.030988, loss_ce: 0.014147
2021-11-30 14:34:04,365 iteration 6381 : loss : 0.022591, loss_ce: 0.009962
2021-11-30 14:34:05,760 iteration 6382 : loss : 0.028777, loss_ce: 0.012808
2021-11-30 14:34:07,301 iteration 6383 : loss : 0.038676, loss_ce: 0.017641
2021-11-30 14:34:08,760 iteration 6384 : loss : 0.025067, loss_ce: 0.012843
2021-11-30 14:34:10,288 iteration 6385 : loss : 0.031039, loss_ce: 0.012053
2021-11-30 14:34:11,821 iteration 6386 : loss : 0.039421, loss_ce: 0.014305
2021-11-30 14:34:13,409 iteration 6387 : loss : 0.040976, loss_ce: 0.021127
2021-11-30 14:34:14,936 iteration 6388 : loss : 0.030587, loss_ce: 0.015047
2021-11-30 14:34:16,310 iteration 6389 : loss : 0.028054, loss_ce: 0.012450
2021-11-30 14:34:17,801 iteration 6390 : loss : 0.036166, loss_ce: 0.018443
2021-11-30 14:34:19,246 iteration 6391 : loss : 0.027760, loss_ce: 0.016387
2021-11-30 14:34:20,818 iteration 6392 : loss : 0.042223, loss_ce: 0.015753
 94%|███████████████████████████▎ | 376/400 [2:52:23<11:12, 28.03s/it]2021-11-30 14:34:22,348 iteration 6393 : loss : 0.032666, loss_ce: 0.015612
2021-11-30 14:34:23,886 iteration 6394 : loss : 0.039341, loss_ce: 0.016748
2021-11-30 14:34:25,356 iteration 6395 : loss : 0.020485, loss_ce: 0.009703
2021-11-30 14:34:26,831 iteration 6396 : loss : 0.038653, loss_ce: 0.014111
2021-11-30 14:34:28,375 iteration 6397 : loss : 0.035538, loss_ce: 0.016864
2021-11-30 14:34:29,899 iteration 6398 : loss : 0.028830, loss_ce: 0.012025
2021-11-30 14:34:31,398 iteration 6399 : loss : 0.036164, loss_ce: 0.013817
2021-11-30 14:34:32,845 iteration 6400 : loss : 0.024895, loss_ce: 0.012302
2021-11-30 14:34:34,363 iteration 6401 : loss : 0.026149, loss_ce: 0.012309
2021-11-30 14:34:35,843 iteration 6402 : loss : 0.033026, loss_ce: 0.019449
2021-11-30 14:34:37,429 iteration 6403 : loss : 0.032548, loss_ce: 0.016259
2021-11-30 14:34:38,912 iteration 6404 : loss : 0.023261, loss_ce: 0.011572
2021-11-30 14:34:40,365 iteration 6405 : loss : 0.042467, loss_ce: 0.014832
2021-11-30 14:34:42,024 iteration 6406 : loss : 0.059023, loss_ce: 0.018903
2021-11-30 14:34:43,556 iteration 6407 : loss : 0.035379, loss_ce: 0.012893
2021-11-30 14:34:44,932 iteration 6408 : loss : 0.021868, loss_ce: 0.010228
2021-11-30 14:34:46,423 iteration 6409 : loss : 0.032221, loss_ce: 0.015536
 94%|███████████████████████████▎ | 377/400 [2:52:48<10:27, 27.30s/it]2021-11-30 14:34:47,904 iteration 6410 : loss : 0.023133, loss_ce: 0.010299
2021-11-30 14:34:49,428 iteration 6411 : loss : 0.029707, loss_ce: 0.014873
2021-11-30 14:34:50,945 iteration 6412 : loss : 0.024688, loss_ce: 0.011015
2021-11-30 14:34:52,413 iteration 6413 : loss : 0.030487, loss_ce: 0.015369
2021-11-30 14:34:53,871 iteration 6414 : loss : 0.025950, loss_ce: 0.012553
2021-11-30 14:34:55,306 iteration 6415 : loss : 0.019775, loss_ce: 0.009323
2021-11-30 14:34:56,810 iteration 6416 : loss : 0.021314, loss_ce: 0.010354
2021-11-30 14:34:58,212 iteration 6417 : loss : 0.026256, loss_ce: 0.013287
2021-11-30 14:34:59,663 iteration 6418 : loss : 0.023910, loss_ce: 0.011934
2021-11-30 14:35:01,241 iteration 6419 : loss : 0.029092, loss_ce: 0.013907
2021-11-30 14:35:02,814 iteration 6420 : loss : 0.039874, loss_ce: 0.013198
2021-11-30 14:35:04,337 iteration 6421 : loss : 0.025381, loss_ce: 0.012004
2021-11-30 14:35:05,832 iteration 6422 : loss : 0.038719, loss_ce: 0.013671
2021-11-30 14:35:07,356 iteration 6423 : loss : 0.029701, loss_ce: 0.014440
2021-11-30 14:35:08,954 iteration 6424 : loss : 0.041814, loss_ce: 0.023782
2021-11-30 14:35:10,415 iteration 6425 : loss : 0.026074, loss_ce: 0.011158
2021-11-30 14:35:11,961 iteration 6426 : loss : 0.033261, loss_ce: 0.013425
 94%|███████████████████████████▍ | 378/400 [2:53:14<09:48, 26.77s/it]2021-11-30 14:35:13,533 iteration 6427 : loss : 0.025795, loss_ce: 0.010886
2021-11-30 14:35:15,017 iteration 6428 : loss : 0.037768, loss_ce: 0.014681
2021-11-30 14:35:16,510 iteration 6429 : loss : 0.025201, loss_ce: 0.010507
2021-11-30 14:35:18,050 iteration 6430 : loss : 0.032558, loss_ce: 0.015352
2021-11-30 14:35:19,483 iteration 6431 : loss : 0.026119, loss_ce: 0.012071
2021-11-30 14:35:20,988 iteration 6432 : loss : 0.027519, loss_ce: 0.015584
2021-11-30 14:35:22,485 iteration 6433 : loss : 0.023580, loss_ce: 0.010430
2021-11-30 14:35:23,945 iteration 6434 : loss : 0.029399, loss_ce: 0.013955
2021-11-30 14:35:25,433 iteration 6435 : loss : 0.037907, loss_ce: 0.017227
2021-11-30 14:35:26,911 iteration 6436 : loss : 0.021040, loss_ce: 0.011521
2021-11-30 14:35:28,383 iteration 6437 : loss : 0.030353, loss_ce: 0.012296
2021-11-30 14:35:29,920 iteration 6438 : loss : 0.034571, loss_ce: 0.013519
2021-11-30 14:35:31,362 iteration 6439 : loss : 0.025445, loss_ce: 0.013526
2021-11-30 14:35:32,891 iteration 6440 : loss : 0.037722, loss_ce: 0.020124
2021-11-30 14:35:34,320 iteration 6441 : loss : 0.021709, loss_ce: 0.011830
2021-11-30 14:35:35,831 iteration 6442 : loss : 0.026732, loss_ce: 0.010760
2021-11-30 14:35:37,241 iteration 6443 : loss : 0.030104, loss_ce: 0.011997
 95%|███████████████████████████▍ | 379/400 [2:53:39<09:12, 26.33s/it]2021-11-30 14:35:38,803 iteration 6444 : loss : 0.030078, loss_ce: 0.016196
2021-11-30 14:35:40,290 iteration 6445 : loss : 0.026810, loss_ce: 0.012782
2021-11-30 14:35:41,769 iteration 6446 : loss : 0.033042, loss_ce: 0.015865
2021-11-30 14:35:43,266 iteration 6447 : loss : 0.081862, loss_ce: 0.019334
2021-11-30 14:35:44,784 iteration 6448 : loss : 0.028287, loss_ce: 0.012329
2021-11-30 14:35:46,248 iteration 6449 : loss : 0.032007, loss_ce: 0.010938
2021-11-30 14:35:47,712 iteration 6450 : loss : 0.036186, loss_ce: 0.018803
2021-11-30 14:35:49,203 iteration 6451 : loss : 0.024787, loss_ce: 0.011632
2021-11-30 14:35:50,771 iteration 6452 : loss : 0.034524, loss_ce: 0.014760
2021-11-30 14:35:52,220 iteration 6453 : loss : 0.035402, loss_ce: 0.017685
2021-11-30 14:35:53,637 iteration 6454 : loss : 0.029864, loss_ce: 0.012924
2021-11-30 14:35:55,173 iteration 6455 : loss : 0.026396, loss_ce: 0.013908
2021-11-30 14:35:56,642 iteration 6456 : loss : 0.042185, loss_ce: 0.017479
2021-11-30 14:35:58,156 iteration 6457 : loss : 0.043924, loss_ce: 0.017143
2021-11-30 14:35:59,598 iteration 6458 : loss : 0.026038, loss_ce: 0.012429
2021-11-30 14:36:01,067 iteration 6459 : loss : 0.029336, loss_ce: 0.010403
2021-11-30 14:36:01,067 Training Data Eval:
2021-11-30 14:36:08,460   Average segmentation loss on training set: 0.0136
2021-11-30 14:36:08,461 Validation Data Eval:
2021-11-30 14:36:11,001   Average segmentation loss on validation set: 0.0882
2021-11-30 14:36:12,567 iteration 6460 : loss : 0.040689, loss_ce: 0.020884
 95%|███████████████████████████▌ | 380/400 [2:54:14<09:40, 29.02s/it]2021-11-30 14:36:14,188 iteration 6461 : loss : 0.032138, loss_ce: 0.017158
2021-11-30 14:36:15,640 iteration 6462 : loss : 0.022825, loss_ce: 0.012026
2021-11-30 14:36:17,196 iteration 6463 : loss : 0.037184, loss_ce: 0.017902
2021-11-30 14:36:18,686 iteration 6464 : loss : 0.040649, loss_ce: 0.015090
2021-11-30 14:36:20,216 iteration 6465 : loss : 0.030620, loss_ce: 0.013467
2021-11-30 14:36:21,643 iteration 6466 : loss : 0.022766, loss_ce: 0.010047
2021-11-30 14:36:23,116 iteration 6467 : loss : 0.033446, loss_ce: 0.014577
2021-11-30 14:36:24,666 iteration 6468 : loss : 0.043102, loss_ce: 0.016058
2021-11-30 14:36:26,134 iteration 6469 : loss : 0.034905, loss_ce: 0.018039
2021-11-30 14:36:27,589 iteration 6470 : loss : 0.023251, loss_ce: 0.009518
2021-11-30 14:36:29,082 iteration 6471 : loss : 0.022831, loss_ce: 0.010635
2021-11-30 14:36:30,585 iteration 6472 : loss : 0.038264, loss_ce: 0.014396
2021-11-30 14:36:32,032 iteration 6473 : loss : 0.024126, loss_ce: 0.010389
2021-11-30 14:36:33,595 iteration 6474 : loss : 0.028186, loss_ce: 0.015280
2021-11-30 14:36:35,112 iteration 6475 : loss : 0.039884, loss_ce: 0.023185
2021-11-30 14:36:36,473 iteration 6476 : loss : 0.033552, loss_ce: 0.015082
2021-11-30 14:36:38,062 iteration 6477 : loss : 0.043619, loss_ce: 0.017455
 95%|███████████████████████████▌ | 381/400 [2:54:40<08:51, 27.96s/it]2021-11-30 14:36:39,647 iteration 6478 : loss : 0.041493, loss_ce: 0.014839
2021-11-30 14:36:41,117 iteration 6479 : loss : 0.031585, loss_ce: 0.013650
2021-11-30 14:36:42,599 iteration 6480 : loss : 0.030819, loss_ce: 0.017820
2021-11-30 14:36:44,134 iteration 6481 : loss : 0.030401, loss_ce: 0.015203
2021-11-30 14:36:45,539 iteration 6482 : loss : 0.026348, loss_ce: 0.012403
2021-11-30 14:36:47,065 iteration 6483 : loss : 0.056101, loss_ce: 0.017579
2021-11-30 14:36:48,503 iteration 6484 : loss : 0.021091, loss_ce: 0.010544
2021-11-30 14:36:49,979 iteration 6485 : loss : 0.031413, loss_ce: 0.015458
2021-11-30 14:36:51,530 iteration 6486 : loss : 0.028265, loss_ce: 0.012022
2021-11-30 14:36:53,142 iteration 6487 : loss : 0.039558, loss_ce: 0.018190
2021-11-30 14:36:54,659 iteration 6488 : loss : 0.041484, loss_ce: 0.017255
2021-11-30 14:36:56,113 iteration 6489 : loss : 0.028448, loss_ce: 0.014430
2021-11-30 14:36:57,614 iteration 6490 : loss : 0.038884, loss_ce: 0.015831
2021-11-30 14:36:59,071 iteration 6491 : loss : 0.029446, loss_ce: 0.013388
2021-11-30 14:37:00,644 iteration 6492 : loss : 0.030822, loss_ce: 0.014636
2021-11-30 14:37:02,139 iteration 6493 : loss : 0.041631, loss_ce: 0.018231
2021-11-30 14:37:03,507 iteration 6494 : loss : 0.018449, loss_ce: 0.009912
 96%|███████████████████████████▋ | 382/400 [2:55:05<08:09, 27.21s/it]2021-11-30 14:37:05,020 iteration 6495 : loss : 0.027197, loss_ce: 0.013210
2021-11-30 14:37:06,509 iteration 6496 : loss : 0.040423, loss_ce: 0.013703
2021-11-30 14:37:08,051 iteration 6497 : loss : 0.022854, loss_ce: 0.011176
2021-11-30 14:37:09,582 iteration 6498 : loss : 0.040651, loss_ce: 0.015837
2021-11-30 14:37:11,098 iteration 6499 : loss : 0.035772, loss_ce: 0.015345
2021-11-30 14:37:12,552 iteration 6500 : loss : 0.022291, loss_ce: 0.010357
2021-11-30 14:37:14,060 iteration 6501 : loss : 0.034744, loss_ce: 0.016932
2021-11-30 14:37:15,617 iteration 6502 : loss : 0.035577, loss_ce: 0.019562
2021-11-30 14:37:17,067 iteration 6503 : loss : 0.019489, loss_ce: 0.008327
2021-11-30 14:37:18,484 iteration 6504 : loss : 0.026144, loss_ce: 0.011493
2021-11-30 14:37:19,842 iteration 6505 : loss : 0.023548, loss_ce: 0.013656
2021-11-30 14:37:21,317 iteration 6506 : loss : 0.038228, loss_ce: 0.013313
2021-11-30 14:37:22,892 iteration 6507 : loss : 0.048506, loss_ce: 0.025595
2021-11-30 14:37:24,416 iteration 6508 : loss : 0.039838, loss_ce: 0.020275
2021-11-30 14:37:25,905 iteration 6509 : loss : 0.032774, loss_ce: 0.015826
2021-11-30 14:37:27,426 iteration 6510 : loss : 0.026238, loss_ce: 0.012564
2021-11-30 14:37:28,832 iteration 6511 : loss : 0.027885, loss_ce: 0.012674
 96%|███████████████████████████▊ | 383/400 [2:55:31<07:32, 26.65s/it]2021-11-30 14:37:30,339 iteration 6512 : loss : 0.024997, loss_ce: 0.011140
2021-11-30 14:37:31,854 iteration 6513 : loss : 0.035179, loss_ce: 0.014166
2021-11-30 14:37:33,477 iteration 6514 : loss : 0.034045, loss_ce: 0.014001
2021-11-30 14:37:35,030 iteration 6515 : loss : 0.034946, loss_ce: 0.013926
2021-11-30 14:37:36,537 iteration 6516 : loss : 0.032024, loss_ce: 0.016922
2021-11-30 14:37:38,006 iteration 6517 : loss : 0.034050, loss_ce: 0.010204
2021-11-30 14:37:39,497 iteration 6518 : loss : 0.033410, loss_ce: 0.014847
2021-11-30 14:37:40,998 iteration 6519 : loss : 0.032837, loss_ce: 0.014463
2021-11-30 14:37:42,560 iteration 6520 : loss : 0.041070, loss_ce: 0.016939
2021-11-30 14:37:44,009 iteration 6521 : loss : 0.036394, loss_ce: 0.018945
2021-11-30 14:37:45,505 iteration 6522 : loss : 0.019850, loss_ce: 0.010073
2021-11-30 14:37:46,910 iteration 6523 : loss : 0.020830, loss_ce: 0.010857
2021-11-30 14:37:48,460 iteration 6524 : loss : 0.044441, loss_ce: 0.021547
2021-11-30 14:37:49,967 iteration 6525 : loss : 0.029106, loss_ce: 0.013929
2021-11-30 14:37:51,468 iteration 6526 : loss : 0.026873, loss_ce: 0.012667
2021-11-30 14:37:52,893 iteration 6527 : loss : 0.027294, loss_ce: 0.013997
2021-11-30 14:37:54,337 iteration 6528 : loss : 0.024206, loss_ce: 0.010391
 96%|███████████████████████████▊ | 384/400 [2:55:56<07:00, 26.30s/it]2021-11-30 14:37:55,939 iteration 6529 : loss : 0.055766, loss_ce: 0.021763
2021-11-30 14:37:57,461 iteration 6530 : loss : 0.025758, loss_ce: 0.011719
2021-11-30 14:37:58,883 iteration 6531 : loss : 0.036322, loss_ce: 0.015638
2021-11-30 14:38:00,419 iteration 6532 : loss : 0.035101, loss_ce: 0.016976
2021-11-30 14:38:01,833 iteration 6533 : loss : 0.016685, loss_ce: 0.008831
2021-11-30 14:38:03,332 iteration 6534 : loss : 0.032982, loss_ce: 0.016748
2021-11-30 14:38:04,825 iteration 6535 : loss : 0.027003, loss_ce: 0.012655
2021-11-30 14:38:06,335 iteration 6536 : loss : 0.029903, loss_ce: 0.012484
2021-11-30 14:38:07,837 iteration 6537 : loss : 0.031192, loss_ce: 0.013655
2021-11-30 14:38:09,348 iteration 6538 : loss : 0.031856, loss_ce: 0.013543
2021-11-30 14:38:10,889 iteration 6539 : loss : 0.031200, loss_ce: 0.015609
2021-11-30 14:38:12,295 iteration 6540 : loss : 0.023567, loss_ce: 0.011809
2021-11-30 14:38:13,751 iteration 6541 : loss : 0.036024, loss_ce: 0.020824
2021-11-30 14:38:15,342 iteration 6542 : loss : 0.029489, loss_ce: 0.011541
2021-11-30 14:38:16,803 iteration 6543 : loss : 0.025475, loss_ce: 0.010001
2021-11-30 14:38:18,336 iteration 6544 : loss : 0.027999, loss_ce: 0.013963
2021-11-30 14:38:18,336 Training Data Eval:
2021-11-30 14:38:25,713   Average segmentation loss on training set: 0.0137
2021-11-30 14:38:25,713 Validation Data Eval:
2021-11-30 14:38:28,284   Average segmentation loss on validation set: 0.0927
2021-11-30 14:38:29,880 iteration 6545 : loss : 0.035749, loss_ce: 0.017370
 96%|███████████████████████████▉ | 385/400 [2:56:32<07:16, 29.07s/it]2021-11-30 14:38:31,566 iteration 6546 : loss : 0.025204, loss_ce: 0.012544
2021-11-30 14:38:33,051 iteration 6547 : loss : 0.036245, loss_ce: 0.018126
2021-11-30 14:38:34,579 iteration 6548 : loss : 0.023901, loss_ce: 0.011172
2021-11-30 14:38:36,040 iteration 6549 : loss : 0.033888, loss_ce: 0.015484
2021-11-30 14:38:37,598 iteration 6550 : loss : 0.031786, loss_ce: 0.013196
2021-11-30 14:38:39,148 iteration 6551 : loss : 0.038785, loss_ce: 0.017328
2021-11-30 14:38:40,762 iteration 6552 : loss : 0.051386, loss_ce: 0.023089
2021-11-30 14:38:42,391 iteration 6553 : loss : 0.044757, loss_ce: 0.017822
2021-11-30 14:38:43,882 iteration 6554 : loss : 0.032208, loss_ce: 0.012866
2021-11-30 14:38:45,373 iteration 6555 : loss : 0.021635, loss_ce: 0.012026
2021-11-30 14:38:46,860 iteration 6556 : loss : 0.043797, loss_ce: 0.014733
2021-11-30 14:38:48,246 iteration 6557 : loss : 0.018069, loss_ce: 0.009569
2021-11-30 14:38:49,741 iteration 6558 : loss : 0.040056, loss_ce: 0.015590
2021-11-30 14:38:51,153 iteration 6559 : loss : 0.021200, loss_ce: 0.009998
2021-11-30 14:38:52,651 iteration 6560 : loss : 0.032352, loss_ce: 0.017490
2021-11-30 14:38:54,177 iteration 6561 : loss : 0.041081, loss_ce: 0.013929
2021-11-30 14:38:55,639 iteration 6562 : loss : 0.021212, loss_ce: 0.010397
 96%|███████████████████████████▉ | 386/400 [2:56:57<06:33, 28.08s/it]2021-11-30 14:38:57,301 iteration 6563 : loss : 0.057810, loss_ce: 0.027244
2021-11-30 14:38:58,768 iteration 6564 : loss : 0.031843, loss_ce: 0.014680
2021-11-30 14:39:00,184 iteration 6565 : loss : 0.038526, loss_ce: 0.011380
2021-11-30 14:39:01,611 iteration 6566 : loss : 0.025679, loss_ce: 0.011841
2021-11-30 14:39:03,004 iteration 6567 : loss : 0.027447, loss_ce: 0.013644
2021-11-30 14:39:04,472 iteration 6568 : loss : 0.022920, loss_ce: 0.012162
2021-11-30 14:39:05,965 iteration 6569 : loss : 0.021353, loss_ce: 0.010877
2021-11-30 14:39:07,406 iteration 6570 : loss : 0.020505, loss_ce: 0.011439
2021-11-30 14:39:08,867 iteration 6571 : loss : 0.032779, loss_ce: 0.013215
2021-11-30 14:39:10,264 iteration 6572 : loss : 0.024037, loss_ce: 0.012367
2021-11-30 14:39:11,836 iteration 6573 : loss : 0.041659, loss_ce: 0.016645
2021-11-30 14:39:13,384 iteration 6574 : loss : 0.019577, loss_ce: 0.010631
2021-11-30 14:39:14,957 iteration 6575 : loss : 0.078252, loss_ce: 0.019054
2021-11-30 14:39:16,407 iteration 6576 : loss : 0.035445, loss_ce: 0.018084
2021-11-30 14:39:17,906 iteration 6577 : loss : 0.054861, loss_ce: 0.016378
2021-11-30 14:39:19,332 iteration 6578 : loss : 0.032628, loss_ce: 0.014533
2021-11-30 14:39:20,833 iteration 6579 : loss : 0.031499, loss_ce: 0.013316
 97%|████████████████████████████ | 387/400 [2:57:23<05:53, 27.21s/it]2021-11-30 14:39:22,386 iteration 6580 : loss : 0.038503, loss_ce: 0.014298
2021-11-30 14:39:23,852 iteration 6581 : loss : 0.025906, loss_ce: 0.011284
2021-11-30 14:39:25,419 iteration 6582 : loss : 0.031393, loss_ce: 0.012939
2021-11-30 14:39:26,901 iteration 6583 : loss : 0.030121, loss_ce: 0.013819
2021-11-30 14:39:28,402 iteration 6584 : loss : 0.029722, loss_ce: 0.015117
2021-11-30 14:39:29,956 iteration 6585 : loss : 0.041779, loss_ce: 0.016434
2021-11-30 14:39:31,463 iteration 6586 : loss : 0.038511, loss_ce: 0.014368
2021-11-30 14:39:32,937 iteration 6587 : loss : 0.018625, loss_ce: 0.010546
2021-11-30 14:39:34,424 iteration 6588 : loss : 0.030144, loss_ce: 0.015428
2021-11-30 14:39:35,848 iteration 6589 : loss : 0.036041, loss_ce: 0.016694
2021-11-30 14:39:37,407 iteration 6590 : loss : 0.033204, loss_ce: 0.017832
2021-11-30 14:39:38,968 iteration 6591 : loss : 0.040880, loss_ce: 0.017551
2021-11-30 14:39:40,412 iteration 6592 : loss : 0.024932, loss_ce: 0.012198
2021-11-30 14:39:41,979 iteration 6593 : loss : 0.061534, loss_ce: 0.016823
2021-11-30 14:39:43,458 iteration 6594 : loss : 0.027716, loss_ce: 0.015328
2021-11-30 14:39:44,920 iteration 6595 : loss : 0.029459, loss_ce: 0.013414
2021-11-30 14:39:46,460 iteration 6596 : loss : 0.045436, loss_ce: 0.016294
 97%|████████████████████████████▏| 388/400 [2:57:48<05:20, 26.74s/it]2021-11-30 14:39:47,982 iteration 6597 : loss : 0.032816, loss_ce: 0.016723
2021-11-30 14:39:49,421 iteration 6598 : loss : 0.049076, loss_ce: 0.014676
2021-11-30 14:39:50,935 iteration 6599 : loss : 0.028183, loss_ce: 0.013490
2021-11-30 14:39:52,302 iteration 6600 : loss : 0.020264, loss_ce: 0.011232
2021-11-30 14:39:53,763 iteration 6601 : loss : 0.032519, loss_ce: 0.013688
2021-11-30 14:39:55,201 iteration 6602 : loss : 0.028290, loss_ce: 0.011517
2021-11-30 14:39:56,695 iteration 6603 : loss : 0.028655, loss_ce: 0.012967
2021-11-30 14:39:58,113 iteration 6604 : loss : 0.016015, loss_ce: 0.009216
2021-11-30 14:39:59,610 iteration 6605 : loss : 0.029310, loss_ce: 0.012467
2021-11-30 14:40:01,120 iteration 6606 : loss : 0.030340, loss_ce: 0.016251
2021-11-30 14:40:02,622 iteration 6607 : loss : 0.024701, loss_ce: 0.012477
2021-11-30 14:40:04,132 iteration 6608 : loss : 0.027739, loss_ce: 0.015194
2021-11-30 14:40:05,579 iteration 6609 : loss : 0.031241, loss_ce: 0.013578
2021-11-30 14:40:07,042 iteration 6610 : loss : 0.034875, loss_ce: 0.019037
2021-11-30 14:40:08,503 iteration 6611 : loss : 0.030706, loss_ce: 0.012304
2021-11-30 14:40:10,038 iteration 6612 : loss : 0.052968, loss_ce: 0.021520
2021-11-30 14:40:11,401 iteration 6613 : loss : 0.025266, loss_ce: 0.012330
 97%|████████████████████████████▏| 389/400 [2:58:13<04:48, 26.20s/it]2021-11-30 14:40:12,946 iteration 6614 : loss : 0.028546, loss_ce: 0.013532
2021-11-30 14:40:14,441 iteration 6615 : loss : 0.031998, loss_ce: 0.016332
2021-11-30 14:40:15,914 iteration 6616 : loss : 0.030442, loss_ce: 0.011664
2021-11-30 14:40:17,527 iteration 6617 : loss : 0.034778, loss_ce: 0.014975
2021-11-30 14:40:19,041 iteration 6618 : loss : 0.025882, loss_ce: 0.009983
2021-11-30 14:40:20,642 iteration 6619 : loss : 0.046192, loss_ce: 0.019042
2021-11-30 14:40:22,188 iteration 6620 : loss : 0.043056, loss_ce: 0.019032
2021-11-30 14:40:23,650 iteration 6621 : loss : 0.035912, loss_ce: 0.013976
2021-11-30 14:40:25,109 iteration 6622 : loss : 0.040731, loss_ce: 0.022531
2021-11-30 14:40:26,590 iteration 6623 : loss : 0.028269, loss_ce: 0.012348
2021-11-30 14:40:28,132 iteration 6624 : loss : 0.047288, loss_ce: 0.022119
2021-11-30 14:40:29,689 iteration 6625 : loss : 0.022862, loss_ce: 0.011614
2021-11-30 14:40:31,159 iteration 6626 : loss : 0.043166, loss_ce: 0.016347
2021-11-30 14:40:32,558 iteration 6627 : loss : 0.024782, loss_ce: 0.010960
2021-11-30 14:40:34,048 iteration 6628 : loss : 0.032946, loss_ce: 0.016439
2021-11-30 14:40:35,519 iteration 6629 : loss : 0.025590, loss_ce: 0.014056
2021-11-30 14:40:35,519 Training Data Eval:
2021-11-30 14:40:42,906   Average segmentation loss on training set: 0.0139
2021-11-30 14:40:42,907 Validation Data Eval:
2021-11-30 14:40:45,464   Average segmentation loss on validation set: 0.0883
2021-11-30 14:40:47,015 iteration 6630 : loss : 0.031519, loss_ce: 0.012102
 98%|████████████████████████████▎| 390/400 [2:58:49<04:50, 29.03s/it]2021-11-30 14:40:48,636 iteration 6631 : loss : 0.037072, loss_ce: 0.016487
2021-11-30 14:40:50,158 iteration 6632 : loss : 0.028380, loss_ce: 0.014100
2021-11-30 14:40:51,669 iteration 6633 : loss : 0.035648, loss_ce: 0.012336
2021-11-30 14:40:53,022 iteration 6634 : loss : 0.023819, loss_ce: 0.010643
2021-11-30 14:40:54,470 iteration 6635 : loss : 0.039222, loss_ce: 0.013841
2021-11-30 14:40:56,029 iteration 6636 : loss : 0.031365, loss_ce: 0.014352
2021-11-30 14:40:57,549 iteration 6637 : loss : 0.024355, loss_ce: 0.012545
2021-11-30 14:40:59,102 iteration 6638 : loss : 0.040736, loss_ce: 0.018088
2021-11-30 14:41:00,676 iteration 6639 : loss : 0.036421, loss_ce: 0.015776
2021-11-30 14:41:02,137 iteration 6640 : loss : 0.039758, loss_ce: 0.014802
2021-11-30 14:41:03,574 iteration 6641 : loss : 0.027537, loss_ce: 0.016192
2021-11-30 14:41:05,142 iteration 6642 : loss : 0.030799, loss_ce: 0.014995
2021-11-30 14:41:06,641 iteration 6643 : loss : 0.031226, loss_ce: 0.015628
2021-11-30 14:41:08,118 iteration 6644 : loss : 0.035030, loss_ce: 0.013665
2021-11-30 14:41:09,527 iteration 6645 : loss : 0.022020, loss_ce: 0.011032
2021-11-30 14:41:10,985 iteration 6646 : loss : 0.023596, loss_ce: 0.012435
2021-11-30 14:41:12,384 iteration 6647 : loss : 0.025991, loss_ce: 0.011369
 98%|████████████████████████████▎| 391/400 [2:59:14<04:11, 27.93s/it]2021-11-30 14:41:13,893 iteration 6648 : loss : 0.032374, loss_ce: 0.013692
2021-11-30 14:41:15,468 iteration 6649 : loss : 0.037999, loss_ce: 0.017435
2021-11-30 14:41:16,889 iteration 6650 : loss : 0.028887, loss_ce: 0.013193
2021-11-30 14:41:18,324 iteration 6651 : loss : 0.019491, loss_ce: 0.009462
2021-11-30 14:41:19,897 iteration 6652 : loss : 0.030162, loss_ce: 0.014806
2021-11-30 14:41:21,455 iteration 6653 : loss : 0.026556, loss_ce: 0.013477
2021-11-30 14:41:22,882 iteration 6654 : loss : 0.022825, loss_ce: 0.011169
2021-11-30 14:41:24,406 iteration 6655 : loss : 0.027685, loss_ce: 0.013278
2021-11-30 14:41:25,955 iteration 6656 : loss : 0.022908, loss_ce: 0.011715
2021-11-30 14:41:27,559 iteration 6657 : loss : 0.044701, loss_ce: 0.019019
2021-11-30 14:41:29,091 iteration 6658 : loss : 0.044017, loss_ce: 0.021317
2021-11-30 14:41:30,622 iteration 6659 : loss : 0.032833, loss_ce: 0.013411
2021-11-30 14:41:32,066 iteration 6660 : loss : 0.032784, loss_ce: 0.013066
2021-11-30 14:41:33,457 iteration 6661 : loss : 0.018979, loss_ce: 0.009376
2021-11-30 14:41:34,964 iteration 6662 : loss : 0.023844, loss_ce: 0.011542
2021-11-30 14:41:36,508 iteration 6663 : loss : 0.024733, loss_ce: 0.012096
2021-11-30 14:41:37,987 iteration 6664 : loss : 0.024392, loss_ce: 0.010740
 98%|████████████████████████████▍| 392/400 [2:59:40<03:37, 27.23s/it]2021-11-30 14:41:39,549 iteration 6665 : loss : 0.033246, loss_ce: 0.014565
2021-11-30 14:41:41,064 iteration 6666 : loss : 0.028768, loss_ce: 0.012534
2021-11-30 14:41:42,507 iteration 6667 : loss : 0.024425, loss_ce: 0.011012
2021-11-30 14:41:43,884 iteration 6668 : loss : 0.019465, loss_ce: 0.010206
2021-11-30 14:41:45,396 iteration 6669 : loss : 0.027978, loss_ce: 0.012248
2021-11-30 14:41:46,797 iteration 6670 : loss : 0.037375, loss_ce: 0.015956
2021-11-30 14:41:48,360 iteration 6671 : loss : 0.029727, loss_ce: 0.015423
2021-11-30 14:41:49,843 iteration 6672 : loss : 0.031665, loss_ce: 0.017157
2021-11-30 14:41:51,420 iteration 6673 : loss : 0.031435, loss_ce: 0.016090
2021-11-30 14:41:52,975 iteration 6674 : loss : 0.039667, loss_ce: 0.019311
2021-11-30 14:41:54,429 iteration 6675 : loss : 0.023235, loss_ce: 0.009586
2021-11-30 14:41:55,826 iteration 6676 : loss : 0.021423, loss_ce: 0.010601
2021-11-30 14:41:57,335 iteration 6677 : loss : 0.025847, loss_ce: 0.009381
2021-11-30 14:41:58,927 iteration 6678 : loss : 0.042397, loss_ce: 0.020502
2021-11-30 14:42:00,433 iteration 6679 : loss : 0.030320, loss_ce: 0.012660
2021-11-30 14:42:01,902 iteration 6680 : loss : 0.045598, loss_ce: 0.021088
2021-11-30 14:42:03,344 iteration 6681 : loss : 0.029347, loss_ce: 0.012457
 98%|████████████████████████████▍| 393/400 [3:00:05<03:06, 26.67s/it]2021-11-30 14:42:04,885 iteration 6682 : loss : 0.035337, loss_ce: 0.013727
2021-11-30 14:42:06,437 iteration 6683 : loss : 0.034718, loss_ce: 0.014229
2021-11-30 14:42:07,958 iteration 6684 : loss : 0.080915, loss_ce: 0.012746
2021-11-30 14:42:09,437 iteration 6685 : loss : 0.024940, loss_ce: 0.012347
2021-11-30 14:42:10,948 iteration 6686 : loss : 0.029992, loss_ce: 0.012913
2021-11-30 14:42:12,456 iteration 6687 : loss : 0.040317, loss_ce: 0.017665
2021-11-30 14:42:13,978 iteration 6688 : loss : 0.030828, loss_ce: 0.013621
2021-11-30 14:42:15,448 iteration 6689 : loss : 0.025849, loss_ce: 0.012636
2021-11-30 14:42:16,992 iteration 6690 : loss : 0.030543, loss_ce: 0.013478
2021-11-30 14:42:18,398 iteration 6691 : loss : 0.026417, loss_ce: 0.012709
2021-11-30 14:42:19,808 iteration 6692 : loss : 0.024552, loss_ce: 0.014168
2021-11-30 14:42:21,324 iteration 6693 : loss : 0.022986, loss_ce: 0.012215
2021-11-30 14:42:22,728 iteration 6694 : loss : 0.023790, loss_ce: 0.012069
2021-11-30 14:42:24,194 iteration 6695 : loss : 0.025072, loss_ce: 0.010488
2021-11-30 14:42:25,689 iteration 6696 : loss : 0.035310, loss_ce: 0.020504
2021-11-30 14:42:27,187 iteration 6697 : loss : 0.024645, loss_ce: 0.011390
2021-11-30 14:42:28,580 iteration 6698 : loss : 0.034793, loss_ce: 0.014971
 98%|████████████████████████████▌| 394/400 [3:00:30<02:37, 26.24s/it]2021-11-30 14:42:30,129 iteration 6699 : loss : 0.031339, loss_ce: 0.011252
2021-11-30 14:42:31,675 iteration 6700 : loss : 0.038503, loss_ce: 0.020582
2021-11-30 14:42:33,247 iteration 6701 : loss : 0.043013, loss_ce: 0.016325
2021-11-30 14:42:34,707 iteration 6702 : loss : 0.034837, loss_ce: 0.014528
2021-11-30 14:42:36,291 iteration 6703 : loss : 0.026944, loss_ce: 0.011451
2021-11-30 14:42:37,791 iteration 6704 : loss : 0.029579, loss_ce: 0.016668
2021-11-30 14:42:39,209 iteration 6705 : loss : 0.027953, loss_ce: 0.011477
2021-11-30 14:42:40,658 iteration 6706 : loss : 0.026891, loss_ce: 0.012756
2021-11-30 14:42:42,137 iteration 6707 : loss : 0.028494, loss_ce: 0.012194
2021-11-30 14:42:43,538 iteration 6708 : loss : 0.027072, loss_ce: 0.010519
2021-11-30 14:42:44,976 iteration 6709 : loss : 0.027981, loss_ce: 0.010752
2021-11-30 14:42:46,481 iteration 6710 : loss : 0.034719, loss_ce: 0.016418
2021-11-30 14:42:47,976 iteration 6711 : loss : 0.022512, loss_ce: 0.012066
2021-11-30 14:42:49,513 iteration 6712 : loss : 0.043310, loss_ce: 0.020121
2021-11-30 14:42:50,931 iteration 6713 : loss : 0.024508, loss_ce: 0.010819
2021-11-30 14:42:52,382 iteration 6714 : loss : 0.023514, loss_ce: 0.012876
2021-11-30 14:42:52,382 Training Data Eval:
2021-11-30 14:42:59,769   Average segmentation loss on training set: 0.0138
2021-11-30 14:42:59,769 Validation Data Eval:
2021-11-30 14:43:02,334   Average segmentation loss on validation set: 0.0894
2021-11-30 14:43:03,789 iteration 6715 : loss : 0.021272, loss_ce: 0.009335
 99%|████████████████████████████▋| 395/400 [3:01:06<02:24, 28.93s/it]2021-11-30 14:43:05,256 iteration 6716 : loss : 0.020280, loss_ce: 0.009339
2021-11-30 14:43:06,778 iteration 6717 : loss : 0.038386, loss_ce: 0.014095
2021-11-30 14:43:08,343 iteration 6718 : loss : 0.051038, loss_ce: 0.026015
2021-11-30 14:43:09,893 iteration 6719 : loss : 0.046312, loss_ce: 0.017857
2021-11-30 14:43:11,429 iteration 6720 : loss : 0.062891, loss_ce: 0.015828
2021-11-30 14:43:12,958 iteration 6721 : loss : 0.034234, loss_ce: 0.016973
2021-11-30 14:43:14,433 iteration 6722 : loss : 0.025517, loss_ce: 0.014095
2021-11-30 14:43:15,919 iteration 6723 : loss : 0.030900, loss_ce: 0.013930
2021-11-30 14:43:17,335 iteration 6724 : loss : 0.036079, loss_ce: 0.015893
2021-11-30 14:43:18,816 iteration 6725 : loss : 0.023822, loss_ce: 0.010907
2021-11-30 14:43:20,210 iteration 6726 : loss : 0.016859, loss_ce: 0.008276
2021-11-30 14:43:21,808 iteration 6727 : loss : 0.040258, loss_ce: 0.017174
2021-11-30 14:43:23,239 iteration 6728 : loss : 0.028683, loss_ce: 0.013730
2021-11-30 14:43:24,780 iteration 6729 : loss : 0.033718, loss_ce: 0.013894
2021-11-30 14:43:26,384 iteration 6730 : loss : 0.031107, loss_ce: 0.014209
2021-11-30 14:43:27,913 iteration 6731 : loss : 0.037343, loss_ce: 0.015955
2021-11-30 14:43:29,384 iteration 6732 : loss : 0.019132, loss_ce: 0.010491
 99%|████████████████████████████▋| 396/400 [3:01:31<01:51, 27.93s/it]2021-11-30 14:43:30,868 iteration 6733 : loss : 0.019803, loss_ce: 0.010868
2021-11-30 14:43:32,338 iteration 6734 : loss : 0.033019, loss_ce: 0.012579
2021-11-30 14:43:33,867 iteration 6735 : loss : 0.029743, loss_ce: 0.013044
2021-11-30 14:43:35,338 iteration 6736 : loss : 0.025651, loss_ce: 0.013470
2021-11-30 14:43:36,742 iteration 6737 : loss : 0.026527, loss_ce: 0.015683
2021-11-30 14:43:38,168 iteration 6738 : loss : 0.027914, loss_ce: 0.011824
2021-11-30 14:43:39,666 iteration 6739 : loss : 0.027511, loss_ce: 0.015355
2021-11-30 14:43:41,195 iteration 6740 : loss : 0.050179, loss_ce: 0.016575
2021-11-30 14:43:42,689 iteration 6741 : loss : 0.039765, loss_ce: 0.018849
2021-11-30 14:43:44,137 iteration 6742 : loss : 0.024412, loss_ce: 0.013683
2021-11-30 14:43:45,554 iteration 6743 : loss : 0.030942, loss_ce: 0.010940
2021-11-30 14:43:47,080 iteration 6744 : loss : 0.042706, loss_ce: 0.019582
2021-11-30 14:43:48,572 iteration 6745 : loss : 0.027801, loss_ce: 0.015048
2021-11-30 14:43:49,991 iteration 6746 : loss : 0.016852, loss_ce: 0.007798
2021-11-30 14:43:51,447 iteration 6747 : loss : 0.024196, loss_ce: 0.011400
2021-11-30 14:43:52,952 iteration 6748 : loss : 0.021945, loss_ce: 0.012683
2021-11-30 14:43:54,399 iteration 6749 : loss : 0.030368, loss_ce: 0.011735
 99%|████████████████████████████▊| 397/400 [3:01:56<01:21, 27.05s/it]2021-11-30 14:43:55,924 iteration 6750 : loss : 0.021745, loss_ce: 0.012599
2021-11-30 14:43:57,464 iteration 6751 : loss : 0.023896, loss_ce: 0.011690
2021-11-30 14:43:58,852 iteration 6752 : loss : 0.021784, loss_ce: 0.010504
2021-11-30 14:44:00,330 iteration 6753 : loss : 0.032566, loss_ce: 0.015957
2021-11-30 14:44:01,856 iteration 6754 : loss : 0.024021, loss_ce: 0.010683
2021-11-30 14:44:03,303 iteration 6755 : loss : 0.034412, loss_ce: 0.013405
2021-11-30 14:44:04,757 iteration 6756 : loss : 0.030192, loss_ce: 0.013884
2021-11-30 14:44:06,314 iteration 6757 : loss : 0.024194, loss_ce: 0.011227
2021-11-30 14:44:07,775 iteration 6758 : loss : 0.034690, loss_ce: 0.021619
2021-11-30 14:44:09,271 iteration 6759 : loss : 0.047298, loss_ce: 0.021223
2021-11-30 14:44:10,833 iteration 6760 : loss : 0.052939, loss_ce: 0.013419
2021-11-30 14:44:12,309 iteration 6761 : loss : 0.028390, loss_ce: 0.014599
2021-11-30 14:44:13,780 iteration 6762 : loss : 0.044257, loss_ce: 0.017978
2021-11-30 14:44:15,332 iteration 6763 : loss : 0.038534, loss_ce: 0.017046
2021-11-30 14:44:16,773 iteration 6764 : loss : 0.045076, loss_ce: 0.019318
2021-11-30 14:44:18,313 iteration 6765 : loss : 0.036260, loss_ce: 0.016047
2021-11-30 14:44:19,760 iteration 6766 : loss : 0.030901, loss_ce: 0.016959
100%|████████████████████████████▊| 398/400 [3:02:22<00:53, 26.55s/it]2021-11-30 14:44:21,294 iteration 6767 : loss : 0.055580, loss_ce: 0.019433
2021-11-30 14:44:22,877 iteration 6768 : loss : 0.036234, loss_ce: 0.015750
2021-11-30 14:44:24,366 iteration 6769 : loss : 0.031227, loss_ce: 0.015031
2021-11-30 14:44:25,823 iteration 6770 : loss : 0.028814, loss_ce: 0.015974
2021-11-30 14:44:27,426 iteration 6771 : loss : 0.054279, loss_ce: 0.016467
2021-11-30 14:44:28,877 iteration 6772 : loss : 0.023676, loss_ce: 0.010729
2021-11-30 14:44:30,228 iteration 6773 : loss : 0.025255, loss_ce: 0.011780
2021-11-30 14:44:31,733 iteration 6774 : loss : 0.053393, loss_ce: 0.017216
2021-11-30 14:44:33,250 iteration 6775 : loss : 0.049115, loss_ce: 0.017465
2021-11-30 14:44:34,712 iteration 6776 : loss : 0.035123, loss_ce: 0.013418
2021-11-30 14:44:36,263 iteration 6777 : loss : 0.036658, loss_ce: 0.020814
2021-11-30 14:44:37,731 iteration 6778 : loss : 0.036726, loss_ce: 0.013479
2021-11-30 14:44:39,324 iteration 6779 : loss : 0.042858, loss_ce: 0.022027
2021-11-30 14:44:40,832 iteration 6780 : loss : 0.028246, loss_ce: 0.014982
2021-11-30 14:44:42,363 iteration 6781 : loss : 0.031134, loss_ce: 0.016184
2021-11-30 14:44:43,892 iteration 6782 : loss : 0.028061, loss_ce: 0.012548
2021-11-30 14:44:45,337 iteration 6783 : loss : 0.031798, loss_ce: 0.014067
100%|████████████████████████████▉| 399/400 [3:02:47<00:26, 26.25s/it]2021-11-30 14:44:46,960 iteration 6784 : loss : 0.029388, loss_ce: 0.012185
2021-11-30 14:44:48,473 iteration 6785 : loss : 0.031186, loss_ce: 0.012574
2021-11-30 14:44:49,896 iteration 6786 : loss : 0.021671, loss_ce: 0.010920
2021-11-30 14:44:51,343 iteration 6787 : loss : 0.026911, loss_ce: 0.011618
2021-11-30 14:44:52,857 iteration 6788 : loss : 0.033569, loss_ce: 0.015975
2021-11-30 14:44:54,361 iteration 6789 : loss : 0.046453, loss_ce: 0.017417
2021-11-30 14:44:55,821 iteration 6790 : loss : 0.027284, loss_ce: 0.013689
2021-11-30 14:44:57,326 iteration 6791 : loss : 0.029942, loss_ce: 0.014233
2021-11-30 14:44:58,760 iteration 6792 : loss : 0.025271, loss_ce: 0.012711
2021-11-30 14:45:00,330 iteration 6793 : loss : 0.036122, loss_ce: 0.014446
2021-11-30 14:45:01,812 iteration 6794 : loss : 0.044946, loss_ce: 0.018575
2021-11-30 14:45:03,337 iteration 6795 : loss : 0.037170, loss_ce: 0.016661
2021-11-30 14:45:04,877 iteration 6796 : loss : 0.039260, loss_ce: 0.022705
2021-11-30 14:45:06,247 iteration 6797 : loss : 0.027051, loss_ce: 0.011455
2021-11-30 14:45:07,772 iteration 6798 : loss : 0.028091, loss_ce: 0.011499
2021-11-30 14:45:09,360 iteration 6799 : loss : 0.040309, loss_ce: 0.019449
2021-11-30 14:45:09,360 Training Data Eval:
2021-11-30 14:45:16,763   Average segmentation loss on training set: 0.0136
2021-11-30 14:45:16,764 Validation Data Eval:
2021-11-30 14:45:19,331   Average segmentation loss on validation set: 0.0853
2021-11-30 14:45:20,788 iteration 6800 : loss : 0.024028, loss_ce: 0.009353
100%|█████████████████████████████| 400/400 [3:03:23<00:00, 29.02s/it]100%|█████████████████████████████| 400/400 [3:03:23<00:00, 27.51s/it]
